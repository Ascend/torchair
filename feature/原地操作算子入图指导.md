# 原地操作算子入图指导

## 前言

原地操作（in-place）算子区别于普通算子，修改了输入的值，对于原地修改操作的算子，**pytorch原生就限制了其直接入图会报错**，会提示用户需要针对pytorch的functionalization做一些额外的适配工作（包括开发一个对应的非原地操作算子），才能支持算子入图。functionalization是一种程序到程序 (program-to-program)的转换。给定一个 PyTorch 运算符的程序/函数，函数化将返回一个新函数，该函数具有与旧函数相同的语义。functionalization的背景知识可以参考[pytorch社区讨论](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know)。文本将针对在python侧注册算子给出相关适配工作的指导，使得用户开发的自定义原地操作算子能够在GE图模式中运行。

* **functionalization的基本实现逻辑**
  1、调用原地操作算子的非原地版本，得到临时输出
  2、将临时输出拷贝给输入，达到修改输入值的目的
* **为什么需要非原地版本的算子**
  [pytorch社区讨论](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know)中有提及，pytorch有着非常多的API支持变量的别名与(原地)修改，但是编译器在许多场景中（如AOT Autograd的min-cut partitioning等特性）无法处理原地修改，所以需要通过functionalization来达到与原算子语义相同，但是去除了原地修改的行为的目的，此操作需要一个非原地版本的算子才能实现。
* **如果不实现functionalization，会有什么问题**
  假设用户在定义算子时，没有表示为一个原地操作算子，欺骗了编译器而入图，可能因为执行顺序的问题造成隐形的精度问题，如下给出一个示例：
  
  &emsp; &emsp; &emsp;&emsp;input &emsp; &emsp;&emsp; &emsp;&emsp; &emsp; &emsp;&emsp;&emsp;input <br/>
  &emsp;&emsp;&emsp;&emsp;\/&emsp; &emsp; &emsp;  \\  &emsp; &emsp;&emsp; &emsp;&emsp; &emsp; \/&emsp; &emsp; &emsp;  \\<br/>
  out-of-place op1&ensp;&nbsp;\\   &ensp;&emsp; &emsp; &emsp;&emsp; &emsp;| &emsp; &emsp;  in-place op2<br/>
  &emsp;&emsp; &emsp;| &emsp; &emsp;  in-place op2 &emsp;  out-of-place op1&ensp;&nbsp;\|<br/>
  &emsp;&emsp; &emsp;\\&emsp; &emsp; &emsp;&emsp;\/ &emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;\\&emsp; &emsp; &emsp;&emsp;&emsp;\/<br/>
  &emsp; &emsp; &emsp;&emsp;output &emsp; &emsp;&emsp; &emsp;&emsp; &emsp; &emsp;&emsp;output<br/>
  如图所示，假设input会被out-of-place op1读且会被in-place op2修改，则示例中左右两种执行顺序不同，执行的结果也截然不同

## 准备工作

在进行原地操作算子入图的适配工作前，需要您确保已经完成了算子基本的交付件，具体工作如下表

| 工作项      | 说明 |指导|
| ----------- | ----------- |----------- |
| 原地操作算子实现      | 使用Ascend C开发原地操作算子的实现|[Ascend C算子实现](https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0031.html) |
| 非原地操作算子实现   | 原地操作算子入图需要额外开发一套相对应算子的非原地操作实现|[Ascend C算子实现](https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0031.html)|
| GE图适配   | 完成InferShape、InferDtype等代码交付件，使得算子能在GE图模式上编译与运行|[算子入图（GE）图开发](https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0077.html)|
| torchair适配   | 生成ge构图api，完成converter编写，使得算子在FX图上的节点能转换为GE图节点|[Converter补齐](https://gitee.com/ascend/torchair/blob/master/CONTRIBUTING.md#converter%E8%A1%A5%E9%BD%90)|

## 适配指导（基于python侧注册算子）

#### 1.开放算子python接口

| 工作项      | 说明 |
| ----------- | ----------- |
| 开放算子的python侧接口      | 开发算子的c++接口绑定的python接口，以便算子能在python代码中被调用|

#### 2.注册算子及meta实现

```python
import torch
from torch.library import Library, impl
import torch._custom_op.impl

# 此处自定义了"my_ops"命名空间，后续将算子注册到该命名空间，就可以使用torch.ops.my_ops.xxx调用该算子
# 注意 "npu_define"已经在torch_npu的c++侧注册，python侧不能重复注册，所以此处不能再使用"npu_define"命名空间
m = Library("my_ops", "DEF")

# 注册非原地操作算子函数签名
m.define("custom_add(Tensor input1, int input2) -> Tensor")
# 注册原地操作算子函数签名，指定输出是input1、input1的别名（原地操作）
m.define("custom_add_(Tensor(a!) input1, Tensor(b!) input2) -> (Tensor(a!), Tensor(b!))")

# 注册原地算子实现，实现是给输入的两个值分别加1、2 (此处是为了方便展示算子的逻辑，实际使用中，如果只运行图模式，可以不注册原地操作算子的实现，只注册Functionalize即可)
@impl(m, "custom_add_", "PrivateUse1")
def plug_custom_add_(x: torch.Tensor, y: torch.Tensor) -> (torch.Tensor, torch.Tensor):
    x.add_(1) # 此处为了示例，调用的是torch.add算子，实际使用中应该调用您自定义的非原地操作算子的python接口
    y.add_(2)
    return x, y

# 注册非原地算子实现
@impl(m, "custom_add", "PrivateUse1")
def plug_custom_add(x: torch.Tensor, num: int) -> torch.Tensor:
    return torch.add(x, num) # 此处为了示例，调用的是torch.add算子，实际使用中应该调用您自定义的非原地操作算子的python接口

# 注册非原地操作算子meta实现，用于torch FX图的shape、dtype等推导
@impl(m, "custom_add", "Meta")
def custom_op_meta(x, y):
    return torch.empty_like(x)

# 注册原地操作算子meta实现，用于torch FX图的shape、dtype等推导
@impl(m, "custom_add_", "Meta")
def custom_add_meta(x, y):
    return torch.empty_like(x), torch.empty_like(y)
```

#### 3.注册原地操作算子的functionalization

```
@impl(m, "custom_add_", "Functionalize")
def custom_add_func(x, y):
    """

    functionalization基本实现逻辑：
    1.调用算子的非原地版本，得到临时输出
    2.将临时输出的值拷贝给输入，实现修改输入值的目的

    torch.func提供了functionalize方法可以生成没有mutation的等效函数，本函数的实现代码是参考functionalize生成的代码，
    然后将torch.ops.aten.add.Tensor替换为了torch.ops.my_ops.custom_add

    functionalize生成代码的方式如下
    from torch.func import functionalize
    from torch.fx.experimental.proxy_tensor import make_fx

    a = torch.ones(1, 1)
    b = torch.ones(1, 1)
    out = make_fx(functionalize(plug_custom_add_))(a, b)
    print(out.code)
    """
    add = torch.ops.my_ops.custom_add(x, 1)
    add_1 = torch.ops.my_ops.custom_add(y, 2)
    copy_ = torch.ops.aten.copy_.default(x, add);  x = None
    copy__1 = torch.ops.aten.copy_.default(y, add_1);  y = None
    return x, y
```

#### 4.完整示例代码

```
from typing import Any
import torch
from torch.library import Library, impl
import torch._custom_op.impl

import torch_npu
import torchair

from torchair.configs.compiler_config import CompilerConfig
from torchair._ge_concrete_graph.fx2ge_converter import register_fx_node_ge_converter
from torchair._ge_concrete_graph import ge_apis as ge
from torchair.ge._ge_graph import Tensor
from torchair._ge_concrete_graph.utils import dtype_promote

# 此处自定义了"my_ops"命名空间，后续将算子注册到该命名空间，就可以使用torch.ops.my_ops.xxx调用该算子
# 注意 "npu_define"已经在torch_npu的c++侧注册，python侧不能重复注册，所以此处不能再使用"npu_define"命名空间
m = Library("my_ops", "DEF")

# 注册非原地操作算子函数签名
m.define("custom_add(Tensor input1, int input2) -> Tensor")
# 注册原地操作算子函数签名，指定输出是input1、input1的别名（原地操作）
m.define("custom_add_(Tensor(a!) input1, Tensor(b!) input2) -> (Tensor(a!), Tensor(b!))")

# 注册原地算子实现，实现是给输入的两个值分别加1、2 (此处是为了方便展示算子的逻辑，实际使用中，如果只运行图模式，可以不注册原地操作算子的实现，只注册Functionalize即可)
@impl(m, "custom_add_", "PrivateUse1")
def plug_custom_add_(x: torch.Tensor, y: torch.Tensor) -> (torch.Tensor, torch.Tensor):
    x.add_(1) # 此处为了示例，调用的是torch.add算子，实际使用中应该调用您自定义的非原地操作算子的python接口
    y.add_(2)
    return x, y

# 注册非原地算子实现
@impl(m, "custom_add", "PrivateUse1")
def plug_custom_add(x: torch.Tensor, num: int) -> torch.Tensor:
    return torch.add(x, num) # 此处为了示例，调用的是torch.add算子，实际使用中应该调用您自定义的非原地操作算子的python接口

# 注册非原地操作算子meta实现，用于torch FX图的shape、dtype推导
@impl(m, "custom_add", "Meta")
def custom_op_meta(x, y):
    return torch.empty_like(x)

# 注册原地操作算子meta实现，用于torch FX图的shape、dtype推导
@impl(m, "custom_add_", "Meta")
def custom_add_meta(x, y):
    return torch.empty_like(x), torch.empty_like(y)

# 注册非原地操作算子的converter，使得该算子能被从FX图节点转换为GE图节点
@register_fx_node_ge_converter(torch.ops.my_ops.custom_add.default)
def converter_custom_add(
        input1: Tensor,
        input2: int,
        out: Tensor = None,
        meta_outputs: Any = None):
    # 将输入的数据类型提升至与输出一致
    input1 = dtype_promote(input1, target_dtype=meta_outputs.dtype)
    # 调用ge构图api
    return ge.Add(input1, input2) # 此处为了演示，调用了已有的ge.Add api，实际使用中，应该调用根据您的非原地操作算子的原型生成的构图api（详见准备工作的最后一项）

# 给原地操作算子注册Functionalize，使得该算子能够被入fx图
@impl(m, "custom_add_", "Functionalize")
def custom_add_func(x, y):
    """

    functionalization基本实现逻辑：
    1.调用算子的非原地版本，得到临时输出
    2.将临时输出的值拷贝给输入，实现修改输入值的目的

    torch.func提供了functionalize方法可以生成没有mutation的等效函数，本函数的实现代码是参考functionalize生成的代码，
    然后将torch.ops.aten.add.Tensor替换为了torch.ops.my_ops.custom_add

    functionalize生成代码的方式如下
    from torch.func import functionalize
    from torch.fx.experimental.proxy_tensor import make_fx

    a = torch.ones(1, 1)
    b = torch.ones(1, 1)
    out = make_fx(functionalize(plug_custom_add_))(a, b)
    print(out.code)
    """
    add = torch.ops.my_ops.custom_add(x, 1)
    add_1 = torch.ops.my_ops.custom_add(y, 2)
    copy_ = torch.ops.aten.copy_.default(x, add);  x = None
    copy__1 = torch.ops.aten.copy_.default(y, add_1);  y = None
    return x, y

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x, y):
        out1, out2 = torch.ops.my_ops.custom_add_(x, y)
        return out1, out2

def main():
    input1 = torch.ones(1, 1).npu()
    input2 = torch.ones(1, 1).npu()
    print(f"before run model input1 is : {input1}")
    print(f"before run model input2 is : {input2}")
    print("--------------")
    config = CompilerConfig()
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = Model()
    c_model = torch.compile(model, backend=npu_backend , fullgraph=True)
    out1, out2 = c_model(input1, input2)
    print("--------------")
    print(f"after run model input1 is : {input1}")
    print(f"after run model input2 is : {input2}")


if __name__ == "__main__":
    main()
```

#### 5.验证脚本

执行脚本后，input1的值加了1，input2的值加了2，输出符合预期

```
before run model input1 is : tensor([[1.]], device='npu:0')
before run model input2 is : tensor([[1.]], device='npu:0')
--------------
after run model input1 is : tensor([[2.]], device='npu:0')
after run model input2 is : tensor([[3.]], device='npu:0')
```
