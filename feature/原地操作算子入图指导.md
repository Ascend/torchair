# 原地操作算子入图指导

## 前言

在讨论原地算子入图之前，先看什么是原地算子，以及原地算子应该怎么定义。

在pytorch中，一般将算子分成两大类：原地算子 inplace op 和 非原地算子 out_of_place op。

* **out_of_place op：**
  一般都是基于输入得到输出，调用过程不涉及对input tensor数据的修改，即不涉及 input mutations。
  如：torch.add()
* **inplace op：**
  一般在计算过程中会直接修改一个或者多个input tensor，即总涉及 input mutations。对于原地算子，返回值是否包含被原地修改的输入tensor 并不固定。
  如：torch.add_()

## 如何定义原地算子？

pytorch中算子定义常用的有两种方式： 直接通过library接口注册算子；定义yaml文件通过codegen生成算子注册；

#### 原地算子注册示例：

关于yaml如何定义算子，重点参考社区yaml中说明的[README文档](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md)

**示例：**

```yaml
- func: scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
  structured_delegate: scatter_add.out
  variants: method

- func: _cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
  variants: function
  dispatch:
    CPU: cummax_helper_cpu
    CUDA: cummax_helper_cuda
```

#### 原地算子注册常见问题：

* 常见错误写法：
  yaml定义算子时是非原地算子，meta函数也是按照非原地去实现的，但是算子适配层和converter却是将输入作为输出返回出去。这就出现了算子本身的多个行为不一致。
  
  如：定义了一个非原地算子func，入参是A,B，返回值是C,D，即：
  C,D = FUNC(A,B)
* 此时会出现两种问题：
  1、此时预期C,D 和A,B 完全无关，但是当后续原地修改C,D时会导致A,B 也被隐式修改。
  2、A,B丢给func作为入参使用之后，A,B的值也被隐式修改了
  以上都是调用func的人，预期之外的问题。

**所以：首先得保持算子定义和算子实现一致，这是所有人能正常工作 前提。**

## Functionalization基本概念

原地操作（in-place）算子区别于普通算子，修改了输入的值，对于原地修改操作的算子，**pytorch原生就限制了其直接入图会报错**，会提示用户需要针对pytorch的functionalization做一些额外的适配工作（包括开发一个对应的非原地操作算子），才能支持算子入图。functionalization是一种程序到程序 (program-to-program)的转换。给定一个 PyTorch 运算符的程序/函数，函数化将返回一个新函数，该函数具有与旧函数相同的语义。functionalization的背景知识可以参考[pytorch社区讨论](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know)。文本将给出相关适配工作的指导，使得用户开发的自定义原地操作算子能够在GE图模式中运行。

* **functionalization的基本实现逻辑**
  1、调用原地操作算子的非原地版本，得到临时输出
  2、将临时输出拷贝给输入，达到修改输入值的目的
* **为什么需要非原地版本的算子**
  [pytorch社区讨论](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know)中有提及，pytorch有着非常多的API支持变量的别名与(原地)修改，但是编译器在许多场景中（如AOT Autograd的min-cut partitioning等特性）无法处理原地修改，所以需要通过functionalization来达到与原算子语义相同，但是去除了原地修改的行为的目的，此操作需要一个非原地版本的算子才能实现。
* **如果不实现functionalization，会有什么问题**
  假设用户在定义算子时，没有表示为一个原地操作算子，欺骗了编译器而入图，可能因为执行顺序的问题造成隐形的精度问题，如下给出一个示例：<br />
  &emsp; &emsp; &emsp;&emsp;input &emsp; &emsp;&emsp; &emsp;&emsp; &emsp; &emsp;&emsp;&emsp;input <br />
  &emsp;&emsp;&emsp;&emsp;\/&emsp; &emsp; &emsp;  \\  &emsp; &emsp;&emsp; &emsp;&emsp; &emsp; \/&emsp; &emsp; &emsp;  \\<br />
  out-of-place op1&ensp;&nbsp;\\   &ensp;&emsp; &emsp; &emsp;&emsp; &emsp;| &emsp; &emsp;  in-place op2<br />
  &emsp;&emsp; &emsp;| &emsp; &emsp;  in-place op2 &emsp;  out-of-place op1&ensp;&nbsp;\|<br />
  &emsp;&emsp; &emsp;\\&emsp; &emsp; &emsp;&emsp;\/ &emsp;&emsp; &emsp;&emsp;&emsp;&emsp;&emsp;\\&emsp; &emsp; &emsp;&emsp;&emsp;\/<br />
  &emsp; &emsp; &emsp;&emsp;output &emsp; &emsp;&emsp; &emsp;&emsp; &emsp; &emsp;&emsp;output<br />
  如图所示，假设input会被out-of-place op1读且会被in-place op2修改，则示例中左右两种执行顺序不同，执行的结果也截然不同

## 准备工作

在进行原地操作算子入图的适配工作前，需要您确保已经完成了算子基本的交付件，具体工作如下表

| 工作项      | 说明 |指导|
| ----------- | ----------- |----------- |
| 原地操作算子实现      | 使用Ascend C开发一套原地操作算子的实现|[Ascend C算子实现](https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0031.html) |
| 非原地操作算子实现   | 由于functionalization的转换，所以需要有对应的非原地算子实现（鉴于已经有了算子实现的接口，可以copy一份输入然后对临时tensor做原地修改，最后返回该临时tensor，来达成非原地算子实现的效果， 无需新增非原地的aclnn算子）|[Ascend C算子实现](https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0031.html)|
| GE图适配   | 完成InferShape、InferDtype等代码交付件，使得算子能在GE图模式上编译与运行|[算子入图（GE）图开发](https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0077.html)|
| torchair适配   | 生成ge构图api，完成converter编写，使得算子在FX图上的节点能转换为GE图节点|[Converter补齐](https://gitee.com/ascend/torchair/blob/master/CONTRIBUTING.md#converter%E8%A1%A5%E9%BD%90)|

## 流程图

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;编写yaml文件<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;|<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| codegen注册代码<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;|<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\\/&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(图模式，生成FX图)<br />
调用library接口 -->&emsp;算子注册到library&emsp; —————————>&emsp;meta函数（FX图推导dtype、shape）<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;| (单算子模式)&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;\\/<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\\/&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Functionalization（将FX图中的原地算子节点替换为非原地）<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;torch_npu适配&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;|&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;\\/<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\\/&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;torchair converter（将FX图中的非原地算子替换为Ascend IR）<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;算子kernel&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;\\/<br />
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;Ascend图引擎执行<br />

## 适配指导（基于python侧注册算子）

#### 1.开放算子python接口

| 工作项      | 说明 |
| ----------- | ----------- |
| 开放算子的python侧接口      | 开发算子的c++接口绑定的python接口，以便算子能在python代码中被调用|

#### 2.注册算子及meta实现

```python
import torch
from torch.library import Library, impl
import torch._custom_op.impl
# 此处自定义了"my_ops"命名空间，后续将算子注册到该命名空间，就可以使用torch.ops.my_ops.xxx调用该算子
# 注意 "npu_define"已经在torch_npu的c++侧注册，python侧不能重复注册，所以此处不能再使用"npu_define"命名空间
m = Library("my_ops", "DEF")

# 注册非原地操作算子函数签名
m.define("custom_add(Tensor input1, Tensor input2) -> (Tensor, Tensor)")
# 注册原地操作算子函数签名，指定输出是input1、input1的别名（原地操作）
m.define("custom_add_(Tensor(a!) input1, Tensor(b!) input2) -> (Tensor(a!), Tensor(b!))")

# 注册原地算子实现，实现是原地修改输入的两个值，分别加1和2 (此处是为了方便展示算子的逻辑，实际使用中，如果只运行图模式，可以不注册原地操作算子的实现，只注册Functionalize即可)
@impl(m, "custom_add_", "PrivateUse1")
def plug_custom_add_(x: torch.Tensor, y: torch.Tensor) -> (torch.Tensor, torch.Tensor):
    x.add_(1) # 此处为了示例，调用的是torch.add_原地算子，实际使用中应该调用您自定义的原地操作算子的python接口
    y.add_(2)
    return x, y

# 注册非原地算子实现，实现是对输入的两个值分别加1和2，并生成新的值返回
@impl(m, "custom_add", "PrivateUse1")
def plug_custom_add(x: torch.Tensor, y: torch.Tensor) -> (torch.Tensor, torch.Tensor):
    x = torch.add(x, 1)  # 此处为了示例，调用的是torch.add非原地算子，实际使用中应该调用您自定义的非原地操作算子的python接口
    y = torch.add(y, 2)
    return x, y

# 注册非原地操作算子meta实现，用于torch FX图的shape、dtype推导
@impl(m, "custom_add", "Meta")
def custom_op_meta(x, y):
    return torch.empty_like(x), torch.empty_like(y)

# 注册原地操作算子meta实现，用于torch FX图的shape、dtype推导
@impl(m, "custom_add_", "Meta")
def custom_add_meta(x, y):
    return torch.empty_like(x), torch.empty_like(y)
```

#### 3.注册原地操作算子的functionalization

```
@impl(m, "custom_add_", "Functionalize")
def custom_add_func(x, y):
    """

    functionalization基本实现逻辑：
    1.调用算子的非原地版本，得到临时输出
    2.将临时输出的值拷贝给输入，实现修改输入值的目的

    torch.func提供了functionalize方法可以生成没有mutation的等效函数，本函数的实现代码是参考functionalize生成的代码，
    然后将torch.ops.aten.add.Tensor替换为了torch.ops.my_ops.custom_add

    functionalize生成代码的方式如下
    from torch.func import functionalize
    from torch.fx.experimental.proxy_tensor import make_fx

    a = torch.ones(1, 1)
    b = torch.ones(1, 1)
    out = make_fx(functionalize(plug_custom_add_))(a, b)
    print(out.code)
    """
    out1, out2 = torch.ops.my_ops.custom_add(x, y)
    copy_ = torch.ops.aten.copy_.default(x, out1);  x = None
    copy__1 = torch.ops.aten.copy_.default(y, out2);  y = None
    return x, y
```

#### 4.实现converter

参考多原地操作算子适配的[第6小节](#实现converter)

#### 5.完整示例代码

```
from typing import Any
import torch
from torch.library import Library, impl
import torch._custom_op.impl

import torch_npu
import torchair

from torchair.configs.compiler_config import CompilerConfig
from torchair._ge_concrete_graph.fx2ge_converter import register_fx_node_ge_converter
from torchair._ge_concrete_graph import ge_apis as ge
from torchair.ge._ge_graph import Tensor
from torchair._ge_concrete_graph.utils import dtype_promote
# 此处自定义了"my_ops"命名空间，后续将算子注册到该命名空间，就可以使用torch.ops.my_ops.xxx调用该算子
# 注意 "npu_define"已经在torch_npu的c++侧注册，python侧不能重复注册，所以此处不能再使用"npu_define"命名空间
m = Library("my_ops", "DEF")

# 注册非原地操作算子函数签名
m.define("custom_add(Tensor input1, Tensor input2) -> (Tensor, Tensor)")
# 注册原地操作算子函数签名，指定输出是input1、input1的别名（原地操作）
m.define("custom_add_(Tensor(a!) input1, Tensor(b!) input2) -> (Tensor(a!), Tensor(b!))")

# 注册原地算子实现，实现是原地修改输入的两个值，分别加1和2 (此处是为了方便展示算子的逻辑，实际使用中，如果只运行图模式，可以不注册原地操作算子的实现，只注册Functionalize即可)
@impl(m, "custom_add_", "PrivateUse1")
def plug_custom_add_(x: torch.Tensor, y: torch.Tensor) -> (torch.Tensor, torch.Tensor):
    x.add_(1) # 此处为了示例，调用的是torch.add_原地算子，实际使用中应该调用您自定义的原地操作算子的python接口
    y.add_(2)
    return x, y

# 注册非原地算子实现，实现是对输入的两个值分别加1和2，并生成新的值返回
@impl(m, "custom_add", "PrivateUse1")
def plug_custom_add(x: torch.Tensor, y: torch.Tensor) -> (torch.Tensor, torch.Tensor):
    x = torch.add(x, 1)  # 此处为了示例，调用的是torch.add非原地算子，实际使用中应该调用您自定义的非原地操作算子的python接口
    y = torch.add(y, 2)
    return x, y

# 注册非原地操作算子meta实现，用于torch FX图的shape、dtype推导
@impl(m, "custom_add", "Meta")
def custom_op_meta(x, y):
    return torch.empty_like(x), torch.empty_like(y)

# 注册原地操作算子meta实现，用于torch FX图的shape、dtype推导
@impl(m, "custom_add_", "Meta")
def custom_add_meta(x, y):
    return torch.empty_like(x), torch.empty_like(y)

# 注册非原地操作算子的converter，使得该算子能被从FX图节点转换为GE图节点
@register_fx_node_ge_converter(torch.ops.my_ops.custom_add.default)
def converter_custom_add(
        input1: Tensor,
        input2: Tensor,
        meta_outputs: Any = None):
    # 调用ge构图api
    return ge.Add(input1, 1), ge.Add(input2, 2) # 此处为了演示，调用了已有的ge.Add api，实际使用中，应该调用根据您的算子的原型生成的构图api（详见准备工作的最后一项）


@impl(m, "custom_add_", "Functionalize")
def custom_add_func(x, y):
    """

    functionalization基本实现逻辑：
    1.调用算子的非原地版本，得到临时输出
    2.将临时输出的值拷贝给输入，实现修改输入值的目的

    torch.func提供了functionalize方法可以生成没有mutation的等效函数，本函数的实现代码是参考functionalize生成的代码，
    然后将torch.ops.aten.add.Tensor替换为了torch.ops.my_ops.custom_add

    functionalize生成代码的方式如下
    from torch.func import functionalize
    from torch.fx.experimental.proxy_tensor import make_fx

    a = torch.ones(1, 1)
    b = torch.ones(1, 1)
    out = make_fx(functionalize(plug_custom_add_))(a, b)
    print(out.code)
    """
    out1, out2 = torch.ops.my_ops.custom_add(x, y)
    copy_ = torch.ops.aten.copy_.default(x, out1);  x = None
    copy__1 = torch.ops.aten.copy_.default(y, out2);  y = None
    return x, y

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self, x, y):
        out1, out2 = torch.ops.my_ops.custom_add_(x, y)
        return out1, out2

def main():
    input1 = torch.ones(1, 1).npu()
    input2 = torch.ones(1, 1).npu()
    print(f"before run model input1 is : {input1}")
    print(f"before run model input2 is : {input2}")
    print("--------------")
    config = CompilerConfig()
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    model = Model()
    c_model = torch.compile(model, backend=npu_backend, fullgraph=True)
    out1, out2 = c_model(input1, input2)
    print("--------------")
    print(f"after run model input1 is : {input1}")
    print(f"after run model input2 is : {input2}")


if __name__ == "__main__":
    main()
```

#### 5.验证脚本

执行脚本后，input1的值加了1，input2的值加了2，输出符合预期

```
before run model input1 is : tensor([[1.]], device='npu:0')
before run model input2 is : tensor([[1.]], device='npu:0')
--------------
after run model input1 is : tensor([[2.]], device='npu:0')
after run model input2 is : tensor([[3.]], device='npu:0')
```

<br />
<br />

---

<br />

## 适配指导（基于op-plugin注册算子）

### 单原地操作算子适配

当您的算子仅有一个输入被原地修改时，可参照如下示例定义一组原地和非原地算子，**op-plugin会为这种单算子的算子自动生成相应的functionalization代码**。

当您完成准备工作中的其他交付件后，便可以使用该原地算子入图
op-plugin注册算子参考：[适配开发](https://www.hiascend.com/document/detail/zh/Pytorch/700/modthirdparty/modparts/thirdpart_0014.html)

```yaml
-- op_plugin/config/op_plugin_functions.yaml

# 非原地操作算子，算子名不带下划线_
- func: scatter_update(Tensor self, Tensor indices, Tensor updates, int axis) -> Tensor
  acl_op: all_version
  op_api: all_version
  exposed: all_version
  
# 原地操作算子，算子名与非原地算子相同，并在结尾添加下划线_。用(a!)表示该入参会被原地修改
- func: scatter_update_(Tensor(a!) self, Tensor indices, Tensor updates, int axis) -> Tensor(a!)
  acl_op: all_version
  op_api: all_version
  exposed: all_version
```

### 多原地操作算子适配

#### 1.注册非原地算子及其交付件

多原地操作算子，torch原生在yaml注册时限制了返回多原地修改，此时op-plugin无法自动生成functionalization代码，需要您手动实现。需要确保非原地算子的实现及交付件已经准备好
op-plugin注册算子参考：[适配开发](https://www.hiascend.com/document/detail/zh/Pytorch/700/modthirdparty/modparts/thirdpart_0014.html)

| 工作项      | 说明 |
| ----------- | ----------- |
| 注册非原地算子、完成相关交付件      | 在op-plugin中注册非原地算子、注册meta实现，<br />并完成其GE适配交付件、torchair converter实现|

#### 2.注册原地操作算子

```yaml
-- op_plugin/config/op_plugin_functions.yaml

# 非原地操作算子
- func: custom_demo_add(Tensor input1, Tensor input2) -> (Tensor, Tensor)
  op_api: all_version

# 多原地操作算子名结尾不能带下划线，且不能返回被原地修改的值（属于社区对yaml 定义算子时 codegen的约束），
# 定义可以参考如下方式。 对于原地算子：
# 如果输出全部是原地修改输入，那么按照前面的要求，这个算子就不定义输出了；
# 如果输出是非原地和原地混合时，那么只需要写上非原地部分输出即可。
- func: _custom_demo_add(Tensor(a!) input1, Tensor(b!) input2) -> ()
  op_api: all_version
```

#### 3.原地操作算子适配实现

* 以op_api接口为例：

```cpp
-- op_plugin/ops/opapi/CustomDemoAddKernelNpuOpApi.cpp

#include "op_plugin/OpApiInterface.h"
#include "op_plugin/utils/op_api_common.h"

namespace op_api {
void _custom_demo_add(
    at::Tensor &input1, // 注意被修改的值不能用const修饰
    at::Tensor &input2)
{
    input1.add_(1);  // 原地操作，实际场景可能是调用您的acl接口
    input2.add_(2); 
}
}
```

#### 4.注册meta实现

meta实现最终的是 返回tensor 需要和算子定义的预期一致。
比如定义了原地修改的tensor要输出（单原地case），那么meta里面应该是 直接返回对应的输入，
如果定义了返回的不是原地修改的tensor，那么需要empty新的tensor去返回
如果 定义的多原地算子，除了被原地修改的输入，无其他返回值，那就pass即可，无需return

```
-- op_plugin/python/meta/_meta_registrations.py

@impl(m, "_custom_demo_add")
def custom_demo_add__meta(input1, input2):
    pass
```

#### 5.实现functionalization

```cpp
-- "在opplugin仓库的算子适配层 下创建RegisterFunctionalization_npu_ops.cpp，
     并参考以下内容实现自己算子的functionalization 函数"

namespace functionalization {
void _custom_demo_add(c10::DispatchKeySet dispatchKeySet, at::Tensor & input1, at::Tensor & input2) {
    // Sync and unwrap functional tensors
    at::functionalization::impl::sync(input1);
    at::functionalization::impl::sync(input2);
    // 注意所有调用非原地算子的参数都需要被from_functional_tensor()解包
    auto input1_unwarp = at::functionalization::impl::from_functional_tensor(input1);
    auto input2_unwarp = at::functionalization::impl::from_functional_tensor(input2);

    // Redispath to the out-of-place op when mutable op is called by user
    at::Tensor tmp_input1;
    at::Tensor tmp_input2;
    {
        at::AutoDispatchSkipFunctionalize guard;
        // 调用非原地算子（入参是from_functional_tensor()解包后的参数）得到临时输出
        auto tmp_result = at_npu::native::custom_ops::custom_demo_add(input1_unwarp, input2_unwarp);        
        tmp_input1 = std::get<0>(tmp_result);
        tmp_input2 = std::get<1>(tmp_result);
    }
    // 将临时输出拷贝给输入
    at::functionalization::impl::replace_(input1, tmp_input1);
    at::functionalization::impl::replace_(input2, tmp_input2);
    at::functionalization::impl::commit_update(input1);
    at::functionalization::impl::commit_update(input2);
    at::functionalization::impl::sync(input1);
    at::functionalization::impl::sync(input2);
}
}

namespace {
// 注册Functionalize
TORCH_LIBRARY_IMPL(npu, Functionalize, m) {
    m.impl("_custom_demo_add", TORCH_FN(functionalization::_custom_demo_add));;
}
```

完整代码及functionalization实现可参考torch_npu/csrc/aten/RegisterFunctionalization_0.cpp `npu_scatter_nd_update_`方法，该方法是torch_npu为单原地算子自动生成的实现。

#### <span id="实现converter" />6.实现converter

在torch侧定义了一个原地算子和一个非原地算子， 进入fx的图的最终是非原地算子，所以converter实现主要是针对非原地算子。

代码在torchair仓库：

```
python/torchair/_ge_concrete_graph/ge_converter/custom/custom_xxx_ops.py
```

代码实现可以参考算子 npu_scatter_nd_update.py 中的实现。

```
@register_fx_node_ge_converter(torch.ops.npu.npu_scatter_nd_update.default)
def conveter_npu_scatter_nd_update_default(
    self: Tensor,
    indices: Tensor,
    updates: Tensor,
    meta_outputs: TensorSpec = None,
):
    """NB: func: scatter_nd_update(Tensor self, Tensor indices, Tensor updates) -> Tensor"""

    copy = ge.TensorMove(self)
    return ge.ScatterNdUpdate(copy, indices, updates)
```

**注意：**

* **此处为什么要加一行TensorMove（将输入拷贝一个新值作为算子输入）？**
  答：因为此处是非原地算子的converter，由于您的两个算子使用的是同个GE IR，且GE IR已经表示了会修改入参，所以为了保证非原地算子在被单独调用时语义正确，此处需要将入参进行一次拷贝，防止入参被修改
* **但是用户可能使用原地算子的预期就是减少拷贝提升性能，此时应该怎么办？**
  一种参考方式：将该非原地算子名前加下划线（如_custom_inplace_xxx），<font color="red">表示为内部算子，不对外提供。并且说明该非原地算子仅用于原地算子入图使用，不能被直接调用，然后把converter中的TensorMove去掉</font>

#### 7.验证脚本

编写python脚本，验证如上流程注册的原地操作算子入图后，执行输出符合预期

```python
import torch
import torch_npu
import torchair
from torchair.configs.compiler_config import CompilerConfig

from torchair._ge_concrete_graph.fx2ge_converter import register_fx_node_ge_converter
from torchair.ge._ge_graph import Tensor
from torchair._ge_concrete_graph import ge_apis as ge
from typing import Any

def cus_func(x, y):
    return torch_npu._custom_demo_add(x, y)

@register_fx_node_ge_converter(torch.ops.npu.custom_demo_add.default)
def conveter_custom_demo_add_default(
        input1: Tensor,
        input2: Tensor,
        meta_outputs: Any = None):
    out1 = ge.Add(input1, 1) #此处为了演示方便，直接调用了ge.Add api，实际使用中应该使用您根据算子原型生成的ge api，参考第二章 准备工作的最后一项
    out2 = ge.Add(input2, 2)
    return out1, out2


def main():
    x = torch.ones(1, 1).npu()
    y = torch.ones(1, 1).npu()
    print(f'x is {x}')
    print(f'y is {y}')
    config = CompilerConfig()
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    compile_func = torch.compile(cus_func, backend=npu_backend, fullgraph=True)
    compile_func(x, y)
    print(f'after run func, x is {x}')
    print(f'after run func, y is {y}')

if __name__ == '__main__':
    main()
```

验证结果如下，入图执行后，输入的值被原地修改

```
x is tensor([[1.]], device='npu:0')
y is tensor([[1.]], device='npu:0')
---------
after run func, x is tensor([[2.]], device='npu:0')
after run func, y is tensor([[3.]], device='npu:0')

```
