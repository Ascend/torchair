
## 单算子入图后输出校验和精度对比指导

#### 前言

用户在实现完自定义算子并在Graph Engine（基于昇腾AI软件栈对不同的机器学习框架提供统一的IR接口，对接上层网络模型框架模式下功能，以下简称GE）的图模式跑通后，或在执行算子遇到精度问题时，需要对算子输出结果的shape、dtype、精度等进行校验和比对，从而确认算子的实现是否正确。torchAir提供了相关的功能和工具，来帮助用户快速校验与对比，本文将给出相关的指导

| 对比对象   | 对比方法|
| ----------- | ----------- |
| GE图模式与FX图输出的tensor shape与dtype      | 自动对比   |
| GE图融合模式与FX图输出的精度  | 工具对比  |
| GE图融合模式与关闭GE融合模式输出的精度| 工具对比 |

#### 1.GE与FX图输出校验

torchAir实现了自动比对GE模式最终输出tensor的shape、dtype与FX图输出是否一致，如不一致则会报错并终止程序
下面给出示例，假设用户已经实现了名为custom_op自定义算子，并有如下示例脚本代码

```import
from torch.library import impl
import torch_npu
import torchair
from torchair.configs.compiler_config import CompilerConfig

@impl(m, "custom_op", "Meta")
def custom_op_meta(x, y):
    return ...

@register_fx_node_ge_converter(torch.ops.npu_define.custom_op.default)
def conveter_custom_op(
        input1: Tensor,
        input2: Tensor,
        out: Tensor = None,
        meta_outputs: Any = None):

    return ...

class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()

    def forward(self,  ...):.
        ...
        x = torch_npu.custom_op(input1, input2)
        ...

def main():
    model = Model()    
    config = CompilerConfig()
    npu_backend = torchair.get_npu_backend(compiler_config=config)
    compile_model = torch.compile(model, backend=npu_backend, fullgraph=True)
    compile_model(...)
```

在执行推理后，如果GE输出tensor的shape和FX图输出的shape不一致，程序会有类似如下报错并终止：

```
RuntimeError: The dim of Ascend GE graph NetOutput: [2, 2] is not equal to FX graph NetOutput: [1, 1]. FX graph NetOutput shapes is : [[1, 1]], Ascend GE graph NetOutput shapes is : [[2, 2]]
```

如果GE图输出tensor的shape和FX图输出的dtype不一致，程序会有类似如下报错并终止：

```
RuntimeError: The dtype in num[0] net output of Ascend output: [DT_FLOAT] is not equal to FX graph NetOutput: [DT_INT32]. FX graph NetOutput dtypes is : [DT_INT32], Ascend GE graph NetOutput dtypes is : [DT_FLOAT]
```

##### 定位思路：

FX图输出的shape、dtype由算子注册的meta实现控制，GE输出的shape、dtype由适配GE交付件中的InferShape实现控制。**在出现以上报错时，应当检查算子的meta实现或者inferShape实现逻辑。**

* 如果FX图输出的shape与预期不一致，则算子的meta注册函数的实现可能存在问题，请检查算子的meta注册函数代码。（如果您使用的是在op-plugin 中注册的算子，其meta函数的实现在op_plugin/python/meta/_meta_registrations.py）
* 如果GE输出的shape与预期不一致，则算子适配GE交付件中的[InferShape函数](https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0078.html)可能存在问题，请检查相关代码

---

如果您运行的是整网脚本，无法直接从输出中看出是哪个算子存在问题，请分别dump torchair构造的Ascend IR原图（图中信息与上文FX图输出相对应）与CANN编译后的Ascend IR图（图中信息与上文GE输出相对应），从图中每个节点的输出信息上找到shape和dtype，逐个比对找到哪个算子开始出现不一致，然后再检查该算子的meta或inferShape实现

* dump torchair构造的Ascend IR原图：[图结构dump功能](https://www.hiascend.com/document/detail/zh/Pytorch/700/modthirdparty/torchairuseguide/torchair_0013.html)
* dump 编译后的Ascend IR图：在环境变量中添加 export DUMP_GE_GRAPH=2、export DUMP_GRAPH_LEVEL=2（变量释义：DUMP_GE_GRAPH=2：dump ge图且不记录图中const变量的值；DUMP_GRAPH_LEVEL=2：不dump子图）

打开Ascend IR原图的 dynamo_optimized_graph***.pbtxt与编译后的ge_onnx_****_PreRunAfterBuild.pbtxt，逐个节点查找是哪个算子出现不一致。
* Ascend IR原图某个节点输出的shape、dtype信息在名为 [o]xxx（xxx为输出变量名） 的Attribute中，样例：
```
Attribute Key: 
[o]y
Attribute Value: 
name: "y" dtype: DT_FLOAT layout: "ND" attr { key: "_fx_tensor_name" value { s: "copy_1-aten.copy.default.OUTPUT.0" } } attr { key: "_meta" value { s: "Tensor(dtype=torch.float32, shape=torch.Size([1, 1])" } }
```
* 编译后Ascend IR图某个节点输出的shape、dtype信息在名为 output_desc_shape:xxx、output_desc_dtype:xxx中 （xxx为该节点输出的标号，第一个输出的标号为0），样例：
```
Attribute Key: output_desc_shape:0  Attribute Value:  128, 2, 10, 128
Attribute Key: output_desc_dtype:0  Attribute Value:  DT_FLOAT16
```

#### 2.GE融合模式输出与FX图输出精度比对

GE图默认开启算子融合模式，如果在该模式下遇到精度问题，请使用msit工具来对比GE融合模式下的输出与FX输出的精度

以下只给出了关键步骤，详细指导请参考[TorchAir场景-整网算子精度比对](https://gitee.com/ascend/msit/blob/master/msit/docs/llm/TorchAir%E5%9C%BA%E6%99%AF-%E6%95%B4%E7%BD%91%E7%AE%97%E5%AD%90%E7%B2%BE%E5%BA%A6%E6%AF%94%E5%AF%B9.md)第1章节

* dump GE模式下的数据

```py
import torch, torch_npu, torchair
from msit_llm.dump import torchair_dump  # 添加导入
...
model = ...
config = torchair_dump.get_ge_dump_config(dump_path="dump")  # 添加获取 config
...
npu_backend = torchair.get_npu_backend(compiler_config=config)
model = torch.compile(model, backend=npu_backend, dynamic=True)
...
```

* dump FX模式下的数据

```py
import torch, torch_npu, torchair
from msit_llm.dump import torchair_dump  # 添加导入
...
model = ...
config = torchair_dump.get_fx_dump_config()  # 添加获取 config
...
npu_backend = torchair.get_npu_backend(compiler_config=config)
model = torch.compile(model, backend=npu_backend, dynamic=True)
...
```

* 执行精度比对

```
msit llm compare --my-path {dump_path}/dump_{time_stamp} --golden-path data_dump
```

分析输出比对结果的csv 文件，若与预期不一致，请参考下一章比对关闭融合模式的精度，或检查算子实现逻辑

#### 3.GE融合模式（默认）输出与GE关闭融合模式输出精度比对

在某些场景下，GE融合模式可能造成精度问题，此时需要比对关闭融合模式后输出的精度

以下只给出了关键步骤，详细指导请参考[TorchAir场景-整网算子精度比对](https://gitee.com/ascend/msit/blob/master/msit/docs/llm/TorchAir%E5%9C%BA%E6%99%AF-%E6%95%B4%E7%BD%91%E7%AE%97%E5%AD%90%E7%B2%BE%E5%BA%A6%E6%AF%94%E5%AF%B9.md)第2章节

* 参考创建 `fusion_switch.json` 文件关闭算子融合功能

```json
{
  "Switch": {
    "GraphFusion": {
      "ALL": "off"
    },
    "UBFusion": {
      "ALL": "off"
    }
  }
}
```

* dump 关闭GE融合模式后的数据

```py
import torch, torch_npu, torchair
from msit_llm.dump import torchair_dump  # 添加导入
...
model = ...
config = torchair_dump.get_ge_dump_config(dump_path="dump", fusion_switch_file="fusion_switch.json")  # 添加获取 config
...
npu_backend = torchair.get_npu_backend(compiler_config=config)
model = torch.compile(model, backend=npu_backend, dynamic=True)
...
```

* 按照第2小节的步骤，dump 开启GE融合模式的数据，并执行精度比对
* 分析输出比对结果的csv 文件，若与预期不一致，请检查算子实现逻辑
