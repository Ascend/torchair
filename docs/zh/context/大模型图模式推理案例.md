# 大模型图模式推理案例

## 样例须知

-   面向熟悉Transformer模型结构的用户，例如了解注意力机制、DeepSpeed分布式计算等，以便更好地进行大模型优化和执行。
-   面向推理服务场景。
-   该案例目前支持<term>Atlas A2 训练系列产品/Atlas A2 推理系列产品</term>。

## 样例获取


| 样例名称 | 样例获取 | 样例介绍 |
| --- | --- | --- |
| 图模式下LLaMA 2模型DeepSpeed分布式推理样例 | 访问[TorchAir仓](https://gitcode.com/Ascend/torchair/tree/master/npu_tuned_model/llm/llama)中npu_tuned_model/llm/llama目录，阅读README.md了解详情。 | 该样例介绍了LLaMA 2模型迁移、优化、执行过程，详细阐述了各种模型优化方法，如固定KV Cache大小、QKV融合、小算子替换为融合算子等方法。<br>优化后的模型，可供开发者直接进行应用开发，也可为自定义的大模型进行NPU迁移提供参考。 |
| 图模式下LLaMA 2模型分离部署迁移样例 | 访问[TorchAir仓](https://gitcode.com/Ascend/torchair/tree/master/npu_tuned_model/llm/llama)中npu_tuned_model/llm/llama/benchmark/pd_separate目录，阅读README.md了解详情。 | 该样例介绍了LLaMA 2模型脚本如何迁移为可以全量和增量分离部署的过程，详细阐述了脚本改造过程，包括如何修改脚本保证一次调用只会推理一次、如何拆分为全量/增量执行脚本、如何调整预处理/后处理代码等。<br>全量和增量模型分离部署后，可以减少计算资源的浪费，还可以提升模型在单位时间内处理用户请求的数量，即模型吞吐量。 |

