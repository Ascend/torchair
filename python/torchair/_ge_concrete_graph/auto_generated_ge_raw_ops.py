# This file is auto-generated by 'all_ops.h'
# Summary: total 1627, generated 1610, skipped 17
from typing import Any, Dict, List, Tuple, Union, Callable, Optional
from torchair._ge_concrete_graph.ge_ir_pb2 import GraphDef, OpDef, TensorDescriptor, TensorDef
from torchair.ge._ge_graph import get_default_ge_graph, next_unique_name
from torchair.ge._ge_graph import auto_convert_to_tensor
from torchair.ge._ge_graph import Tensor, TensorSpec, DataType, TensorType
from torchair.ge._ge_graph import compat_as_bytes, compat_as_bytes_list
from torchair.ge._ge_graph import trans_to_list_list_int, trans_to_list_list_float
from torchair.ge._ge_graph import get_invalid_desc
from torchair._ge_concrete_graph.compat_ir import ge_op, IrDef
from torchair.ge import attr
# IR Const skipped as Deformed prototype
# IR Constant skipped as Deformed prototype
# IR Data skipped as Deformed prototype
# IR PlaceholderWithDefault skipped as Deformed prototype
# IR Cast skipped as Deformed prototype
# IR _If skipped as graph
# IR StatelessIf skipped as graph
# IR If skipped as graph
# IR StatelessCase skipped as dynamic graph
# IR Case skipped as dynamic graph
# IR _While skipped as graph
# IR While skipped as graph
# IR StatelessWhile skipped as graph
# IR For skipped as graph
# IR PartitionedCall skipped as graph
# IR StatefulPartitionedCall skipped as graph



# This api is auto-generated from IR Aipp
@auto_convert_to_tensor([False, False], [False, True])
def Aipp(images: Tensor, params: Optional[Tensor], *, aipp_config_path: str="./aipp.cfg", dependencies=[], node_name=None):
    """REG_OP(Aipp)\n
.INPUT(images, TensorType{DT_UINT8})\n
.OPTIONAL_INPUT(params, TensorType{DT_UINT8})\n
.OUTPUT(features, TensorType({DT_FLOAT16, DT_UINT8}))\n
.ATTR(aipp_config_path, String, "./aipp.cfg")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Aipp"
    op.name = next_unique_name(node_name, "Aipp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    if params is not None:
        op.input.append(params.tensor)
        op.input_desc.add().CopyFrom(params.desc)
        op.input_desc[-1].name = "params"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "params"

    # process attrs
    op.attr["aipp_config_path"].s = compat_as_bytes(aipp_config_path)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "features"
    features = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return features


# This api is auto-generated from IR AippData
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def AippData(data: Tensor, *, index: int=0, dependencies=[], node_name=None):
    """REG_OP(AippData)\n
.INPUT(data, TensorType::ALL())\n
.OUTPUT(out, TensorType::ALL())\n
.ATTR(index, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AippData"
    op.name = next_unique_name(node_name, "AippData")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"

    # process attrs
    op.attr["index"].i = index

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR LowerBound
@auto_convert_to_tensor([False, False], [False, False])
def LowerBound(sorted_x: Tensor, values: Tensor, *, out_type: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(LowerBound)\n
.INPUT(sorted_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(out_type, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LowerBound"
    op.name = next_unique_name(node_name, "LowerBound")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(sorted_x.tensor)
    op.input_desc.add().CopyFrom(sorted_x.desc)
    op.input_desc[-1].name = "sorted_x"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"

    # process attrs
    op.attr["out_type"].dt = out_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReverseSequence
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ReverseSequence(x: Tensor, seq_lengths: Tensor, *, seq_dim: int, batch_dim: int=0, dependencies=[], node_name=None):
    """REG_OP(ReverseSequence)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(seq_lengths, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.REQUIRED_ATTR(seq_dim, Int)\n
.ATTR(batch_dim, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReverseSequence"
    op.name = next_unique_name(node_name, "ReverseSequence")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(seq_lengths.tensor)
    op.input_desc.add().CopyFrom(seq_lengths.desc)
    op.input_desc[-1].name = "seq_lengths"

    # process attrs
    op.attr["seq_dim"].i = seq_dim
    op.attr["batch_dim"].i = batch_dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixBandPart
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def MatrixBandPart(x: Tensor, num_lower: Tensor, num_upper: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixBandPart)\n
.INPUT(x, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128 }))\n
.INPUT(num_lower, TensorType({ DT_INT32, DT_INT64 }))\n
.INPUT(num_upper, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixBandPart"
    op.name = next_unique_name(node_name, "MatrixBandPart")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(num_lower.tensor)
    op.input_desc.add().CopyFrom(num_lower.desc)
    op.input_desc[-1].name = "num_lower"
    op.input.append(num_upper.tensor)
    op.input_desc.add().CopyFrom(num_upper.desc)
    op.input_desc[-1].name = "num_upper"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UniqueWithCounts
@auto_convert_to_tensor([False], [False])
def UniqueWithCounts(x: Tensor, *, out_idx: int, dependencies=[], node_name=None):
    """REG_OP(UniqueWithCounts)\n
.INPUT(x, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_STRING }))\n
.OUTPUT(y, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_STRING }))\n
.OUTPUT(idx, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(count, TensorType({ DT_INT32, DT_INT64 }))\n
.REQUIRED_ATTR(out_idx, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UniqueWithCounts"
    op.name = next_unique_name(node_name, "UniqueWithCounts")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["out_idx"].dt = out_idx

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "count"
    count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, idx, count


# This api is auto-generated from IR Unique
@auto_convert_to_tensor([False], [False])
def Unique(x: Tensor, *, out_idx: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(Unique)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(idx, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(out_idx, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Unique"
    op.name = next_unique_name(node_name, "Unique")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["out_idx"].dt = out_idx

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, idx


# This api is auto-generated from IR UniqueExt2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def UniqueExt2(x: Tensor, axis: Tensor, *, out_idx: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(UniqueExt2)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(axis, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(idx, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(out_idx, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UniqueExt2"
    op.name = next_unique_name(node_name, "UniqueExt2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs
    op.attr["out_idx"].dt = out_idx

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, idx


# This api is auto-generated from IR InvertPermutation
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def InvertPermutation(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InvertPermutation)\n
.INPUT(x, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InvertPermutation"
    op.name = next_unique_name(node_name, "InvertPermutation")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CheckNumerics
@auto_convert_to_tensor([False], [False])
def CheckNumerics(x: Tensor, *, message: str, dependencies=[], node_name=None):
    """REG_OP(CheckNumerics)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(message, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CheckNumerics"
    op.name = next_unique_name(node_name, "CheckNumerics")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["message"].s = compat_as_bytes(message)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnravelIndex
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def UnravelIndex(indices: Tensor, dims: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(UnravelIndex)\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(dims, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnravelIndex"
    op.name = next_unique_name(node_name, "UnravelIndex")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(dims.tensor)
    op.input_desc.add().CopyFrom(dims.desc)
    op.input_desc[-1].name = "dims"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UpperBound
@auto_convert_to_tensor([False, False], [False, False])
def UpperBound(sorted_x: Tensor, values: Tensor, *, out_type: int, dependencies=[], node_name=None):
    """REG_OP(UpperBound)\n
.INPUT(sorted_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(out_type, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpperBound"
    op.name = next_unique_name(node_name, "UpperBound")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(sorted_x.tensor)
    op.input_desc.add().CopyFrom(sorted_x.desc)
    op.input_desc[-1].name = "sorted_x"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"

    # process attrs
    op.attr["out_type"].dt = out_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UniqueWithCountsExt2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def UniqueWithCountsExt2(x: Tensor, axis: Tensor, *, out_idx: int, dependencies=[], node_name=None):
    """REG_OP(UniqueWithCountsExt2)\n
.INPUT(x, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_STRING }))\n
.INPUT(axis, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_STRING }))\n
.OUTPUT(idx, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(count, TensorType({ DT_INT32, DT_INT64 }))\n
.REQUIRED_ATTR(out_idx, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UniqueWithCountsExt2"
    op.name = next_unique_name(node_name, "UniqueWithCountsExt2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs
    op.attr["out_idx"].dt = out_idx

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "count"
    count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, idx, count


# This api is auto-generated from IR MirrorPad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def MirrorPad(x: Tensor, paddings: Tensor, *, mode: str, dependencies=[], node_name=None):
    """REG_OP(MirrorPad)\n
.INPUT(x, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128 }))\n
.INPUT(paddings, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128 }))\n
.REQUIRED_ATTR(mode, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MirrorPad"
    op.name = next_unique_name(node_name, "MirrorPad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ListDiff
@auto_convert_to_tensor([False, False], [False, False])
def ListDiff(x: Tensor, y: Tensor, *, out_idx: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(ListDiff)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64}))\n
.INPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64}))\n
.OUTPUT(out, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64}))\n
.OUTPUT(idx, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(out_idx, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ListDiff"
    op.name = next_unique_name(node_name, "ListDiff")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"

    # process attrs
    op.attr["out_idx"].dt = out_idx

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out, idx


# This api is auto-generated from IR _ParallelConcatStart
@auto_convert_to_tensor([], [])
def _ParallelConcatStart(*, dtype: int=DataType.DT_INT32, shape: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(_ParallelConcatStart)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.ATTR(dtype, Type, DT_INT32)\n
.ATTR(shape, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "_ParallelConcatStart"
    op.name = next_unique_name(node_name, "_ParallelConcatStart")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FileConstant
@auto_convert_to_tensor([], [])
def FileConstant(*, shape: List[int], dtype: int, file_path: str="", file_id: str="", dependencies=[], node_name=None):
    """REG_OP(FileConstant)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.ATTR(file_path, String, "")\n
.ATTR(file_id, String, "")\n
.REQUIRED_ATTR(shape, ListInt)\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FileConstant"
    op.name = next_unique_name(node_name, "FileConstant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["dtype"].dt = dtype
    op.attr["file_path"].s = compat_as_bytes(file_path)
    op.attr["file_id"].s = compat_as_bytes(file_id)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Snapshot
@auto_convert_to_tensor([False], [False])
def Snapshot(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Snapshot)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Snapshot"
    op.name = next_unique_name(node_name, "Snapshot")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GuaranteeConst
@auto_convert_to_tensor([False], [False])
def GuaranteeConst(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(GuaranteeConst)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GuaranteeConst"
    op.name = next_unique_name(node_name, "GuaranteeConst")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BroadcastArgs
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def BroadcastArgs(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BroadcastArgs)\n
.INPUT(x1, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(x2, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BroadcastArgs"
    op.name = next_unique_name(node_name, "BroadcastArgs")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PreventGradient
@auto_convert_to_tensor([False], [False])
def PreventGradient(x: Tensor, *, message: str="", dependencies=[], node_name=None):
    """REG_OP(PreventGradient)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.ATTR(message, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PreventGradient"
    op.name = next_unique_name(node_name, "PreventGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["message"].s = compat_as_bytes(message)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BroadcastGradientArgs
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def BroadcastGradientArgs(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BroadcastGradientArgs)\n
.INPUT(x1, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(x2, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y1, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y2, TensorType({DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BroadcastGradientArgs"
    op.name = next_unique_name(node_name, "BroadcastGradientArgs")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR StopGradient
@auto_convert_to_tensor([False], [False])
def StopGradient(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StopGradient)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StopGradient"
    op.name = next_unique_name(node_name, "StopGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Identity
@auto_convert_to_tensor([False], [False])
def Identity(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Identity)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Identity"
    op.name = next_unique_name(node_name, "Identity")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IdentityN
@auto_convert_to_tensor([True], [False])
def _IdentityN(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(IdentityN)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IdentityN"
    op.name = next_unique_name(node_name, "IdentityN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ExpandDims
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ExpandDims(x: Tensor, axis: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ExpandDims)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.INPUT(axis, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ExpandDims"
    op.name = next_unique_name(node_name, "ExpandDims")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Unsqueeze
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def Unsqueeze(x: Tensor, *, axes: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(Unsqueeze)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(axes, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Unsqueeze"
    op.name = next_unique_name(node_name, "Unsqueeze")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsqueezeV2
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def UnsqueezeV2(x: Tensor, *, axis: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(UnsqueezeV2)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(axis, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsqueezeV2"
    op.name = next_unique_name(node_name, "UnsqueezeV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsqueezeV3
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_ALL, TensorType.TT_UNKNOWN])
def UnsqueezeV3(x: Tensor, axes: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(UnsqueezeV3)\n
.INPUT(x, TensorType::ALL())\n
.INPUT(axes, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType::ALL())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsqueezeV3"
    op.name = next_unique_name(node_name, "UnsqueezeV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Reshape
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_ALL, TensorType.TT_INDEX_NUMBER])
def Reshape(x: Tensor, shape: Tensor, *, axis: int=0, num_axes: int=-1, dependencies=[], node_name=None):
    """REG_OP(Reshape)\n
.INPUT(x, TensorType::ALL())\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(axis, Int, 0)\n
.ATTR(num_axes, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Reshape"
    op.name = next_unique_name(node_name, "Reshape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["num_axes"].i = num_axes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Squeeze
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def Squeeze(x: Tensor, *, axis: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(Squeeze)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(axis, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Squeeze"
    op.name = next_unique_name(node_name, "Squeeze")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SqueezeV2
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def SqueezeV2(x: Tensor, *, axis: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(SqueezeV2)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(axis, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SqueezeV2"
    op.name = next_unique_name(node_name, "SqueezeV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SqueezeV3
@auto_convert_to_tensor([False, False], [False, True], inputs_tensor_type=[TensorType.TT_ALL, TensorType.TT_UNKNOWN])
def SqueezeV3(x: Tensor, axes: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(SqueezeV3)\n
.INPUT(x, TensorType::ALL())\n
.OPTIONAL_INPUT(axes, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType::ALL())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SqueezeV3"
    op.name = next_unique_name(node_name, "SqueezeV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if axes is not None:
        op.input.append(axes.tensor)
        op.input_desc.add().CopyFrom(axes.desc)
        op.input_desc[-1].name = "axes"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "axes"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Rank
@auto_convert_to_tensor([False], [False])
def Rank(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Rank)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Rank"
    op.name = next_unique_name(node_name, "Rank")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Size
@auto_convert_to_tensor([False], [False])
def Size(x: Tensor, *, dtype: int=3, dependencies=[], node_name=None):
    """REG_OP(Size)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT32,DT_INT64}))\n
.ATTR(dtype, Int, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Size"
    op.name = next_unique_name(node_name, "Size")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PlaceHolder
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def PlaceHolder(x: Tensor, *, peerIndex: int=0, parentId: str="", parentOpType: str="", anchorIndex: int=0, dependencies=[], node_name=None):
    """REG_OP(PlaceHolder)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(peerIndex, Int, 0)\n
.ATTR(parentId, String, "")\n
.ATTR(parentOpType, String, "")\n
.ATTR(anchorIndex, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PlaceHolder"
    op.name = next_unique_name(node_name, "PlaceHolder")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["peerIndex"].i = peerIndex
    op.attr["parentId"].s = compat_as_bytes(parentId)
    op.attr["parentOpType"].s = compat_as_bytes(parentOpType)
    op.attr["anchorIndex"].i = anchorIndex

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReadVariableOp
@auto_convert_to_tensor([False], [False])
def ReadVariableOp(x: Tensor, *, dtype: int=3, dependencies=[], node_name=None):
    """REG_OP(ReadVariableOp)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.ATTR(dtype, Int, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReadVariableOp"
    op.name = next_unique_name(node_name, "ReadVariableOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR End
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def End(x: Tensor, *, peerIndex: int=0, parentOpType: str="", dependencies=[], node_name=None):
    """REG_OP(End)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(peerIndex, Int, 0)\n
.ATTR(parentOpType, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "End"
    op.name = next_unique_name(node_name, "End")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["peerIndex"].i = peerIndex
    op.attr["parentOpType"].s = compat_as_bytes(parentOpType)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Summary
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def Summary(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Summary)\n
.INPUT(x, TensorType::ALL())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Summary"
    op.name = next_unique_name(node_name, "Summary")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR Shape
@auto_convert_to_tensor([False], [False])
def Shape(x: Tensor, *, dtype: int=3, dependencies=[], node_name=None):
    """REG_OP(Shape)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(dtype, Int, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Shape"
    op.name = next_unique_name(node_name, "Shape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GatherShapes
@auto_convert_to_tensor([True], [False], inputs_tensor_type=[TensorType.TT_ALL])
def GatherShapes(x: List[Tensor], *, axes: List[List[int]], dtype: int=3, dependencies=[], node_name=None):
    """REG_OP(GatherShapes)\n
.DYNAMIC_INPUT(x, TensorType::ALL())\n
.OUTPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(axes, ListListInt)\n
.ATTR(dtype, Int, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GatherShapes"
    op.name = next_unique_name(node_name, "GatherShapes")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["axes"].list_list_int.CopyFrom(trans_to_list_list_int(axes))
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "shape"
    shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return shape


# This api is auto-generated from IR ShapeN
@auto_convert_to_tensor([True], [False])
def _ShapeN(x: List[Tensor], *, size_of_y: int, dtype: int=3, dependencies=[], node_name=None):
    """REG_OP(ShapeN)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(dtype, Int, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ShapeN"
    op.name = next_unique_name(node_name, "ShapeN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR Empty
@auto_convert_to_tensor([False], [False])
def Empty(shape: Tensor, *, dtype: int=3, init: bool=False, dependencies=[], node_name=None):
    """REG_OP(Empty)\n
.INPUT(shape, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.ATTR(dtype, Int, DT_INT32)\n
.ATTR(init, Bool, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Empty"
    op.name = next_unique_name(node_name, "Empty")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["dtype"].i = dtype
    op.attr["init"].b = init

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MirrorPadGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def MirrorPadGrad(x: Tensor, paddings: Tensor, *, mode: str, dependencies=[], node_name=None):
    """REG_OP(MirrorPadGrad)\n
.INPUT(x, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128 }))\n
.INPUT(paddings, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128 }))\n
.REQUIRED_ATTR(mode, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MirrorPadGrad"
    op.name = next_unique_name(node_name, "MirrorPadGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Where
@auto_convert_to_tensor([False], [False])
def Where(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Where)\n
.INPUT(x, TensorType({BasicType(), DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Where"
    op.name = next_unique_name(node_name, "Where")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Copy
@auto_convert_to_tensor([False], [False])
def _Copy(x: Tensor, *, size_of_y: int, N: int, dependencies=[], node_name=None):
    """REG_OP(Copy)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Copy"
    op.name = next_unique_name(node_name, "Copy")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ViewCopy
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def ViewCopy(dst: Tensor, dst_size: Tensor, dst_stride: Tensor, dst_storage_offset: Tensor, src: Tensor, src_size: Tensor, src_stride: Tensor, src_storage_offset: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ViewCopy)\n
.INPUT(dst, TensorType::BasicType())\n
.INPUT(dst_size, TensorType::IndexNumberType())\n
.INPUT(dst_stride, TensorType::IndexNumberType())\n
.INPUT(dst_storage_offset, TensorType::IndexNumberType())\n
.INPUT(src, TensorType::BasicType())\n
.INPUT(src_size, TensorType::IndexNumberType())\n
.INPUT(src_stride, TensorType::IndexNumberType())\n
.INPUT(src_storage_offset, TensorType::IndexNumberType())\n
.OUTPUT(dst, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ViewCopy"
    op.name = next_unique_name(node_name, "ViewCopy")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dst.tensor)
    op.input_desc.add().CopyFrom(dst.desc)
    op.input_desc[-1].name = "dst"
    op.input.append(dst_size.tensor)
    op.input_desc.add().CopyFrom(dst_size.desc)
    op.input_desc[-1].name = "dst_size"
    op.input.append(dst_stride.tensor)
    op.input_desc.add().CopyFrom(dst_stride.desc)
    op.input_desc[-1].name = "dst_stride"
    op.input.append(dst_storage_offset.tensor)
    op.input_desc.add().CopyFrom(dst_storage_offset.desc)
    op.input_desc[-1].name = "dst_storage_offset"
    op.input.append(src.tensor)
    op.input_desc.add().CopyFrom(src.desc)
    op.input_desc[-1].name = "src"
    op.input.append(src_size.tensor)
    op.input_desc.add().CopyFrom(src_size.desc)
    op.input_desc[-1].name = "src_size"
    op.input.append(src_stride.tensor)
    op.input_desc.add().CopyFrom(src_stride.desc)
    op.input_desc[-1].name = "src_stride"
    op.input.append(src_storage_offset.tensor)
    op.input_desc.add().CopyFrom(src_storage_offset.desc)
    op.input_desc[-1].name = "src_storage_offset"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dst"
    dst = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dst


# This api is auto-generated from IR Fingerprint
@auto_convert_to_tensor([False, False], [False, False])
def Fingerprint(data: Tensor, method: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Fingerprint)\n
.INPUT(data, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL}))\n
.INPUT(method, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Fingerprint"
    op.name = next_unique_name(node_name, "Fingerprint")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(method.tensor)
    op.input_desc.add().CopyFrom(method.desc)
    op.input_desc[-1].name = "method"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TransShape
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def TransShape(x: Tensor, *, outShape: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(TransShape)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(outShape, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TransShape"
    op.name = next_unique_name(node_name, "TransShape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["outShape"].list.val_type = 2
    op.attr["outShape"].list.i.extend(outShape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EditDistance
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_BASIC, TensorType.TT_UNKNOWN])
def EditDistance(hypothesis_indices: Tensor, hypothesis_values: Tensor, hypothesis_shape: Tensor, truth_indices: Tensor, truth_values: Tensor, truth_shape: Tensor, *, normalize: bool=True, dependencies=[], node_name=None):
    """REG_OP(EditDistance)\n
.INPUT(hypothesis_indices, TensorType({DT_INT64}))\n
.INPUT(hypothesis_values, TensorType::BasicType())\n
.INPUT(hypothesis_shape, TensorType({DT_INT64}))\n
.INPUT(truth_indices, TensorType({DT_INT64}))\n
.INPUT(truth_values, TensorType::BasicType())\n
.INPUT(truth_shape, TensorType({DT_INT64}))\n
.ATTR(normalize, Bool, true)\n
.OUTPUT(output, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EditDistance"
    op.name = next_unique_name(node_name, "EditDistance")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(hypothesis_indices.tensor)
    op.input_desc.add().CopyFrom(hypothesis_indices.desc)
    op.input_desc[-1].name = "hypothesis_indices"
    op.input.append(hypothesis_values.tensor)
    op.input_desc.add().CopyFrom(hypothesis_values.desc)
    op.input_desc[-1].name = "hypothesis_values"
    op.input.append(hypothesis_shape.tensor)
    op.input_desc.add().CopyFrom(hypothesis_shape.desc)
    op.input_desc[-1].name = "hypothesis_shape"
    op.input.append(truth_indices.tensor)
    op.input_desc.add().CopyFrom(truth_indices.desc)
    op.input_desc[-1].name = "truth_indices"
    op.input.append(truth_values.tensor)
    op.input_desc.add().CopyFrom(truth_values.desc)
    op.input_desc[-1].name = "truth_values"
    op.input.append(truth_shape.tensor)
    op.input_desc.add().CopyFrom(truth_shape.desc)
    op.input_desc[-1].name = "truth_shape"

    # process attrs
    op.attr["normalize"].b = normalize

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR SortV2
@auto_convert_to_tensor([False], [False])
def SortV2(x: Tensor, *, axis: int=-1, descending: bool=False, dependencies=[], node_name=None):
    """REG_OP(SortV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(axis, Int, -1)\n
.ATTR(descending, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SortV2"
    op.name = next_unique_name(node_name, "SortV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["descending"].b = descending

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Expand
@auto_convert_to_tensor([False, False], [False, False])
def Expand(x: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Expand)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32,DT_INT64, DT_INT8, DT_UINT8, DT_BOOL}))\n
.INPUT(shape, TensorType({DT_INT16, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32,DT_INT64, DT_INT8, DT_UINT8, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Expand"
    op.name = next_unique_name(node_name, "Expand")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NonZero
@auto_convert_to_tensor([False], [False])
def NonZero(x: Tensor, *, transpose: bool=False, dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(NonZero)\n
.INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_INT64, DT_INT32}))\n
.ATTR(transpose, Bool, false)\n
.ATTR(dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonZero"
    op.name = next_unique_name(node_name, "NonZero")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["transpose"].b = transpose
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NonZeroWithValue
@auto_convert_to_tensor([False], [False])
def NonZeroWithValue(x: Tensor, *, transpose: bool=False, dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(NonZeroWithValue)\n
.INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL}))\n
.OUTPUT(value, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL}))\n
.OUTPUT(index, TensorType({DT_INT32}))\n
.OUTPUT(count, TensorType({DT_INT32}))\n
.ATTR(transpose, Bool, false)\n
.ATTR(dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonZeroWithValue"
    op.name = next_unique_name(node_name, "NonZeroWithValue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["transpose"].b = transpose
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "value"
    value = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "index"
    index = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "count"
    count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return value, index, count


# This api is auto-generated from IR NonZeroWithValueShape
@auto_convert_to_tensor([False, False, False], [False, False, False])
def NonZeroWithValueShape(value: Tensor, index: Tensor, count: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NonZeroWithValueShape)\n
.INPUT(value, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL}))\n
.INPUT(index, TensorType({DT_INT32}))\n
.INPUT(count, TensorType({DT_INT32}))\n
.OUTPUT(out_value, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL}))\n
.OUTPUT(out_index, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonZeroWithValueShape"
    op.name = next_unique_name(node_name, "NonZeroWithValueShape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_value"
    out_value = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "out_index"
    out_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_value, out_index


# This api is auto-generated from IR ExpandD
@auto_convert_to_tensor([False], [False])
def ExpandD(x: Tensor, *, shape: List[int], dependencies=[], node_name=None):
    """REG_OP(ExpandD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_BOOL}))\n
.REQUIRED_ATTR(shape, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ExpandD"
    op.name = next_unique_name(node_name, "ExpandD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GetShape
@auto_convert_to_tensor([True], [False])
def GetShape(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(GetShape)\n
.DYNAMIC_INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GetShape"
    op.name = next_unique_name(node_name, "GetShape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UpdateTensorDesc
@auto_convert_to_tensor([False], [False])
def UpdateTensorDesc(x: Tensor, *, shape: List[int], dependencies=[], node_name=None):
    """REG_OP(UpdateTensorDesc)\n
.INPUT(x, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT32, DT_UINT8, DT_INT64, DT_UINT64, DT_INT16, DT_UINT16, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT32, DT_UINT8, DT_INT64, DT_UINT64, DT_INT16, DT_UINT16, DT_DOUBLE}))\n
.REQUIRED_ATTR(shape, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpdateTensorDesc"
    op.name = next_unique_name(node_name, "UpdateTensorDesc")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR QueueData
@auto_convert_to_tensor([], [])
def QueueData(*, index: int=0, queue_name: str="", output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], dependencies=[], node_name=None):
    """REG_OP(QueueData)\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
.ATTR(index, Int, 0)\n
.ATTR(queue_name, String, "")\n
.ATTR(output_types, ListType, {})\n
.ATTR(output_shapes, ListListInt, {{}, {}})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueData"
    op.name = next_unique_name(node_name, "QueueData")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["index"].i = index
    op.attr["queue_name"].s = compat_as_bytes(queue_name)
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EnsureShape
@auto_convert_to_tensor([False], [False])
def EnsureShape(input: Tensor, *, shape: List[int], dependencies=[], node_name=None):
    """REG_OP(EnsureShape)\n
.INPUT(input, TensorType({DT_INT8,DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(output, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT,DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.REQUIRED_ATTR(shape, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EnsureShape"
    op.name = next_unique_name(node_name, "EnsureShape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR UniqueConsecutive
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def UniqueConsecutive(x: Tensor, *, return_idx: bool=False, return_counts: bool=False, axis: int=1000, dependencies=[], node_name=None):
    """REG_OP(UniqueConsecutive)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.OUTPUT(idx, TensorType::IndexNumberType())\n
.OUTPUT(count, TensorType::IndexNumberType())\n
.ATTR(return_idx, Bool, false)\n
.ATTR(return_counts, Bool, false)\n
.ATTR(axis, Int, 1000)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UniqueConsecutive"
    op.name = next_unique_name(node_name, "UniqueConsecutive")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["return_idx"].b = return_idx
    op.attr["return_counts"].b = return_counts
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "count"
    count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, idx, count


# This api is auto-generated from IR RaggedTensorFromVariant
@auto_convert_to_tensor([False], [False])
def _RaggedTensorFromVariant(encoded_ragged: Tensor, *, size_of_output_nested_splits: int, input_ragged_rank: int, output_ragged_rank: int, Tvalues: int, Tsplits: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(RaggedTensorFromVariant)\n
.INPUT(encoded_ragged, TensorType({DT_VARIANT}))\n
.DYNAMIC_OUTPUT(output_nested_splits, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(output_dense_values, TensorType::BasicType())\n
.REQUIRED_ATTR(input_ragged_rank, Int)\n
.REQUIRED_ATTR(output_ragged_rank, Int)\n
.REQUIRED_ATTR(Tvalues, Type)\n
.ATTR(Tsplits, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedTensorFromVariant"
    op.name = next_unique_name(node_name, "RaggedTensorFromVariant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(encoded_ragged.tensor)
    op.input_desc.add().CopyFrom(encoded_ragged.desc)
    op.input_desc[-1].name = "encoded_ragged"

    # process attrs
    op.attr["input_ragged_rank"].i = input_ragged_rank
    op.attr["output_ragged_rank"].i = output_ragged_rank
    op.attr["Tvalues"].dt = Tvalues
    op.attr["Tsplits"].dt = Tsplits

    # process outputs
    output_index = 0
    output_nested_splits = []
    for i in range(output_index, output_index + size_of_output_nested_splits):
        op.output_desc.add().name = "output_nested_splits" + str(i - output_index)
        output_nested_splits.append(Tensor(op, i))
    output_index += size_of_output_nested_splits
    op.output_desc.add().name = "output_dense_values"
    output_dense_values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_nested_splits, output_dense_values


# This api is auto-generated from IR UniqueWithCountsAndSorting
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def UniqueWithCountsAndSorting(x: Tensor, *, return_inverse: bool=False, return_counts: bool=False, sorted: bool=True, dependencies=[], node_name=None):
    """REG_OP(UniqueWithCountsAndSorting)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.OUTPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(counts, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(return_inverse, Bool, false)\n
.ATTR(return_counts, Bool, false)\n
.ATTR(sorted, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UniqueWithCountsAndSorting"
    op.name = next_unique_name(node_name, "UniqueWithCountsAndSorting")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["return_inverse"].b = return_inverse
    op.attr["return_counts"].b = return_counts
    op.attr["sorted"].b = sorted

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "counts"
    counts = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, indices, counts


# This api is auto-generated from IR RefData
@auto_convert_to_tensor([False], [False])
def RefData(x: Tensor, *, index: int=0, dependencies=[], node_name=None):
    """REG_OP(RefData)\n
.INPUT(x, "T")\n
.OUTPUT(y, "T")\n
.ATTR(index, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RefData"
    op.name = next_unique_name(node_name, "RefData")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["index"].i = index

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Mfcc
@auto_convert_to_tensor([False, False], [False, False])
def Mfcc(spectrogram: Tensor, sample_rate: Tensor, *, upper_frequency_limit: float=4000.000000, lower_frequency_limit: float=20.000000, filterbank_channel_count: int=40, dct_coefficient_count: int=13, dependencies=[], node_name=None):
    """REG_OP(Mfcc)\n
.INPUT(spectrogram, TensorType({DT_FLOAT}))\n
.INPUT(sample_rate, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(upper_frequency_limit, Float, 4000)\n
.ATTR(lower_frequency_limit, Float, 20)\n
.ATTR(filterbank_channel_count, Int, 40)\n
.ATTR(dct_coefficient_count, Int, 13)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Mfcc"
    op.name = next_unique_name(node_name, "Mfcc")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(spectrogram.tensor)
    op.input_desc.add().CopyFrom(spectrogram.desc)
    op.input_desc[-1].name = "spectrogram"
    op.input.append(sample_rate.tensor)
    op.input_desc.add().CopyFrom(sample_rate.desc)
    op.input_desc[-1].name = "sample_rate"

    # process attrs
    op.attr["upper_frequency_limit"].f = upper_frequency_limit
    op.attr["lower_frequency_limit"].f = lower_frequency_limit
    op.attr["filterbank_channel_count"].i = filterbank_channel_count
    op.attr["dct_coefficient_count"].i = dct_coefficient_count

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AudioSpectrogram
@auto_convert_to_tensor([False], [False])
def AudioSpectrogram(x: Tensor, *, window_size: int, stride: int, magnitude_squared: bool=False, dependencies=[], node_name=None):
    """REG_OP(AudioSpectrogram)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.OUTPUT(spectrogram, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(window_size, Int)\n
.REQUIRED_ATTR(stride, Int)\n
.ATTR(magnitude_squared, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AudioSpectrogram"
    op.name = next_unique_name(node_name, "AudioSpectrogram")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["window_size"].i = window_size
    op.attr["stride"].i = stride
    op.attr["magnitude_squared"].b = magnitude_squared

    # process outputs
    output_index = 0
    op.output_desc.add().name = "spectrogram"
    spectrogram = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return spectrogram


# This api is auto-generated from IR DecodeWav
@auto_convert_to_tensor([False], [False])
def DecodeWav(contents: Tensor, *, desired_channels: int=-1, desired_samples: int=-1, dependencies=[], node_name=None):
    """REG_OP(DecodeWav)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(audio, TensorType({DT_FLOAT}))\n
.OUTPUT(sample_rate, TensorType({DT_INT32}))\n
.ATTR(desired_channels, Int, -1)\n
.ATTR(desired_samples, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeWav"
    op.name = next_unique_name(node_name, "DecodeWav")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs
    op.attr["desired_channels"].i = desired_channels
    op.attr["desired_samples"].i = desired_samples

    # process outputs
    output_index = 0
    op.output_desc.add().name = "audio"
    audio = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sample_rate"
    sample_rate = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return audio, sample_rate


# This api is auto-generated from IR EncodeWav
@auto_convert_to_tensor([False, False], [False, False])
def EncodeWav(audio: Tensor, sample_rate: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(EncodeWav)\n
.INPUT(audio, TensorType({DT_FLOAT}))\n
.INPUT(sample_rate, TensorType({DT_INT32}))\n
.OUTPUT(contents, TensorType({DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EncodeWav"
    op.name = next_unique_name(node_name, "EncodeWav")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(audio.tensor)
    op.input_desc.add().CopyFrom(audio.desc)
    op.input_desc[-1].name = "audio"
    op.input.append(sample_rate.tensor)
    op.input_desc.add().CopyFrom(sample_rate.desc)
    op.input_desc[-1].name = "sample_rate"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "contents"
    contents = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return contents


# This api is auto-generated from IR AvgPool1DAvgMatrix
@auto_convert_to_tensor([False], [False])
def AvgPool1DAvgMatrix(x: Tensor, *, ksize: int, strides: int, pads: List[int], ceil_mode: bool=False, count_include_pad: bool=False, dependencies=[], node_name=None):
    """REG_OP(AvgPool1DAvgMatrix)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, Int)\n
.REQUIRED_ATTR(strides, Int)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(count_include_pad, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool1DAvgMatrix"
    op.name = next_unique_name(node_name, "AvgPool1DAvgMatrix")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].i = ksize
    op.attr["strides"].i = strides
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["count_include_pad"].b = count_include_pad

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Batch
@auto_convert_to_tensor([True], [False])
def _Batch(x_tensors: List[Tensor], *, size_of_y_tensors: int, num_batch_threads: int, max_batch_size: int, batch_timeout_micros: int, grad_timeout_micros: int, max_enqueued_batches: int=10, allowed_batch_sizes: List[int]=[], container: str="", shared_name: str="", batching_queue: str="", dependencies=[], node_name=None):
    """REG_OP(Batch)\n
.DYNAMIC_INPUT(x_tensors, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE}))\n
.DYNAMIC_OUTPUT(y_tensors, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_BOOL}))\n
.OUTPUT(y_index, TensorType({ DT_INT64 }))\n
.OUTPUT(y_id, TensorType({ DT_INT64 }))\n
.REQUIRED_ATTR(num_batch_threads, Int)\n
.REQUIRED_ATTR(max_batch_size, Int)\n
.ATTR(max_enqueued_batches, Int, 10)\n
.REQUIRED_ATTR(batch_timeout_micros, Int)\n
.ATTR(allowed_batch_sizes, ListInt, {})\n
.REQUIRED_ATTR(grad_timeout_micros, Int)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(batching_queue, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Batch"
    op.name = next_unique_name(node_name, "Batch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x_tensors, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x_tensors):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x_tensors" + str(i)

    # process attrs
    op.attr["num_batch_threads"].i = num_batch_threads
    op.attr["max_batch_size"].i = max_batch_size
    op.attr["batch_timeout_micros"].i = batch_timeout_micros
    op.attr["grad_timeout_micros"].i = grad_timeout_micros
    op.attr["max_enqueued_batches"].i = max_enqueued_batches
    op.attr["allowed_batch_sizes"].list.val_type = 2
    op.attr["allowed_batch_sizes"].list.i.extend(allowed_batch_sizes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["batching_queue"].s = compat_as_bytes(batching_queue)

    # process outputs
    output_index = 0
    y_tensors = []
    for i in range(output_index, output_index + size_of_y_tensors):
        op.output_desc.add().name = "y_tensors" + str(i - output_index)
        y_tensors.append(Tensor(op, i))
    output_index += size_of_y_tensors
    op.output_desc.add().name = "y_index"
    y_index = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_id"
    y_id = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_tensors, y_index, y_id


# This api is auto-generated from IR Unbatch
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Unbatch(x_tensor: Tensor, index: Tensor, id: Tensor, *, timeout_micros: int, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(Unbatch)\n
.INPUT(x_tensor, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(index, TensorType({DT_INT64}))\n
.INPUT(id, TensorType({DT_INT64}))\n
.OUTPUT(y_tensor, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.REQUIRED_ATTR(timeout_micros, Int)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Unbatch"
    op.name = next_unique_name(node_name, "Unbatch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_tensor.tensor)
    op.input_desc.add().CopyFrom(x_tensor.desc)
    op.input_desc[-1].name = "x_tensor"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"
    op.input.append(id.tensor)
    op.input_desc.add().CopyFrom(id.desc)
    op.input_desc[-1].name = "id"

    # process attrs
    op.attr["timeout_micros"].i = timeout_micros
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_tensor"
    y_tensor = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_tensor


# This api is auto-generated from IR UnbatchGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def UnbatchGrad(x_input: Tensor, index: Tensor, grad: Tensor, id: Tensor, *, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(UnbatchGrad)\n
.INPUT(x_input, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(index, TensorType({DT_INT64}))\n
.INPUT(grad, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(id, TensorType({DT_INT64}))\n
.OUTPUT(y_grad, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnbatchGrad"
    op.name = next_unique_name(node_name, "UnbatchGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_input.tensor)
    op.input_desc.add().CopyFrom(x_input.desc)
    op.input_desc[-1].name = "x_input"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(id.tensor)
    op.input_desc.add().CopyFrom(id.desc)
    op.input_desc[-1].name = "id"

    # process attrs
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_grad"
    y_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_grad


# This api is auto-generated from IR LeftShift
@auto_convert_to_tensor([False, False], [False, False])
def LeftShift(x: Tensor, y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LeftShift)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.INPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.OUTPUT(z, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LeftShift"
    op.name = next_unique_name(node_name, "LeftShift")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR RightShift
@auto_convert_to_tensor([False, False], [False, False])
def RightShift(x: Tensor, y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RightShift)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.INPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.OUTPUT(z, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RightShift"
    op.name = next_unique_name(node_name, "RightShift")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR BoostedTreesBucketize
@auto_convert_to_tensor([True, True], [False, False])
def _BoostedTreesBucketize(float_values: List[Tensor], bucket_boundaries: List[Tensor], *, size_of_y: int, num_features: int, dependencies=[], node_name=None):
    """REG_OP(BoostedTreesBucketize)\n
.DYNAMIC_INPUT(float_values, TensorType({DT_FLOAT}))\n
.DYNAMIC_INPUT(bucket_boundaries, TensorType({DT_FLOAT}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(num_features, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BoostedTreesBucketize"
    op.name = next_unique_name(node_name, "BoostedTreesBucketize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(float_values, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(float_values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "float_values" + str(i)
    if not isinstance(bucket_boundaries, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(bucket_boundaries):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "bucket_boundaries" + str(i)

    # process attrs
    op.attr["num_features"].i = num_features

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ThreadUnsafeUnigramCandidateSampler
@auto_convert_to_tensor([False], [False])
def ThreadUnsafeUnigramCandidateSampler(true_classes: Tensor, *, num_true: int, num_sampled: int, unique: bool, range_max: int, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(ThreadUnsafeUnigramCandidateSampler)\n
.INPUT(true_classes, TensorType({ DT_INT64 }))\n
.OUTPUT(sampled_candidates, TensorType({ DT_INT64 }))\n
.OUTPUT(true_expected_count, TensorType({ DT_FLOAT }))\n
.OUTPUT(sampled_expected_count, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(num_true, Int)\n
.REQUIRED_ATTR(num_sampled, Int)\n
.REQUIRED_ATTR(unique, Bool)\n
.REQUIRED_ATTR(range_max, Int)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ThreadUnsafeUnigramCandidateSampler"
    op.name = next_unique_name(node_name, "ThreadUnsafeUnigramCandidateSampler")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(true_classes.tensor)
    op.input_desc.add().CopyFrom(true_classes.desc)
    op.input_desc[-1].name = "true_classes"

    # process attrs
    op.attr["num_true"].i = num_true
    op.attr["num_sampled"].i = num_sampled
    op.attr["unique"].b = unique
    op.attr["range_max"].i = range_max
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sampled_candidates"
    sampled_candidates = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "true_expected_count"
    true_expected_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sampled_expected_count"
    sampled_expected_count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sampled_candidates, true_expected_count, sampled_expected_count


# This api is auto-generated from IR UniformCandidateSampler
@auto_convert_to_tensor([False], [False])
def UniformCandidateSampler(true_classes: Tensor, *, num_true: int, num_sampled: int, unique: bool, range_max: int, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(UniformCandidateSampler)\n
.INPUT(true_classes, TensorType({ DT_INT64 }))\n
.OUTPUT(sampled_candidates, TensorType({ DT_INT64 }))\n
.OUTPUT(true_expected_count, TensorType({ DT_FLOAT }))\n
.OUTPUT(sampled_expected_count, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(num_true, Int)\n
.REQUIRED_ATTR(num_sampled, Int)\n
.REQUIRED_ATTR(unique, Bool)\n
.REQUIRED_ATTR(range_max, Int)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UniformCandidateSampler"
    op.name = next_unique_name(node_name, "UniformCandidateSampler")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(true_classes.tensor)
    op.input_desc.add().CopyFrom(true_classes.desc)
    op.input_desc[-1].name = "true_classes"

    # process attrs
    op.attr["num_true"].i = num_true
    op.attr["num_sampled"].i = num_sampled
    op.attr["unique"].b = unique
    op.attr["range_max"].i = range_max
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sampled_candidates"
    sampled_candidates = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "true_expected_count"
    true_expected_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sampled_expected_count"
    sampled_expected_count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sampled_candidates, true_expected_count, sampled_expected_count


# This api is auto-generated from IR FixedUnigramCandidateSampler
@auto_convert_to_tensor([False], [False])
def FixedUnigramCandidateSampler(true_classes: Tensor, *, unigrams: List[float], num_true: int=0, num_sampled: int=0, unique: bool=False, range_max: int=0, vocab_file: str="", distortion: float=1.000000, num_reserved_ids: int=0, num_shards: int=1, shard: int=0, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(FixedUnigramCandidateSampler)\n
.INPUT(true_classes, TensorType({ DT_INT64 }))\n
.OUTPUT(sampled_candidates, TensorType({ DT_INT64 }))\n
.OUTPUT(true_expected_count, TensorType({ DT_FLOAT }))\n
.OUTPUT(sampled_expected_count, TensorType({ DT_FLOAT }))\n
.ATTR(num_true, Int, 0)\n
.ATTR(num_sampled, Int, 0)\n
.ATTR(unique, Bool, false)\n
.ATTR(range_max, Int, 0)\n
.ATTR(vocab_file, String, "")\n
.ATTR(distortion, Float, 1.0)\n
.ATTR(num_reserved_ids, Int, 0)\n
.ATTR(num_shards, Int, 1)\n
.ATTR(shard, Int, 0)\n
.REQUIRED_ATTR(unigrams, ListFloat)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FixedUnigramCandidateSampler"
    op.name = next_unique_name(node_name, "FixedUnigramCandidateSampler")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(true_classes.tensor)
    op.input_desc.add().CopyFrom(true_classes.desc)
    op.input_desc[-1].name = "true_classes"

    # process attrs
    op.attr["unigrams"].list.val_type = 3
    op.attr["unigrams"].list.f.extend(unigrams)
    op.attr["num_true"].i = num_true
    op.attr["num_sampled"].i = num_sampled
    op.attr["unique"].b = unique
    op.attr["range_max"].i = range_max
    op.attr["vocab_file"].s = compat_as_bytes(vocab_file)
    op.attr["distortion"].f = distortion
    op.attr["num_reserved_ids"].i = num_reserved_ids
    op.attr["num_shards"].i = num_shards
    op.attr["shard"].i = shard
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sampled_candidates"
    sampled_candidates = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "true_expected_count"
    true_expected_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sampled_expected_count"
    sampled_expected_count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sampled_candidates, true_expected_count, sampled_expected_count


# This api is auto-generated from IR LearnedUnigramCandidateSampler
@auto_convert_to_tensor([False], [False])
def LearnedUnigramCandidateSampler(true_classes: Tensor, *, num_true: int, num_sampled: int, unique: bool, range_max: int, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(LearnedUnigramCandidateSampler)\n
.INPUT(true_classes, TensorType({ DT_INT64 }))\n
.OUTPUT(sampled_candidates, TensorType({ DT_INT64 }))\n
.OUTPUT(true_expected_count, TensorType({ DT_FLOAT }))\n
.OUTPUT(sampled_expected_count, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(num_true, Int)\n
.REQUIRED_ATTR(num_sampled, Int)\n
.REQUIRED_ATTR(unique, Bool)\n
.REQUIRED_ATTR(range_max, Int)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LearnedUnigramCandidateSampler"
    op.name = next_unique_name(node_name, "LearnedUnigramCandidateSampler")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(true_classes.tensor)
    op.input_desc.add().CopyFrom(true_classes.desc)
    op.input_desc[-1].name = "true_classes"

    # process attrs
    op.attr["num_true"].i = num_true
    op.attr["num_sampled"].i = num_sampled
    op.attr["unique"].b = unique
    op.attr["range_max"].i = range_max
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sampled_candidates"
    sampled_candidates = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "true_expected_count"
    true_expected_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sampled_expected_count"
    sampled_expected_count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sampled_candidates, true_expected_count, sampled_expected_count


# This api is auto-generated from IR LogUniformCandidateSampler
@auto_convert_to_tensor([False], [False])
def LogUniformCandidateSampler(true_classes: Tensor, *, num_true: int, num_sampled: int, unique: bool, range_max: int, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(LogUniformCandidateSampler)\n
.INPUT(true_classes, TensorType({ DT_INT64 }))\n
.OUTPUT(sampled_candidates, TensorType({ DT_INT64 }))\n
.OUTPUT(true_expected_count, TensorType({ DT_FLOAT }))\n
.OUTPUT(sampled_expected_count, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(num_true, Int)\n
.REQUIRED_ATTR(num_sampled, Int)\n
.REQUIRED_ATTR(unique, Bool)\n
.REQUIRED_ATTR(range_max, Int)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogUniformCandidateSampler"
    op.name = next_unique_name(node_name, "LogUniformCandidateSampler")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(true_classes.tensor)
    op.input_desc.add().CopyFrom(true_classes.desc)
    op.input_desc[-1].name = "true_classes"

    # process attrs
    op.attr["num_true"].i = num_true
    op.attr["num_sampled"].i = num_sampled
    op.attr["unique"].b = unique
    op.attr["range_max"].i = range_max
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sampled_candidates"
    sampled_candidates = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "true_expected_count"
    true_expected_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sampled_expected_count"
    sampled_expected_count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sampled_candidates, true_expected_count, sampled_expected_count


# This api is auto-generated from IR AllCandidateSampler
@auto_convert_to_tensor([False], [False])
def AllCandidateSampler(true_classes: Tensor, *, num_true: int, num_sampled: int, unique: bool, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(AllCandidateSampler)\n
.INPUT(true_classes, TensorType({ DT_INT64 }))\n
.OUTPUT(sampled_candidates, TensorType({ DT_INT64 }))\n
.OUTPUT(true_expected_count, TensorType({ DT_FLOAT }))\n
.OUTPUT(sampled_expected_count, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(num_true, Int)\n
.REQUIRED_ATTR(num_sampled, Int)\n
.REQUIRED_ATTR(unique, Bool)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AllCandidateSampler"
    op.name = next_unique_name(node_name, "AllCandidateSampler")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(true_classes.tensor)
    op.input_desc.add().CopyFrom(true_classes.desc)
    op.input_desc[-1].name = "true_classes"

    # process attrs
    op.attr["num_true"].i = num_true
    op.attr["num_sampled"].i = num_sampled
    op.attr["unique"].b = unique
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sampled_candidates"
    sampled_candidates = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "true_expected_count"
    true_expected_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sampled_expected_count"
    sampled_expected_count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sampled_candidates, true_expected_count, sampled_expected_count


# This api is auto-generated from IR ComputeAccidentalHits
@auto_convert_to_tensor([False, False], [False, False])
def ComputeAccidentalHits(true_classes: Tensor, sampled_candidates: Tensor, *, num_true: int, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(ComputeAccidentalHits)\n
.INPUT(true_classes, TensorType({ DT_INT64 }))\n
.INPUT(sampled_candidates, TensorType({ DT_INT64 }))\n
.OUTPUT(indices, TensorType({ DT_INT32 }))\n
.OUTPUT(ids, TensorType({ DT_INT64 }))\n
.OUTPUT(weights, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(num_true, Int)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ComputeAccidentalHits"
    op.name = next_unique_name(node_name, "ComputeAccidentalHits")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(true_classes.tensor)
    op.input_desc.add().CopyFrom(true_classes.desc)
    op.input_desc[-1].name = "true_classes"
    op.input.append(sampled_candidates.tensor)
    op.input_desc.add().CopyFrom(sampled_candidates.desc)
    op.input_desc[-1].name = "sampled_candidates"

    # process attrs
    op.attr["num_true"].i = num_true
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ids"
    ids = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "weights"
    weights = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, ids, weights


# This api is auto-generated from IR KMeansCentroids
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True])
def KMeansCentroids(x: Tensor, y: Tensor, sum_square_y: Tensor, sum_square_x: Optional[Tensor], *, use_actual_distance: bool=False, dependencies=[], node_name=None):
    """REG_OP(KMeansCentroids)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(y, TensorType({DT_FLOAT}))\n
.INPUT(sum_square_y, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(sum_square_x, TensorType({DT_FLOAT}))\n
.OUTPUT(segment_sum, TensorType({DT_FLOAT}))\n
.OUTPUT(segment_count, TensorType({DT_FLOAT}))\n
.OUTPUT(kmean_total_sum, TensorType({DT_FLOAT}))\n
.ATTR(use_actual_distance, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "KMeansCentroids"
    op.name = next_unique_name(node_name, "KMeansCentroids")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(sum_square_y.tensor)
    op.input_desc.add().CopyFrom(sum_square_y.desc)
    op.input_desc[-1].name = "sum_square_y"
    if sum_square_x is not None:
        op.input.append(sum_square_x.tensor)
        op.input_desc.add().CopyFrom(sum_square_x.desc)
        op.input_desc[-1].name = "sum_square_x"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "sum_square_x"

    # process attrs
    op.attr["use_actual_distance"].b = use_actual_distance

    # process outputs
    output_index = 0
    op.output_desc.add().name = "segment_sum"
    segment_sum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "segment_count"
    segment_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "kmean_total_sum"
    kmean_total_sum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return segment_sum, segment_count, kmean_total_sum


# This api is auto-generated from IR CondTake
@auto_convert_to_tensor([False, False], [False, False])
def CondTake(data: Tensor, mask: Tensor, *, mode: str, val: float, eps: float=0.000001, dependencies=[], node_name=None):
    """REG_OP(CondTake)\n
.INPUT(data, TensorType({DT_FLOAT}))\n
.INPUT(mask, TensorType({DT_FLOAT}))\n
.OUTPUT(out_data, TensorType({DT_FLOAT}))\n
.OUTPUT(out_index, TensorType({DT_INT32}))\n
.OUTPUT(valid_num, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(mode, String)\n
.REQUIRED_ATTR(val, Float)\n
.ATTR(eps, Float, 1e-06)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CondTake"
    op.name = next_unique_name(node_name, "CondTake")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["val"].f = val
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_data"
    out_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "out_index"
    out_index = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "valid_num"
    valid_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_data, out_index, valid_num


# This api is auto-generated from IR Merge
@auto_convert_to_tensor([True], [False])
def Merge(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(Merge)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(value_index, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Merge"
    op.name = next_unique_name(node_name, "Merge")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_index"
    value_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, value_index


# This api is auto-generated from IR RefMerge
@auto_convert_to_tensor([True], [False])
def RefMerge(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(RefMerge)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
.OUTPUT(value_index, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RefMerge"
    op.name = next_unique_name(node_name, "RefMerge")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError

    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_index"
    value_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, value_index


# This api is auto-generated from IR Switch
@auto_convert_to_tensor([False, False], [False, False])
def Switch(data: Tensor, pred: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Switch)\n
.INPUT(data, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
.INPUT(pred, TensorType({DT_BOOL}))\n
.OUTPUT(output_false, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
.OUTPUT(output_true, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Switch"
    op.name = next_unique_name(node_name, "Switch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(pred.tensor)
    op.input_desc.add().CopyFrom(pred.desc)
    op.input_desc[-1].name = "pred"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_false"
    output_false = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_true"
    output_true = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_false, output_true


# This api is auto-generated from IR RefSwitch
@auto_convert_to_tensor([False, False], [False, False])
def RefSwitch(data: Tensor, pred: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RefSwitch)\n
.INPUT(data, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
.INPUT(pred, TensorType({DT_BOOL}))\n
.OUTPUT(output_false, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
.OUTPUT(output_true, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RefSwitch"
    op.name = next_unique_name(node_name, "RefSwitch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(pred.tensor)
    op.input_desc.add().CopyFrom(pred.desc)
    op.input_desc[-1].name = "pred"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_false"
    output_false = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_true"
    output_true = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_false, output_true


# This api is auto-generated from IR SwitchN
@auto_convert_to_tensor([False, False], [False, False])
def _SwitchN(data: Tensor, pred_value: Tensor, *, size_of_output: int, dependencies=[], node_name=None):
    """REG_OP(SwitchN)\n
.INPUT(data, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.INPUT(pred_value, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(output, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SwitchN"
    op.name = next_unique_name(node_name, "SwitchN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(pred_value.tensor)
    op.input_desc.add().CopyFrom(pred_value.desc)
    op.input_desc[-1].name = "pred_value"

    # process attrs

    # process outputs
    output_index = 0
    output = []
    for i in range(output_index, output_index + size_of_output):
        op.output_desc.add().name = "output" + str(i - output_index)
        output.append(Tensor(op, i))
    output_index += size_of_output

    # return outputs
    return output


# This api is auto-generated from IR Enter
@auto_convert_to_tensor([False], [False])
def Enter(x: Tensor, *, frame_name: str, is_constant: bool, dependencies=[], node_name=None):
    """REG_OP(Enter)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.REQUIRED_ATTR(frame_name, String)\n
.REQUIRED_ATTR(is_constant, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Enter"
    op.name = next_unique_name(node_name, "Enter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["frame_name"].s = compat_as_bytes(frame_name)
    op.attr["is_constant"].b = is_constant

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RefEnter
@auto_convert_to_tensor([False], [False])
def RefEnter(x: Tensor, *, frame_name: str, is_constant: bool, dependencies=[], node_name=None):
    """REG_OP(RefEnter)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.REQUIRED_ATTR(frame_name, String)\n
.REQUIRED_ATTR(is_constant, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RefEnter"
    op.name = next_unique_name(node_name, "RefEnter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["frame_name"].s = compat_as_bytes(frame_name)
    op.attr["is_constant"].b = is_constant

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LoopCond
@auto_convert_to_tensor([False], [False])
def LoopCond(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LoopCond)\n
.INPUT(x, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LoopCond"
    op.name = next_unique_name(node_name, "LoopCond")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NextIteration
@auto_convert_to_tensor([False], [False])
def NextIteration(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NextIteration)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NextIteration"
    op.name = next_unique_name(node_name, "NextIteration")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RefNextIteration
@auto_convert_to_tensor([False], [False])
def RefNextIteration(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RefNextIteration)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RefNextIteration"
    op.name = next_unique_name(node_name, "RefNextIteration")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Exit
@auto_convert_to_tensor([False], [False])
def Exit(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Exit)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Exit"
    op.name = next_unique_name(node_name, "Exit")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RefExit
@auto_convert_to_tensor([False], [False])
def RefExit(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RefExit)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RefExit"
    op.name = next_unique_name(node_name, "RefExit")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ControlTrigger
@auto_convert_to_tensor([], [])
def ControlTrigger(*, dependencies=[], node_name=None):
    """REG_OP(ControlTrigger)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ControlTrigger"
    op.name = next_unique_name(node_name, "ControlTrigger")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR MapIndex
@auto_convert_to_tensor([False, False, False], [False, False, True])
def MapIndex(x: Tensor, data_seq: Tensor, level_index: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(MapIndex)\n
.INPUT(x, TensorType({DT_INT32}))\n
.INPUT(data_seq, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(level_index, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapIndex"
    op.name = next_unique_name(node_name, "MapIndex")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(data_seq.tensor)
    op.input_desc.add().CopyFrom(data_seq.desc)
    op.input_desc[-1].name = "data_seq"
    if level_index is not None:
        op.input.append(level_index.tensor)
        op.input_desc.add().CopyFrom(level_index.desc)
        op.input_desc[-1].name = "level_index"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "level_index"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Correlation
@auto_convert_to_tensor([False, False], [False, False])
def Correlation(filter: Tensor, x: Tensor, *, groups: int=1, dependencies=[], node_name=None):
    """REG_OP(Correlation)\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_INT8}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_INT32}))\n
.ATTR(groups, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Correlation"
    op.name = next_unique_name(node_name, "Correlation")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["groups"].i = groups

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CTCLoss
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def CTCLoss(inputs: Tensor, labels_indices: Tensor, labels_values: Tensor, sequence_length: Tensor, *, preprocess_collapse_repeated: bool=False, ctc_merge_repeated: bool=True, ignore_longer_outputs_than_inputs: bool=False, dependencies=[], node_name=None):
    """REG_OP(CTCLoss)\n
.INPUT(inputs, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(labels_indices, TensorType({DT_INT64}))\n
.INPUT(labels_values, TensorType({DT_INT32}))\n
.INPUT(sequence_length, TensorType({DT_INT32}))\n
.OUTPUT(loss, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(gradient, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(preprocess_collapse_repeated, Bool, false)\n
.ATTR(ctc_merge_repeated, Bool, true)\n
.ATTR(ignore_longer_outputs_than_inputs, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CTCLoss"
    op.name = next_unique_name(node_name, "CTCLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(inputs.tensor)
    op.input_desc.add().CopyFrom(inputs.desc)
    op.input_desc[-1].name = "inputs"
    op.input.append(labels_indices.tensor)
    op.input_desc.add().CopyFrom(labels_indices.desc)
    op.input_desc[-1].name = "labels_indices"
    op.input.append(labels_values.tensor)
    op.input_desc.add().CopyFrom(labels_values.desc)
    op.input_desc[-1].name = "labels_values"
    op.input.append(sequence_length.tensor)
    op.input_desc.add().CopyFrom(sequence_length.desc)
    op.input_desc[-1].name = "sequence_length"

    # process attrs
    op.attr["preprocess_collapse_repeated"].b = preprocess_collapse_repeated
    op.attr["ctc_merge_repeated"].b = ctc_merge_repeated
    op.attr["ignore_longer_outputs_than_inputs"].b = ignore_longer_outputs_than_inputs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "gradient"
    gradient = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss, gradient


# This api is auto-generated from IR CTCGreedyDecoder
@auto_convert_to_tensor([False, False], [False, False])
def CTCGreedyDecoder(inputs: Tensor, sequence_length: Tensor, *, merge_repeated: bool=False, dependencies=[], node_name=None):
    """REG_OP(CTCGreedyDecoder)\n
.INPUT(inputs, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(sequence_length, TensorType({DT_INT32}))\n
.ATTR(merge_repeated, Bool, false)\n
.OUTPUT(decoded_indices, TensorType({DT_INT64}))\n
.OUTPUT(decoded_values, TensorType({DT_INT64}))\n
.OUTPUT(decoded_shape, TensorType({DT_INT64}))\n
.OUTPUT(log_probability, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CTCGreedyDecoder"
    op.name = next_unique_name(node_name, "CTCGreedyDecoder")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(inputs.tensor)
    op.input_desc.add().CopyFrom(inputs.desc)
    op.input_desc[-1].name = "inputs"
    op.input.append(sequence_length.tensor)
    op.input_desc.add().CopyFrom(sequence_length.desc)
    op.input_desc[-1].name = "sequence_length"

    # process attrs
    op.attr["merge_repeated"].b = merge_repeated

    # process outputs
    output_index = 0
    op.output_desc.add().name = "decoded_indices"
    decoded_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "decoded_values"
    decoded_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "decoded_shape"
    decoded_shape = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "log_probability"
    log_probability = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return decoded_indices, decoded_values, decoded_shape, log_probability


# This api is auto-generated from IR CTCBeamSearchDecoder
@auto_convert_to_tensor([False, False], [False, False])
def _CTCBeamSearchDecoder(inputs: Tensor, sequence_length: Tensor, *, size_of_decoded_indices: int, size_of_decoded_values: int, size_of_decoded_shape: int, beam_width: int, top_paths: int, merge_repeated: bool=True, dependencies=[], node_name=None):
    """REG_OP(CTCBeamSearchDecoder)\n
.INPUT(inputs, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(sequence_length, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(beam_width, Int)\n
.REQUIRED_ATTR(top_paths, Int)\n
.ATTR(merge_repeated, Bool, true)\n
.DYNAMIC_OUTPUT(decoded_indices, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(decoded_values, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(decoded_shape, TensorType({DT_INT64}))\n
.OUTPUT(log_probability, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CTCBeamSearchDecoder"
    op.name = next_unique_name(node_name, "CTCBeamSearchDecoder")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(inputs.tensor)
    op.input_desc.add().CopyFrom(inputs.desc)
    op.input_desc[-1].name = "inputs"
    op.input.append(sequence_length.tensor)
    op.input_desc.add().CopyFrom(sequence_length.desc)
    op.input_desc[-1].name = "sequence_length"

    # process attrs
    op.attr["beam_width"].i = beam_width
    op.attr["top_paths"].i = top_paths
    op.attr["merge_repeated"].b = merge_repeated

    # process outputs
    output_index = 0
    decoded_indices = []
    for i in range(output_index, output_index + size_of_decoded_indices):
        op.output_desc.add().name = "decoded_indices" + str(i - output_index)
        decoded_indices.append(Tensor(op, i))
    output_index += size_of_decoded_indices
    decoded_values = []
    for i in range(output_index, output_index + size_of_decoded_values):
        op.output_desc.add().name = "decoded_values" + str(i - output_index)
        decoded_values.append(Tensor(op, i))
    output_index += size_of_decoded_values
    decoded_shape = []
    for i in range(output_index, output_index + size_of_decoded_shape):
        op.output_desc.add().name = "decoded_shape" + str(i - output_index)
        decoded_shape.append(Tensor(op, i))
    output_index += size_of_decoded_shape
    op.output_desc.add().name = "log_probability"
    log_probability = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return decoded_indices, decoded_values, decoded_shape, log_probability


# This api is auto-generated from IR CTCLossV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def CTCLossV2(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, *, blank: int=0, reduction: str="mean", zero_infinity: bool=False, dependencies=[], node_name=None):
    """REG_OP(CTCLossV2)\n
.INPUT(log_probs, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(targets, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(input_lengths, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(target_lengths, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(neg_log_likelihood, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(log_alpha, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(blank, Int, 0)\n
.ATTR(reduction, String, "mean")\n
.ATTR(zero_infinity, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CTCLossV2"
    op.name = next_unique_name(node_name, "CTCLossV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(log_probs.tensor)
    op.input_desc.add().CopyFrom(log_probs.desc)
    op.input_desc[-1].name = "log_probs"
    op.input.append(targets.tensor)
    op.input_desc.add().CopyFrom(targets.desc)
    op.input_desc[-1].name = "targets"
    op.input.append(input_lengths.tensor)
    op.input_desc.add().CopyFrom(input_lengths.desc)
    op.input_desc[-1].name = "input_lengths"
    op.input.append(target_lengths.tensor)
    op.input_desc.add().CopyFrom(target_lengths.desc)
    op.input_desc[-1].name = "target_lengths"

    # process attrs
    op.attr["blank"].i = blank
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["zero_infinity"].b = zero_infinity

    # process outputs
    output_index = 0
    op.output_desc.add().name = "neg_log_likelihood"
    neg_log_likelihood = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "log_alpha"
    log_alpha = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return neg_log_likelihood, log_alpha


# This api is auto-generated from IR CTCLossV2Grad
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def CTCLossV2Grad(grad_out: Tensor, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, neg_log_likelihood: Tensor, log_alpha: Tensor, *, blank: int=0, reduction: str="mean", zero_infinity: bool=False, dependencies=[], node_name=None):
    """REG_OP(CTCLossV2Grad)\n
.INPUT(grad_out, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(log_probs, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(targets, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(input_lengths, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(target_lengths, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(neg_log_likelihood, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(log_alpha, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(grad, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(blank, Int, 0)\n
.ATTR(reduction, String, "mean")\n
.ATTR(zero_infinity, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CTCLossV2Grad"
    op.name = next_unique_name(node_name, "CTCLossV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad_out.tensor)
    op.input_desc.add().CopyFrom(grad_out.desc)
    op.input_desc[-1].name = "grad_out"
    op.input.append(log_probs.tensor)
    op.input_desc.add().CopyFrom(log_probs.desc)
    op.input_desc[-1].name = "log_probs"
    op.input.append(targets.tensor)
    op.input_desc.add().CopyFrom(targets.desc)
    op.input_desc[-1].name = "targets"
    op.input.append(input_lengths.tensor)
    op.input_desc.add().CopyFrom(input_lengths.desc)
    op.input_desc[-1].name = "input_lengths"
    op.input.append(target_lengths.tensor)
    op.input_desc.add().CopyFrom(target_lengths.desc)
    op.input_desc[-1].name = "target_lengths"
    op.input.append(neg_log_likelihood.tensor)
    op.input_desc.add().CopyFrom(neg_log_likelihood.desc)
    op.input_desc[-1].name = "neg_log_likelihood"
    op.input.append(log_alpha.tensor)
    op.input_desc.add().CopyFrom(log_alpha.desc)
    op.input_desc[-1].name = "log_alpha"

    # process attrs
    op.attr["blank"].i = blank
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["zero_infinity"].b = zero_infinity

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad"
    grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad


# This api is auto-generated from IR QueueIsClosed
@auto_convert_to_tensor([False], [False])
def QueueIsClosed(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(QueueIsClosed)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(is_closed, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueIsClosed"
    op.name = next_unique_name(node_name, "QueueIsClosed")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "is_closed"
    is_closed = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return is_closed


# This api is auto-generated from IR QueueSize
@auto_convert_to_tensor([False], [False])
def QueueSize(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(QueueSize)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(size, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueSize"
    op.name = next_unique_name(node_name, "QueueSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR FIFOQueue
@auto_convert_to_tensor([], [])
def FIFOQueue(*, component_types: List[int], shapes: List[List[int]]=[], capacity: int=-1, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(FIFOQueue)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.REQUIRED_ATTR(component_types, ListType)\n
.ATTR(shapes, ListListInt, {})\n
.ATTR(capacity, Int, -1)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FIFOQueue"
    op.name = next_unique_name(node_name, "FIFOQueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["shapes"].list_list_int.CopyFrom(trans_to_list_list_int(shapes))
    op.attr["capacity"].i = capacity
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR QueueEnqueue
@auto_convert_to_tensor([False, True], [False, False])
def QueueEnqueue(handle: Tensor, components: List[Tensor], *, timeout_ms: int=-1, dependencies=[], node_name=None):
    """REG_OP(QueueEnqueue)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.DYNAMIC_INPUT(components, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT16, DT_QUINT16, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.ATTR(timeout_ms, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueEnqueue"
    op.name = next_unique_name(node_name, "QueueEnqueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    if not isinstance(components, (tuple, list)):
        raise AssertionError("components must be a tuple or a list.")
    for i, v in enumerate(components):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "components" + str(i)

    # process attrs
    op.attr["timeout_ms"].i = timeout_ms

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR QueueEnqueueMany
@auto_convert_to_tensor([False, True], [False, False])
def QueueEnqueueMany(handle: Tensor, components: List[Tensor], *, timeout_ms: int=-1, dependencies=[], node_name=None):
    """REG_OP(QueueEnqueueMany)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.DYNAMIC_INPUT(components, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT16, DT_QUINT16, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.ATTR(timeout_ms, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueEnqueueMany"
    op.name = next_unique_name(node_name, "QueueEnqueueMany")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    if not isinstance(components, (tuple, list)):
        raise AssertionError("components must be a tuple or a list.")
    for i, v in enumerate(components):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "components" + str(i)

    # process attrs
    op.attr["timeout_ms"].i = timeout_ms

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR QueueDequeue
@auto_convert_to_tensor([False], [False])
def _QueueDequeue(handle: Tensor, *, size_of_components: int, component_types: List[int], timeout_ms: int=-1, dependencies=[], node_name=None):
    """REG_OP(QueueDequeue)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.DYNAMIC_OUTPUT(components, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT16, DT_QUINT16, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.ATTR(timeout_ms, Int, -1)\n
.REQUIRED_ATTR(component_types, ListType)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueDequeue"
    op.name = next_unique_name(node_name, "QueueDequeue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["timeout_ms"].i = timeout_ms

    # process outputs
    output_index = 0
    components = []
    for i in range(output_index, output_index + size_of_components):
        op.output_desc.add().name = "components" + str(i - output_index)
        components.append(Tensor(op, i))
    output_index += size_of_components

    # return outputs
    return components


# This api is auto-generated from IR QueueDequeueMany
@auto_convert_to_tensor([False, False], [False, False])
def _QueueDequeueMany(handle: Tensor, n: Tensor, *, size_of_components: int, component_types: List[int], timeout_ms: int=-1, dependencies=[], node_name=None):
    """REG_OP(QueueDequeueMany)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(n, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(components, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT16, DT_QUINT16, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.ATTR(timeout_ms, Int, -1)\n
.REQUIRED_ATTR(component_types, ListType)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueDequeueMany"
    op.name = next_unique_name(node_name, "QueueDequeueMany")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(n.tensor)
    op.input_desc.add().CopyFrom(n.desc)
    op.input_desc[-1].name = "n"

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["timeout_ms"].i = timeout_ms

    # process outputs
    output_index = 0
    components = []
    for i in range(output_index, output_index + size_of_components):
        op.output_desc.add().name = "components" + str(i - output_index)
        components.append(Tensor(op, i))
    output_index += size_of_components

    # return outputs
    return components


# This api is auto-generated from IR QueueDequeueUpTo
@auto_convert_to_tensor([False, False], [False, False])
def _QueueDequeueUpTo(handle: Tensor, n: Tensor, *, size_of_components: int, component_types: List[int], timeout_ms: int=-1, dependencies=[], node_name=None):
    """REG_OP(QueueDequeueUpTo)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(n, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(components, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT16, DT_QUINT16, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.ATTR(timeout_ms, Int, -1)\n
.REQUIRED_ATTR(component_types, ListType)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueDequeueUpTo"
    op.name = next_unique_name(node_name, "QueueDequeueUpTo")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(n.tensor)
    op.input_desc.add().CopyFrom(n.desc)
    op.input_desc[-1].name = "n"

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["timeout_ms"].i = timeout_ms

    # process outputs
    output_index = 0
    components = []
    for i in range(output_index, output_index + size_of_components):
        op.output_desc.add().name = "components" + str(i - output_index)
        components.append(Tensor(op, i))
    output_index += size_of_components

    # return outputs
    return components


# This api is auto-generated from IR Stage
@auto_convert_to_tensor([True], [False])
def Stage(values: List[Tensor], *, capacity: int=0, memory_limit: int=0, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(Stage)\n
.DYNAMIC_INPUT(values, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Stage"
    op.name = next_unique_name(node_name, "Stage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(values, (tuple, list)):
        raise AssertionError("values must be a tuple or a list.")
    for i, v in enumerate(values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "values" + str(i)

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR StageClear
@auto_convert_to_tensor([], [])
def StageClear(*, capacity: int=0, memory_limit: int=0, container: str="", shared_name: str="", dtypes: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(StageClear)\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(dtypes, ListType, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StageClear"
    op.name = next_unique_name(node_name, "StageClear")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR StagePeek
@auto_convert_to_tensor([False], [False])
def _StagePeek(index: Tensor, *, size_of_y: int, capacity: int=0, memory_limit: int=0, container: str="", shared_name: str="", dtypes: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(StagePeek)\n
.INPUT(index, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(dtypes, ListType, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StagePeek"
    op.name = next_unique_name(node_name, "StagePeek")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR StageSize
@auto_convert_to_tensor([], [])
def StageSize(*, capacity: int=0, memory_limit: int=0, container: str="", shared_name: str="", dtypes: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(StageSize)\n
.OUTPUT(size, TensorType({DT_INT32}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(dtypes, ListType, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StageSize"
    op.name = next_unique_name(node_name, "StageSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR StackPop
@auto_convert_to_tensor([False], [False])
def StackPop(handle: Tensor, *, elem_type: int, dependencies=[], node_name=None):
    """REG_OP(StackPop)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(element, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64}))\n
.REQUIRED_ATTR(elem_type, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StackPop"
    op.name = next_unique_name(node_name, "StackPop")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs
    op.attr["elem_type"].dt = elem_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "element"
    element = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return element


# This api is auto-generated from IR StackPush
@auto_convert_to_tensor([False, False], [False, False])
def StackPush(handle: Tensor, element: Tensor, *, swap_memory: bool=False, dependencies=[], node_name=None):
    """REG_OP(StackPush)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(element, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64}))\n
.ATTR(swap_memory, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StackPush"
    op.name = next_unique_name(node_name, "StackPush")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(element.tensor)
    op.input_desc.add().CopyFrom(element.desc)
    op.input_desc[-1].name = "element"

    # process attrs
    op.attr["swap_memory"].b = swap_memory

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StackClose
@auto_convert_to_tensor([False], [False])
def StackClose(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StackClose)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StackClose"
    op.name = next_unique_name(node_name, "StackClose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR Stack
@auto_convert_to_tensor([False], [False])
def Stack(max_size: Tensor, *, elem_type: int, stack_name: str="", dependencies=[], node_name=None):
    """REG_OP(Stack)\n
.INPUT(max_size, TensorType({DT_INT32}))\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(stack_name, String, "")\n
.REQUIRED_ATTR(elem_type, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Stack"
    op.name = next_unique_name(node_name, "Stack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(max_size.tensor)
    op.input_desc.add().CopyFrom(max_size.desc)
    op.input_desc[-1].name = "max_size"

    # process attrs
    op.attr["elem_type"].dt = elem_type
    op.attr["stack_name"].s = compat_as_bytes(stack_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR DynamicPartition
@auto_convert_to_tensor([False, False], [False, False])
def _DynamicPartition(x: Tensor, partitions: Tensor, *, size_of_y: int, num_partitions: int=1, dependencies=[], node_name=None):
    """REG_OP(DynamicPartition)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.INPUT(partitions, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.ATTR(num_partitions, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicPartition"
    op.name = next_unique_name(node_name, "DynamicPartition")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(partitions.tensor)
    op.input_desc.add().CopyFrom(partitions.desc)
    op.input_desc[-1].name = "partitions"

    # process attrs
    op.attr["num_partitions"].i = num_partitions

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR DynamicStitch
@auto_convert_to_tensor([True, True], [False, False])
def DynamicStitch(indices: List[Tensor], x: List[Tensor], *, N: int=1, dependencies=[], node_name=None):
    """REG_OP(DynamicStitch)\n
.DYNAMIC_INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_QINT32, DT_QUINT8, DT_QINT8, DT_STRING, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_QINT32, DT_QUINT8, DT_QINT8, DT_STRING, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(N, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicStitch"
    op.name = next_unique_name(node_name, "DynamicStitch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(indices, (tuple, list)):
        raise AssertionError("indices must be a tuple or a list.")
    for i, v in enumerate(indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "indices" + str(i)
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ParallelDynamicStitch
@auto_convert_to_tensor([True, True], [False, False])
def ParallelDynamicStitch(indices: List[Tensor], x: List[Tensor], *, N: int=1, dependencies=[], node_name=None):
    """REG_OP(ParallelDynamicStitch)\n
.DYNAMIC_INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_INPUT(x, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32 }))\n
.OUTPUT(y, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32 }))\n
.ATTR(N, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ParallelDynamicStitch"
    op.name = next_unique_name(node_name, "ParallelDynamicStitch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(indices, (tuple, list)):
        raise AssertionError("indices must be a tuple or a list.")
    for i, v in enumerate(indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "indices" + str(i)
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MapClear
@auto_convert_to_tensor([], [])
def MapClear(*, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(MapClear)\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapClear"
    op.name = next_unique_name(node_name, "MapClear")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR MapIncompleteSize
@auto_convert_to_tensor([], [])
def MapIncompleteSize(*, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(MapIncompleteSize)\n
.OUTPUT(size, TensorType({DT_INT32}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapIncompleteSize"
    op.name = next_unique_name(node_name, "MapIncompleteSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR Unstage
@auto_convert_to_tensor([], [])
def _Unstage(*, size_of_y: int, dtypes: List[int], capacity: int=0, memory_limit: int=0, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(Unstage)\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.REQUIRED_ATTR(dtypes, ListType)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Unstage"
    op.name = next_unique_name(node_name, "Unstage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR MapStage
@auto_convert_to_tensor([False, False, True], [False, False, False])
def MapStage(key: Tensor, indices: Tensor, values: List[Tensor], *, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(MapStage)\n
.INPUT(key, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_INPUT(values, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32 }))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapStage"
    op.name = next_unique_name(node_name, "MapStage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    if not isinstance(values, (tuple, list)):
        raise AssertionError("values must be a tuple or a list.")
    for i, v in enumerate(values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "values" + str(i)

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR MapUnstage
@auto_convert_to_tensor([False, False], [False, False])
def _MapUnstage(key: Tensor, indices: Tensor, *, size_of_values: int, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(MapUnstage)\n
.INPUT(key, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(values, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32 }))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapUnstage"
    op.name = next_unique_name(node_name, "MapUnstage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    values = []
    for i in range(output_index, output_index + size_of_values):
        op.output_desc.add().name = "values" + str(i - output_index)
        values.append(Tensor(op, i))
    output_index += size_of_values

    # return outputs
    return values


# This api is auto-generated from IR MapUnstageNoKey
@auto_convert_to_tensor([False], [False])
def _MapUnstageNoKey(indices: Tensor, *, size_of_values: int, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(MapUnstageNoKey)\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(key, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(values, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32 }))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapUnstageNoKey"
    op.name = next_unique_name(node_name, "MapUnstageNoKey")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "key"
    key = Tensor(op, output_index)
    output_index += 1
    values = []
    for i in range(output_index, output_index + size_of_values):
        op.output_desc.add().name = "values" + str(i - output_index)
        values.append(Tensor(op, i))
    output_index += size_of_values

    # return outputs
    return key, values


# This api is auto-generated from IR MapPeek
@auto_convert_to_tensor([False, False], [False, False])
def _MapPeek(key: Tensor, indices: Tensor, *, size_of_values: int, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(MapPeek)\n
.INPUT(key, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(values, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32 }))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapPeek"
    op.name = next_unique_name(node_name, "MapPeek")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    values = []
    for i in range(output_index, output_index + size_of_values):
        op.output_desc.add().name = "values" + str(i - output_index)
        values.append(Tensor(op, i))
    output_index += size_of_values

    # return outputs
    return values


# This api is auto-generated from IR MapSize
@auto_convert_to_tensor([], [])
def MapSize(*, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(MapSize)\n
.OUTPUT(size, TensorType({DT_INT32}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MapSize"
    op.name = next_unique_name(node_name, "MapSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR TensorArray
@auto_convert_to_tensor([False], [False])
def TensorArray(size: Tensor, *, dtype: int, element_shape: List[int]=[-2], dynamic_size: bool=False, clear_after_read: bool=True, identical_element_shapes: bool=False, tensor_array_name: str="", dependencies=[], node_name=None):
    """REG_OP(TensorArray)\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(flow, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(element_shape, ListInt, ge::UNKNOWN_RANK)\n
.ATTR(dynamic_size, Bool, false)\n
.ATTR(clear_after_read, Bool, true)\n
.ATTR(identical_element_shapes, Bool, false)\n
.ATTR(tensor_array_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArray"
    op.name = next_unique_name(node_name, "TensorArray")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["element_shape"].list.val_type = 2
    op.attr["element_shape"].list.i.extend(element_shape)
    op.attr["dynamic_size"].b = dynamic_size
    op.attr["clear_after_read"].b = clear_after_read
    op.attr["identical_element_shapes"].b = identical_element_shapes
    op.attr["tensor_array_name"].s = compat_as_bytes(tensor_array_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "flow"
    flow = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle, flow


# This api is auto-generated from IR TensorArrayClose
@auto_convert_to_tensor([False], [False])
def TensorArrayClose(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorArrayClose)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayClose"
    op.name = next_unique_name(node_name, "TensorArrayClose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR TensorArrayConcat
@auto_convert_to_tensor([False, False], [False, False])
def TensorArrayConcat(handle: Tensor, flow_in: Tensor, *, dtype: int, element_shape_except0: List[int]=[-2], dependencies=[], node_name=None):
    """REG_OP(TensorArrayConcat)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(flow_in, TensorType({DT_FLOAT}))\n
.OUTPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.OUTPUT(lengths, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(element_shape_except0, ListInt, ge::UNKNOWN_RANK)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayConcat"
    op.name = next_unique_name(node_name, "TensorArrayConcat")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["element_shape_except0"].list.val_type = 2
    op.attr["element_shape_except0"].list.i.extend(element_shape_except0)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "value"
    value = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "lengths"
    lengths = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return value, lengths


# This api is auto-generated from IR TensorArrayGather
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TensorArrayGather(handle: Tensor, indices: Tensor, flow_in: Tensor, *, dtype: int, element_shape: List[int]=[-2], dependencies=[], node_name=None):
    """REG_OP(TensorArrayGather)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(flow_in, TensorType({DT_FLOAT}))\n
.OUTPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(element_shape, ListInt, ge::UNKNOWN_RANK)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayGather"
    op.name = next_unique_name(node_name, "TensorArrayGather")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["element_shape"].list.val_type = 2
    op.attr["element_shape"].list.i.extend(element_shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "value"
    value = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return value


# This api is auto-generated from IR TensorArrayGrad
@auto_convert_to_tensor([False, False], [False, False])
def TensorArrayGrad(handle: Tensor, flow_in: Tensor, *, source: str, dependencies=[], node_name=None):
    """REG_OP(TensorArrayGrad)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(flow_in, TensorType({DT_FLOAT}))\n
.OUTPUT(grad_handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(flow_out, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(source, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayGrad"
    op.name = next_unique_name(node_name, "TensorArrayGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs
    op.attr["source"].s = compat_as_bytes(source)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_handle"
    grad_handle = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "flow_out"
    flow_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_handle, flow_out


# This api is auto-generated from IR TensorArrayWrite
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def TensorArrayWrite(handle: Tensor, index: Tensor, value: Tensor, flow_in: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorArrayWrite)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(index, TensorType({DT_INT32}))\n
.INPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_STRING, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(flow_in, TensorType({DT_FLOAT}))\n
.OUTPUT(flow_out, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayWrite"
    op.name = next_unique_name(node_name, "TensorArrayWrite")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "flow_out"
    flow_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return flow_out


# This api is auto-generated from IR TensorArrayGradWithShape
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TensorArrayGradWithShape(handle: Tensor, flow_in: Tensor, shape_to_prepend: Tensor, *, source: str="", dependencies=[], node_name=None):
    """REG_OP(TensorArrayGradWithShape)\n
.INPUT(handle, TensorType({ DT_RESOURCE }))\n
.INPUT(flow_in, TensorType({ DT_FLOAT }))\n
.INPUT(shape_to_prepend, TensorType({ DT_INT32 }))\n
.OUTPUT(grad_handle, TensorType({ DT_RESOURCE }))\n
.OUTPUT(flow_out, TensorType({ DT_FLOAT }))\n
.ATTR(source, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayGradWithShape"
    op.name = next_unique_name(node_name, "TensorArrayGradWithShape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"
    op.input.append(shape_to_prepend.tensor)
    op.input_desc.add().CopyFrom(shape_to_prepend.desc)
    op.input_desc[-1].name = "shape_to_prepend"

    # process attrs
    op.attr["source"].s = compat_as_bytes(source)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_handle"
    grad_handle = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "flow_out"
    flow_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_handle, flow_out


# This api is auto-generated from IR TensorArrayRead
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TensorArrayRead(handle: Tensor, index: Tensor, flow_in: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(TensorArrayRead)\n
.INPUT(handle, TensorType({ DT_RESOURCE }))\n
.INPUT(index, TensorType({ DT_INT32 }))\n
.INPUT(flow_in, TensorType({ DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayRead"
    op.name = next_unique_name(node_name, "TensorArrayRead")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TensorArrayScatter
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def TensorArrayScatter(handle: Tensor, indices: Tensor, value: Tensor, flow_in: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorArrayScatter)\n
.INPUT(handle, TensorType({ DT_RESOURCE }))\n
.INPUT(indices, TensorType({ DT_INT32 }))\n
.INPUT(value, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128 }))\n
.INPUT(flow_in, TensorType({ DT_FLOAT }))\n
.OUTPUT(flow_out, TensorType({ DT_FLOAT }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArrayScatter"
    op.name = next_unique_name(node_name, "TensorArrayScatter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "flow_out"
    flow_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return flow_out


# This api is auto-generated from IR TensorArraySplit
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def TensorArraySplit(handle: Tensor, value: Tensor, lengths: Tensor, flow_in: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorArraySplit)\n
.INPUT(handle, TensorType({ DT_RESOURCE }))\n
.INPUT(value, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128 }))\n
.INPUT(lengths, TensorType({ DT_INT64 }))\n
.INPUT(flow_in, TensorType({ DT_FLOAT }))\n
.OUTPUT(flow_out, TensorType({ DT_FLOAT }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArraySplit"
    op.name = next_unique_name(node_name, "TensorArraySplit")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(lengths.tensor)
    op.input_desc.add().CopyFrom(lengths.desc)
    op.input_desc[-1].name = "lengths"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "flow_out"
    flow_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return flow_out


# This api is auto-generated from IR TensorArraySize
@auto_convert_to_tensor([False, False], [False, False])
def TensorArraySize(handle: Tensor, flow_in: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorArraySize)\n
.INPUT(handle, TensorType({ DT_RESOURCE }))\n
.INPUT(flow_in, TensorType({ DT_FLOAT }))\n
.OUTPUT(size, TensorType({ DT_INT32 }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorArraySize"
    op.name = next_unique_name(node_name, "TensorArraySize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(flow_in.tensor)
    op.input_desc.add().CopyFrom(flow_in.desc)
    op.input_desc[-1].name = "flow_in"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR RandomShuffleQueue
@auto_convert_to_tensor([], [])
def RandomShuffleQueue(*, component_types: List[int], shapes: List[List[int]]=[], capacity: int=-1, min_after_dequeue: int=0, seed: int=0, seed2: int=0, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(RandomShuffleQueue)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.REQUIRED_ATTR(component_types, ListType)\n
.ATTR(shapes, ListListInt, {})\n
.ATTR(capacity, Int, -1)\n
.ATTR(min_after_dequeue, Int, 0)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomShuffleQueue"
    op.name = next_unique_name(node_name, "RandomShuffleQueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["shapes"].list_list_int.CopyFrom(trans_to_list_list_int(shapes))
    op.attr["capacity"].i = capacity
    op.attr["min_after_dequeue"].i = min_after_dequeue
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR PaddingFIFOQueue
@auto_convert_to_tensor([], [])
def PaddingFIFOQueue(*, component_types: List[int], shapes: List[List[int]]=[], capacity: int=-1, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(PaddingFIFOQueue)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.REQUIRED_ATTR(component_types, ListType)\n
.ATTR(shapes, ListListInt, {})\n
.ATTR(capacity, Int, -1)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PaddingFIFOQueue"
    op.name = next_unique_name(node_name, "PaddingFIFOQueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["shapes"].list_list_int.CopyFrom(trans_to_list_list_int(shapes))
    op.attr["capacity"].i = capacity
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR PriorityQueue
@auto_convert_to_tensor([], [])
def PriorityQueue(*, component_types: List[int]=[], shapes: List[List[int]]=[], capacity: int=-1, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(PriorityQueue)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(component_types, ListType, {})\n
.ATTR(shapes, ListListInt, {})\n
.ATTR(capacity, Int, -1)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PriorityQueue"
    op.name = next_unique_name(node_name, "PriorityQueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["shapes"].list_list_int.CopyFrom(trans_to_list_list_int(shapes))
    op.attr["capacity"].i = capacity
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR QueueClose
@auto_convert_to_tensor([False], [False])
def QueueClose(handle: Tensor, *, cancel_pending_enqueues: bool=False, dependencies=[], node_name=None):
    """REG_OP(QueueClose)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(cancel_pending_enqueues, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QueueClose"
    op.name = next_unique_name(node_name, "QueueClose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs
    op.attr["cancel_pending_enqueues"].b = cancel_pending_enqueues

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR OrderedMapStage
@auto_convert_to_tensor([False, False, True], [False, False, False])
def OrderedMapStage(key: Tensor, indices: Tensor, values: List[Tensor], *, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(OrderedMapStage)\n
.INPUT(key, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_INPUT(values, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32 }))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OrderedMapStage"
    op.name = next_unique_name(node_name, "OrderedMapStage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    if not isinstance(values, (tuple, list)):
        raise AssertionError("values must be a tuple or a list.")
    for i, v in enumerate(values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "values" + str(i)

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR OrderedMapSize
@auto_convert_to_tensor([], [])
def OrderedMapSize(*, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(OrderedMapSize)\n
.OUTPUT(size, TensorType({DT_INT32}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OrderedMapSize"
    op.name = next_unique_name(node_name, "OrderedMapSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR OrderedMapClear
@auto_convert_to_tensor([], [])
def OrderedMapClear(*, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(OrderedMapClear)\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OrderedMapClear"
    op.name = next_unique_name(node_name, "OrderedMapClear")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR FakeQueue
@auto_convert_to_tensor([False], [False])
def FakeQueue(resource: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FakeQueue)\n
.INPUT(resource, TensorType({DT_RESOURCE}))\n
.OUTPUT(handle, TensorType({DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FakeQueue"
    op.name = next_unique_name(node_name, "FakeQueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(resource.tensor)
    op.input_desc.add().CopyFrom(resource.desc)
    op.input_desc[-1].name = "resource"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR OrderedMapIncompleteSize
@auto_convert_to_tensor([], [])
def OrderedMapIncompleteSize(*, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(OrderedMapIncompleteSize)\n
.OUTPUT(size, TensorType({DT_INT32}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OrderedMapIncompleteSize"
    op.name = next_unique_name(node_name, "OrderedMapIncompleteSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR OrderedMapPeek
@auto_convert_to_tensor([False, False], [False, False])
def _OrderedMapPeek(key: Tensor, indices: Tensor, *, size_of_values: int, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(OrderedMapPeek)\n
.INPUT(key, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(values, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32 }))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OrderedMapPeek"
    op.name = next_unique_name(node_name, "OrderedMapPeek")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    values = []
    for i in range(output_index, output_index + size_of_values):
        op.output_desc.add().name = "values" + str(i - output_index)
        values.append(Tensor(op, i))
    output_index += size_of_values

    # return outputs
    return values


# This api is auto-generated from IR OrderedMapUnstageNoKey
@auto_convert_to_tensor([False], [False])
def _OrderedMapUnstageNoKey(indices: Tensor, *, size_of_values: int, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(OrderedMapUnstageNoKey)\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(key, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(values, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32 }))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OrderedMapUnstageNoKey"
    op.name = next_unique_name(node_name, "OrderedMapUnstageNoKey")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "key"
    key = Tensor(op, output_index)
    output_index += 1
    values = []
    for i in range(output_index, output_index + size_of_values):
        op.output_desc.add().name = "values" + str(i - output_index)
        values.append(Tensor(op, i))
    output_index += size_of_values

    # return outputs
    return key, values


# This api is auto-generated from IR OrderedMapUnstage
@auto_convert_to_tensor([False, False], [False, False])
def _OrderedMapUnstage(key: Tensor, indices: Tensor, *, size_of_values: int, capacity: int=0, memory_limit: int=0, dtypes: List[int]=[], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(OrderedMapUnstage)\n
.INPUT(key, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_BOOL, DT_UINT32, DT_UINT64}))\n
.ATTR(capacity, Int, 0)\n
.ATTR(memory_limit, Int, 0)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OrderedMapUnstage"
    op.name = next_unique_name(node_name, "OrderedMapUnstage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["capacity"].i = capacity
    op.attr["memory_limit"].i = memory_limit
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    values = []
    for i in range(output_index, output_index + size_of_values):
        op.output_desc.add().name = "values" + str(i - output_index)
        values.append(Tensor(op, i))
    output_index += size_of_values

    # return outputs
    return values


# This api is auto-generated from IR Barrier
@auto_convert_to_tensor([], [])
def Barrier(*, component_types: List[int], shapes: List[List[int]]=[], capacity: int=-1, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(Barrier)\n
.OUTPUT(handle, TensorType({DT_STRING_REF}))\n
.REQUIRED_ATTR(component_types, ListType)\n
.ATTR(shapes, ListListInt, {})\n
.ATTR(capacity, Int, -1)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Barrier"
    op.name = next_unique_name(node_name, "Barrier")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["shapes"].list_list_int.CopyFrom(trans_to_list_list_int(shapes))
    op.attr["capacity"].i = capacity
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR BarrierInsertMany
@auto_convert_to_tensor([False, False, False], [False, False, False])
def BarrierInsertMany(handle: Tensor, keys: Tensor, values: Tensor, *, component_index: int, dependencies=[], node_name=None):
    """REG_OP(BarrierInsertMany)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.INPUT(keys, TensorType({DT_STRING}))\n
.INPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.REQUIRED_ATTR(component_index, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BarrierInsertMany"
    op.name = next_unique_name(node_name, "BarrierInsertMany")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"

    # process attrs
    op.attr["component_index"].i = component_index

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR BarrierTakeMany
@auto_convert_to_tensor([False, False], [False, False])
def _BarrierTakeMany(handle: Tensor, num_elements: Tensor, *, size_of_values: int, component_types: List[int], allow_small_batch: bool=False, wait_for_incomplete: bool=False, timeout_ms: int=-1, dependencies=[], node_name=None):
    """REG_OP(BarrierTakeMany)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.INPUT(num_elements, TensorType(DT_INT32))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(keys, TensorType({DT_STRING}))\n
.DYNAMIC_OUTPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.REQUIRED_ATTR(component_types, ListType)\n
.ATTR(allow_small_batch, Bool, false)\n
.ATTR(wait_for_incomplete, Bool, false)\n
.ATTR(timeout_ms, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BarrierTakeMany"
    op.name = next_unique_name(node_name, "BarrierTakeMany")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(num_elements.tensor)
    op.input_desc.add().CopyFrom(num_elements.desc)
    op.input_desc[-1].name = "num_elements"

    # process attrs
    op.attr["component_types"].list.val_type = 10
    op.attr["component_types"].list.dt.extend(component_types)
    op.attr["allow_small_batch"].b = allow_small_batch
    op.attr["wait_for_incomplete"].b = wait_for_incomplete
    op.attr["timeout_ms"].i = timeout_ms

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "keys"
    keys = Tensor(op, output_index)
    output_index += 1
    values = []
    for i in range(output_index, output_index + size_of_values):
        op.output_desc.add().name = "values" + str(i - output_index)
        values.append(Tensor(op, i))
    output_index += size_of_values

    # return outputs
    return indices, keys, values


# This api is auto-generated from IR BarrierClose
@auto_convert_to_tensor([False], [False])
def BarrierClose(handle: Tensor, *, cancel_pending_enqueues: bool=False, dependencies=[], node_name=None):
    """REG_OP(BarrierClose)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.ATTR(cancel_pending_enqueues, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BarrierClose"
    op.name = next_unique_name(node_name, "BarrierClose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs
    op.attr["cancel_pending_enqueues"].b = cancel_pending_enqueues

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR BarrierReadySize
@auto_convert_to_tensor([False], [False])
def BarrierReadySize(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BarrierReadySize)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.OUTPUT(size, TensorType(DT_INT32))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BarrierReadySize"
    op.name = next_unique_name(node_name, "BarrierReadySize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR BarrierIncompleteSize
@auto_convert_to_tensor([False], [False])
def BarrierIncompleteSize(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BarrierIncompleteSize)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.OUTPUT(size, TensorType(DT_INT32))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BarrierIncompleteSize"
    op.name = next_unique_name(node_name, "BarrierIncompleteSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR RecordInput
@auto_convert_to_tensor([], [])
def RecordInput(*, file_pattern: str, file_random_seed: int=301, file_shuffle_shift_ratio: float=0.000000, file_buffer_size: int=10000, file_parallelism: int=16, batch_size: int=32, compression_type: str="", dependencies=[], node_name=None):
    """REG_OP(RecordInput)\n
.OUTPUT(records, TensorType({DT_STRING}))\n
.REQUIRED_ATTR(file_pattern, String)\n
.ATTR(file_random_seed, Int, 301)\n
.ATTR(file_shuffle_shift_ratio, Float, 0)\n
.ATTR(file_buffer_size, Int, 10000)\n
.ATTR(file_parallelism, Int, 16)\n
.ATTR(batch_size, Int, 32)\n
.ATTR(compression_type, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RecordInput"
    op.name = next_unique_name(node_name, "RecordInput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["file_pattern"].s = compat_as_bytes(file_pattern)
    op.attr["file_random_seed"].i = file_random_seed
    op.attr["file_shuffle_shift_ratio"].f = file_shuffle_shift_ratio
    op.attr["file_buffer_size"].i = file_buffer_size
    op.attr["file_parallelism"].i = file_parallelism
    op.attr["batch_size"].i = batch_size
    op.attr["compression_type"].s = compat_as_bytes(compression_type)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "records"
    records = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return records


# This api is auto-generated from IR ConditionalAccumulator
@auto_convert_to_tensor([], [])
def ConditionalAccumulator(*, dtype: int, shape: List[int], container: str="", shared_name: str="", reduction_type: str="MEAN", dependencies=[], node_name=None):
    """REG_OP(ConditionalAccumulator)\n
.OUTPUT(handle, TensorType({DT_STRING_REF}))\n
.REQUIRED_ATTR(dtype, Type)\n
.REQUIRED_ATTR(shape, ListInt)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(reduction_type, String, "MEAN")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConditionalAccumulator"
    op.name = next_unique_name(node_name, "ConditionalAccumulator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["reduction_type"].s = compat_as_bytes(reduction_type)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR AccumulatorApplyGradient
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AccumulatorApplyGradient(handle: Tensor, local_step: Tensor, gradient: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(AccumulatorApplyGradient)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.INPUT(local_step, TensorType({DT_INT64}))\n
.INPUT(gradient, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AccumulatorApplyGradient"
    op.name = next_unique_name(node_name, "AccumulatorApplyGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(local_step.tensor)
    op.input_desc.add().CopyFrom(local_step.desc)
    op.input_desc[-1].name = "local_step"
    op.input.append(gradient.tensor)
    op.input_desc.add().CopyFrom(gradient.desc)
    op.input_desc[-1].name = "gradient"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR AccumulatorNumAccumulated
@auto_convert_to_tensor([False], [False])
def AccumulatorNumAccumulated(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AccumulatorNumAccumulated)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AccumulatorNumAccumulated"
    op.name = next_unique_name(node_name, "AccumulatorNumAccumulated")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AccumulatorSetGlobalStep
@auto_convert_to_tensor([False, False], [False, False])
def AccumulatorSetGlobalStep(handle: Tensor, new_global_step: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AccumulatorSetGlobalStep)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.INPUT(new_global_step, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AccumulatorSetGlobalStep"
    op.name = next_unique_name(node_name, "AccumulatorSetGlobalStep")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(new_global_step.tensor)
    op.input_desc.add().CopyFrom(new_global_step.desc)
    op.input_desc[-1].name = "new_global_step"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR AccumulatorTakeGradient
@auto_convert_to_tensor([False, False], [False, False])
def AccumulatorTakeGradient(handle: Tensor, num_required: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(AccumulatorTakeGradient)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.INPUT(num_required, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AccumulatorTakeGradient"
    op.name = next_unique_name(node_name, "AccumulatorTakeGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(num_required.tensor)
    op.input_desc.add().CopyFrom(num_required.desc)
    op.input_desc[-1].name = "num_required"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseConditionalAccumulator
@auto_convert_to_tensor([], [])
def SparseConditionalAccumulator(*, shape: List[int], dtype: int, container: str="", shared_name: str="", reduction_type: str="MEAN", dependencies=[], node_name=None):
    """REG_OP(SparseConditionalAccumulator)\n
.OUTPUT(handle, TensorType({DT_STRING_REF}))\n
.REQUIRED_ATTR(shape, ListInt)\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(reduction_type, String, "MEAN")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseConditionalAccumulator"
    op.name = next_unique_name(node_name, "SparseConditionalAccumulator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["dtype"].dt = dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["reduction_type"].s = compat_as_bytes(reduction_type)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR SparseAccumulatorApplyGradient
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def SparseAccumulatorApplyGradient(handle: Tensor, local_step: Tensor, indices: Tensor, values: Tensor, shape: Tensor, *, has_known_shape: bool, dtype: int, dependencies=[], node_name=None):
    """REG_OP(SparseAccumulatorApplyGradient)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.INPUT(local_step, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_UINT32, DT_UINT64, DT_COMPLEX64, DT_COMPLEX128, DT_QINT16, DT_QUINT16, DT_QINT8, DT_QUINT8, DT_QINT32}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(has_known_shape, Bool)\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseAccumulatorApplyGradient"
    op.name = next_unique_name(node_name, "SparseAccumulatorApplyGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(local_step.tensor)
    op.input_desc.add().CopyFrom(local_step.desc)
    op.input_desc[-1].name = "local_step"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["has_known_shape"].b = has_known_shape
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR SparseAccumulatorTakeGradient
@auto_convert_to_tensor([False, False], [False, False])
def SparseAccumulatorTakeGradient(handle: Tensor, num_required: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(SparseAccumulatorTakeGradient)\n
.INPUT(handle, TensorType({DT_STRING_REF}))\n
.INPUT(num_required, TensorType({DT_INT32}))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT}))\n
.OUTPUT(shape, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseAccumulatorTakeGradient"
    op.name = next_unique_name(node_name, "SparseAccumulatorTakeGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(num_required.tensor)
    op.input_desc.add().CopyFrom(num_required.desc)
    op.input_desc[-1].name = "num_required"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "shape"
    shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, values, shape


# This api is auto-generated from IR ResourceConditionalAccumulator
@auto_convert_to_tensor([], [])
def ResourceConditionalAccumulator(*, dtype: int, shape: List[int], container: str="", shared_name: str="", reduction_type: str="MEAN", dependencies=[], node_name=None):
    """REG_OP(ResourceConditionalAccumulator)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.REQUIRED_ATTR(dtype, Type)\n
.REQUIRED_ATTR(shape, ListInt)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(reduction_type, String, "MEAN")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResourceConditionalAccumulator"
    op.name = next_unique_name(node_name, "ResourceConditionalAccumulator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["reduction_type"].s = compat_as_bytes(reduction_type)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR ResourceAccumulatorApplyGradient
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ResourceAccumulatorApplyGradient(handle: Tensor, local_step: Tensor, gradient: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ResourceAccumulatorApplyGradient)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(local_step, TensorType({DT_INT64}))\n
.INPUT(gradient, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResourceAccumulatorApplyGradient"
    op.name = next_unique_name(node_name, "ResourceAccumulatorApplyGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(local_step.tensor)
    op.input_desc.add().CopyFrom(local_step.desc)
    op.input_desc[-1].name = "local_step"
    op.input.append(gradient.tensor)
    op.input_desc.add().CopyFrom(gradient.desc)
    op.input_desc[-1].name = "gradient"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ResourceAccumulatorNumAccumulated
@auto_convert_to_tensor([False], [False])
def ResourceAccumulatorNumAccumulated(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ResourceAccumulatorNumAccumulated)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(num_accumulated, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResourceAccumulatorNumAccumulated"
    op.name = next_unique_name(node_name, "ResourceAccumulatorNumAccumulated")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "num_accumulated"
    num_accumulated = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return num_accumulated


# This api is auto-generated from IR ResourceAccumulatorSetGlobalStep
@auto_convert_to_tensor([False, False], [False, False])
def ResourceAccumulatorSetGlobalStep(handle: Tensor, new_global_step: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ResourceAccumulatorSetGlobalStep)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(new_global_step, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResourceAccumulatorSetGlobalStep"
    op.name = next_unique_name(node_name, "ResourceAccumulatorSetGlobalStep")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(new_global_step.tensor)
    op.input_desc.add().CopyFrom(new_global_step.desc)
    op.input_desc[-1].name = "new_global_step"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ResourceAccumulatorTakeGradient
@auto_convert_to_tensor([False, False], [False, False])
def ResourceAccumulatorTakeGradient(handle: Tensor, num_required: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(ResourceAccumulatorTakeGradient)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(num_required, TensorType({DT_INT32}))\n
.OUTPUT(average, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResourceAccumulatorTakeGradient"
    op.name = next_unique_name(node_name, "ResourceAccumulatorTakeGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(num_required.tensor)
    op.input_desc.add().CopyFrom(num_required.desc)
    op.input_desc[-1].name = "num_required"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "average"
    average = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return average


# This api is auto-generated from IR OutfeedEnqueueOp
@auto_convert_to_tensor([True], [False])
def OutfeedEnqueueOp(x: List[Tensor], *, channel_name: str="", dependencies=[], node_name=None):
    """REG_OP(OutfeedEnqueueOp)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.ATTR(channel_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OutfeedEnqueueOp"
    op.name = next_unique_name(node_name, "OutfeedEnqueueOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["channel_name"].s = compat_as_bytes(channel_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR OutfeedEnqueueOpV2
@auto_convert_to_tensor([True, False], [False, False])
def OutfeedEnqueueOpV2(x: List[Tensor], tensor_name: Tensor, *, channel_name: str="", dependencies=[], node_name=None):
    """REG_OP(OutfeedEnqueueOpV2)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.INPUT(tensor_name, TensorType({DT_STRING}))\n
.ATTR(channel_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OutfeedEnqueueOpV2"
    op.name = next_unique_name(node_name, "OutfeedEnqueueOpV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(tensor_name.tensor)
    op.input_desc.add().CopyFrom(tensor_name.desc)
    op.input_desc[-1].name = "tensor_name"

    # process attrs
    op.attr["channel_name"].s = compat_as_bytes(channel_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR LruCache
@auto_convert_to_tensor([], [])
def LruCache(*, dtype: int, container: str="", shared_name: str="LruCache", cache_size: int=100000, load_factor: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(LruCache)\n
.OUTPUT(cache, TensorType({DT_RESOURCE}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "LruCache")\n
.ATTR(cache_size, Int, 100000)\n
.ATTR(load_factor, Float, 1)\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LruCache"
    op.name = next_unique_name(node_name, "LruCache")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["cache_size"].i = cache_size
    op.attr["load_factor"].f = load_factor

    # process outputs
    output_index = 0
    op.output_desc.add().name = "cache"
    cache = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return cache


# This api is auto-generated from IR CacheAdd
@auto_convert_to_tensor([False, False], [False, False])
def CacheAdd(cache: Tensor, ids: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(CacheAdd)\n
.INPUT(cache, TensorType({DT_RESOURCE}))\n
.INPUT(ids, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
.OUTPUT(swap_in_id, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
.OUTPUT(swap_in_idx, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
.OUTPUT(swap_out_id, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
.OUTPUT(swap_out_idx, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CacheAdd"
    op.name = next_unique_name(node_name, "CacheAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(cache.tensor)
    op.input_desc.add().CopyFrom(cache.desc)
    op.input_desc[-1].name = "cache"
    op.input.append(ids.tensor)
    op.input_desc.add().CopyFrom(ids.desc)
    op.input_desc[-1].name = "ids"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "swap_in_id"
    swap_in_id = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "swap_in_idx"
    swap_in_idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "swap_out_id"
    swap_out_id = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "swap_out_idx"
    swap_out_idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return swap_in_id, swap_in_idx, swap_out_id, swap_out_idx


# This api is auto-generated from IR CacheRemoteIndexToLocal
@auto_convert_to_tensor([False, False], [False, False])
def CacheRemoteIndexToLocal(cache: Tensor, ids: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(CacheRemoteIndexToLocal)\n
.INPUT(cache, TensorType({DT_RESOURCE}))\n
.INPUT(ids, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
.OUTPUT(local_idx, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CacheRemoteIndexToLocal"
    op.name = next_unique_name(node_name, "CacheRemoteIndexToLocal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(cache.tensor)
    op.input_desc.add().CopyFrom(cache.desc)
    op.input_desc[-1].name = "cache"
    op.input.append(ids.tensor)
    op.input_desc.add().CopyFrom(ids.desc)
    op.input_desc[-1].name = "ids"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "local_idx"
    local_idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return local_idx


# This api is auto-generated from IR CacheAllIndexToLocal
@auto_convert_to_tensor([False], [False])
def CacheAllIndexToLocal(cache: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(CacheAllIndexToLocal)\n
.INPUT(cache, TensorType({DT_RESOURCE}))\n
.OUTPUT(local_idx, TensorType({DT_INT64, DT_INT32, DT_UINT64, DT_UINT32}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CacheAllIndexToLocal"
    op.name = next_unique_name(node_name, "CacheAllIndexToLocal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(cache.tensor)
    op.input_desc.add().CopyFrom(cache.desc)
    op.input_desc[-1].name = "cache"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "local_idx"
    local_idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return local_idx


# This api is auto-generated from IR LRUCacheV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_BASIC])
def LRUCacheV2(index_list: Tensor, data: Tensor, cache: Tensor, tag: Tensor, is_last_call: Tensor, *, pre_route_count: int, dependencies=[], node_name=None):
    """REG_OP(LRUCacheV2)\n
.INPUT(index_list, TensorType::BasicType())\n
.INPUT(data, TensorType::BasicType())\n
.INPUT(cache, TensorType::BasicType())\n
.INPUT(tag, TensorType::BasicType())\n
.INPUT(is_last_call, TensorType::BasicType())\n
.OUTPUT(data, TensorType::BasicType())\n
.OUTPUT(cache, TensorType::BasicType())\n
.OUTPUT(tag, TensorType::BasicType())\n
.OUTPUT(index_offset_list, TensorType::BasicType())\n
.OUTPUT(not_in_cache_index_list, TensorType::BasicType())\n
.OUTPUT(not_in_cache_number, TensorType::BasicType())\n
.REQUIRED_ATTR(pre_route_count, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LRUCacheV2"
    op.name = next_unique_name(node_name, "LRUCacheV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(index_list.tensor)
    op.input_desc.add().CopyFrom(index_list.desc)
    op.input_desc[-1].name = "index_list"
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(cache.tensor)
    op.input_desc.add().CopyFrom(cache.desc)
    op.input_desc[-1].name = "cache"
    op.input.append(tag.tensor)
    op.input_desc.add().CopyFrom(tag.desc)
    op.input_desc[-1].name = "tag"
    op.input.append(is_last_call.tensor)
    op.input_desc.add().CopyFrom(is_last_call.desc)
    op.input_desc[-1].name = "is_last_call"

    # process attrs
    op.attr["pre_route_count"].i = pre_route_count

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "cache"
    cache = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tag"
    tag = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "index_offset_list"
    index_offset_list = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "not_in_cache_index_list"
    not_in_cache_index_list = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "not_in_cache_number"
    not_in_cache_number = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data, cache, tag, index_offset_list, not_in_cache_index_list, not_in_cache_number


# This api is auto-generated from IR DynamicGetNext
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def _DynamicGetNext(x: Tensor, *, size_of_y: int, output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], _dynamic_graph_execute_mode: str="lazy_recompile", _getnext_inputs_shape_range: str="", dependencies=[], node_name=None):
    """REG_OP(DynamicGetNext)\n
.INPUT(x, TensorType::ALL())\n
.DYNAMIC_OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListType, {})\n
.ATTR(output_shapes, ListListInt, {{}, {}})\n
.ATTR(_dynamic_graph_execute_mode, String, "lazy_recompile")\n
.ATTR(_getnext_inputs_shape_range, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicGetNext"
    op.name = next_unique_name(node_name, "DynamicGetNext")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["_dynamic_graph_execute_mode"].s = compat_as_bytes(_dynamic_graph_execute_mode)
    op.attr["_getnext_inputs_shape_range"].s = compat_as_bytes(_getnext_inputs_shape_range)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR DynamicGetNextV2
@auto_convert_to_tensor([], [])
def _DynamicGetNextV2(*, size_of_y: int, output_types: List[int]=[], channel_name: str="", output_shapes: List[List[int]]=[[], []], _dynamic_graph_execute_mode: str="lazy_recompile", _getnext_inputs_shape_range: str="", dependencies=[], node_name=None):
    """REG_OP(DynamicGetNextV2)\n
.DYNAMIC_OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListType, {})\n
.ATTR(channel_name, String, "")\n
.ATTR(output_shapes, ListListInt, {{}, {}})\n
.ATTR(_dynamic_graph_execute_mode, String, "lazy_recompile")\n
.ATTR(_getnext_inputs_shape_range, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicGetNextV2"
    op.name = next_unique_name(node_name, "DynamicGetNextV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["channel_name"].s = compat_as_bytes(channel_name)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["_dynamic_graph_execute_mode"].s = compat_as_bytes(_dynamic_graph_execute_mode)
    op.attr["_getnext_inputs_shape_range"].s = compat_as_bytes(_getnext_inputs_shape_range)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR AdpGetNext
@auto_convert_to_tensor([], [])
def _AdpGetNext(*, size_of_y: int, output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], queue_name: str="", dependencies=[], node_name=None):
    """REG_OP(AdpGetNext)\n
.DYNAMIC_OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListType, {})\n
.ATTR(output_shapes, ListListInt, {{}, {}})\n
.ATTR(queue_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdpGetNext"
    op.name = next_unique_name(node_name, "AdpGetNext")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["queue_name"].s = compat_as_bytes(queue_name)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR GetNextV2
@auto_convert_to_tensor([], [])
def _GetNextV2(*, size_of_y: int, output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], channel_name: str="", dependencies=[], node_name=None):
    """REG_OP(GetNextV2)\n
.DYNAMIC_OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListType, {})\n
.ATTR(output_shapes, ListListInt, {{}, {}})\n
.ATTR(channel_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GetNextV2"
    op.name = next_unique_name(node_name, "GetNextV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["channel_name"].s = compat_as_bytes(channel_name)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR GetNextFromQueue
@auto_convert_to_tensor([False], [False])
def _GetNextFromQueue(x: Tensor, *, size_of_y: int, output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], dependencies=[], node_name=None):
    """REG_OP(GetNextFromQueue)\n
.INPUT(x, TensorType({DT_UINT8}))\n
.DYNAMIC_OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListType, {})\n
.ATTR(output_shapes, ListListInt, {{}, {}})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GetNextFromQueue"
    op.name = next_unique_name(node_name, "GetNextFromQueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR PeekData
@auto_convert_to_tensor([], [])
def _PeekData(*, size_of_y: int, output_types: List[int]=[], output_shapes: List[List[int]]=[], channel_name: str="", dependencies=[], node_name=None):
    """REG_OP(PeekData)\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL}))\n
.ATTR(output_types, ListType, {})\n
.ATTR(output_shapes, ListListInt, {})\n
.ATTR(channel_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PeekData"
    op.name = next_unique_name(node_name, "PeekData")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["channel_name"].s = compat_as_bytes(channel_name)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR OptionalGetValue
@auto_convert_to_tensor([False], [False])
def _OptionalGetValue(optional: Tensor, *, size_of_components: int, output_types: List[int], output_shapes: List[List[int]], dependencies=[], node_name=None):
    """REG_OP(OptionalGetValue)\n
.INPUT(optional, TensorType({DT_VARIANT}))\n
.DYNAMIC_OUTPUT(components, TensorType::BasicType())\n
.REQUIRED_ATTR(output_types, ListType)\n
.REQUIRED_ATTR(output_shapes, ListListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OptionalGetValue"
    op.name = next_unique_name(node_name, "OptionalGetValue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(optional.tensor)
    op.input_desc.add().CopyFrom(optional.desc)
    op.input_desc[-1].name = "optional"

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))

    # process outputs
    output_index = 0
    components = []
    for i in range(output_index, output_index + size_of_components):
        op.output_desc.add().name = "components" + str(i - output_index)
        components.append(Tensor(op, i))
    output_index += size_of_components

    # return outputs
    return components


# This api is auto-generated from IR FlowFunc
@auto_convert_to_tensor([True], [False])
def _FlowFunc(x: List[Tensor], *, size_of_y: int, bin_path: str, func_name: str, output_types: List[int], output_shapes: List[List[int]]=[], dependencies=[], node_name=None):
    """REG_OP(FlowFunc)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.REQUIRED_ATTR(bin_path, String)\n
.REQUIRED_ATTR(func_name, String)\n
.ATTR(output_shapes, ListListInt, {})\n
.REQUIRED_ATTR(output_types, ListType)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FlowFunc"
    op.name = next_unique_name(node_name, "FlowFunc")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["bin_path"].s = compat_as_bytes(bin_path)
    op.attr["func_name"].s = compat_as_bytes(func_name)
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR SequenceAt
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def SequenceAt(handle: Tensor, index: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SequenceAt)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(index, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SequenceAt"
    op.name = next_unique_name(node_name, "SequenceAt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SequenceConstruct
@auto_convert_to_tensor([True], [False])
def SequenceConstruct(inputs: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(SequenceConstruct)\n
.DYNAMIC_INPUT(inputs, TensorType({DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SequenceConstruct"
    op.name = next_unique_name(node_name, "SequenceConstruct")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(inputs, (tuple, list)):
        raise AssertionError("inputs must be a tuple or a list.")
    for i, v in enumerate(inputs):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "inputs" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR SequenceEmpty
@auto_convert_to_tensor([], [])
def SequenceEmpty(*, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(SequenceEmpty)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SequenceEmpty"
    op.name = next_unique_name(node_name, "SequenceEmpty")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR SequenceErase
@auto_convert_to_tensor([False, False], [False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def SequenceErase(handle: Tensor, index: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(SequenceErase)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OPTIONAL_INPUT(index, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(handle_y, TensorType({DT_RESOURCE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SequenceErase"
    op.name = next_unique_name(node_name, "SequenceErase")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    if index is not None:
        op.input.append(index.tensor)
        op.input_desc.add().CopyFrom(index.desc)
        op.input_desc[-1].name = "index"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "index"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle_y"
    handle_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle_y


# This api is auto-generated from IR SequenceInsert
@auto_convert_to_tensor([False, False, False], [False, False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def SequenceInsert(handle: Tensor, value: Tensor, index: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(SequenceInsert)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(value, TensorType({DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
.OPTIONAL_INPUT(index, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(handle_y, TensorType({DT_RESOURCE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SequenceInsert"
    op.name = next_unique_name(node_name, "SequenceInsert")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    if index is not None:
        op.input.append(index.tensor)
        op.input_desc.add().CopyFrom(index.desc)
        op.input_desc[-1].name = "index"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "index"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle_y"
    handle_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle_y


# This api is auto-generated from IR SequenceLength
@auto_convert_to_tensor([False], [False])
def SequenceLength(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SequenceLength)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(length, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SequenceLength"
    op.name = next_unique_name(node_name, "SequenceLength")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "length"
    length = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return length


# This api is auto-generated from IR SplitToSequence
@auto_convert_to_tensor([False, False], [False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def SplitToSequence(x: Tensor, split: Optional[Tensor], *, axis: int=0, keepdims: bool=True, dependencies=[], node_name=None):
    """REG_OP(SplitToSequence)\n
.INPUT(x, TensorType({DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
.OPTIONAL_INPUT(split, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(axis, Int, 0)\n
.ATTR(keepdims, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SplitToSequence"
    op.name = next_unique_name(node_name, "SplitToSequence")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if split is not None:
        op.input.append(split.tensor)
        op.input_desc.add().CopyFrom(split.desc)
        op.input_desc[-1].name = "split"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "split"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["keepdims"].b = keepdims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR ConcatFromSequence
@auto_convert_to_tensor([False], [False])
def ConcatFromSequence(handle: Tensor, *, axis: int, new_axis: int=0, dependencies=[], node_name=None):
    """REG_OP(ConcatFromSequence)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(y, TensorType({DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
.REQUIRED_ATTR(axis, Int)\n
.ATTR(new_axis, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConcatFromSequence"
    op.name = next_unique_name(node_name, "ConcatFromSequence")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["new_axis"].i = new_axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TabulateFusion
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def TabulateFusion(table: Tensor, table_info: Tensor, em_x: Tensor, em: Tensor, *, last_layer_size: int, dependencies=[], node_name=None):
    """REG_OP(TabulateFusion)\n
.INPUT(table, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(table_info, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(em_x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(em, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(descriptor, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(last_layer_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TabulateFusion"
    op.name = next_unique_name(node_name, "TabulateFusion")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table.tensor)
    op.input_desc.add().CopyFrom(table.desc)
    op.input_desc[-1].name = "table"
    op.input.append(table_info.tensor)
    op.input_desc.add().CopyFrom(table_info.desc)
    op.input_desc[-1].name = "table_info"
    op.input.append(em_x.tensor)
    op.input_desc.add().CopyFrom(em_x.desc)
    op.input_desc[-1].name = "em_x"
    op.input.append(em.tensor)
    op.input_desc.add().CopyFrom(em.desc)
    op.input_desc[-1].name = "em"

    # process attrs
    op.attr["last_layer_size"].i = last_layer_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "descriptor"
    descriptor = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return descriptor


# This api is auto-generated from IR ProdEnvMatA
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def ProdEnvMatA(coord: Tensor, type: Tensor, natoms: Tensor, box: Tensor, mesh: Tensor, davg: Tensor, dstd: Tensor, *, rcut_a: float=1.000000, rcut_r: float=1.000000, rcut_r_smth: float=1.000000, sel_a: List[int]=[], sel_r: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(ProdEnvMatA)\n
.INPUT(coord, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(type, TensorType({DT_INT32}))\n
.INPUT(natoms, TensorType({DT_INT32}))\n
.INPUT(box, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(mesh, TensorType({DT_INT32}))\n
.INPUT(davg, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(dstd, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(descrpt, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(descrpt_deriv, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(rij, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(nlist, TensorType({DT_INT32}))\n
.ATTR(rcut_a, Float, 1.0)\n
.ATTR(rcut_r, Float, 1.0)\n
.ATTR(rcut_r_smth, Float, 1.0)\n
.ATTR(sel_a, ListInt, {})\n
.ATTR(sel_r, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ProdEnvMatA"
    op.name = next_unique_name(node_name, "ProdEnvMatA")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(coord.tensor)
    op.input_desc.add().CopyFrom(coord.desc)
    op.input_desc[-1].name = "coord"
    op.input.append(type.tensor)
    op.input_desc.add().CopyFrom(type.desc)
    op.input_desc[-1].name = "type"
    op.input.append(natoms.tensor)
    op.input_desc.add().CopyFrom(natoms.desc)
    op.input_desc[-1].name = "natoms"
    op.input.append(box.tensor)
    op.input_desc.add().CopyFrom(box.desc)
    op.input_desc[-1].name = "box"
    op.input.append(mesh.tensor)
    op.input_desc.add().CopyFrom(mesh.desc)
    op.input_desc[-1].name = "mesh"
    op.input.append(davg.tensor)
    op.input_desc.add().CopyFrom(davg.desc)
    op.input_desc[-1].name = "davg"
    op.input.append(dstd.tensor)
    op.input_desc.add().CopyFrom(dstd.desc)
    op.input_desc[-1].name = "dstd"

    # process attrs
    op.attr["rcut_a"].f = rcut_a
    op.attr["rcut_r"].f = rcut_r
    op.attr["rcut_r_smth"].f = rcut_r_smth
    op.attr["sel_a"].list.val_type = 2
    op.attr["sel_a"].list.i.extend(sel_a)
    op.attr["sel_r"].list.val_type = 2
    op.attr["sel_r"].list.i.extend(sel_r)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "descrpt"
    descrpt = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "descrpt_deriv"
    descrpt_deriv = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rij"
    rij = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nlist"
    nlist = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return descrpt, descrpt_deriv, rij, nlist


# This api is auto-generated from IR ProdEnvMatACalcRij
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def ProdEnvMatACalcRij(coord: Tensor, type: Tensor, natoms: Tensor, box: Tensor, mesh: Tensor, *, rcut_a: float=1.000000, rcut_r: float=1.000000, rcut_r_smth: float=1.000000, sel_a: List[int]=[], sel_r: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(ProdEnvMatACalcRij)\n
.INPUT(coord, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(type, TensorType({DT_INT32}))\n
.INPUT(natoms, TensorType({DT_INT32}))\n
.INPUT(box, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(mesh, TensorType({DT_INT32}))\n
.OUTPUT(rij, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(nlist, TensorType({DT_INT32}))\n
.OUTPUT(distance, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(rij_x, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(rij_y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(rij_z, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(rcut_a, Float, 1.0)\n
.ATTR(rcut_r, Float, 1.0)\n
.ATTR(rcut_r_smth, Float, 1.0)\n
.ATTR(sel_a, ListInt, {})\n
.ATTR(sel_r, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ProdEnvMatACalcRij"
    op.name = next_unique_name(node_name, "ProdEnvMatACalcRij")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(coord.tensor)
    op.input_desc.add().CopyFrom(coord.desc)
    op.input_desc[-1].name = "coord"
    op.input.append(type.tensor)
    op.input_desc.add().CopyFrom(type.desc)
    op.input_desc[-1].name = "type"
    op.input.append(natoms.tensor)
    op.input_desc.add().CopyFrom(natoms.desc)
    op.input_desc[-1].name = "natoms"
    op.input.append(box.tensor)
    op.input_desc.add().CopyFrom(box.desc)
    op.input_desc[-1].name = "box"
    op.input.append(mesh.tensor)
    op.input_desc.add().CopyFrom(mesh.desc)
    op.input_desc[-1].name = "mesh"

    # process attrs
    op.attr["rcut_a"].f = rcut_a
    op.attr["rcut_r"].f = rcut_r
    op.attr["rcut_r_smth"].f = rcut_r_smth
    op.attr["sel_a"].list.val_type = 2
    op.attr["sel_a"].list.i.extend(sel_a)
    op.attr["sel_r"].list.val_type = 2
    op.attr["sel_r"].list.i.extend(sel_r)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "rij"
    rij = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nlist"
    nlist = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "distance"
    distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rij_x"
    rij_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rij_y"
    rij_y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rij_z"
    rij_z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return rij, nlist, distance, rij_x, rij_y, rij_z


# This api is auto-generated from IR ProdEnvMatACalcDescrpt
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False])
def ProdEnvMatACalcDescrpt(distance: Tensor, rij_x: Tensor, rij_y: Tensor, rij_z: Tensor, type: Tensor, natoms: Tensor, mesh: Tensor, davg: Tensor, dstd: Tensor, *, rcut_a: float=1.000000, rcut_r: float=1.000000, rcut_r_smth: float=1.000000, sel_a: List[int]=[], sel_r: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(ProdEnvMatACalcDescrpt)\n
.INPUT(distance, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(rij_x, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(rij_y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(rij_z, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(type, TensorType({DT_INT32}))\n
.INPUT(natoms, TensorType({DT_INT32}))\n
.INPUT(mesh, TensorType({DT_INT32}))\n
.INPUT(davg, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(dstd, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(descrpt, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(descrpt_deriv, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(rcut_a, Float, 1.0)\n
.ATTR(rcut_r, Float, 1.0)\n
.ATTR(rcut_r_smth, Float, 1.0)\n
.ATTR(sel_a, ListInt, {})\n
.ATTR(sel_r, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ProdEnvMatACalcDescrpt"
    op.name = next_unique_name(node_name, "ProdEnvMatACalcDescrpt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(distance.tensor)
    op.input_desc.add().CopyFrom(distance.desc)
    op.input_desc[-1].name = "distance"
    op.input.append(rij_x.tensor)
    op.input_desc.add().CopyFrom(rij_x.desc)
    op.input_desc[-1].name = "rij_x"
    op.input.append(rij_y.tensor)
    op.input_desc.add().CopyFrom(rij_y.desc)
    op.input_desc[-1].name = "rij_y"
    op.input.append(rij_z.tensor)
    op.input_desc.add().CopyFrom(rij_z.desc)
    op.input_desc[-1].name = "rij_z"
    op.input.append(type.tensor)
    op.input_desc.add().CopyFrom(type.desc)
    op.input_desc[-1].name = "type"
    op.input.append(natoms.tensor)
    op.input_desc.add().CopyFrom(natoms.desc)
    op.input_desc[-1].name = "natoms"
    op.input.append(mesh.tensor)
    op.input_desc.add().CopyFrom(mesh.desc)
    op.input_desc[-1].name = "mesh"
    op.input.append(davg.tensor)
    op.input_desc.add().CopyFrom(davg.desc)
    op.input_desc[-1].name = "davg"
    op.input.append(dstd.tensor)
    op.input_desc.add().CopyFrom(dstd.desc)
    op.input_desc[-1].name = "dstd"

    # process attrs
    op.attr["rcut_a"].f = rcut_a
    op.attr["rcut_r"].f = rcut_r
    op.attr["rcut_r_smth"].f = rcut_r_smth
    op.attr["sel_a"].list.val_type = 2
    op.attr["sel_a"].list.i.extend(sel_a)
    op.attr["sel_r"].list.val_type = 2
    op.attr["sel_r"].list.i.extend(sel_r)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "descrpt"
    descrpt = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "descrpt_deriv"
    descrpt_deriv = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return descrpt, descrpt_deriv


# This api is auto-generated from IR ProdForceSeA
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def ProdForceSeA(net_deriv: Tensor, in_deriv: Tensor, nlist: Tensor, natoms: Tensor, *, n_a_sel: int, n_r_sel: int, dependencies=[], node_name=None):
    """REG_OP(ProdForceSeA)\n
.INPUT(net_deriv, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(in_deriv, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(nlist, TensorType({DT_INT32}))\n
.INPUT(natoms, TensorType({DT_INT32}))\n
.OUTPUT(atom_force, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(n_a_sel, Int)\n
.REQUIRED_ATTR(n_r_sel, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ProdForceSeA"
    op.name = next_unique_name(node_name, "ProdForceSeA")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(net_deriv.tensor)
    op.input_desc.add().CopyFrom(net_deriv.desc)
    op.input_desc[-1].name = "net_deriv"
    op.input.append(in_deriv.tensor)
    op.input_desc.add().CopyFrom(in_deriv.desc)
    op.input_desc[-1].name = "in_deriv"
    op.input.append(nlist.tensor)
    op.input_desc.add().CopyFrom(nlist.desc)
    op.input_desc[-1].name = "nlist"
    op.input.append(natoms.tensor)
    op.input_desc.add().CopyFrom(natoms.desc)
    op.input_desc[-1].name = "natoms"

    # process attrs
    op.attr["n_a_sel"].i = n_a_sel
    op.attr["n_r_sel"].i = n_r_sel

    # process outputs
    output_index = 0
    op.output_desc.add().name = "atom_force"
    atom_force = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return atom_force


# This api is auto-generated from IR ProdVirialSeA
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def ProdVirialSeA(net_deriv: Tensor, in_deriv: Tensor, rij: Tensor, nlist: Tensor, natoms: Tensor, *, n_a_sel: int, n_r_sel: int, dependencies=[], node_name=None):
    """REG_OP(ProdVirialSeA)\n
.INPUT(net_deriv, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(in_deriv, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(rij, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(nlist, TensorType({DT_INT32}))\n
.INPUT(natoms, TensorType({DT_INT32}))\n
.OUTPUT(virial, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(atom_virial, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(n_a_sel, Int)\n
.REQUIRED_ATTR(n_r_sel, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ProdVirialSeA"
    op.name = next_unique_name(node_name, "ProdVirialSeA")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(net_deriv.tensor)
    op.input_desc.add().CopyFrom(net_deriv.desc)
    op.input_desc[-1].name = "net_deriv"
    op.input.append(in_deriv.tensor)
    op.input_desc.add().CopyFrom(in_deriv.desc)
    op.input_desc[-1].name = "in_deriv"
    op.input.append(rij.tensor)
    op.input_desc.add().CopyFrom(rij.desc)
    op.input_desc[-1].name = "rij"
    op.input.append(nlist.tensor)
    op.input_desc.add().CopyFrom(nlist.desc)
    op.input_desc[-1].name = "nlist"
    op.input.append(natoms.tensor)
    op.input_desc.add().CopyFrom(natoms.desc)
    op.input_desc[-1].name = "natoms"

    # process attrs
    op.attr["n_a_sel"].i = n_a_sel
    op.attr["n_r_sel"].i = n_r_sel

    # process outputs
    output_index = 0
    op.output_desc.add().name = "virial"
    virial = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "atom_virial"
    atom_virial = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return virial, atom_virial


# This api is auto-generated from IR TabulateFusionGrad
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def TabulateFusionGrad(table: Tensor, table_info: Tensor, em_x: Tensor, em: Tensor, dy: Tensor, descriptor: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TabulateFusionGrad)\n
.INPUT(table, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(table_info, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(em_x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(em, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(descriptor, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(dy_dem_x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(dy_dem, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TabulateFusionGrad"
    op.name = next_unique_name(node_name, "TabulateFusionGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table.tensor)
    op.input_desc.add().CopyFrom(table.desc)
    op.input_desc[-1].name = "table"
    op.input.append(table_info.tensor)
    op.input_desc.add().CopyFrom(table_info.desc)
    op.input_desc[-1].name = "table_info"
    op.input.append(em_x.tensor)
    op.input_desc.add().CopyFrom(em_x.desc)
    op.input_desc[-1].name = "em_x"
    op.input.append(em.tensor)
    op.input_desc.add().CopyFrom(em.desc)
    op.input_desc[-1].name = "em"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(descriptor.tensor)
    op.input_desc.add().CopyFrom(descriptor.desc)
    op.input_desc[-1].name = "descriptor"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dy_dem_x"
    dy_dem_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dy_dem"
    dy_dem = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dy_dem_x, dy_dem


# This api is auto-generated from IR AddN
@auto_convert_to_tensor([True], [False])
def AddN(x: List[Tensor], *, N: int, dependencies=[], node_name=None):
    """REG_OP(AddN)\n
.DYNAMIC_INPUT(x, TensorType({NumberType(), DT_VARIANT}))\n
.OUTPUT(y, TensorType({NumberType(), DT_VARIANT}))\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AddN"
    op.name = next_unique_name(node_name, "AddN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaximumGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MaximumGrad(grads: Tensor, x1: Tensor, x2: Tensor, *, grad_x: bool=True, grad_y: bool=True, dependencies=[], node_name=None):
    """REG_OP(MaximumGrad)\n
.INPUT(grads, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.ATTR(grad_x, Bool, true)\n
.ATTR(grad_y, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaximumGrad"
    op.name = next_unique_name(node_name, "MaximumGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["grad_x"].b = grad_x
    op.attr["grad_y"].b = grad_y

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR MinimumGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MinimumGrad(grads: Tensor, x1: Tensor, x2: Tensor, *, grad_x: bool=True, grad_y: bool=True, dependencies=[], node_name=None):
    """REG_OP(MinimumGrad)\n
.INPUT(grads, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.ATTR(grad_x, Bool, true)\n
.ATTR(grad_y, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MinimumGrad"
    op.name = next_unique_name(node_name, "MinimumGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["grad_x"].b = grad_x
    op.attr["grad_y"].b = grad_y

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR GreaterEqual
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def GreaterEqual(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(GreaterEqual)\n
.INPUT(x1, TensorType::RealNumberType())\n
.INPUT(x2, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GreaterEqual"
    op.name = next_unique_name(node_name, "GreaterEqual")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Less
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def Less(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Less)\n
.INPUT(x1, TensorType::RealNumberType())\n
.INPUT(x2, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Less"
    op.name = next_unique_name(node_name, "Less")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RealDiv
@auto_convert_to_tensor([False, False], [False, False])
def RealDiv(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RealDiv)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RealDiv"
    op.name = next_unique_name(node_name, "RealDiv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Sqrt
@auto_convert_to_tensor([False], [False])
def Sqrt(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Sqrt)\n
.INPUT(x, TensorType{(DT_BF16, DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128)})\n
.OUTPUT(y, TensorType{(DT_BF16, DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128)})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Sqrt"
    op.name = next_unique_name(node_name, "Sqrt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Maximum
@auto_convert_to_tensor([False, False], [False, False])
def Maximum(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Maximum)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Maximum"
    op.name = next_unique_name(node_name, "Maximum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Minimum
@auto_convert_to_tensor([False, False], [False, False])
def Minimum(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Minimum)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Minimum"
    op.name = next_unique_name(node_name, "Minimum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Reciprocal
@auto_convert_to_tensor([False], [False])
def Reciprocal(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Reciprocal)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_FLOAT16 DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Reciprocal"
    op.name = next_unique_name(node_name, "Reciprocal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Sub
@auto_convert_to_tensor([False, False], [False, False])
def Sub(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Sub)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Sub"
    op.name = next_unique_name(node_name, "Sub")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Gcd
@auto_convert_to_tensor([False, False], [False, False])
def Gcd(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Gcd)\n
.INPUT(x1, "T")\n
.INPUT(x2, "T")\n
.OUTPUT(y, "T")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Gcd"
    op.name = next_unique_name(node_name, "Gcd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Abs
@auto_convert_to_tensor([False], [False])
def Abs(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Abs)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Abs"
    op.name = next_unique_name(node_name, "Abs")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AbsGrad
@auto_convert_to_tensor([False, False], [False, False])
def AbsGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AbsGrad)\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AbsGrad"
    op.name = next_unique_name(node_name, "AbsGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Sign
@auto_convert_to_tensor([False], [False])
def Sign(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Sign)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Sign"
    op.name = next_unique_name(node_name, "Sign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SquaredDifference
@auto_convert_to_tensor([False, False], [False, False])
def SquaredDifference(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SquaredDifference)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SquaredDifference"
    op.name = next_unique_name(node_name, "SquaredDifference")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Cos
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Cos(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Cos)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cos"
    op.name = next_unique_name(node_name, "Cos")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Div
@auto_convert_to_tensor([False, False], [False, False])
def Div(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Div)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT32, DT_DOUBLE, DT_INT64, DT_UINT16, DT_INT16, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT32, DT_DOUBLE, DT_INT64, DT_UINT16, DT_INT16, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT32, DT_DOUBLE, DT_INT64, DT_UINT16, DT_INT16, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Div"
    op.name = next_unique_name(node_name, "Div")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Equal
@auto_convert_to_tensor([False, False], [False, False])
def Equal(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Equal)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_BF16, DT_FLOAT16, DT_INT32, DT_INT8, DT_UINT8, DT_DOUBLE, DT_INT16, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_QUINT8, DT_QINT8, DT_QINT32, DT_STRING, DT_BOOL}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_BF16, DT_FLOAT16, DT_INT32, DT_INT8, DT_UINT8, DT_DOUBLE, DT_INT16, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_QUINT8, DT_QINT8, DT_QINT32, DT_STRING, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Equal"
    op.name = next_unique_name(node_name, "Equal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Exp
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Exp(x: Tensor, *, base: float=-1.000000, scale: float=1.000000, shift: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Exp)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
.ATTR(base, Float, -1.0)\n
.ATTR(scale, Float, 1.0)\n
.ATTR(shift, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Exp"
    op.name = next_unique_name(node_name, "Exp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["base"].f = base
    op.attr["scale"].f = scale
    op.attr["shift"].f = shift

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Expm1
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Expm1(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Expm1)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Expm1"
    op.name = next_unique_name(node_name, "Expm1")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Expint
@auto_convert_to_tensor([False], [False])
def Expint(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Expint)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Expint"
    op.name = next_unique_name(node_name, "Expint")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Inv
@auto_convert_to_tensor([False], [False])
def Inv(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Inv)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64,DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Inv"
    op.name = next_unique_name(node_name, "Inv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InvGrad
@auto_convert_to_tensor([False, False], [False, False])
def InvGrad(x: Tensor, grad: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InvGrad)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.INPUT(grad, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InvGrad"
    op.name = next_unique_name(node_name, "InvGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LessEqual
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def LessEqual(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LessEqual)\n
.INPUT(x1, TensorType::RealNumberType())\n
.INPUT(x2, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LessEqual"
    op.name = next_unique_name(node_name, "LessEqual")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Log1p
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Log1p(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Log1p)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Log1p"
    op.name = next_unique_name(node_name, "Log1p")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Mod
@auto_convert_to_tensor([False, False], [False, False])
def Mod(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Mod)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_INT64, DT_DOUBLE}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_INT64, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Mod"
    op.name = next_unique_name(node_name, "Mod")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NotEqual
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def NotEqual(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NotEqual)\n
.INPUT(x1, TensorType::RealNumberType())\n
.INPUT(x2, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NotEqual"
    op.name = next_unique_name(node_name, "NotEqual")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Ndtri
@auto_convert_to_tensor([False], [False])
def Ndtri(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Ndtri)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Ndtri"
    op.name = next_unique_name(node_name, "Ndtri")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Neg
@auto_convert_to_tensor([False], [False])
def Neg(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Neg)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Neg"
    op.name = next_unique_name(node_name, "Neg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TruncateDiv
@auto_convert_to_tensor([False, False], [False, False])
def TruncateDiv(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TruncateDiv)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT32, DT_DOUBLE, DT_UINT16, DT_INT16, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT32, DT_DOUBLE, DT_UINT16, DT_INT16, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT32, DT_DOUBLE, DT_UINT16, DT_INT16, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TruncateDiv"
    op.name = next_unique_name(node_name, "TruncateDiv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Xdivy
@auto_convert_to_tensor([False, False], [False, False])
def Xdivy(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Xdivy)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Xdivy"
    op.name = next_unique_name(node_name, "Xdivy")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Xlog1py
@auto_convert_to_tensor([False, False], [False, False])
def Xlog1py(x: Tensor, y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Xlog1py)\n
.INPUT(x, TensorType({DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(y, TensorType({DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(z, TensorType({DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Xlog1py"
    op.name = next_unique_name(node_name, "Xlog1py")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Xlogy
@auto_convert_to_tensor([False, False], [False, False])
def Xlogy(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Xlogy)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Xlogy"
    op.name = next_unique_name(node_name, "Xlogy")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Square
@auto_convert_to_tensor([False], [False])
def Square(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Square)\n
.INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT16, DT_FLOAT, DT_BF16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_DOUBLE, DT_FLOAT16, DT_FLOAT, DT_BF16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Square"
    op.name = next_unique_name(node_name, "Square")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Rsqrt
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Rsqrt(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Rsqrt)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Rsqrt"
    op.name = next_unique_name(node_name, "Rsqrt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Asin
@auto_convert_to_tensor([False], [False])
def Asin(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Asin)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Asin"
    op.name = next_unique_name(node_name, "Asin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AsinGrad
@auto_convert_to_tensor([False, False], [False, False])
def AsinGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AsinGrad)\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(z, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AsinGrad"
    op.name = next_unique_name(node_name, "AsinGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Acos
@auto_convert_to_tensor([False], [False])
def Acos(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Acos)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Acos"
    op.name = next_unique_name(node_name, "Acos")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AcosGrad
@auto_convert_to_tensor([False, False], [False, False])
def AcosGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AcosGrad)\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AcosGrad"
    op.name = next_unique_name(node_name, "AcosGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Acosh
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Acosh(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Acosh)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Acosh"
    op.name = next_unique_name(node_name, "Acosh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AcoshGrad
@auto_convert_to_tensor([False, False], [False, False])
def AcoshGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AcoshGrad)\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AcoshGrad"
    op.name = next_unique_name(node_name, "AcoshGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR LogicalOr
@auto_convert_to_tensor([False, False], [False, False])
def LogicalOr(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LogicalOr)\n
.INPUT(x1, TensorType({DT_BOOL}))\n
.INPUT(x2, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogicalOr"
    op.name = next_unique_name(node_name, "LogicalOr")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Spence
@auto_convert_to_tensor([False], [False])
def Spence(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Spence)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Spence"
    op.name = next_unique_name(node_name, "Spence")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LogicalAnd
@auto_convert_to_tensor([False, False], [False, False])
def LogicalAnd(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LogicalAnd)\n
.INPUT(x1, TensorType({DT_BOOL}))\n
.INPUT(x2, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogicalAnd"
    op.name = next_unique_name(node_name, "LogicalAnd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BesselI0e
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def BesselI0e(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BesselI0e)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BesselI0e"
    op.name = next_unique_name(node_name, "BesselI0e")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BesselI1e
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def BesselI1e(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BesselI1e)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BesselI1e"
    op.name = next_unique_name(node_name, "BesselI1e")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Log
@auto_convert_to_tensor([False], [False])
def Log(x: Tensor, *, base: float=-1.000000, scale: float=1.000000, shift: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Log)\n
.INPUT(x, TensorType({DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_BF16, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType::UnaryDataType())\n
.ATTR(base, Float, -1.0)\n
.ATTR(scale, Float, 1.0)\n
.ATTR(shift, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Log"
    op.name = next_unique_name(node_name, "Log")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["base"].f = base
    op.attr["scale"].f = scale
    op.attr["shift"].f = shift

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Mul
@auto_convert_to_tensor([False, False], [False, False])
def Mul(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Mul)\n
.INPUT(x1, "T1")\n
.INPUT(x2, "T2")\n
.OUTPUT(y, "T3")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Mul"
    op.name = next_unique_name(node_name, "Mul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SqrtGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNARY, TensorType.TT_UNARY])
def SqrtGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SqrtGrad)\n
.INPUT(y, TensorType(UnaryDataType))\n
.INPUT(dy, TensorType(UnaryDataType))\n
.OUTPUT(z, TensorType(UnaryDataType))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SqrtGrad"
    op.name = next_unique_name(node_name, "SqrtGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Add
@auto_convert_to_tensor([False, False], [False, False])
def Add(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Add)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_INT32, DT_INT64, DT_FLOAT16, DT_BF16, DT_INT16, DT_INT8, DT_UINT8, DT_DOUBLE, DT_COMPLEX128, DT_COMPLEX64, DT_STRING}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_INT32, DT_INT64, DT_FLOAT16, DT_BF16, DT_INT16, DT_INT8, DT_UINT8, DT_DOUBLE, DT_COMPLEX128, DT_COMPLEX64, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT64, DT_FLOAT16, DT_BF16, DT_INT16, DT_INT8, DT_UINT8, DT_DOUBLE, DT_COMPLEX128, DT_COMPLEX64, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Add"
    op.name = next_unique_name(node_name, "Add")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FusedMulAdd
@auto_convert_to_tensor([False, False, False], [False, False, False])
def FusedMulAdd(x1: Tensor, x2: Tensor, x3: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FusedMulAdd)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x3, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedMulAdd"
    op.name = next_unique_name(node_name, "FusedMulAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(x3.tensor)
    op.input_desc.add().CopyFrom(x3.desc)
    op.input_desc[-1].name = "x3"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FusedMulAddAdd
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def FusedMulAddAdd(x1: Tensor, x2: Tensor, x3: Tensor, x4: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FusedMulAddAdd)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x3, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(x4, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedMulAddAdd"
    op.name = next_unique_name(node_name, "FusedMulAddAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(x3.tensor)
    op.input_desc.add().CopyFrom(x3.desc)
    op.input_desc[-1].name = "x3"
    op.input.append(x4.tensor)
    op.input_desc.add().CopyFrom(x4.desc)
    op.input_desc[-1].name = "x4"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AddV2
@auto_convert_to_tensor([False, False], [False, False])
def AddV2(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AddV2)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_INT32, DT_INT64, DT_FLOAT16, DT_INT16, DT_INT8, DT_UINT8, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_INT32, DT_INT64, DT_FLOAT16, DT_INT16, DT_INT8, DT_UINT8, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT64, DT_FLOAT16, DT_INT16, DT_INT8, DT_UINT8, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AddV2"
    op.name = next_unique_name(node_name, "AddV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AssignAdd
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC])
def AssignAdd(ref: Tensor, value: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(AssignAdd)\n
.INPUT(ref, TensorType::BasicType())\n
.INPUT(value, TensorType::BasicType())\n
.OUTPUT(ref, TensorType::BasicType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AssignAdd"
    op.name = next_unique_name(node_name, "AssignAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(ref.tensor)
    op.input_desc.add().CopyFrom(ref.desc)
    op.input_desc[-1].name = "ref"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "ref"
    ref = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return ref


# This api is auto-generated from IR Assign
@auto_convert_to_tensor([False, False], [False, False])
def Assign(ref: Tensor, value: Tensor, *, validate_shape: bool=True, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(Assign)\n
.INPUT(ref, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.INPUT(value, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.OUTPUT(ref, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.ATTR(validate_shape, Bool, true)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Assign"
    op.name = next_unique_name(node_name, "Assign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(ref.tensor)
    op.input_desc.add().CopyFrom(ref.desc)
    op.input_desc[-1].name = "ref"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["validate_shape"].b = validate_shape
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "ref"
    ref = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return ref


# This api is auto-generated from IR AssignSub
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def AssignSub(var: Tensor, value: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(AssignSub)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(value, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AssignSub"
    op.name = next_unique_name(node_name, "AssignSub")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR RsqrtGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNARY, TensorType.TT_UNARY])
def RsqrtGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RsqrtGrad)\n
.INPUT(y, TensorType({UnaryDataType,int32,int8}))\n
.INPUT(dy, TensorType({UnaryDataType,int32,int8}))\n
.OUTPUT(z, TensorType({UnaryDataType,int32,int8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RsqrtGrad"
    op.name = next_unique_name(node_name, "RsqrtGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Sinh
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Sinh(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Sinh)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Sinh"
    op.name = next_unique_name(node_name, "Sinh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ClipByValue
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ClipByValue(x: Tensor, clip_value_min: Tensor, clip_value_max: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ClipByValue)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(clip_value_min, TensorType::NumberType())\n
.INPUT(clip_value_max, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ClipByValue"
    op.name = next_unique_name(node_name, "ClipByValue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(clip_value_min.tensor)
    op.input_desc.add().CopyFrom(clip_value_min.desc)
    op.input_desc[-1].name = "clip_value_min"
    op.input.append(clip_value_max.tensor)
    op.input_desc.add().CopyFrom(clip_value_max.desc)
    op.input_desc[-1].name = "clip_value_max"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Cosh
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Cosh(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Cosh)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cosh"
    op.name = next_unique_name(node_name, "Cosh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DivNoNan
@auto_convert_to_tensor([False, False], [False, False])
def DivNoNan(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DivNoNan)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_UINT8, DT_INT8, DT_INT32, DT_FLOAT16, DT_DOUBLE}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_UINT8, DT_INT8, DT_INT32, DT_FLOAT16, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_UINT8, DT_INT8, DT_INT32, DT_FLOAT16, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DivNoNan"
    op.name = next_unique_name(node_name, "DivNoNan")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Invert
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INTEGER])
def Invert(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Invert)\n
.INPUT(x, TensorType::IntegerDataType())\n
.OUTPUT(y, TensorType::IntegerDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Invert"
    op.name = next_unique_name(node_name, "Invert")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR OnesLike
@auto_convert_to_tensor([False], [False])
def OnesLike(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(OnesLike)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_UINT8, DT_INT16, DI_UINT16, DT_INT32, DT_INT64, DT_COMPLEX128, DT_BOOL, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_UINT8, DT_INT16, DI_UINT16, DT_INT32, DT_INT64, DT_COMPLEX128, DT_BOOL, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OnesLike"
    op.name = next_unique_name(node_name, "OnesLike")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReciprocalGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNARY, TensorType.TT_UNARY])
def ReciprocalGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ReciprocalGrad)\n
.INPUT(y, TensorType::UnaryDataType())\n
.INPUT(dy, TensorType::UnaryDataType())\n
.OUTPUT(z, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReciprocalGrad"
    op.name = next_unique_name(node_name, "ReciprocalGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Greater
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def Greater(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Greater)\n
.INPUT(x1, TensorType::RealNumberType())\n
.INPUT(x2, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Greater"
    op.name = next_unique_name(node_name, "Greater")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ZerosLike
@auto_convert_to_tensor([False], [False])
def ZerosLike(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ZerosLike)\n
.INPUT(x, TensorType({BasicType(), DT_VARIANT}))\n
.OUTPUT(y, TensorType({BasicType(), DT_VARIANT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ZerosLike"
    op.name = next_unique_name(node_name, "ZerosLike")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LogicalNot
@auto_convert_to_tensor([False], [False])
def LogicalNot(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LogicalNot)\n
.INPUT(x, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogicalNot"
    op.name = next_unique_name(node_name, "LogicalNot")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Asinh
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Asinh(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Asinh)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Asinh"
    op.name = next_unique_name(node_name, "Asinh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AsinhGrad
@auto_convert_to_tensor([False, False], [False, False])
def AsinhGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AsinhGrad)\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AsinhGrad"
    op.name = next_unique_name(node_name, "AsinhGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Atanh
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Atanh(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Atanh)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Atanh"
    op.name = next_unique_name(node_name, "Atanh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Atan
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Atan(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Atan)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Atan"
    op.name = next_unique_name(node_name, "Atan")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AtanGrad
@auto_convert_to_tensor([False, False], [False, False])
def AtanGrad(y: Tensor, dy: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AtanGrad)\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AtanGrad"
    op.name = next_unique_name(node_name, "AtanGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Atan2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_FLOATING, TensorType.TT_FLOATING])
def Atan2(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Atan2)\n
.INPUT(x1, TensorType::FloatingDataType())\n
.INPUT(x2, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Atan2"
    op.name = next_unique_name(node_name, "Atan2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FresnelCos
@auto_convert_to_tensor([False], [False])
def FresnelCos(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FresnelCos)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FresnelCos"
    op.name = next_unique_name(node_name, "FresnelCos")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FresnelSin
@auto_convert_to_tensor([False], [False])
def FresnelSin(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FresnelSin)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FresnelSin"
    op.name = next_unique_name(node_name, "FresnelSin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ApproximateEqual
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApproximateEqual(x1: Tensor, x2: Tensor, *, tolerance: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(ApproximateEqual)\n
.INPUT(x1, TensorType::NumberType())\n
.INPUT(x2, TensorType::NumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
.ATTR(tolerance, Float, 1e-5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApproximateEqual"
    op.name = next_unique_name(node_name, "ApproximateEqual")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["tolerance"].f = tolerance

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AccumulateNV2
@auto_convert_to_tensor([True], [False], inputs_tensor_type=[TensorType.TT_NUMBER])
def AccumulateNV2(x: List[Tensor], *, N: int, dependencies=[], node_name=None):
    """REG_OP(AccumulateNV2)\n
.DYNAMIC_INPUT(x, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AccumulateNV2"
    op.name = next_unique_name(node_name, "AccumulateNV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FakeQuantWithMinMaxArgs
@auto_convert_to_tensor([False], [False])
def FakeQuantWithMinMaxArgs(x: Tensor, *, min: float=-6.000000, max: float=6.000000, num_bits: int=8, narrow_range: bool=False, dependencies=[], node_name=None):
    """REG_OP(FakeQuantWithMinMaxArgs)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(min, Float, -6.0)\n
.ATTR(max, Float, 6.0)\n
.ATTR(num_bits, Int, 8)\n
.ATTR(narrow_range, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FakeQuantWithMinMaxArgs"
    op.name = next_unique_name(node_name, "FakeQuantWithMinMaxArgs")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["min"].f = min
    op.attr["max"].f = max
    op.attr["num_bits"].i = num_bits
    op.attr["narrow_range"].b = narrow_range

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FakeQuantWithMinMaxArgsGradient
@auto_convert_to_tensor([False, False], [False, False])
def FakeQuantWithMinMaxArgsGradient(gradients: Tensor, x: Tensor, *, min: float=-6.000000, max: float=6.000000, num_bits: int=8, narrow_range: bool=False, dependencies=[], node_name=None):
    """REG_OP(FakeQuantWithMinMaxArgsGradient)\n
.INPUT(gradients, TensorType({DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(min, Float, -6.0)\n
.ATTR(max, Float, 6.0)\n
.ATTR(num_bits, Int, 8)\n
.ATTR(narrow_range, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FakeQuantWithMinMaxArgsGradient"
    op.name = next_unique_name(node_name, "FakeQuantWithMinMaxArgsGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["min"].f = min
    op.attr["max"].f = max
    op.attr["num_bits"].i = num_bits
    op.attr["narrow_range"].b = narrow_range

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FakeQuantWithMinMaxVars
@auto_convert_to_tensor([False, False, False], [False, False, False])
def FakeQuantWithMinMaxVars(x: Tensor, min: Tensor, max: Tensor, *, num_bits: int=8, narrow_range: bool=False, dependencies=[], node_name=None):
    """REG_OP(FakeQuantWithMinMaxVars)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(min, TensorType({DT_FLOAT}))\n
.INPUT(max, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(num_bits, Int, 8)\n
.ATTR(narrow_range, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FakeQuantWithMinMaxVars"
    op.name = next_unique_name(node_name, "FakeQuantWithMinMaxVars")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs
    op.attr["num_bits"].i = num_bits
    op.attr["narrow_range"].b = narrow_range

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FakeQuantWithMinMaxVarsGradient
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def FakeQuantWithMinMaxVarsGradient(gradients: Tensor, x: Tensor, min: Tensor, max: Tensor, *, num_bits: int=8, narrow_range: bool=False, dependencies=[], node_name=None):
    """REG_OP(FakeQuantWithMinMaxVarsGradient)\n
.INPUT(gradients, TensorType({DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(min, TensorType({DT_FLOAT}))\n
.INPUT(max, TensorType({DT_FLOAT}))\n
.OUTPUT(backprops_wrt_x, TensorType({DT_FLOAT}))\n
.OUTPUT(backprops_wrt_min, TensorType({DT_FLOAT}))\n
.OUTPUT(backprops_wrt_max, TensorType({DT_FLOAT}))\n
.ATTR(num_bits, Int, 8)\n
.ATTR(narrow_range, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FakeQuantWithMinMaxVarsGradient"
    op.name = next_unique_name(node_name, "FakeQuantWithMinMaxVarsGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs
    op.attr["num_bits"].i = num_bits
    op.attr["narrow_range"].b = narrow_range

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops_wrt_x"
    backprops_wrt_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "backprops_wrt_min"
    backprops_wrt_min = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "backprops_wrt_max"
    backprops_wrt_max = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops_wrt_x, backprops_wrt_min, backprops_wrt_max


# This api is auto-generated from IR FakeQuantWithMinMaxVarsPerChannel
@auto_convert_to_tensor([False, False, False], [False, False, False])
def FakeQuantWithMinMaxVarsPerChannel(x: Tensor, min: Tensor, max: Tensor, *, num_bits: int=8, narrow_range: bool=False, dependencies=[], node_name=None):
    """REG_OP(FakeQuantWithMinMaxVarsPerChannel)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(min, TensorType({DT_FLOAT}))\n
.INPUT(max, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(num_bits, Int, 8)\n
.ATTR(narrow_range, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FakeQuantWithMinMaxVarsPerChannel"
    op.name = next_unique_name(node_name, "FakeQuantWithMinMaxVarsPerChannel")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs
    op.attr["num_bits"].i = num_bits
    op.attr["narrow_range"].b = narrow_range

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FakeQuantWithMinMaxVarsPerChannelGradient
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def FakeQuantWithMinMaxVarsPerChannelGradient(gradients: Tensor, x: Tensor, min: Tensor, max: Tensor, *, num_bits: int=8, narrow_range: bool=False, dependencies=[], node_name=None):
    """REG_OP(FakeQuantWithMinMaxVarsPerChannelGradient)\n
.INPUT(gradients, TensorType({DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(min, TensorType({DT_FLOAT}))\n
.INPUT(max, TensorType({DT_FLOAT}))\n
.OUTPUT(backprops_wrt_x, TensorType({DT_FLOAT}))\n
.OUTPUT(backprops_wrt_min, TensorType({DT_FLOAT}))\n
.OUTPUT(backprops_wrt_max, TensorType({DT_FLOAT}))\n
.ATTR(num_bits, Int, 8)\n
.ATTR(narrow_range, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FakeQuantWithMinMaxVarsPerChannelGradient"
    op.name = next_unique_name(node_name, "FakeQuantWithMinMaxVarsPerChannelGradient")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs
    op.attr["num_bits"].i = num_bits
    op.attr["narrow_range"].b = narrow_range

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops_wrt_x"
    backprops_wrt_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "backprops_wrt_min"
    backprops_wrt_min = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "backprops_wrt_max"
    backprops_wrt_max = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops_wrt_x, backprops_wrt_min, backprops_wrt_max


# This api is auto-generated from IR BitwiseAnd
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INTEGER, TensorType.TT_INTEGER])
def BitwiseAnd(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BitwiseAnd)\n
.INPUT(x1, TensorType::IntegerDataType())\n
.INPUT(x2, TensorType::IntegerDataType())\n
.OUTPUT(y, TensorType::IntegerDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BitwiseAnd"
    op.name = next_unique_name(node_name, "BitwiseAnd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BitwiseOr
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INTEGER, TensorType.TT_INTEGER])
def BitwiseOr(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BitwiseOr)\n
.INPUT(x1, TensorType::IntegerDataType())\n
.INPUT(x2, TensorType::IntegerDataType())\n
.OUTPUT(y, TensorType::IntegerDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BitwiseOr"
    op.name = next_unique_name(node_name, "BitwiseOr")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BitwiseXor
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INTEGER, TensorType.TT_INTEGER])
def BitwiseXor(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BitwiseXor)\n
.INPUT(x1, TensorType::IntegerDataType())\n
.INPUT(x2, TensorType::IntegerDataType())\n
.OUTPUT(y, TensorType::IntegerDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BitwiseXor"
    op.name = next_unique_name(node_name, "BitwiseXor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Ceil
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def Ceil(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Ceil)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Ceil"
    op.name = next_unique_name(node_name, "Ceil")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Floor
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def Floor(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Floor)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Floor"
    op.name = next_unique_name(node_name, "Floor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FloorDiv
@auto_convert_to_tensor([False, False], [False, False])
def FloorDiv(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FloorDiv)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_INT64, DT_INT16, DT_UINT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_INT64, DT_INT16,DT_UINT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_INT64, DT_INT16,DT_UINT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FloorDiv"
    op.name = next_unique_name(node_name, "FloorDiv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FloorMod
@auto_convert_to_tensor([False, False], [False, False])
def FloorMod(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FloorMod)\n
.INPUT(x1, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
.INPUT(x2, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FloorMod"
    op.name = next_unique_name(node_name, "FloorMod")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Pow
@auto_convert_to_tensor([False, False], [False, False])
def Pow(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Pow)\n
.INPUT(x1, "T1")\n
.INPUT(x2, "T2")\n
.OUTPUT(y, "T3")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Pow"
    op.name = next_unique_name(node_name, "Pow")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Rint
@auto_convert_to_tensor([False], [False])
def Rint(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Rint)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Rint"
    op.name = next_unique_name(node_name, "Rint")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Round
@auto_convert_to_tensor([False], [False])
def Round(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Round)\n
.INPUT(x, TensorType(DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128))\n
.OUTPUT(y, TensorType(DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Round"
    op.name = next_unique_name(node_name, "Round")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Sin
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Sin(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Sin)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Sin"
    op.name = next_unique_name(node_name, "Sin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Tan
@auto_convert_to_tensor([False], [False])
def Tan(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Tan)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Tan"
    op.name = next_unique_name(node_name, "Tan")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TruncateMod
@auto_convert_to_tensor([False, False], [False, False])
def TruncateMod(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TruncateMod)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TruncateMod"
    op.name = next_unique_name(node_name, "TruncateMod")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BiasAdd
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def BiasAdd(x: Tensor, bias: Tensor, *, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(BiasAdd)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(bias, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BiasAdd"
    op.name = next_unique_name(node_name, "BiasAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMin
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ArgMin(x: Tensor, dimension: Tensor, *, dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(ArgMin)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(dimension, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMin"
    op.name = next_unique_name(node_name, "ArgMin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(dimension.tensor)
    op.input_desc.add().CopyFrom(dimension.desc)
    op.input_desc[-1].name = "dimension"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMinD
@auto_convert_to_tensor([False], [False])
def ArgMinD(x: Tensor, *, dimension: int, dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(ArgMinD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(dimension, Int)\n
.ATTR(dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMinD"
    op.name = next_unique_name(node_name, "ArgMinD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dimension"].i = dimension
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMaxV2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ArgMaxV2(x: Tensor, dimension: Tensor, *, dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(ArgMaxV2)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(dimension, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMaxV2"
    op.name = next_unique_name(node_name, "ArgMaxV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(dimension.tensor)
    op.input_desc.add().CopyFrom(dimension.desc)
    op.input_desc[-1].name = "dimension"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMaxD
@auto_convert_to_tensor([False], [False])
def ArgMaxD(x: Tensor, *, dimension: int, dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(ArgMaxD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(dimension, Int)\n
.ATTR(dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMaxD"
    op.name = next_unique_name(node_name, "ArgMaxD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dimension"].i = dimension
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMaxWithValue
@auto_convert_to_tensor([False], [False])
def ArgMaxWithValue(x: Tensor, *, dimension: int, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ArgMaxWithValue)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(indice, TensorType({DT_INT32}))\n
.OUTPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.REQUIRED_ATTR(dimension, Int)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMaxWithValue"
    op.name = next_unique_name(node_name, "ArgMaxWithValue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dimension"].i = dimension
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indice"
    indice = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indice, values


# This api is auto-generated from IR ArgMinWithValue
@auto_convert_to_tensor([False], [False])
def ArgMinWithValue(x: Tensor, *, dimension: int, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ArgMinWithValue)\n
.INPUT(x, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.OUTPUT(indice, TensorType({DT_INT32}))\n
.OUTPUT(values, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.REQUIRED_ATTR(dimension, Int)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMinWithValue"
    op.name = next_unique_name(node_name, "ArgMinWithValue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dimension"].i = dimension
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indice"
    indice = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indice, values


# This api is auto-generated from IR Eltwise
@auto_convert_to_tensor([True], [False])
def Eltwise(x: List[Tensor], *, N: int, mode: int=1, coeff: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(Eltwise)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(N, Int)\n
.ATTR(mode, Int, 1)\n
.ATTR(coeff, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Eltwise"
    op.name = next_unique_name(node_name, "Eltwise")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N
    op.attr["mode"].i = mode
    op.attr["coeff"].list.val_type = 3
    op.attr["coeff"].list.f.extend(coeff)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Erfinv
@auto_convert_to_tensor([False], [False])
def Erfinv(input_x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Erfinv)\n
.INPUT(input_x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Erfinv"
    op.name = next_unique_name(node_name, "Erfinv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR PopulationCount
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INTEGER])
def PopulationCount(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(PopulationCount)\n
.INPUT(x, TensorType::IntegerDataType())\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PopulationCount"
    op.name = next_unique_name(node_name, "PopulationCount")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LambNextMVWithDecay
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False])
def LambNextMVWithDecay(input_mul3: Tensor, input_mul2: Tensor, input_realdiv1: Tensor, input_mul1: Tensor, input_mul0: Tensor, input_realdiv0: Tensor, input_mul4: Tensor, mul0_x: Tensor, mul1_sub: Tensor, mul2_x: Tensor, mul3_sub1: Tensor, mul4_x: Tensor, add2_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LambNextMVWithDecay)\n
.INPUT(input_mul3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_realdiv1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_realdiv0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul0_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul1_sub, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_sub1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul4_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LambNextMVWithDecay"
    op.name = next_unique_name(node_name, "LambNextMVWithDecay")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_mul3.tensor)
    op.input_desc.add().CopyFrom(input_mul3.desc)
    op.input_desc[-1].name = "input_mul3"
    op.input.append(input_mul2.tensor)
    op.input_desc.add().CopyFrom(input_mul2.desc)
    op.input_desc[-1].name = "input_mul2"
    op.input.append(input_realdiv1.tensor)
    op.input_desc.add().CopyFrom(input_realdiv1.desc)
    op.input_desc[-1].name = "input_realdiv1"
    op.input.append(input_mul1.tensor)
    op.input_desc.add().CopyFrom(input_mul1.desc)
    op.input_desc[-1].name = "input_mul1"
    op.input.append(input_mul0.tensor)
    op.input_desc.add().CopyFrom(input_mul0.desc)
    op.input_desc[-1].name = "input_mul0"
    op.input.append(input_realdiv0.tensor)
    op.input_desc.add().CopyFrom(input_realdiv0.desc)
    op.input_desc[-1].name = "input_realdiv0"
    op.input.append(input_mul4.tensor)
    op.input_desc.add().CopyFrom(input_mul4.desc)
    op.input_desc[-1].name = "input_mul4"
    op.input.append(mul0_x.tensor)
    op.input_desc.add().CopyFrom(mul0_x.desc)
    op.input_desc[-1].name = "mul0_x"
    op.input.append(mul1_sub.tensor)
    op.input_desc.add().CopyFrom(mul1_sub.desc)
    op.input_desc[-1].name = "mul1_sub"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_sub1.tensor)
    op.input_desc.add().CopyFrom(mul3_sub1.desc)
    op.input_desc[-1].name = "mul3_sub1"
    op.input.append(mul4_x.tensor)
    op.input_desc.add().CopyFrom(mul4_x.desc)
    op.input_desc[-1].name = "mul4_x"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y3"
    y3 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y4"
    y4 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2, y3, y4


# This api is auto-generated from IR LambNextMV
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False])
def LambNextMV(input_mul3: Tensor, input_mul2: Tensor, input_realdiv1: Tensor, input_mul1: Tensor, input_mul0: Tensor, input_realdiv0: Tensor, input_mul4: Tensor, mul0_x: Tensor, mul1_sub: Tensor, mul2_x: Tensor, mul3_sub1: Tensor, mul4_x: Tensor, add2_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LambNextMV)\n
.INPUT(input_mul3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_realdiv1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_realdiv0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul0_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul1_sub, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_sub1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul4_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LambNextMV"
    op.name = next_unique_name(node_name, "LambNextMV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_mul3.tensor)
    op.input_desc.add().CopyFrom(input_mul3.desc)
    op.input_desc[-1].name = "input_mul3"
    op.input.append(input_mul2.tensor)
    op.input_desc.add().CopyFrom(input_mul2.desc)
    op.input_desc[-1].name = "input_mul2"
    op.input.append(input_realdiv1.tensor)
    op.input_desc.add().CopyFrom(input_realdiv1.desc)
    op.input_desc[-1].name = "input_realdiv1"
    op.input.append(input_mul1.tensor)
    op.input_desc.add().CopyFrom(input_mul1.desc)
    op.input_desc[-1].name = "input_mul1"
    op.input.append(input_mul0.tensor)
    op.input_desc.add().CopyFrom(input_mul0.desc)
    op.input_desc[-1].name = "input_mul0"
    op.input.append(input_realdiv0.tensor)
    op.input_desc.add().CopyFrom(input_realdiv0.desc)
    op.input_desc[-1].name = "input_realdiv0"
    op.input.append(input_mul4.tensor)
    op.input_desc.add().CopyFrom(input_mul4.desc)
    op.input_desc[-1].name = "input_mul4"
    op.input.append(mul0_x.tensor)
    op.input_desc.add().CopyFrom(mul0_x.desc)
    op.input_desc[-1].name = "mul0_x"
    op.input.append(mul1_sub.tensor)
    op.input_desc.add().CopyFrom(mul1_sub.desc)
    op.input_desc[-1].name = "mul1_sub"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_sub1.tensor)
    op.input_desc.add().CopyFrom(mul3_sub1.desc)
    op.input_desc[-1].name = "mul3_sub1"
    op.input.append(mul4_x.tensor)
    op.input_desc.add().CopyFrom(mul4_x.desc)
    op.input_desc[-1].name = "mul4_x"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y3"
    y3 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y4"
    y4 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2, y3, y4


# This api is auto-generated from IR LambNextRight
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def LambNextRight(input_square: Tensor, input_mul2: Tensor, mul2_x: Tensor, mul3_x: Tensor, truediv1_recip: Tensor, add2_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LambNextRight)\n
.INPUT(input_square, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(truediv1_recip, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LambNextRight"
    op.name = next_unique_name(node_name, "LambNextRight")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_square.tensor)
    op.input_desc.add().CopyFrom(input_square.desc)
    op.input_desc[-1].name = "input_square"
    op.input.append(input_mul2.tensor)
    op.input_desc.add().CopyFrom(input_mul2.desc)
    op.input_desc[-1].name = "input_mul2"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_x.tensor)
    op.input_desc.add().CopyFrom(mul3_x.desc)
    op.input_desc[-1].name = "mul3_x"
    op.input.append(truediv1_recip.tensor)
    op.input_desc.add().CopyFrom(truediv1_recip.desc)
    op.input_desc[-1].name = "truediv1_recip"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR LambUpdateWithLr
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False])
def LambUpdateWithLr(input_greater1: Tensor, input_greater_realdiv: Tensor, input_realdiv: Tensor, input_mul0: Tensor, input_mul1: Tensor, input_sub: Tensor, greater_y: Tensor, select_e: Tensor, minimum_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LambUpdateWithLr)\n
.INPUT(input_greater1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_greater_realdiv, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_realdiv, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_mul1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_sub, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(greater_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(select_e, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(minimum_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LambUpdateWithLr"
    op.name = next_unique_name(node_name, "LambUpdateWithLr")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_greater1.tensor)
    op.input_desc.add().CopyFrom(input_greater1.desc)
    op.input_desc[-1].name = "input_greater1"
    op.input.append(input_greater_realdiv.tensor)
    op.input_desc.add().CopyFrom(input_greater_realdiv.desc)
    op.input_desc[-1].name = "input_greater_realdiv"
    op.input.append(input_realdiv.tensor)
    op.input_desc.add().CopyFrom(input_realdiv.desc)
    op.input_desc[-1].name = "input_realdiv"
    op.input.append(input_mul0.tensor)
    op.input_desc.add().CopyFrom(input_mul0.desc)
    op.input_desc[-1].name = "input_mul0"
    op.input.append(input_mul1.tensor)
    op.input_desc.add().CopyFrom(input_mul1.desc)
    op.input_desc[-1].name = "input_mul1"
    op.input.append(input_sub.tensor)
    op.input_desc.add().CopyFrom(input_sub.desc)
    op.input_desc[-1].name = "input_sub"
    op.input.append(greater_y.tensor)
    op.input_desc.add().CopyFrom(greater_y.desc)
    op.input_desc[-1].name = "greater_y"
    op.input.append(select_e.tensor)
    op.input_desc.add().CopyFrom(select_e.desc)
    op.input_desc[-1].name = "select_e"
    op.input.append(minimum_y.tensor)
    op.input_desc.add().CopyFrom(minimum_y.desc)
    op.input_desc[-1].name = "minimum_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LambUpdateWithLrV2
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def LambUpdateWithLrV2(x1: Tensor, x2: Tensor, x3: Tensor, x4: Tensor, x5: Tensor, greater_y: Tensor, select_e: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LambUpdateWithLrV2)\n
.INPUT(x1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x5, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(greater_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(select_e, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LambUpdateWithLrV2"
    op.name = next_unique_name(node_name, "LambUpdateWithLrV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(x3.tensor)
    op.input_desc.add().CopyFrom(x3.desc)
    op.input_desc[-1].name = "x3"
    op.input.append(x4.tensor)
    op.input_desc.add().CopyFrom(x4.desc)
    op.input_desc[-1].name = "x4"
    op.input.append(x5.tensor)
    op.input_desc.add().CopyFrom(x5.desc)
    op.input_desc[-1].name = "x5"
    op.input.append(greater_y.tensor)
    op.input_desc.add().CopyFrom(greater_y.desc)
    op.input_desc[-1].name = "greater_y"
    op.input.append(select_e.tensor)
    op.input_desc.add().CopyFrom(select_e.desc)
    op.input_desc[-1].name = "select_e"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdamApplyOneWithDecay
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False])
def AdamApplyOneWithDecay(input0: Tensor, input1: Tensor, input2: Tensor, input3: Tensor, input4: Tensor, mul0_x: Tensor, mul1_x: Tensor, mul2_x: Tensor, mul3_x: Tensor, mul4_x: Tensor, add2_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdamApplyOneWithDecay)\n
.INPUT(input0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul0_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul1_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul4_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdamApplyOneWithDecay"
    op.name = next_unique_name(node_name, "AdamApplyOneWithDecay")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input0.tensor)
    op.input_desc.add().CopyFrom(input0.desc)
    op.input_desc[-1].name = "input0"
    op.input.append(input1.tensor)
    op.input_desc.add().CopyFrom(input1.desc)
    op.input_desc[-1].name = "input1"
    op.input.append(input2.tensor)
    op.input_desc.add().CopyFrom(input2.desc)
    op.input_desc[-1].name = "input2"
    op.input.append(input3.tensor)
    op.input_desc.add().CopyFrom(input3.desc)
    op.input_desc[-1].name = "input3"
    op.input.append(input4.tensor)
    op.input_desc.add().CopyFrom(input4.desc)
    op.input_desc[-1].name = "input4"
    op.input.append(mul0_x.tensor)
    op.input_desc.add().CopyFrom(mul0_x.desc)
    op.input_desc[-1].name = "mul0_x"
    op.input.append(mul1_x.tensor)
    op.input_desc.add().CopyFrom(mul1_x.desc)
    op.input_desc[-1].name = "mul1_x"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_x.tensor)
    op.input_desc.add().CopyFrom(mul3_x.desc)
    op.input_desc[-1].name = "mul3_x"
    op.input.append(mul4_x.tensor)
    op.input_desc.add().CopyFrom(mul4_x.desc)
    op.input_desc[-1].name = "mul4_x"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output0"
    output0 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output1"
    output1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output2"
    output2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output0, output1, output2


# This api is auto-generated from IR AdamApplyOne
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False])
def AdamApplyOne(input0: Tensor, input1: Tensor, input2: Tensor, input3: Tensor, input4: Tensor, mul0_x: Tensor, mul1_x: Tensor, mul2_x: Tensor, mul3_x: Tensor, add2_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdamApplyOne)\n
.INPUT(input0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul0_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul1_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdamApplyOne"
    op.name = next_unique_name(node_name, "AdamApplyOne")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input0.tensor)
    op.input_desc.add().CopyFrom(input0.desc)
    op.input_desc[-1].name = "input0"
    op.input.append(input1.tensor)
    op.input_desc.add().CopyFrom(input1.desc)
    op.input_desc[-1].name = "input1"
    op.input.append(input2.tensor)
    op.input_desc.add().CopyFrom(input2.desc)
    op.input_desc[-1].name = "input2"
    op.input.append(input3.tensor)
    op.input_desc.add().CopyFrom(input3.desc)
    op.input_desc[-1].name = "input3"
    op.input.append(input4.tensor)
    op.input_desc.add().CopyFrom(input4.desc)
    op.input_desc[-1].name = "input4"
    op.input.append(mul0_x.tensor)
    op.input_desc.add().CopyFrom(mul0_x.desc)
    op.input_desc[-1].name = "mul0_x"
    op.input.append(mul1_x.tensor)
    op.input_desc.add().CopyFrom(mul1_x.desc)
    op.input_desc[-1].name = "mul1_x"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_x.tensor)
    op.input_desc.add().CopyFrom(mul3_x.desc)
    op.input_desc[-1].name = "mul3_x"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output0"
    output0 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output1"
    output1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output2"
    output2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output0, output1, output2


# This api is auto-generated from IR AdamApplyOneWithDecayAssign
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False])
def AdamApplyOneWithDecayAssign(input0: Tensor, input1: Tensor, input2: Tensor, input3: Tensor, input4: Tensor, mul0_x: Tensor, mul1_x: Tensor, mul2_x: Tensor, mul3_x: Tensor, mul4_x: Tensor, add2_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdamApplyOneWithDecayAssign)\n
.INPUT(input0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul0_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul1_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul4_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdamApplyOneWithDecayAssign"
    op.name = next_unique_name(node_name, "AdamApplyOneWithDecayAssign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input0.tensor)
    op.input_desc.add().CopyFrom(input0.desc)
    op.input_desc[-1].name = "input0"
    op.input.append(input1.tensor)
    op.input_desc.add().CopyFrom(input1.desc)
    op.input_desc[-1].name = "input1"
    op.input.append(input2.tensor)
    op.input_desc.add().CopyFrom(input2.desc)
    op.input_desc[-1].name = "input2"
    op.input.append(input3.tensor)
    op.input_desc.add().CopyFrom(input3.desc)
    op.input_desc[-1].name = "input3"
    op.input.append(input4.tensor)
    op.input_desc.add().CopyFrom(input4.desc)
    op.input_desc[-1].name = "input4"
    op.input.append(mul0_x.tensor)
    op.input_desc.add().CopyFrom(mul0_x.desc)
    op.input_desc[-1].name = "mul0_x"
    op.input.append(mul1_x.tensor)
    op.input_desc.add().CopyFrom(mul1_x.desc)
    op.input_desc[-1].name = "mul1_x"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_x.tensor)
    op.input_desc.add().CopyFrom(mul3_x.desc)
    op.input_desc[-1].name = "mul3_x"
    op.input.append(mul4_x.tensor)
    op.input_desc.add().CopyFrom(mul4_x.desc)
    op.input_desc[-1].name = "mul4_x"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "input1"
    input1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "input2"
    input2 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "input3"
    input3 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return input1, input2, input3


# This api is auto-generated from IR AdamApplyOneAssign
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False])
def AdamApplyOneAssign(input0: Tensor, input1: Tensor, input2: Tensor, input3: Tensor, input4: Tensor, mul0_x: Tensor, mul1_x: Tensor, mul2_x: Tensor, mul3_x: Tensor, add2_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdamApplyOneAssign)\n
.INPUT(input0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input4, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul0_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul1_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdamApplyOneAssign"
    op.name = next_unique_name(node_name, "AdamApplyOneAssign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input0.tensor)
    op.input_desc.add().CopyFrom(input0.desc)
    op.input_desc[-1].name = "input0"
    op.input.append(input1.tensor)
    op.input_desc.add().CopyFrom(input1.desc)
    op.input_desc[-1].name = "input1"
    op.input.append(input2.tensor)
    op.input_desc.add().CopyFrom(input2.desc)
    op.input_desc[-1].name = "input2"
    op.input.append(input3.tensor)
    op.input_desc.add().CopyFrom(input3.desc)
    op.input_desc[-1].name = "input3"
    op.input.append(input4.tensor)
    op.input_desc.add().CopyFrom(input4.desc)
    op.input_desc[-1].name = "input4"
    op.input.append(mul0_x.tensor)
    op.input_desc.add().CopyFrom(mul0_x.desc)
    op.input_desc[-1].name = "mul0_x"
    op.input.append(mul1_x.tensor)
    op.input_desc.add().CopyFrom(mul1_x.desc)
    op.input_desc[-1].name = "mul1_x"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_x.tensor)
    op.input_desc.add().CopyFrom(mul3_x.desc)
    op.input_desc[-1].name = "mul3_x"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "input1"
    input1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "input2"
    input2 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "input3"
    input3 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return input1, input2, input3


# This api is auto-generated from IR LambApplyOptimizerAssign
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False])
def LambApplyOptimizerAssign(grad: Tensor, inputv: Tensor, inputm: Tensor, input3: Tensor, mul0_x: Tensor, mul1_x: Tensor, mul2_x: Tensor, mul3_x: Tensor, add2_y: Tensor, steps: Tensor, do_use_weight: Tensor, weight_decay_rate: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LambApplyOptimizerAssign)\n
.INPUT(grad, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(inputv, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(inputm, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul0_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul1_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul2_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mul3_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(add2_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(steps, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(do_use_weight, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(weight_decay_rate, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(inputv, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(inputm, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LambApplyOptimizerAssign"
    op.name = next_unique_name(node_name, "LambApplyOptimizerAssign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(inputv.tensor)
    op.input_desc.add().CopyFrom(inputv.desc)
    op.input_desc[-1].name = "inputv"
    op.input.append(inputm.tensor)
    op.input_desc.add().CopyFrom(inputm.desc)
    op.input_desc[-1].name = "inputm"
    op.input.append(input3.tensor)
    op.input_desc.add().CopyFrom(input3.desc)
    op.input_desc[-1].name = "input3"
    op.input.append(mul0_x.tensor)
    op.input_desc.add().CopyFrom(mul0_x.desc)
    op.input_desc[-1].name = "mul0_x"
    op.input.append(mul1_x.tensor)
    op.input_desc.add().CopyFrom(mul1_x.desc)
    op.input_desc[-1].name = "mul1_x"
    op.input.append(mul2_x.tensor)
    op.input_desc.add().CopyFrom(mul2_x.desc)
    op.input_desc[-1].name = "mul2_x"
    op.input.append(mul3_x.tensor)
    op.input_desc.add().CopyFrom(mul3_x.desc)
    op.input_desc[-1].name = "mul3_x"
    op.input.append(add2_y.tensor)
    op.input_desc.add().CopyFrom(add2_y.desc)
    op.input_desc[-1].name = "add2_y"
    op.input.append(steps.tensor)
    op.input_desc.add().CopyFrom(steps.desc)
    op.input_desc[-1].name = "steps"
    op.input.append(do_use_weight.tensor)
    op.input_desc.add().CopyFrom(do_use_weight.desc)
    op.input_desc[-1].name = "do_use_weight"
    op.input.append(weight_decay_rate.tensor)
    op.input_desc.add().CopyFrom(weight_decay_rate.desc)
    op.input_desc[-1].name = "weight_decay_rate"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output0"
    output0 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "inputv"
    inputv = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "inputm"
    inputm = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output0, inputv, inputm


# This api is auto-generated from IR LambApplyWeightAssign
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def LambApplyWeightAssign(input0: Tensor, input1: Tensor, input2: Tensor, input3: Tensor, input_param: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LambApplyWeightAssign)\n
.INPUT(input0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input3, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_param, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(input_param, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LambApplyWeightAssign"
    op.name = next_unique_name(node_name, "LambApplyWeightAssign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input0.tensor)
    op.input_desc.add().CopyFrom(input0.desc)
    op.input_desc[-1].name = "input0"
    op.input.append(input1.tensor)
    op.input_desc.add().CopyFrom(input1.desc)
    op.input_desc[-1].name = "input1"
    op.input.append(input2.tensor)
    op.input_desc.add().CopyFrom(input2.desc)
    op.input_desc[-1].name = "input2"
    op.input.append(input3.tensor)
    op.input_desc.add().CopyFrom(input3.desc)
    op.input_desc[-1].name = "input3"
    op.input.append(input_param.tensor)
    op.input_desc.add().CopyFrom(input_param.desc)
    op.input_desc[-1].name = "input_param"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "input_param"
    input_param = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return input_param


# This api is auto-generated from IR ClipByNormNoDivSum
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def ClipByNormNoDivSum(x: Tensor, greater_zeros: Tensor, select_ones: Tensor, maximum_ones: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ClipByNormNoDivSum)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(greater_zeros, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(select_ones, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(maximum_ones, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ClipByNormNoDivSum"
    op.name = next_unique_name(node_name, "ClipByNormNoDivSum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(greater_zeros.tensor)
    op.input_desc.add().CopyFrom(greater_zeros.desc)
    op.input_desc[-1].name = "greater_zeros"
    op.input.append(select_ones.tensor)
    op.input_desc.add().CopyFrom(select_ones.desc)
    op.input_desc[-1].name = "select_ones"
    op.input.append(maximum_ones.tensor)
    op.input_desc.add().CopyFrom(maximum_ones.desc)
    op.input_desc[-1].name = "maximum_ones"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SquareSumV2
@auto_convert_to_tensor([False], [False])
def SquareSumV2(x: Tensor, *, axis: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(SquareSumV2)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.REQUIRED_ATTR(axis, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SquareSumV2"
    op.name = next_unique_name(node_name, "SquareSumV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR SquareSumV1
@auto_convert_to_tensor([False], [False])
def SquareSumV1(x: Tensor, *, axis: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(SquareSumV1)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.REQUIRED_ATTR(axis, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SquareSumV1"
    op.name = next_unique_name(node_name, "SquareSumV1")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SquareSumAll
@auto_convert_to_tensor([False, False], [False, False])
def SquareSumAll(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SquareSumAll)\n
.INPUT(x1, TensorType({DT_FLOAT}))\n
.INPUT(x2, TensorType({DT_FLOAT}))\n
.OUTPUT(y1, TensorType({DT_FLOAT}))\n
.OUTPUT(y2, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SquareSumAll"
    op.name = next_unique_name(node_name, "SquareSumAll")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR FusedMulAddN
@auto_convert_to_tensor([False, False, False], [False, False, False])
def FusedMulAddN(x1: Tensor, x2: Tensor, x3: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FusedMulAddN)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.INPUT(x3, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedMulAddN"
    op.name = next_unique_name(node_name, "FusedMulAddN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(x3.tensor)
    op.input_desc.add().CopyFrom(x3.desc)
    op.input_desc[-1].name = "x3"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Bias
@auto_convert_to_tensor([False, False], [False, False])
def Bias(x: Tensor, bias: Tensor, *, axis: int=1, num_axes: int=1, bias_from_blob: bool=True, dependencies=[], node_name=None):
    """REG_OP(Bias)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(axis, Int, 1)\n
.ATTR(num_axes, Int, 1)\n
.ATTR(bias_from_blob, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Bias"
    op.name = next_unique_name(node_name, "Bias")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["num_axes"].i = num_axes
    op.attr["bias_from_blob"].b = bias_from_blob

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConfusionMulGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ConfusionMulGrad(input0: Tensor, input1: Tensor, input2: Tensor, *, axes: List[int]=[], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ConfusionMulGrad)\n
.INPUT(input0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output0, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(axes, ListInt, {})\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConfusionMulGrad"
    op.name = next_unique_name(node_name, "ConfusionMulGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input0.tensor)
    op.input_desc.add().CopyFrom(input0.desc)
    op.input_desc[-1].name = "input0"
    op.input.append(input1.tensor)
    op.input_desc.add().CopyFrom(input1.desc)
    op.input_desc[-1].name = "input1"
    op.input.append(input2.tensor)
    op.input_desc.add().CopyFrom(input2.desc)
    op.input_desc[-1].name = "input2"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output0"
    output0 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output1"
    output1 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output0, output1


# This api is auto-generated from IR FusedMulAddNL2loss
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def FusedMulAddNL2loss(x1: Tensor, x2: Tensor, x3: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FusedMulAddNL2loss)\n
.INPUT(x1, TensorType::NumberType())\n
.INPUT(x2, TensorType::NumberType())\n
.INPUT(x3, TensorType::NumberType())\n
.OUTPUT(y1, TensorType::NumberType())\n
.OUTPUT(y2, TensorType::NumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedMulAddNL2loss"
    op.name = next_unique_name(node_name, "FusedMulAddNL2loss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(x3.tensor)
    op.input_desc.add().CopyFrom(x3.desc)
    op.input_desc[-1].name = "x3"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR Threshold
@auto_convert_to_tensor([False], [False])
def Threshold(x: Tensor, *, threshold: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Threshold)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(threshold, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Threshold"
    op.name = next_unique_name(node_name, "Threshold")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["threshold"].f = threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMaxWithK
@auto_convert_to_tensor([False], [False])
def ArgMaxWithK(x: Tensor, *, axis: int=10000, out_max_val: bool=False, topk: int=1, dependencies=[], node_name=None):
    """REG_OP(ArgMaxWithK)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(indices, TensorType({DT_INT32, DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(values, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(axis, Int, 10000)\n
.ATTR(out_max_val, Bool, false)\n
.ATTR(topk, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMaxWithK"
    op.name = next_unique_name(node_name, "ArgMaxWithK")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["out_max_val"].b = out_max_val
    op.attr["topk"].i = topk

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, values


# This api is auto-generated from IR Muls
@auto_convert_to_tensor([False], [False])
def Muls(x: Tensor, *, value: float, dependencies=[], node_name=None):
    """REG_OP(Muls)\n
.INPUT(x, TensorType({DT_FLOAT,DT_INT16,DT_INT32,DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT,DT_INT16,DT_INT32,DT_FLOAT16}))\n
.REQUIRED_ATTR(value, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Muls"
    op.name = next_unique_name(node_name, "Muls")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["value"].f = value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Fills
@auto_convert_to_tensor([False], [False])
def Fills(x: Tensor, *, value: float, dependencies=[], node_name=None):
    """REG_OP(Fills)\n
.INPUT(x, TensorType({BasicType(), DT_BOOL}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL}))\n
.REQUIRED_ATTR(value, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Fills"
    op.name = next_unique_name(node_name, "Fills")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["value"].f = value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Adds
@auto_convert_to_tensor([False], [False])
def Adds(x: Tensor, *, value: float, dependencies=[], node_name=None):
    """REG_OP(Adds)\n
.INPUT(x, TensorType({DT_FLOAT,DT_INT16,DT_INT32,DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT,DT_INT16,DT_INT32,DT_FLOAT16}))\n
.REQUIRED_ATTR(value, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Adds"
    op.name = next_unique_name(node_name, "Adds")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["value"].f = value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MulNoNan
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def MulNoNan(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MulNoNan)\n
.INPUT(x1, TensorType::NumberType())\n
.INPUT(x2, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MulNoNan"
    op.name = next_unique_name(node_name, "MulNoNan")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Axpy
@auto_convert_to_tensor([False, False], [False, False])
def Axpy(x1: Tensor, x2: Tensor, *, alpha: float, dependencies=[], node_name=None):
    """REG_OP(Axpy)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16}))\n
.REQUIRED_ATTR(alpha, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Axpy"
    op.name = next_unique_name(node_name, "Axpy")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["alpha"].f = alpha

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CosineEmbeddingLoss
@auto_convert_to_tensor([False, False, False], [False, False, False])
def CosineEmbeddingLoss(x1: Tensor, x2: Tensor, target: Tensor, *, margin: float=0.000000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(CosineEmbeddingLoss)\n
.INPUT(x1, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(x2, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(target, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(margin, Float, 0)\n
.ATTR(reduction, String, "mean")\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CosineEmbeddingLoss"
    op.name = next_unique_name(node_name, "CosineEmbeddingLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"

    # process attrs
    op.attr["margin"].f = margin
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR KLDiv
@auto_convert_to_tensor([False, False], [False, False])
def KLDiv(x: Tensor, target: Tensor, *, reduction: str, dependencies=[], node_name=None):
    """REG_OP(KLDiv)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(target, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(reduction, String)\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "KLDiv"
    op.name = next_unique_name(node_name, "KLDiv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TensorMove
@auto_convert_to_tensor([False], [False])
def TensorMove(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorMove)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT32, DT_INT8, DT_UINT8, DT_UINT64, DT_INT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT32, DT_INT8, DT_UINT8, DT_UINT64, DT_INT64, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorMove"
    op.name = next_unique_name(node_name, "TensorMove")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TensorRedirect
@auto_convert_to_tensor([False], [False])
def TensorRedirect(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorRedirect)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_INT64, DT_INT16, DT_UINT16, DT_UINT64, DT_UINT32}))\n
.OUTPUT(output_x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_INT64, DT_INT16, DT_UINT16, DT_UINT64, DT_UINT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorRedirect"
    op.name = next_unique_name(node_name, "TensorRedirect")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_x"
    output_x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_x


# This api is auto-generated from IR Addcdiv
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def Addcdiv(input_data: Tensor, x1: Tensor, x2: Tensor, value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Addcdiv)\n
.INPUT(input_data, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE, DT_INT64}))\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE, DT_INT64}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE, DT_INT64}))\n
.INPUT(value, TensorType({ DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT32, DT_DOUBLE, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Addcdiv"
    op.name = next_unique_name(node_name, "Addcdiv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_data.tensor)
    op.input_desc.add().CopyFrom(input_data.desc)
    op.input_desc[-1].name = "input_data"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Addcmul
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def Addcmul(input_data: Tensor, x1: Tensor, x2: Tensor, value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Addcmul)\n
.INPUT(input_data, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_DOUBLE, DT_INT64}))\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_DOUBLE, DT_INT64}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_DOUBLE, DT_INT64}))\n
.INPUT(value, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_DOUBLE, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8, DT_DOUBLE, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Addcmul"
    op.name = next_unique_name(node_name, "Addcmul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_data.tensor)
    op.input_desc.add().CopyFrom(input_data.desc)
    op.input_desc[-1].name = "input_data"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AxpyV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AxpyV2(x1: Tensor, x2: Tensor, alpha: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AxpyV2)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_BF16}))\n
.INPUT(alpha, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AxpyV2"
    op.name = next_unique_name(node_name, "AxpyV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StrideAdd
@auto_convert_to_tensor([False, False], [False, False])
def StrideAdd(x1: Tensor, x2: Tensor, *, x1_c1_offset: int, x2_c1_offset: int, c1_len: int, dependencies=[], node_name=None):
    """REG_OP(StrideAdd)\n
.INPUT(x1, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(x2, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.REQUIRED_ATTR(x1_c1_offset, Int)\n
.REQUIRED_ATTR(x2_c1_offset, Int)\n
.REQUIRED_ATTR(c1_len, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StrideAdd"
    op.name = next_unique_name(node_name, "StrideAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["x1_c1_offset"].i = x1_c1_offset
    op.attr["x2_c1_offset"].i = x2_c1_offset
    op.attr["c1_len"].i = c1_len

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TensorEqual
@auto_convert_to_tensor([False, False], [False, False])
def TensorEqual(input_x: Tensor, input_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorEqual)\n
.INPUT(input_x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_INT8, DT_UINT8, DT_BOOL}))\n
.INPUT(input_y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_INT8, DT_UINT8, DT_BOOL}))\n
.OUTPUT(output_z, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorEqual"
    op.name = next_unique_name(node_name, "TensorEqual")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"
    op.input.append(input_y.tensor)
    op.input_desc.add().CopyFrom(input_y.desc)
    op.input_desc[-1].name = "input_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_z"
    output_z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_z


# This api is auto-generated from IR MaxN
@auto_convert_to_tensor([True], [False])
def MaxN(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(MaxN)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_FLOAT64, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_FLOAT64, DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxN"
    op.name = next_unique_name(node_name, "MaxN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaskedScale
@auto_convert_to_tensor([False, False], [False, False])
def MaskedScale(x: Tensor, mask: Tensor, *, value: float, dependencies=[], node_name=None):
    """REG_OP(MaskedScale)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.INPUT(mask, TensorType({DT_INT8, DT_FLOAT16, DT_FLOAT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.REQUIRED_ATTR(value, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaskedScale"
    op.name = next_unique_name(node_name, "MaskedScale")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["value"].f = value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Lerp
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Lerp(start: Tensor, end: Tensor, weight: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Lerp)\n
.INPUT(start, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(end, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Lerp"
    op.name = next_unique_name(node_name, "Lerp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(start.tensor)
    op.input_desc.add().CopyFrom(start.desc)
    op.input_desc[-1].name = "start"
    op.input.append(end.tensor)
    op.input_desc.add().CopyFrom(end.desc)
    op.input_desc[-1].name = "end"
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DataCompare
@auto_convert_to_tensor([False, False], [False, False])
def DataCompare(x1: Tensor, x2: Tensor, *, atol: float=0.000010, rtol: float=0.001000, dependencies=[], node_name=None):
    """REG_OP(DataCompare)\n
.INPUT(x1, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT32}))\n
.INPUT(x2, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT32}))\n
.OUTPUT(num, TensorType({DT_FLOAT}))\n
.ATTR(atol, Float, 1e-5)\n
.ATTR(rtol, Float, 1e-3)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DataCompare"
    op.name = next_unique_name(node_name, "DataCompare")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["atol"].f = atol
    op.attr["rtol"].f = rtol

    # process outputs
    output_index = 0
    op.output_desc.add().name = "num"
    num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return num


# This api is auto-generated from IR HardMax
@auto_convert_to_tensor([False], [False])
def HardMax(x: Tensor, *, axis: int=-1, dependencies=[], node_name=None):
    """REG_OP(HardMax)\n
.INPUT(x, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(axis, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardMax"
    op.name = next_unique_name(node_name, "HardMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Dot
@auto_convert_to_tensor([False, False], [False, False])
def Dot(input_x: Tensor, input_y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Dot)\n
.INPUT(input_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16, DT_UINT8, DT_INT8, DT_INT32}))\n
.INPUT(input_y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16, DT_UINT8, DT_INT8, DT_INT32}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16, DT_UINT8, DT_INT8, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dot"
    op.name = next_unique_name(node_name, "Dot")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"
    op.input.append(input_y.tensor)
    op.input_desc.add().CopyFrom(input_y.desc)
    op.input_desc[-1].name = "input_y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR IsClose
@auto_convert_to_tensor([False, False], [False, False])
def IsClose(x1: Tensor, x2: Tensor, *, rtol: float=0.000010, atol: float=0.000000, equal_nan: bool=False, dependencies=[], node_name=None):
    """REG_OP(IsClose)\n
.INPUT(x1, TensorType({BasicType(), DT_BOOL}))\n
.INPUT(x2, TensorType({BasicType(), DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
.ATTR(rtol, Float, 1e-05)\n
.ATTR(atol, Float, 1e-08)\n
.ATTR(equal_nan, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsClose"
    op.name = next_unique_name(node_name, "IsClose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["rtol"].f = rtol
    op.attr["atol"].f = atol
    op.attr["equal_nan"].b = equal_nan

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMaxGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ArgMaxGrad(var: Tensor, indices: Tensor, updates: Tensor, *, dimension: int, dependencies=[], node_name=None):
    """REG_OP(ArgMaxGrad)\n
.INPUT(var, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(updates, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.REQUIRED_ATTR(dimension, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMaxGrad"
    op.name = next_unique_name(node_name, "ArgMaxGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["dimension"].i = dimension

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ArgMaxGradD
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def ArgMaxGradD(var: Tensor, indices: Tensor, updates: Tensor, assist: Tensor, *, dimension: int, dependencies=[], node_name=None):
    """REG_OP(ArgMaxGradD)\n
.INPUT(var, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(updates, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.INPUT(assist, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8}))\n
.REQUIRED_ATTR(dimension, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ArgMaxGradD"
    op.name = next_unique_name(node_name, "ArgMaxGradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs
    op.attr["dimension"].i = dimension

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AddMatMatElements
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def AddMatMatElements(c: Tensor, a: Tensor, b: Tensor, beta: Tensor, alpha: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AddMatMatElements)\n
.INPUT(c, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(a, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(b, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(c, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AddMatMatElements"
    op.name = next_unique_name(node_name, "AddMatMatElements")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(a.tensor)
    op.input_desc.add().CopyFrom(a.desc)
    op.input_desc[-1].name = "a"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "c"
    c = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return c


# This api is auto-generated from IR CosineSimilarity
@auto_convert_to_tensor([False, False], [False, False])
def CosineSimilarity(input_x1: Tensor, input_x2: Tensor, *, dim: int=1, eps: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(CosineSimilarity)\n
.INPUT(input_x1, TensorType({DT_FLOAT}))\n
.INPUT(input_x2, TensorType({DT_FLOAT}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT}))\n
.ATTR(dim, Int, 1)\n
.ATTR(eps, Float, 1e-8)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CosineSimilarity"
    op.name = next_unique_name(node_name, "CosineSimilarity")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x1.tensor)
    op.input_desc.add().CopyFrom(input_x1.desc)
    op.input_desc[-1].name = "input_x1"
    op.input.append(input_x2.tensor)
    op.input_desc.add().CopyFrom(input_x2.desc)
    op.input_desc[-1].name = "input_x2"

    # process attrs
    op.attr["dim"].i = dim
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR ApplyAdamV2
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, True, False, False, True])
def ApplyAdamV2(var: Tensor, m: Tensor, v: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, max_grad_norm: Optional[Tensor], global_grad_norm: Tensor, weight_decay: Tensor, step_size: Optional[Tensor], *, adam_mode: str="adam", dependencies=[], node_name=None):
    """REG_OP(ApplyAdamV2)\n
.INPUT(var, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(m, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(v, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(lr, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(beta1, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(beta2, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(epsilon, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(grad, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OPTIONAL_INPUT(max_grad_norm, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(global_grad_norm, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(weight_decay, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OPTIONAL_INPUT(step_size, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(var, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(m, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(v, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.ATTR(adam_mode, String, "adam")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdamV2"
    op.name = next_unique_name(node_name, "ApplyAdamV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    if max_grad_norm is not None:
        op.input.append(max_grad_norm.tensor)
        op.input_desc.add().CopyFrom(max_grad_norm.desc)
        op.input_desc[-1].name = "max_grad_norm"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "max_grad_norm"
    op.input.append(global_grad_norm.tensor)
    op.input_desc.add().CopyFrom(global_grad_norm.desc)
    op.input_desc[-1].name = "global_grad_norm"
    op.input.append(weight_decay.tensor)
    op.input_desc.add().CopyFrom(weight_decay.desc)
    op.input_desc[-1].name = "weight_decay"
    if step_size is not None:
        op.input.append(step_size.tensor)
        op.input_desc.add().CopyFrom(step_size.desc)
        op.input_desc[-1].name = "step_size"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "step_size"

    # process attrs
    op.attr["adam_mode"].s = compat_as_bytes(adam_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "v"
    v = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m, v


# This api is auto-generated from IR Dawsn
@auto_convert_to_tensor([False], [False])
def Dawsn(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Dawsn)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dawsn"
    op.name = next_unique_name(node_name, "Dawsn")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NanToNum
@auto_convert_to_tensor([False], [False])
def NanToNum(x: Tensor, *, nan: float, posinf: float, neginf: float, dependencies=[], node_name=None):
    """REG_OP(NanToNum)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(nan, Float)\n
.REQUIRED_ATTR(posinf, Float)\n
.REQUIRED_ATTR(neginf, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NanToNum"
    op.name = next_unique_name(node_name, "NanToNum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["nan"].f = nan
    op.attr["posinf"].f = posinf
    op.attr["neginf"].f = neginf

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CastLike
@auto_convert_to_tensor([False, False], [False, False])
def CastLike(x: Tensor, target: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(CastLike)\n
.INPUT(x, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.INPUT(target, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CastLike"
    op.name = next_unique_name(node_name, "CastLike")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ClipByValueV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ClipByValueV2(x: Tensor, clip_value_min: Tensor, clip_value_max: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ClipByValueV2)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(clip_value_min, TensorType::NumberType())\n
.INPUT(clip_value_max, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ClipByValueV2"
    op.name = next_unique_name(node_name, "ClipByValueV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(clip_value_min.tensor)
    op.input_desc.add().CopyFrom(clip_value_min.desc)
    op.input_desc[-1].name = "clip_value_min"
    op.input.append(clip_value_max.tensor)
    op.input_desc.add().CopyFrom(clip_value_max.desc)
    op.input_desc[-1].name = "clip_value_max"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LDPCDecode
@auto_convert_to_tensor([False, False], [False, False])
def LDPCDecode(valid_num: Tensor, matrix_info: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LDPCDecode)\n
.INPUT(valid_num, TensorType({DT_INT32}))\n
.INPUT(matrix_info, TensorType({DT_INT32}))\n
.OUTPUT(indices, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LDPCDecode"
    op.name = next_unique_name(node_name, "LDPCDecode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(valid_num.tensor)
    op.input_desc.add().CopyFrom(valid_num.desc)
    op.input_desc[-1].name = "valid_num"
    op.input.append(matrix_info.tensor)
    op.input_desc.add().CopyFrom(matrix_info.desc)
    op.input_desc[-1].name = "matrix_info"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices


# This api is auto-generated from IR ApplyAdamW
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, True], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdamW(var: Tensor, m: Tensor, v: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, weight_decay: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, max_grad_norm: Optional[Tensor], *, amsgrad: bool=False, maximize: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdamW)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(v, TensorType::NumberType())\n
.INPUT(beta1_power, TensorType::NumberType())\n
.INPUT(beta2_power, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(weight_decay, TensorType::NumberType())\n
.INPUT(beta1, TensorType::NumberType())\n
.INPUT(beta2, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OPTIONAL_INPUT(max_grad_norm, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(m, TensorType::NumberType())\n
.OUTPUT(v, TensorType::NumberType())\n
.ATTR(amsgrad, Bool, false)\n
.ATTR(maximize, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdamW"
    op.name = next_unique_name(node_name, "ApplyAdamW")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(weight_decay.tensor)
    op.input_desc.add().CopyFrom(weight_decay.desc)
    op.input_desc[-1].name = "weight_decay"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    if max_grad_norm is not None:
        op.input.append(max_grad_norm.tensor)
        op.input_desc.add().CopyFrom(max_grad_norm.desc)
        op.input_desc[-1].name = "max_grad_norm"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "max_grad_norm"

    # process attrs
    op.attr["amsgrad"].b = amsgrad
    op.attr["maximize"].b = maximize

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "v"
    v = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m, v


# This api is auto-generated from IR ScanSQCodes
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def ScanSQCodes(ivf: Tensor, query: Tensor, bucket_list: Tensor, bucket_limits: Tensor, bucket_offsets: Tensor, vmin: Tensor, vdiff: Tensor, *, total_limit: int, group_size: int=64, extreme_mode: int=0, dependencies=[], node_name=None):
    """REG_OP(ScanSQCodes)\n
.INPUT(ivf, TensorType({DT_UINT8}))\n
.INPUT(query, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bucket_list, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(bucket_limits, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(bucket_offsets, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(vmin, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(vdiff, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(actual_count, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(sq_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(grouped_extreme_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(sq_ivf, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(sq_index, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(total_limit, Int)\n
.ATTR(group_size, Int, 64)\n
.ATTR(extreme_mode, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScanSQCodes"
    op.name = next_unique_name(node_name, "ScanSQCodes")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(ivf.tensor)
    op.input_desc.add().CopyFrom(ivf.desc)
    op.input_desc[-1].name = "ivf"
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(bucket_list.tensor)
    op.input_desc.add().CopyFrom(bucket_list.desc)
    op.input_desc[-1].name = "bucket_list"
    op.input.append(bucket_limits.tensor)
    op.input_desc.add().CopyFrom(bucket_limits.desc)
    op.input_desc[-1].name = "bucket_limits"
    op.input.append(bucket_offsets.tensor)
    op.input_desc.add().CopyFrom(bucket_offsets.desc)
    op.input_desc[-1].name = "bucket_offsets"
    op.input.append(vmin.tensor)
    op.input_desc.add().CopyFrom(vmin.desc)
    op.input_desc[-1].name = "vmin"
    op.input.append(vdiff.tensor)
    op.input_desc.add().CopyFrom(vdiff.desc)
    op.input_desc[-1].name = "vdiff"

    # process attrs
    op.attr["total_limit"].i = total_limit
    op.attr["group_size"].i = group_size
    op.attr["extreme_mode"].i = extreme_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "actual_count"
    actual_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sq_distance"
    sq_distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "grouped_extreme_distance"
    grouped_extreme_distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sq_ivf"
    sq_ivf = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sq_index"
    sq_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return actual_count, sq_distance, grouped_extreme_distance, sq_ivf, sq_index


# This api is auto-generated from IR TransposeBatchMatMul
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def TransposeBatchMatMul(x1: Tensor, x2: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, perm_x1: List[int]=[], perm_x2: List[int]=[], perm_y: List[int]=[], offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(TransposeBatchMatMul)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8, DT_INT4}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.ATTR(perm_x1, ListInt, {})\n
.ATTR(perm_x2, ListInt, {})\n
.ATTR(perm_y, ListInt, {})\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TransposeBatchMatMul"
    op.name = next_unique_name(node_name, "TransposeBatchMatMul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["perm_x1"].list.val_type = 2
    op.attr["perm_x1"].list.i.extend(perm_x1)
    op.attr["perm_x2"].list.val_type = 2
    op.attr["perm_x2"].list.i.extend(perm_x2)
    op.attr["perm_y"].list.val_type = 2
    op.attr["perm_y"].list.i.extend(perm_y)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RotatedNMS
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def RotatedNMS(boxes: Tensor, scores: Tensor, labels: Tensor, *, iou_threshold: float, dependencies=[], node_name=None):
    """REG_OP(RotatedNMS)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(labels, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(selected_detections, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(keep_indices, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(iou_threshold, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotatedNMS"
    op.name = next_unique_name(node_name, "RotatedNMS")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"

    # process attrs
    op.attr["iou_threshold"].f = iou_threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_detections"
    selected_detections = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "keep_indices"
    keep_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_detections, keep_indices


@auto_convert_to_tensor([False, True],
                        [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN])
def IndexByTensor(x: Tensor,
          indices: List[Tensor],
          *,
          indices_mask: List[int],
          dependencies=[],
          node_name=None):
    """REG_OP(IndexByTensor)\n
.INPUT(x, TensorType::BasicType())\n
.DYNAMIC_INPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(indices_mask, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IndexByTensor"
    op.name = next_unique_name(node_name, "IndexByTensor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if not isinstance(indices, (tuple, list)):
        raise AssertionError("indices must be a tuple or a list.")
    for i, v in enumerate(indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "indices" + str(i)

    # process attrs
    op.attr["indices_mask"].list.val_type = 2
    op.attr["indices_mask"].list.i.extend(indices_mask)
    
    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    return y


# This api is auto-generated from IR Index
@auto_convert_to_tensor([False, False, False, True], [False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def Index(x: Tensor, indexed_sizes: Tensor, indexed_strides: Tensor, indices: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(Index)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(indexed_sizes, TensorType({DT_INT64}))\n
.INPUT(indexed_strides, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Index"
    op.name = next_unique_name(node_name, "Index")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indexed_sizes.tensor)
    op.input_desc.add().CopyFrom(indexed_sizes.desc)
    op.input_desc[-1].name = "indexed_sizes"
    op.input.append(indexed_strides.tensor)
    op.input_desc.add().CopyFrom(indexed_strides.desc)
    op.input_desc[-1].name = "indexed_strides"
    if not isinstance(indices, (tuple, list)):
        raise AssertionError("indices must be a tuple or a list.")
    for i, v in enumerate(indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "indices" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IndexPutV2
@auto_convert_to_tensor([False, False, False, False, True], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def IndexPutV2(x: Tensor, value: Tensor, indexed_sizes: Tensor, indexed_strides: Tensor, indices: List[Tensor], *, accumulate: bool=False, dependencies=[], node_name=None):
    """REG_OP(IndexPutV2)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(value, TensorType::BasicType())\n
.INPUT(indexed_sizes, TensorType({DT_INT64}))\n
.INPUT(indexed_strides, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(x, TensorType::BasicType())\n
.ATTR(accumulate, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IndexPutV2"
    op.name = next_unique_name(node_name, "IndexPutV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(indexed_sizes.tensor)
    op.input_desc.add().CopyFrom(indexed_sizes.desc)
    op.input_desc[-1].name = "indexed_sizes"
    op.input.append(indexed_strides.tensor)
    op.input_desc.add().CopyFrom(indexed_strides.desc)
    op.input_desc[-1].name = "indexed_strides"
    if not isinstance(indices, (tuple, list)):
        raise AssertionError("indices must be a tuple or a list.")
    for i, v in enumerate(indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "indices" + str(i)

    # process attrs
    op.attr["accumulate"].b = accumulate

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x"
    x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x


# This api is auto-generated from IR AvgPoolUpdate
@auto_convert_to_tensor([False, False], [False, False])
def AvgPoolUpdate(x1: Tensor, x2: Tensor, *, ksize: List[int], strides: List[int], padding_mode: str="CALCULATED", pads: List[int]=[0, 0, 0, 0], data_format: str="NHWC", ceil_mode: bool=False, exclusive: bool=True, dependencies=[], node_name=None):
    """REG_OP(AvgPoolUpdate)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x2, TensorType({DA_INT4, DT_INT8, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(padding_mode, String, "CALCULATED")\n
.ATTR(pads, ListInt, {0, 0, 0, 0})\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(exclusive, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPoolUpdate"
    op.name = next_unique_name(node_name, "AvgPoolUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["exclusive"].b = exclusive

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR YUVToRGB
@auto_convert_to_tensor([False, False], [False, True])
def YUVToRGB(x: Tensor, matrix: Optional[Tensor], *, matrix_type: int=0, rb_swap: int=0, dependencies=[], node_name=None):
    """REG_OP(YUVToRGB)\n
.INPUT(x, TensorType({DT_UINT8}))\n
.OPTIONAL_INPUT(matrix, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
.ATTR(matrix_type, Int, 0)\n
.ATTR(rb_swap, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YUVToRGB"
    op.name = next_unique_name(node_name, "YUVToRGB")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if matrix is not None:
        op.input.append(matrix.tensor)
        op.input_desc.add().CopyFrom(matrix.desc)
        op.input_desc[-1].name = "matrix"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "matrix"

    # process attrs
    op.attr["matrix_type"].i = matrix_type
    op.attr["rb_swap"].i = rb_swap

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DecodeJpegPre
@auto_convert_to_tensor([False], [False])
def DecodeJpegPre(contents: Tensor, *, w_range: List[int], h_range: List[int], dependencies=[], node_name=None):
    """REG_OP(DecodeJpegPre)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(dvpp_support, BOOL)\n
.REQUIRED_ATTR(w_range, ListInt)\n
.REQUIRED_ATTR(h_range, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeJpegPre"
    op.name = next_unique_name(node_name, "DecodeJpegPre")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs
    op.attr["w_range"].list.val_type = 2
    op.attr["w_range"].list.i.extend(w_range)
    op.attr["h_range"].list.val_type = 2
    op.attr["h_range"].list.i.extend(h_range)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dvpp_support"
    dvpp_support = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dvpp_support


# This api is auto-generated from IR InitPartitionMap
@auto_convert_to_tensor([False, False], [False, False])
def InitPartitionMap(ps_num: Tensor, ps_ids: Tensor, *, partition_num: int=65537, dependencies=[], node_name=None):
    """REG_OP(InitPartitionMap)\n
.INPUT(ps_num, TensorType({DT_INT32}))\n
.INPUT(ps_ids, TensorType({DT_INT32}))\n
.ATTR(partition_num, Int, 65537)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InitPartitionMap"
    op.name = next_unique_name(node_name, "InitPartitionMap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(ps_num.tensor)
    op.input_desc.add().CopyFrom(ps_num.desc)
    op.input_desc[-1].name = "ps_num"
    op.input.append(ps_ids.tensor)
    op.input_desc.add().CopyFrom(ps_ids.desc)
    op.input_desc[-1].name = "ps_ids"

    # process attrs
    op.attr["partition_num"].i = partition_num

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR UninitPartitionMap
@auto_convert_to_tensor([], [])
def UninitPartitionMap(*, dependencies=[], node_name=None):
    """REG_OP(UninitPartitionMap)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UninitPartitionMap"
    op.name = next_unique_name(node_name, "UninitPartitionMap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR InitEmbeddingHashmap
@auto_convert_to_tensor([False], [False])
def InitEmbeddingHashmap(table_id: Tensor, *, value_total_len: int, embedding_dim: int, bucket_size: int=0, dtype: int=DataType.DT_FLOAT, initializer_mode: str="", constant_value: float=0.000000, min: float=-2.000000, max: float=2.000000, mu: float=0.000000, sigma: float=1.000000, seed: int=0, seed2: int=0, filter_mode: str="no_filter", optimizer_mode: str="", optimizer_params: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(InitEmbeddingHashmap)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.ATTR(bucket_size, Int, 0)\n
.REQUIRED_ATTR(value_total_len, Int)\n
.REQUIRED_ATTR(embedding_dim, Int)\n
.ATTR(dtype, Type, DT_FLOAT)\n
.ATTR(initializer_mode, String, "")\n
.ATTR(constant_value, Float, 0)\n
.ATTR(min, Float, -2)\n
.ATTR(max, Float, 2)\n
.ATTR(mu, Float, 0)\n
.ATTR(sigma, Float, 1)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
.ATTR(filter_mode, String, "no_filter")\n
.ATTR(optimizer_mode, String, "")\n
.ATTR(optimizer_params, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InitEmbeddingHashmap"
    op.name = next_unique_name(node_name, "InitEmbeddingHashmap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs
    op.attr["value_total_len"].i = value_total_len
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["bucket_size"].i = bucket_size
    op.attr["dtype"].dt = dtype
    op.attr["initializer_mode"].s = compat_as_bytes(initializer_mode)
    op.attr["constant_value"].f = constant_value
    op.attr["min"].f = min
    op.attr["max"].f = max
    op.attr["mu"].f = mu
    op.attr["sigma"].f = sigma
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2
    op.attr["filter_mode"].s = compat_as_bytes(filter_mode)
    op.attr["optimizer_mode"].s = compat_as_bytes(optimizer_mode)
    op.attr["optimizer_params"].list.val_type = 3
    op.attr["optimizer_params"].list.f.extend(optimizer_params)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR EmbeddingTableImport
@auto_convert_to_tensor([False, False, False], [False, False, False])
def EmbeddingTableImport(file_path: Tensor, ps_id: Tensor, table_id: Tensor, *, embedding_dim: List[int], value_total_len: List[int], only_var_flag: bool=False, file_type: str="bin", table_name: List[str]=[], dependencies=[], node_name=None):
    """REG_OP(EmbeddingTableImport)\n
.INPUT(file_path, TensorType({DT_STRING}))\n
.INPUT(ps_id, TensorType({DT_INT32}))\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(embedding_dim, ListInt)\n
.REQUIRED_ATTR(value_total_len, ListInt)\n
.ATTR(only_var_flag, Bool, false)\n
.ATTR(file_type, String, "bin")\n
.ATTR(table_name, ListString, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingTableImport"
    op.name = next_unique_name(node_name, "EmbeddingTableImport")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(file_path.tensor)
    op.input_desc.add().CopyFrom(file_path.desc)
    op.input_desc[-1].name = "file_path"
    op.input.append(ps_id.tensor)
    op.input_desc.add().CopyFrom(ps_id.desc)
    op.input_desc[-1].name = "ps_id"
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs
    op.attr["embedding_dim"].list.val_type = 2
    op.attr["embedding_dim"].list.i.extend(embedding_dim)
    op.attr["value_total_len"].list.val_type = 2
    op.attr["value_total_len"].list.i.extend(value_total_len)
    op.attr["only_var_flag"].b = only_var_flag
    op.attr["file_type"].s = compat_as_bytes(file_type)
    op.attr["table_name"].list.val_type = 0
    op.attr["table_name"].list.s.extend(compat_as_bytes_list(table_name))

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR EmbeddingTableFind
@auto_convert_to_tensor([False, False], [False, False])
def EmbeddingTableFind(table_id: Tensor, keys: Tensor, *, embedding_dim: int, dependencies=[], node_name=None):
    """REG_OP(EmbeddingTableFind)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(embedding_dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingTableFind"
    op.name = next_unique_name(node_name, "EmbeddingTableFind")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"

    # process attrs
    op.attr["embedding_dim"].i = embedding_dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values


# This api is auto-generated from IR UninitEmbeddingHashmap
@auto_convert_to_tensor([False], [False])
def UninitEmbeddingHashmap(table_id: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(UninitEmbeddingHashmap)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UninitEmbeddingHashmap"
    op.name = next_unique_name(node_name, "UninitEmbeddingHashmap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR EmbeddingTableFindAndInit
@auto_convert_to_tensor([False, False], [False, False])
def EmbeddingTableFindAndInit(table_id: Tensor, keys: Tensor, *, embedding_dim: int, value_total_len: int, initializer_mode: str="random_uniform", constant_value: float=0.000000, min: float=-2.000000, max: float=2.000000, mu: float=0.000000, sigma: float=1.000000, seed: int=0, seed2: int=0, filter_mode: str="no_filter", filter_freq: int=0, default_key_or_value: bool=False, default_key: int=0, default_value: float=0.000000, optimizer_mode: str="", optimizer_params: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(EmbeddingTableFindAndInit)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(embedding_dim, Int)\n
.REQUIRED_ATTR(value_total_len, Int)\n
.ATTR(initializer_mode, String, "random_uniform")\n
.ATTR(constant_value, Float, 0)\n
.ATTR(min, Float, -2)\n
.ATTR(max, Float, 2)\n
.ATTR(mu, Float, 0)\n
.ATTR(sigma, Float, 1)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
.ATTR(filter_mode, String, "no_filter")\n
.ATTR(filter_freq, Int, 0)\n
.ATTR(default_key_or_value, Bool, false)\n
.ATTR(default_key, Int, 0)\n
.ATTR(default_value, Float, 0)\n
.ATTR(optimizer_mode, String, "")\n
.ATTR(optimizer_params, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingTableFindAndInit"
    op.name = next_unique_name(node_name, "EmbeddingTableFindAndInit")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"

    # process attrs
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["value_total_len"].i = value_total_len
    op.attr["initializer_mode"].s = compat_as_bytes(initializer_mode)
    op.attr["constant_value"].f = constant_value
    op.attr["min"].f = min
    op.attr["max"].f = max
    op.attr["mu"].f = mu
    op.attr["sigma"].f = sigma
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2
    op.attr["filter_mode"].s = compat_as_bytes(filter_mode)
    op.attr["filter_freq"].i = filter_freq
    op.attr["default_key_or_value"].b = default_key_or_value
    op.attr["default_key"].i = default_key
    op.attr["default_value"].f = default_value
    op.attr["optimizer_mode"].s = compat_as_bytes(optimizer_mode)
    op.attr["optimizer_params"].list.val_type = 3
    op.attr["optimizer_params"].list.f.extend(optimizer_params)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values


# This api is auto-generated from IR EmbeddingApplyAdam
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def EmbeddingApplyAdam(var_handle: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, keys: Tensor, global_step: Tensor, *, embedding_dim: int, dependencies=[], node_name=None):
    """REG_OP(EmbeddingApplyAdam)\n
.INPUT(var_handle, TensorType({DT_RESOURCE}))\n
.INPUT(beta1_power, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta2_power, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(lr, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta1, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta2, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(epsilon, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(grad, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.INPUT(global_step, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(var_handle, TensorType({DT_RESOURCE}))\n
.REQUIRED_ATTR(embedding_dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingApplyAdam"
    op.name = next_unique_name(node_name, "EmbeddingApplyAdam")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var_handle.tensor)
    op.input_desc.add().CopyFrom(var_handle.desc)
    op.input_desc[-1].name = "var_handle"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(global_step.tensor)
    op.input_desc.add().CopyFrom(global_step.desc)
    op.input_desc[-1].name = "global_step"

    # process attrs
    op.attr["embedding_dim"].i = embedding_dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var_handle"
    var_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var_handle


# This api is auto-generated from IR EmbeddingApplyAdamW
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, True])
def EmbeddingApplyAdamW(var_handle: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, weight_decay: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, keys: Tensor, max_grad_norm: Optional[Tensor], *, embedding_dim: int, amsgrad: bool=False, maximize: bool=False, dependencies=[], node_name=None):
    """REG_OP(EmbeddingApplyAdamW)\n
.INPUT(var_handle, TensorType({DT_RESOURCE}))\n
.INPUT(beta1_power, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta2_power, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(lr, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(weight_decay, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta1, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta2, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(epsilon, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(grad, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(max_grad_norm, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(var_handle, TensorType({DT_RESOURCE}))\n
.REQUIRED_ATTR(embedding_dim, Int)\n
.ATTR(amsgrad, Bool, false)\n
.ATTR(maximize, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingApplyAdamW"
    op.name = next_unique_name(node_name, "EmbeddingApplyAdamW")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var_handle.tensor)
    op.input_desc.add().CopyFrom(var_handle.desc)
    op.input_desc[-1].name = "var_handle"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(weight_decay.tensor)
    op.input_desc.add().CopyFrom(weight_decay.desc)
    op.input_desc[-1].name = "weight_decay"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    if max_grad_norm is not None:
        op.input.append(max_grad_norm.tensor)
        op.input_desc.add().CopyFrom(max_grad_norm.desc)
        op.input_desc[-1].name = "max_grad_norm"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "max_grad_norm"

    # process attrs
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["amsgrad"].b = amsgrad
    op.attr["maximize"].b = maximize

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var_handle"
    var_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var_handle


# This api is auto-generated from IR EmbeddingTableExport
@auto_convert_to_tensor([False, False, False], [False, False, False])
def EmbeddingTableExport(file_path: Tensor, ps_id: Tensor, table_id: Tensor, *, embedding_dim: List[int], value_total_len: List[int], export_mode: str="all", only_var_flag: bool=False, file_type: str="bin", table_name: List[str]=[], filter_export_flag: bool=False, dependencies=[], node_name=None):
    """REG_OP(EmbeddingTableExport)\n
.INPUT(file_path, TensorType({DT_STRING}))\n
.INPUT(ps_id, TensorType({DT_INT32}))\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(embedding_dim, ListInt)\n
.REQUIRED_ATTR(value_total_len, ListInt)\n
.ATTR(export_mode, String, "all")\n
.ATTR(only_var_flag, Bool, false)\n
.ATTR(file_type, String, "bin")\n
.ATTR(table_name, ListString, {})\n
.ATTR(filter_export_flag, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingTableExport"
    op.name = next_unique_name(node_name, "EmbeddingTableExport")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(file_path.tensor)
    op.input_desc.add().CopyFrom(file_path.desc)
    op.input_desc[-1].name = "file_path"
    op.input.append(ps_id.tensor)
    op.input_desc.add().CopyFrom(ps_id.desc)
    op.input_desc[-1].name = "ps_id"
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs
    op.attr["embedding_dim"].list.val_type = 2
    op.attr["embedding_dim"].list.i.extend(embedding_dim)
    op.attr["value_total_len"].list.val_type = 2
    op.attr["value_total_len"].list.i.extend(value_total_len)
    op.attr["export_mode"].s = compat_as_bytes(export_mode)
    op.attr["only_var_flag"].b = only_var_flag
    op.attr["file_type"].s = compat_as_bytes(file_type)
    op.attr["table_name"].list.val_type = 0
    op.attr["table_name"].list.s.extend(compat_as_bytes_list(table_name))
    op.attr["filter_export_flag"].b = filter_export_flag

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR TableToResource
@auto_convert_to_tensor([False], [False])
def TableToResource(table_id: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TableToResource)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.OUTPUT(table_handle, TensorType({DT_RESOURCE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TableToResource"
    op.name = next_unique_name(node_name, "TableToResource")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "table_handle"
    table_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return table_handle


# This api is auto-generated from IR EmbeddingFeatureMapping
@auto_convert_to_tensor([False], [False])
def EmbeddingFeatureMapping(feature_id: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(EmbeddingFeatureMapping)\n
.INPUT(feature_id, TensorType({DT_INT64}))\n
.OUTPUT(offset_id, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingFeatureMapping"
    op.name = next_unique_name(node_name, "EmbeddingFeatureMapping")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(feature_id.tensor)
    op.input_desc.add().CopyFrom(feature_id.desc)
    op.input_desc[-1].name = "feature_id"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "offset_id"
    offset_id = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return offset_id


# This api is auto-generated from IR EmbeddingApplyAdaGrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def EmbeddingApplyAdaGrad(var_handle: Tensor, lr: Tensor, grad: Tensor, keys: Tensor, global_step: Tensor, *, embedding_dim: int, dependencies=[], node_name=None):
    """REG_OP(EmbeddingApplyAdaGrad)\n
.INPUT(var_handle, TensorType({DT_RESOURCE}))\n
.INPUT(lr, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(grad, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.INPUT(global_step, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(var_handle, TensorType({DT_RESOURCE}))\n
.REQUIRED_ATTR(embedding_dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingApplyAdaGrad"
    op.name = next_unique_name(node_name, "EmbeddingApplyAdaGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var_handle.tensor)
    op.input_desc.add().CopyFrom(var_handle.desc)
    op.input_desc[-1].name = "var_handle"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(global_step.tensor)
    op.input_desc.add().CopyFrom(global_step.desc)
    op.input_desc[-1].name = "global_step"

    # process attrs
    op.attr["embedding_dim"].i = embedding_dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var_handle"
    var_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var_handle


# This api is auto-generated from IR EmbeddingComputeVarExport
@auto_convert_to_tensor([False, False, False], [False, False, False])
def EmbeddingComputeVarExport(file_path: Tensor, ps_id: Tensor, table_id: Tensor, *, table_name: List[str]=[], dependencies=[], node_name=None):
    """REG_OP(EmbeddingComputeVarExport)\n
.INPUT(file_path, TensorType({DT_STRING}))\n
.INPUT(ps_id, TensorType({DT_INT32}))\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.ATTR(table_name, ListString, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingComputeVarExport"
    op.name = next_unique_name(node_name, "EmbeddingComputeVarExport")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(file_path.tensor)
    op.input_desc.add().CopyFrom(file_path.desc)
    op.input_desc[-1].name = "file_path"
    op.input.append(ps_id.tensor)
    op.input_desc.add().CopyFrom(ps_id.desc)
    op.input_desc[-1].name = "ps_id"
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs
    op.attr["table_name"].list.val_type = 0
    op.attr["table_name"].list.s.extend(compat_as_bytes_list(table_name))

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR EmbeddingComputeVarImport
@auto_convert_to_tensor([False, False, False], [False, False, False])
def EmbeddingComputeVarImport(file_path: Tensor, ps_id: Tensor, table_id: Tensor, *, table_name: List[str]=[], dependencies=[], node_name=None):
    """REG_OP(EmbeddingComputeVarImport)\n
.INPUT(file_path, TensorType({DT_STRING}))\n
.INPUT(ps_id, TensorType({DT_INT32}))\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.ATTR(table_name, ListString, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingComputeVarImport"
    op.name = next_unique_name(node_name, "EmbeddingComputeVarImport")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(file_path.tensor)
    op.input_desc.add().CopyFrom(file_path.desc)
    op.input_desc[-1].name = "file_path"
    op.input.append(ps_id.tensor)
    op.input_desc.add().CopyFrom(ps_id.desc)
    op.input_desc[-1].name = "ps_id"
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs
    op.attr["table_name"].list.val_type = 0
    op.attr["table_name"].list.s.extend(compat_as_bytes_list(table_name))

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR FusedBiasLeakyRelu
@auto_convert_to_tensor([False, False], [False, False])
def FusedBiasLeakyRelu(x: Tensor, bias: Tensor, *, negative_slope: float=0.200000, scale: float=1.414214, dependencies=[], node_name=None):
    """REG_OP(FusedBiasLeakyRelu)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
.INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
.ATTR(negative_slope, Float, 0.2)\n
.ATTR(scale, Float, 1.414213562373)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedBiasLeakyRelu"
    op.name = next_unique_name(node_name, "FusedBiasLeakyRelu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["negative_slope"].f = negative_slope
    op.attr["scale"].f = scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FusedBiasLeakyReluGrad
@auto_convert_to_tensor([False, False], [False, False])
def FusedBiasLeakyReluGrad(y_grad: Tensor, features: Tensor, *, negative_slope: float=0.200000, scale: float=1.414214, dependencies=[], node_name=None):
    """REG_OP(FusedBiasLeakyReluGrad)\n
.INPUT(y_grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(features, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(negative_slope, Float, 0.2)\n
.ATTR(scale, Float, 1.414213562373)\n
.OUTPUT(x_grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedBiasLeakyReluGrad"
    op.name = next_unique_name(node_name, "FusedBiasLeakyReluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs
    op.attr["negative_slope"].f = negative_slope
    op.attr["scale"].f = scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR MemSet
@auto_convert_to_tensor([], [])
def MemSet(*, sizes: List[int], dtypes: List[int]=[], values_int: List[int]=[], values_float: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(MemSet)\n
.REQUIRED_ATTR(sizes, ListInt)\n
.ATTR(dtypes, ListType, {})\n
.ATTR(values_int, ListInt, {})\n
.ATTR(values_float, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MemSet"
    op.name = next_unique_name(node_name, "MemSet")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["sizes"].list.val_type = 2
    op.attr["sizes"].list.i.extend(sizes)
    op.attr["dtypes"].list.val_type = 10
    op.attr["dtypes"].list.dt.extend(dtypes)
    op.attr["values_int"].list.val_type = 2
    op.attr["values_int"].list.i.extend(values_int)
    op.attr["values_float"].list.val_type = 3
    op.attr["values_float"].list.f.extend(values_float)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR DeformableRoiPoolGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True])
def DeformableRoiPoolGrad(grad: Tensor, x: Tensor, rois: Tensor, offset: Optional[Tensor], *, output_size: List[int], spatial_scale: float=1.000000, sampling_ratio: int=0, gamma: float=0.100000, dependencies=[], node_name=None):
    """REG_OP(DeformableRoiPoolGrad)\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(offset, TensorType({DT_FLOAT}))\n
.OUTPUT(grad_x, TensorType({DT_FLOAT}))\n
.OUTPUT(grad_offset, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(output_size, ListInt)\n
.ATTR(spatial_scale, Float, 1.0)\n
.ATTR(sampling_ratio, Int, 0)\n
.ATTR(gamma, Float, 0.1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeformableRoiPoolGrad"
    op.name = next_unique_name(node_name, "DeformableRoiPoolGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["sampling_ratio"].i = sampling_ratio
    op.attr["gamma"].f = gamma

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_x"
    grad_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "grad_offset"
    grad_offset = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_x, grad_offset


# This api is auto-generated from IR SearchN
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SearchN(x: Tensor, scale_d: Tensor, scale_w: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SearchN)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(scale_d, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(scale_w, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(n, TensorType({DT_INT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SearchN"
    op.name = next_unique_name(node_name, "SearchN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale_d.tensor)
    op.input_desc.add().CopyFrom(scale_d.desc)
    op.input_desc[-1].name = "scale_d"
    op.input.append(scale_w.tensor)
    op.input_desc.add().CopyFrom(scale_w.desc)
    op.input_desc[-1].name = "scale_w"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "n"
    n = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return n


# This api is auto-generated from IR AdaptiveAvgPoolAssistMatrix
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def AdaptiveAvgPoolAssistMatrix(input_size: Tensor, output_size: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdaptiveAvgPoolAssistMatrix)\n
.INPUT(input_size, TensorType({DT_INT64, DT_INT32}))\n
.INPUT(output_size, TensorType({DT_INT64, DT_INT32}))\n
.OUTPUT(left_matrix, TensorType({DT_FLOAT}))\n
.OUTPUT(right_matrix, TensorType({DT_FLOAT}))\n
.OUTPUT(weight_matrix, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdaptiveAvgPoolAssistMatrix"
    op.name = next_unique_name(node_name, "AdaptiveAvgPoolAssistMatrix")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_size.tensor)
    op.input_desc.add().CopyFrom(input_size.desc)
    op.input_desc[-1].name = "input_size"
    op.input.append(output_size.tensor)
    op.input_desc.add().CopyFrom(output_size.desc)
    op.input_desc[-1].name = "output_size"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "left_matrix"
    left_matrix = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "right_matrix"
    right_matrix = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "weight_matrix"
    weight_matrix = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return left_matrix, right_matrix, weight_matrix


# This api is auto-generated from IR AdaptiveAvgPool2dAssistMatrix
@auto_convert_to_tensor([False], [False])
def AdaptiveAvgPool2dAssistMatrix(input_size: Tensor, *, output_size: List[int], dependencies=[], node_name=None):
    """REG_OP(AdaptiveAvgPool2dAssistMatrix)\n
.INPUT(input_size, TensorType({DT_INT64}))\n
.OUTPUT(left_matrix, TensorType({DT_FLOAT}))\n
.OUTPUT(right_matrix, TensorType({DT_FLOAT}))\n
.OUTPUT(weight_matrix, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(output_size, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdaptiveAvgPool2dAssistMatrix"
    op.name = next_unique_name(node_name, "AdaptiveAvgPool2dAssistMatrix")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_size.tensor)
    op.input_desc.add().CopyFrom(input_size.desc)
    op.input_desc[-1].name = "input_size"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "left_matrix"
    left_matrix = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "right_matrix"
    right_matrix = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "weight_matrix"
    weight_matrix = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return left_matrix, right_matrix, weight_matrix


# This api is auto-generated from IR CorrectBBox
@auto_convert_to_tensor([False, False, False], [False, False, False])
def CorrectBBox(x: Tensor, grid: Tensor, anchor_grid: Tensor, *, stride: int, yolo_version: str, dependencies=[], node_name=None):
    """REG_OP(CorrectBBox)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(grid, TensorType({DT_FLOAT16}))\n
.INPUT(anchor_grid, TensorType({DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(stride, Int)\n
.REQUIRED_ATTR(yolo_version, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CorrectBBox"
    op.name = next_unique_name(node_name, "CorrectBBox")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grid.tensor)
    op.input_desc.add().CopyFrom(grid.desc)
    op.input_desc[-1].name = "grid"
    op.input.append(anchor_grid.tensor)
    op.input_desc.add().CopyFrom(anchor_grid.desc)
    op.input_desc[-1].name = "anchor_grid"

    # process attrs
    op.attr["stride"].i = stride
    op.attr["yolo_version"].s = compat_as_bytes(yolo_version)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DeformableRoiPool
@auto_convert_to_tensor([False, False, False], [False, False, True])
def DeformableRoiPool(x: Tensor, rois: Tensor, offset: Optional[Tensor], *, output_size: List[int], spatial_scale: float=1.000000, sampling_ratio: int=0, gamma: float=0.100000, dependencies=[], node_name=None):
    """REG_OP(DeformableRoiPool)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(offset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(spatial_scale, Float, 1.0)\n
.REQUIRED_ATTR(output_size, ListInt)\n
.ATTR(sampling_ratio, Int, 0)\n
.ATTR(gamma, Float, 0.1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeformableRoiPool"
    op.name = next_unique_name(node_name, "DeformableRoiPool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["sampling_ratio"].i = sampling_ratio
    op.attr["gamma"].f = gamma

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PSAMask
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def PSAMask(x: Tensor, *, psa_type: int, num: int, h_feature: int, w_feature: int, h_mask: int, w_mask: int, half_h_mask: int, half_w_mask: int, dependencies=[], node_name=None):
    """REG_OP(PSAMask)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(psa_type, Int)\n
.REQUIRED_ATTR(num, Int)\n
.REQUIRED_ATTR(h_feature, Int)\n
.REQUIRED_ATTR(w_feature, Int)\n
.REQUIRED_ATTR(h_mask, Int)\n
.REQUIRED_ATTR(w_mask, Int)\n
.REQUIRED_ATTR(half_h_mask, Int)\n
.REQUIRED_ATTR(half_w_mask, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PSAMask"
    op.name = next_unique_name(node_name, "PSAMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["psa_type"].i = psa_type
    op.attr["num"].i = num
    op.attr["h_feature"].i = h_feature
    op.attr["w_feature"].i = w_feature
    op.attr["h_mask"].i = h_mask
    op.attr["w_mask"].i = w_mask
    op.attr["half_h_mask"].i = half_h_mask
    op.attr["half_w_mask"].i = half_w_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PSAMaskGrad
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def PSAMaskGrad(y_grad: Tensor, *, psa_type: int, num: int, h_feature: int, w_feature: int, h_mask: int, w_mask: int, half_h_mask: int, half_w_mask: int, dependencies=[], node_name=None):
    """REG_OP(PSAMaskGrad)\n
.INPUT(y_grad, TensorType::BasicType())\n
.OUTPUT(x_grad, TensorType::BasicType())\n
.REQUIRED_ATTR(psa_type, Int)\n
.REQUIRED_ATTR(num, Int)\n
.REQUIRED_ATTR(h_feature, Int)\n
.REQUIRED_ATTR(w_feature, Int)\n
.REQUIRED_ATTR(h_mask, Int)\n
.REQUIRED_ATTR(w_mask, Int)\n
.REQUIRED_ATTR(half_h_mask, Int)\n
.REQUIRED_ATTR(half_w_mask, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PSAMaskGrad"
    op.name = next_unique_name(node_name, "PSAMaskGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"

    # process attrs
    op.attr["psa_type"].i = psa_type
    op.attr["num"].i = num
    op.attr["h_feature"].i = h_feature
    op.attr["w_feature"].i = w_feature
    op.attr["h_mask"].i = h_mask
    op.attr["w_mask"].i = w_mask
    op.attr["half_h_mask"].i = half_h_mask
    op.attr["half_w_mask"].i = half_w_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR BallQuery
@auto_convert_to_tensor([False, False], [False, False])
def BallQuery(xyz: Tensor, center_xyz: Tensor, *, min_radius: float, max_radius: float, sample_num: int, dependencies=[], node_name=None):
    """REG_OP(BallQuery)\n
.INPUT(xyz, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(center_xyz, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(idx, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(min_radius, Float)\n
.REQUIRED_ATTR(max_radius, Float)\n
.REQUIRED_ATTR(sample_num, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BallQuery"
    op.name = next_unique_name(node_name, "BallQuery")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(xyz.tensor)
    op.input_desc.add().CopyFrom(xyz.desc)
    op.input_desc[-1].name = "xyz"
    op.input.append(center_xyz.tensor)
    op.input_desc.add().CopyFrom(center_xyz.desc)
    op.input_desc[-1].name = "center_xyz"

    # process attrs
    op.attr["min_radius"].f = min_radius
    op.attr["max_radius"].f = max_radius
    op.attr["sample_num"].i = sample_num

    # process outputs
    output_index = 0
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return idx


# This api is auto-generated from IR StackBallQuery
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def StackBallQuery(xyz: Tensor, center_xyz: Tensor, xyz_batch_cnt: Tensor, center_xyz_batch_cnt: Tensor, *, max_radius: float, sample_num: int, dependencies=[], node_name=None):
    """REG_OP(StackBallQuery)\n
.INPUT(xyz, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(center_xyz, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(xyz_batch_cnt, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(center_xyz_batch_cnt, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(idx, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(max_radius, Float)\n
.REQUIRED_ATTR(sample_num, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StackBallQuery"
    op.name = next_unique_name(node_name, "StackBallQuery")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(xyz.tensor)
    op.input_desc.add().CopyFrom(xyz.desc)
    op.input_desc[-1].name = "xyz"
    op.input.append(center_xyz.tensor)
    op.input_desc.add().CopyFrom(center_xyz.desc)
    op.input_desc[-1].name = "center_xyz"
    op.input.append(xyz_batch_cnt.tensor)
    op.input_desc.add().CopyFrom(xyz_batch_cnt.desc)
    op.input_desc[-1].name = "xyz_batch_cnt"
    op.input.append(center_xyz_batch_cnt.tensor)
    op.input_desc.add().CopyFrom(center_xyz_batch_cnt.desc)
    op.input_desc[-1].name = "center_xyz_batch_cnt"

    # process attrs
    op.attr["max_radius"].f = max_radius
    op.attr["sample_num"].i = sample_num

    # process outputs
    output_index = 0
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return idx


# This api is auto-generated from IR HcomRemoteLookup
@auto_convert_to_tensor([False, False], [False, False])
def HcomRemoteLookup(keys: Tensor, table_id: Tensor, *, tag: int, max_num: int, embedding_dim: int, insert_option: int=0, dependencies=[], node_name=None):
    """REG_OP(HcomRemoteLookup)\n
.INPUT(keys, TensorType({DT_INT64}))\n
.INPUT(table_id, Int)\n
.OUTPUT(values, TensorType({DT_FP32}))\n
.REQUIRED_ATTR(tag, Int)\n
.ATTR(insert_option, Int, 0)\n
.REQUIRED_ATTR(max_num, Int)\n
.REQUIRED_ATTR(embedding_dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomRemoteLookup"
    op.name = next_unique_name(node_name, "HcomRemoteLookup")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"

    # process attrs
    op.attr["tag"].i = tag
    op.attr["max_num"].i = max_num
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["insert_option"].i = insert_option

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values


# This api is auto-generated from IR HcomCollRemoteLookup
@auto_convert_to_tensor([False, False], [False, False])
def HcomCollRemoteLookup(table_id: Tensor, keys: Tensor, *, tag: int, max_num: int, embedding_dim: int, insert_option: int=0, group: str="hccl_world_group", flags: int=0, dependencies=[], node_name=None):
    """REG_OP(HcomCollRemoteLookup)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_FP32}))\n
.REQUIRED_ATTR(tag, Int)\n
.ATTR(insert_option, Int, 0)\n
.ATTR(group, String, "hccl_world_group")\n
.REQUIRED_ATTR(max_num, Int)\n
.REQUIRED_ATTR(embedding_dim, Int)\n
.ATTR(flags, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomCollRemoteLookup"
    op.name = next_unique_name(node_name, "HcomCollRemoteLookup")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"

    # process attrs
    op.attr["tag"].i = tag
    op.attr["max_num"].i = max_num
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["insert_option"].i = insert_option
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["flags"].i = flags

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values


# This api is auto-generated from IR HcomCollRemoteUpdate
@auto_convert_to_tensor([False, False, False], [False, False, False])
def HcomCollRemoteUpdate(table_id: Tensor, keys: Tensor, values: Tensor, *, tag: int, max_num: int, embedding_dim: int, group: str="hccl_world_group", dependencies=[], node_name=None):
    """REG_OP(HcomCollRemoteUpdate)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_FP32}))\n
.REQUIRED_ATTR(tag, Int)\n
.ATTR(group, String, "hccl_world_group")\n
.REQUIRED_ATTR(max_num, Int)\n
.REQUIRED_ATTR(embedding_dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomCollRemoteUpdate"
    op.name = next_unique_name(node_name, "HcomCollRemoteUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"

    # process attrs
    op.attr["tag"].i = tag
    op.attr["max_num"].i = max_num
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["group"].s = compat_as_bytes(group)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR HcomCollRemoteLookupPaired
@auto_convert_to_tensor([False, False], [False, False])
def HcomCollRemoteLookupPaired(table_id: Tensor, keys: Tensor, *, tag: int, max_num: int, embedding_dim: int, insert_option: int=0, group: str="hccl_world_group", flags: int=0, dependencies=[], node_name=None):
    """REG_OP(HcomCollRemoteLookupPaired)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_FP32}))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(num_uniqued, TensorType({DT_INT64}))\n
.OUTPUT(ps_segments, TesnorType({DT_INT64}))\n
.OUTPUT(ps_segments_num, TesnorType({DT_INT64}))\n
.REQUIRED_ATTR(tag, Int)\n
.ATTR(insert_option, Int, 0)\n
.ATTR(group, String, "hccl_world_group")\n
.REQUIRED_ATTR(max_num, Int)\n
.REQUIRED_ATTR(embedding_dim, Int)\n
.ATTR(flags, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomCollRemoteLookupPaired"
    op.name = next_unique_name(node_name, "HcomCollRemoteLookupPaired")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"

    # process attrs
    op.attr["tag"].i = tag
    op.attr["max_num"].i = max_num
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["insert_option"].i = insert_option
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["flags"].i = flags

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "num_uniqued"
    num_uniqued = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ps_segments"
    ps_segments = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ps_segments_num"
    ps_segments_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values, indices, num_uniqued, ps_segments, ps_segments_num


# This api is auto-generated from IR HcomCollRemoteUpdatePaired
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def HcomCollRemoteUpdatePaired(table_id: Tensor, keys: Tensor, values: Tensor, indices: Tensor, num_uniqued: Tensor, ps_segments: Tensor, ps_segments_num: Tensor, *, tag: int, max_num: int, embedding_dim: int, group: str="hccl_world_group", dependencies=[], node_name=None):
    """REG_OP(HcomCollRemoteUpdatePaired)\n
.INPUT(table_id, TensorType({DT_INT32}))\n
.INPUT(keys, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_FP32}))\n
.INPUT(indices, TesnorType({DT_INT64}))\n
.INPUT(num_uniqued, TesnorType({DT_INT64}))\n
.INPUT(ps_segments, TesnorType({DT_INT64}))\n
.INPUT(ps_segments_num, TesnorType({DT_INT64}))\n
.REQUIRED_ATTR(tag, Int)\n
.ATTR(group, String, "hccl_world_group")\n
.REQUIRED_ATTR(max_num, Int)\n
.REQUIRED_ATTR(embedding_dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomCollRemoteUpdatePaired"
    op.name = next_unique_name(node_name, "HcomCollRemoteUpdatePaired")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_id.tensor)
    op.input_desc.add().CopyFrom(table_id.desc)
    op.input_desc[-1].name = "table_id"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(num_uniqued.tensor)
    op.input_desc.add().CopyFrom(num_uniqued.desc)
    op.input_desc[-1].name = "num_uniqued"
    op.input.append(ps_segments.tensor)
    op.input_desc.add().CopyFrom(ps_segments.desc)
    op.input_desc[-1].name = "ps_segments"
    op.input.append(ps_segments_num.tensor)
    op.input_desc.add().CopyFrom(ps_segments_num.desc)
    op.input_desc[-1].name = "ps_segments_num"

    # process attrs
    op.attr["tag"].i = tag
    op.attr["max_num"].i = max_num
    op.attr["embedding_dim"].i = embedding_dim
    op.attr["group"].s = compat_as_bytes(group)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR HcomGather
@auto_convert_to_tensor([False], [False])
def HcomGather(x: Tensor, *, root_rank: int, group: str, rank_size: int, dependencies=[], node_name=None):
    """REG_OP(HcomGather)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64}))\n
.REQUIRED_ATTR(root_rank, Int)\n
.REQUIRED_ATTR(group, String)\n
.REQUIRED_ATTR(rank_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomGather"
    op.name = next_unique_name(node_name, "HcomGather")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["root_rank"].i = root_rank
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["rank_size"].i = rank_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MinAreaPolygons
@auto_convert_to_tensor([False], [False])
def MinAreaPolygons(pointsets: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MinAreaPolygons)\n
.INPUT(pointsets, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(polygons, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MinAreaPolygons"
    op.name = next_unique_name(node_name, "MinAreaPolygons")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(pointsets.tensor)
    op.input_desc.add().CopyFrom(pointsets.desc)
    op.input_desc[-1].name = "pointsets"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "polygons"
    polygons = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return polygons


# This api is auto-generated from IR PointsInPolygons
@auto_convert_to_tensor([False, False], [False, False])
def PointsInPolygons(points: Tensor, polygons: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(PointsInPolygons)\n
.INPUT(points, TensorType({DT_FLOAT}))\n
.INPUT(polygons, TensorType({DT_FLOAT}))\n
.OUTPUT(output, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PointsInPolygons"
    op.name = next_unique_name(node_name, "PointsInPolygons")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(points.tensor)
    op.input_desc.add().CopyFrom(points.desc)
    op.input_desc[-1].name = "points"
    op.input.append(polygons.tensor)
    op.input_desc.add().CopyFrom(polygons.desc)
    op.input_desc[-1].name = "polygons"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR ThreeNN
@auto_convert_to_tensor([False, False], [False, False])
def ThreeNN(xyz1: Tensor, xyz2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ThreeNN)\n
.INPUT(xyz1, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(xyz2, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(dist, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(idx, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ThreeNN"
    op.name = next_unique_name(node_name, "ThreeNN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(xyz1.tensor)
    op.input_desc.add().CopyFrom(xyz1.desc)
    op.input_desc[-1].name = "xyz1"
    op.input.append(xyz2.tensor)
    op.input_desc.add().CopyFrom(xyz2.desc)
    op.input_desc[-1].name = "xyz2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dist"
    dist = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "idx"
    idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dist, idx


# This api is auto-generated from IR Voxelization
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Voxelization(points: Tensor, voxel_size: Tensor, coors_range: Tensor, *, max_points: int=35, max_voxels: int=20000, deterministic: bool=True, dependencies=[], node_name=None):
    """REG_OP(Voxelization)\n
.INPUT(points, TensorType({DT_DOUBLE,DT_FLOAT,DT_FLOAT16}))\n
.INPUT(voxel_size, TensorType({DT_DOUBLE,DT_FLOAT,DT_FLOAT16}))\n
.INPUT(coors_range, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(voxels, TensorType({DT_DOUBLE,DT_FLOAT,DT_FLOAT16}))\n
.OUTPUT(coors, TensorType({DT_INT32}))\n
.OUTPUT(num_points_per_voxel, TensorType({DT_INT32}))\n
.OUTPUT(voxel_num, TensorType({DT_INT32}))\n
.ATTR(max_points, Int, 35)\n
.ATTR(max_voxels, Int, 20000)\n
.ATTR(deterministic, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Voxelization"
    op.name = next_unique_name(node_name, "Voxelization")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(points.tensor)
    op.input_desc.add().CopyFrom(points.desc)
    op.input_desc[-1].name = "points"
    op.input.append(voxel_size.tensor)
    op.input_desc.add().CopyFrom(voxel_size.desc)
    op.input_desc[-1].name = "voxel_size"
    op.input.append(coors_range.tensor)
    op.input_desc.add().CopyFrom(coors_range.desc)
    op.input_desc[-1].name = "coors_range"

    # process attrs
    op.attr["max_points"].i = max_points
    op.attr["max_voxels"].i = max_voxels
    op.attr["deterministic"].b = deterministic

    # process outputs
    output_index = 0
    op.output_desc.add().name = "voxels"
    voxels = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "coors"
    coors = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "num_points_per_voxel"
    num_points_per_voxel = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "voxel_num"
    voxel_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return voxels, coors, num_points_per_voxel, voxel_num


# This api is auto-generated from IR ActiveRotatedFilter
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ActiveRotatedFilter(x: Tensor, indices: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ActiveRotatedFilter)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32}))\n
.INPUT(indices, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ActiveRotatedFilter"
    op.name = next_unique_name(node_name, "ActiveRotatedFilter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ActiveRotatedFilterGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ActiveRotatedFilterGrad(y_grad: Tensor, indices: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ActiveRotatedFilterGrad)\n
.INPUT(y_grad, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32}))\n
.INPUT(indices, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(x_grad, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ActiveRotatedFilterGrad"
    op.name = next_unique_name(node_name, "ActiveRotatedFilterGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR BlendFaceBgPartOne
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def BlendFaceBgPartOne(face_img: Tensor, face_rect: Tensor, face_mask: Tensor, acc_face: Tensor, acc_mask: Tensor, max_mask: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BlendFaceBgPartOne)\n
.INPUT(face_img, TensorType({DT_UINT8, DT_FLOAT}))\n
.INPUT(face_rect, TensorType({DT_INT32}))\n
.INPUT(face_mask, TensorType({DT_FLOAT}))\n
.INPUT(acc_face, TensorType({DT_FLOAT}))\n
.INPUT(acc_mask, TensorType({DT_FLOAT}))\n
.INPUT(max_mask, TensorType({DT_FLOAT}))\n
.OUTPUT(acc_face, TensorType({DT_FLOAT}))\n
.OUTPUT(acc_mask, TensorType({DT_FLOAT}))\n
.OUTPUT(max_mask, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BlendFaceBgPartOne"
    op.name = next_unique_name(node_name, "BlendFaceBgPartOne")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(face_img.tensor)
    op.input_desc.add().CopyFrom(face_img.desc)
    op.input_desc[-1].name = "face_img"
    op.input.append(face_rect.tensor)
    op.input_desc.add().CopyFrom(face_rect.desc)
    op.input_desc[-1].name = "face_rect"
    op.input.append(face_mask.tensor)
    op.input_desc.add().CopyFrom(face_mask.desc)
    op.input_desc[-1].name = "face_mask"
    op.input.append(acc_face.tensor)
    op.input_desc.add().CopyFrom(acc_face.desc)
    op.input_desc[-1].name = "acc_face"
    op.input.append(acc_mask.tensor)
    op.input_desc.add().CopyFrom(acc_mask.desc)
    op.input_desc[-1].name = "acc_mask"
    op.input.append(max_mask.tensor)
    op.input_desc.add().CopyFrom(max_mask.desc)
    op.input_desc[-1].name = "max_mask"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "acc_face"
    acc_face = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "acc_mask"
    acc_mask = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "max_mask"
    max_mask = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return acc_face, acc_mask, max_mask


# This api is auto-generated from IR BlendFaceBgPartTwo
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def BlendFaceBgPartTwo(acc_face: Tensor, acc_mask: Tensor, max_mask: Tensor, bg_img: Tensor, *, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(BlendFaceBgPartTwo)\n
.INPUT(acc_face, TensorType({DT_FLOAT}))\n
.INPUT(acc_mask, TensorType({DT_FLOAT}))\n
.INPUT(max_mask, TensorType({DT_FLOAT}))\n
.INPUT(bg_img, TensorType({DT_UINT8, DT_FLOAT}))\n
.OUTPUT(fused_img, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 1e-12)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BlendFaceBgPartTwo"
    op.name = next_unique_name(node_name, "BlendFaceBgPartTwo")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(acc_face.tensor)
    op.input_desc.add().CopyFrom(acc_face.desc)
    op.input_desc[-1].name = "acc_face"
    op.input.append(acc_mask.tensor)
    op.input_desc.add().CopyFrom(acc_mask.desc)
    op.input_desc[-1].name = "acc_mask"
    op.input.append(max_mask.tensor)
    op.input_desc.add().CopyFrom(max_mask.desc)
    op.input_desc[-1].name = "max_mask"
    op.input.append(bg_img.tensor)
    op.input_desc.add().CopyFrom(bg_img.desc)
    op.input_desc[-1].name = "bg_img"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "fused_img"
    fused_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return fused_img


# This api is auto-generated from IR ImgRawDecodePostHandle
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def ImgRawDecodePostHandle(img_channel_0: Tensor, img_channel_1: Tensor, img_channel_2: Tensor, img_channel_3: Tensor, img_size: Tensor, gamma: Tensor, *, bayer_pattern: str="binning", dependencies=[], node_name=None):
    """REG_OP(ImgRawDecodePostHandle)\n
.INPUT(img_channel_0, TensorType({DT_UINT16}))\n
.INPUT(img_channel_1, TensorType({DT_UINT16}))\n
.INPUT(img_channel_2, TensorType({DT_UINT16}))\n
.INPUT(img_channel_3, TensorType({DT_UINT16}))\n
.INPUT(img_size, TensorType({DT_INT32}))\n
.INPUT(gamma, TensorType({DT_FLOAT}))\n
.OUTPUT(raw_img, TensorType({DT_UINT16}))\n
.ATTR(bayer_pattern, String, "binning")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ImgRawDecodePostHandle"
    op.name = next_unique_name(node_name, "ImgRawDecodePostHandle")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img_channel_0.tensor)
    op.input_desc.add().CopyFrom(img_channel_0.desc)
    op.input_desc[-1].name = "img_channel_0"
    op.input.append(img_channel_1.tensor)
    op.input_desc.add().CopyFrom(img_channel_1.desc)
    op.input_desc[-1].name = "img_channel_1"
    op.input.append(img_channel_2.tensor)
    op.input_desc.add().CopyFrom(img_channel_2.desc)
    op.input_desc[-1].name = "img_channel_2"
    op.input.append(img_channel_3.tensor)
    op.input_desc.add().CopyFrom(img_channel_3.desc)
    op.input_desc[-1].name = "img_channel_3"
    op.input.append(img_size.tensor)
    op.input_desc.add().CopyFrom(img_size.desc)
    op.input_desc[-1].name = "img_size"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs
    op.attr["bayer_pattern"].s = compat_as_bytes(bayer_pattern)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "raw_img"
    raw_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return raw_img


# This api is auto-generated from IR ImgRawDecodePostHandleV2
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False])
def ImgRawDecodePostHandleV2(img_channel_0: Tensor, img_channel_1: Tensor, img_channel_2: Tensor, img_channel_3: Tensor, gamma: Tensor, bayer_coordinate: Tensor, bayer_params: Tensor, bayer_ptn: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ImgRawDecodePostHandleV2)\n
.INPUT(img_channel_0, TensorType({DT_UINT16}))\n
.INPUT(img_channel_1, TensorType({DT_UINT16}))\n
.INPUT(img_channel_2, TensorType({DT_UINT16}))\n
.INPUT(img_channel_3, TensorType({DT_UINT16}))\n
.INPUT(gamma, TensorType({DT_FLOAT}))\n
.INPUT(bayer_coordinate, TensorType({DT_INT32}))\n
.INPUT(bayer_params, TensorType({DT_FLOAT}))\n
.INPUT(bayer_ptn, TensorType({DT_INT32}))\n
.OUTPUT(raw_img, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ImgRawDecodePostHandleV2"
    op.name = next_unique_name(node_name, "ImgRawDecodePostHandleV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img_channel_0.tensor)
    op.input_desc.add().CopyFrom(img_channel_0.desc)
    op.input_desc[-1].name = "img_channel_0"
    op.input.append(img_channel_1.tensor)
    op.input_desc.add().CopyFrom(img_channel_1.desc)
    op.input_desc[-1].name = "img_channel_1"
    op.input.append(img_channel_2.tensor)
    op.input_desc.add().CopyFrom(img_channel_2.desc)
    op.input_desc[-1].name = "img_channel_2"
    op.input.append(img_channel_3.tensor)
    op.input_desc.add().CopyFrom(img_channel_3.desc)
    op.input_desc[-1].name = "img_channel_3"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(bayer_coordinate.tensor)
    op.input_desc.add().CopyFrom(bayer_coordinate.desc)
    op.input_desc[-1].name = "bayer_coordinate"
    op.input.append(bayer_params.tensor)
    op.input_desc.add().CopyFrom(bayer_params.desc)
    op.input_desc[-1].name = "bayer_params"
    op.input.append(bayer_ptn.tensor)
    op.input_desc.add().CopyFrom(bayer_ptn.desc)
    op.input_desc[-1].name = "bayer_ptn"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "raw_img"
    raw_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return raw_img


# This api is auto-generated from IR YUV4442YUV422
@auto_convert_to_tensor([False], [False])
def YUV4442YUV422(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(YUV4442YUV422)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YUV4442YUV422"
    op.name = next_unique_name(node_name, "YUV4442YUV422")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RGB2YUV422
@auto_convert_to_tensor([False], [False])
def RGB2YUV422(rgb: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RGB2YUV422)\n
.INPUT(rgb, TensorType({DT_UINT8}))\n
.OUTPUT(yuv, TensorType({DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RGB2YUV422"
    op.name = next_unique_name(node_name, "RGB2YUV422")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(rgb.tensor)
    op.input_desc.add().CopyFrom(rgb.desc)
    op.input_desc[-1].name = "rgb"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "yuv"
    yuv = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return yuv


# This api is auto-generated from IR FlashAttentionScore
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True, True, True, True, True])
def FlashAttentionScore(query: Tensor, key: Tensor, value: Tensor, real_shift: Optional[Tensor], drop_mask: Optional[Tensor], padding_mask: Optional[Tensor], atten_mask: Optional[Tensor], prefix: Optional[Tensor], actual_seq_qlen: Optional[Tensor], actual_seq_kvlen: Optional[Tensor], q_start_idx: Optional[Tensor], kv_start_idx: Optional[Tensor], *, head_num: int, input_layout: str, scale_value: float=1.000000, keep_prob: float=1.000000, pre_tockens: int=2147483647, next_tockens: int=2147483647, inner_precise: int=1, sparse_mode: int=0, dependencies=[], node_name=None):
    """REG_OP(FlashAttentionScore)\n
.INPUT(query, TensorType({DT_FLOAT16, DT_BF16}))\n
.INPUT(key, TensorType({DT_FLOAT16, DT_BF16}))\n
.INPUT(value, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(real_shift, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(drop_mask, TensorType({DT_UINT8}))\n
.OPTIONAL_INPUT(padding_mask, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(atten_mask, TensorType({DT_BOOL, DT_UINT8}))\n
.OPTIONAL_INPUT(prefix, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(actual_seq_qlen, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(actual_seq_kvlen, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(q_start_idx, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(kv_start_idx, TensorType({DT_INT64}))\n
.OUTPUT(softmax_max, TensorType({DT_FLOAT32}))\n
.OUTPUT(softmax_sum, TensorType({DT_FLOAT32}))\n
.OUTPUT(softmax_out, TensorType({DT_FLOAT16, DT_BF16}))\n
.OUTPUT(attention_out, TensorType({DT_FLOAT16, DT_BF16}))\n
.ATTR(scale_value, Float, 1.0)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(pre_tockens, Int, 2147483647)\n
.ATTR(next_tockens, Int, 2147483647)\n
.REQUIRED_ATTR(head_num, Int)\n
.REQUIRED_ATTR(input_layout, String)\n
.ATTR(inner_precise, Int, 1)\n
.ATTR(sparse_mode, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FlashAttentionScore"
    op.name = next_unique_name(node_name, "FlashAttentionScore")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    if real_shift is not None:
        op.input.append(real_shift.tensor)
        op.input_desc.add().CopyFrom(real_shift.desc)
        op.input_desc[-1].name = "real_shift"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "real_shift"
    if drop_mask is not None:
        op.input.append(drop_mask.tensor)
        op.input_desc.add().CopyFrom(drop_mask.desc)
        op.input_desc[-1].name = "drop_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "drop_mask"
    if padding_mask is not None:
        op.input.append(padding_mask.tensor)
        op.input_desc.add().CopyFrom(padding_mask.desc)
        op.input_desc[-1].name = "padding_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "padding_mask"
    if atten_mask is not None:
        op.input.append(atten_mask.tensor)
        op.input_desc.add().CopyFrom(atten_mask.desc)
        op.input_desc[-1].name = "atten_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "atten_mask"
    if prefix is not None:
        op.input.append(prefix.tensor)
        op.input_desc.add().CopyFrom(prefix.desc)
        op.input_desc[-1].name = "prefix"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "prefix"
    if actual_seq_qlen is not None:
        op.input.append(actual_seq_qlen.tensor)
        op.input_desc.add().CopyFrom(actual_seq_qlen.desc)
        op.input_desc[-1].name = "actual_seq_qlen"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_qlen"
    if actual_seq_kvlen is not None:
        op.input.append(actual_seq_kvlen.tensor)
        op.input_desc.add().CopyFrom(actual_seq_kvlen.desc)
        op.input_desc[-1].name = "actual_seq_kvlen"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_kvlen"
    if q_start_idx is not None:
        op.input.append(q_start_idx.tensor)
        op.input_desc.add().CopyFrom(q_start_idx.desc)
        op.input_desc[-1].name = "q_start_idx"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "q_start_idx"
    if kv_start_idx is not None:
        op.input.append(kv_start_idx.tensor)
        op.input_desc.add().CopyFrom(kv_start_idx.desc)
        op.input_desc[-1].name = "kv_start_idx"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "kv_start_idx"

    # process attrs
    op.attr["head_num"].i = head_num
    op.attr["input_layout"].s = compat_as_bytes(input_layout)
    op.attr["scale_value"].f = scale_value
    op.attr["keep_prob"].f = keep_prob
    op.attr["pre_tockens"].i = pre_tockens
    op.attr["next_tockens"].i = next_tockens
    op.attr["inner_precise"].i = inner_precise
    op.attr["sparse_mode"].i = sparse_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "softmax_max"
    softmax_max = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "softmax_sum"
    softmax_sum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "softmax_out"
    softmax_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "attention_out"
    attention_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return softmax_max, softmax_sum, softmax_out, attention_out


# This api is auto-generated from IR FlashAttentionScoreGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True])
def FlashAttentionScoreGrad(query: Tensor, key: Tensor, value: Tensor, dy: Tensor, pse_shift: Optional[Tensor], drop_mask: Optional[Tensor], padding_mask: Optional[Tensor], atten_mask: Optional[Tensor], softmax_max: Optional[Tensor], softmax_sum: Optional[Tensor], softmax_in: Optional[Tensor], attention_in: Optional[Tensor], prefix: Optional[Tensor], actual_seq_qlen: Optional[Tensor], actual_seq_kvlen: Optional[Tensor], q_start_idx: Optional[Tensor], kv_start_idx: Optional[Tensor], *, head_num: int, input_layout: str, scale_value: float=1.000000, keep_prob: float=1.000000, pre_tockens: int=65536, next_tockens: int=65536, inner_precise: int=1, sparse_mode: int=0, dependencies=[], node_name=None):
    """REG_OP(FlashAttentionScoreGrad)\n
.INPUT(query, TensorType({DT_FLOAT16, DT_BF16}))\n
.INPUT(key, TensorType({DT_FLOAT16, DT_BF16}))\n
.INPUT(value, TensorType({DT_FLOAT16, DT_BF16}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(pse_shift, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(drop_mask, TensorType({DT_UINT8}))\n
.OPTIONAL_INPUT(padding_mask, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(atten_mask, TensorType({DT_BOOL, DT_UINT8}))\n
.OPTIONAL_INPUT(softmax_max, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(softmax_sum, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(softmax_in, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(attention_in, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(prefix, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(actual_seq_qlen, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(actual_seq_kvlen, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(q_start_idx, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(kv_start_idx, TensorType({DT_INT64}))\n
.OUTPUT(dq, TensorType({DT_FLOAT16, DT_BF16}))\n
.OUTPUT(dk, TensorType({DT_FLOAT16, DT_BF16}))\n
.OUTPUT(dv, TensorType({DT_FLOAT16, DT_BF16}))\n
.OUTPUT(dpse, TensorType({DT_FLOAT16, DT_BF16}))\n
.ATTR(scale_value, Float, 1.0)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(pre_tockens, Int, 65536)\n
.ATTR(next_tockens, Int, 65536)\n
.REQUIRED_ATTR(head_num, Int)\n
.REQUIRED_ATTR(input_layout, String)\n
.ATTR(inner_precise, Int, 1)\n
.ATTR(sparse_mode, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FlashAttentionScoreGrad"
    op.name = next_unique_name(node_name, "FlashAttentionScoreGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    if pse_shift is not None:
        op.input.append(pse_shift.tensor)
        op.input_desc.add().CopyFrom(pse_shift.desc)
        op.input_desc[-1].name = "pse_shift"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pse_shift"
    if drop_mask is not None:
        op.input.append(drop_mask.tensor)
        op.input_desc.add().CopyFrom(drop_mask.desc)
        op.input_desc[-1].name = "drop_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "drop_mask"
    if padding_mask is not None:
        op.input.append(padding_mask.tensor)
        op.input_desc.add().CopyFrom(padding_mask.desc)
        op.input_desc[-1].name = "padding_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "padding_mask"
    if atten_mask is not None:
        op.input.append(atten_mask.tensor)
        op.input_desc.add().CopyFrom(atten_mask.desc)
        op.input_desc[-1].name = "atten_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "atten_mask"
    if softmax_max is not None:
        op.input.append(softmax_max.tensor)
        op.input_desc.add().CopyFrom(softmax_max.desc)
        op.input_desc[-1].name = "softmax_max"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "softmax_max"
    if softmax_sum is not None:
        op.input.append(softmax_sum.tensor)
        op.input_desc.add().CopyFrom(softmax_sum.desc)
        op.input_desc[-1].name = "softmax_sum"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "softmax_sum"
    if softmax_in is not None:
        op.input.append(softmax_in.tensor)
        op.input_desc.add().CopyFrom(softmax_in.desc)
        op.input_desc[-1].name = "softmax_in"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "softmax_in"
    if attention_in is not None:
        op.input.append(attention_in.tensor)
        op.input_desc.add().CopyFrom(attention_in.desc)
        op.input_desc[-1].name = "attention_in"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "attention_in"
    if prefix is not None:
        op.input.append(prefix.tensor)
        op.input_desc.add().CopyFrom(prefix.desc)
        op.input_desc[-1].name = "prefix"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "prefix"
    if actual_seq_qlen is not None:
        op.input.append(actual_seq_qlen.tensor)
        op.input_desc.add().CopyFrom(actual_seq_qlen.desc)
        op.input_desc[-1].name = "actual_seq_qlen"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_qlen"
    if actual_seq_kvlen is not None:
        op.input.append(actual_seq_kvlen.tensor)
        op.input_desc.add().CopyFrom(actual_seq_kvlen.desc)
        op.input_desc[-1].name = "actual_seq_kvlen"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_kvlen"
    if q_start_idx is not None:
        op.input.append(q_start_idx.tensor)
        op.input_desc.add().CopyFrom(q_start_idx.desc)
        op.input_desc[-1].name = "q_start_idx"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "q_start_idx"
    if kv_start_idx is not None:
        op.input.append(kv_start_idx.tensor)
        op.input_desc.add().CopyFrom(kv_start_idx.desc)
        op.input_desc[-1].name = "kv_start_idx"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "kv_start_idx"

    # process attrs
    op.attr["head_num"].i = head_num
    op.attr["input_layout"].s = compat_as_bytes(input_layout)
    op.attr["scale_value"].f = scale_value
    op.attr["keep_prob"].f = keep_prob
    op.attr["pre_tockens"].i = pre_tockens
    op.attr["next_tockens"].i = next_tockens
    op.attr["inner_precise"].i = inner_precise
    op.attr["sparse_mode"].i = sparse_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dq"
    dq = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dk"
    dk = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dv"
    dv = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dpse"
    dpse = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dq, dk, dv, dpse


# This api is auto-generated from IR ScatterList
@auto_convert_to_tensor([True, False, False, False], [False, False, False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def _ScatterList(var: List[Tensor], indice: Tensor, updates: Tensor, mask: Optional[Tensor], *, size_of_var: int, reduce: str="update", axis: int=-2, dependencies=[], node_name=None):
    """REG_OP(ScatterList)\n
.DYNAMIC_INPUT(var, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.INPUT(indice, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.DYNAMIC_OUTPUT(var, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.ATTR(reduce, String, "update")\n
.ATTR(axis, Int, -2)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterList"
    op.name = next_unique_name(node_name, "ScatterList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(var, (tuple, list)):
        raise AssertionError("var must be a tuple or a list.")
    for i, v in enumerate(var):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "var" + str(i)
    op.input.append(indice.tensor)
    op.input_desc.add().CopyFrom(indice.desc)
    op.input_desc[-1].name = "indice"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["reduce"].s = compat_as_bytes(reduce)
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    var = []
    for i in range(output_index, output_index + size_of_var):
        op.output_desc.add().name = "var" + str(i - output_index)
        var.append(Tensor(op, i))
    output_index += size_of_var

    # return outputs
    return var


# This api is auto-generated from IR MultiHeadAttentionScoreGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, True, True, True, True, True, True])
def MultiHeadAttentionScoreGrad(query: Tensor, key: Tensor, value: Tensor, dy: Tensor, pse_shift: Optional[Tensor], drop_mask: Optional[Tensor], padding_mask: Optional[Tensor], atten_mask: Optional[Tensor], softmax_in: Optional[Tensor], attention_in: Optional[Tensor], *, head_num: int, input_layout: str, scale_value: float=1.000000, keep_prob: float=1.000000, pre_tockens: int=65536, next_tockens: int=65536, dependencies=[], node_name=None):
    """REG_OP(MultiHeadAttentionScoreGrad)\n
.INPUT(query, TensorType({DT_FLOAT32}))\n
.INPUT(key, TensorType({DT_FLOAT32}))\n
.INPUT(value, TensorType({DT_FLOAT32}))\n
.INPUT(dy, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(pse_shift, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(drop_mask, TensorType({DT_UINT8}))\n
.OPTIONAL_INPUT(padding_mask, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(atten_mask, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(softmax_in, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(attention_in, TensorType({DT_FLOAT32}))\n
.OUTPUT(dq, TensorType({DT_FLOAT32}))\n
.OUTPUT(dk, TensorType({DT_FLOAT32}))\n
.OUTPUT(dv, TensorType({DT_FLOAT32}))\n
.OUTPUT(dpse, TensorType({DT_FLOAT32}))\n
.ATTR(scale_value, Float, 1.0)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(pre_tockens, Int, 65536)\n
.ATTR(next_tockens, Int, 65536)\n
.REQUIRED_ATTR(head_num, Int)\n
.REQUIRED_ATTR(input_layout, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultiHeadAttentionScoreGrad"
    op.name = next_unique_name(node_name, "MultiHeadAttentionScoreGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    if pse_shift is not None:
        op.input.append(pse_shift.tensor)
        op.input_desc.add().CopyFrom(pse_shift.desc)
        op.input_desc[-1].name = "pse_shift"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pse_shift"
    if drop_mask is not None:
        op.input.append(drop_mask.tensor)
        op.input_desc.add().CopyFrom(drop_mask.desc)
        op.input_desc[-1].name = "drop_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "drop_mask"
    if padding_mask is not None:
        op.input.append(padding_mask.tensor)
        op.input_desc.add().CopyFrom(padding_mask.desc)
        op.input_desc[-1].name = "padding_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "padding_mask"
    if atten_mask is not None:
        op.input.append(atten_mask.tensor)
        op.input_desc.add().CopyFrom(atten_mask.desc)
        op.input_desc[-1].name = "atten_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "atten_mask"
    if softmax_in is not None:
        op.input.append(softmax_in.tensor)
        op.input_desc.add().CopyFrom(softmax_in.desc)
        op.input_desc[-1].name = "softmax_in"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "softmax_in"
    if attention_in is not None:
        op.input.append(attention_in.tensor)
        op.input_desc.add().CopyFrom(attention_in.desc)
        op.input_desc[-1].name = "attention_in"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "attention_in"

    # process attrs
    op.attr["head_num"].i = head_num
    op.attr["input_layout"].s = compat_as_bytes(input_layout)
    op.attr["scale_value"].f = scale_value
    op.attr["keep_prob"].f = keep_prob
    op.attr["pre_tockens"].i = pre_tockens
    op.attr["next_tockens"].i = next_tockens

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dq"
    dq = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dk"
    dk = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dv"
    dv = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dpse"
    dpse = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dq, dk, dv, dpse


# This api is auto-generated from IR IncreFlashAttention
@auto_convert_to_tensor(
    [False, True, True, False, False, False, False, False, False, False, False, False, False, False, False],
    [False, False, False, True, True, True, True, True, True, True, True, True, True, True, True])
def IncreFlashAttention(query: Tensor,
                        key: List[Tensor],
                        value: List[Tensor],
                        pse_shift: Optional[Tensor],
                        atten_mask: Optional[Tensor],
                        actual_seq_lengths: Optional[Tensor],
                        dequant_scale1: Optional[Tensor],
                        quant_scale1: Optional[Tensor],
                        dequant_scale2: Optional[Tensor],
                        quant_scale2: Optional[Tensor],
                        quant_offset2: Optional[Tensor],
                        antiquant_scale: Optional[Tensor],
                        antiquant_offset: Optional[Tensor],
                        block_table: Optional[Tensor],
                        kv_padding_size: Optional[Tensor],
                        *,
                        num_heads: int,
                        scale_value: float = 1.000000,
                        input_layout: str = "BSH",
                        num_key_value_heads: int = 1,
                        block_size: int = 0,
                        inner_precise: int = 1,
                        dependencies=[],
                        node_name=None):
    """REG_OP(IncreFlashAttention)\n
.INPUT(query, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32, DT_INT8}))\n
.DYNAMIC_INPUT(key, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32, DT_INT8}))\n
.DYNAMIC_INPUT(value, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32, DT_INT8}))\n
.OPTIONAL_INPUT(pse_shift, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(atten_mask, TensorType({DT_FLOAT16, DT_BOOL, DT_FLOAT32}))\n
.OPTIONAL_INPUT(actual_seq_lengths, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(dequant_scale1, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(quant_scale1, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(dequant_scale2, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(quant_scale2, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(quant_offset2, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(antiquant_scale, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(antiquant_offset, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(block_table, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(kv_padding_size, TensorType({DT_INT64}))\n
.OUTPUT(attention_out, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32, DT_INT8}))\n
.REQUIRED_ATTR(num_heads, Int)\n
.ATTR(scale_value, Float, 1.0)\n
.ATTR(input_layout, String, "BSH")\n
.ATTR(num_key_value_heads, Int, 1)\n
.ATTR(block_size, Int, 0)\n
.ATTR(inner_precise, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IncreFlashAttention"
    op.name = next_unique_name(node_name, "IncreFlashAttention")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    if not isinstance(key, (tuple, list)):
        raise AssertionError("key must be a tuple or a list.")
    for i, v in enumerate(key):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "key" + str(i)
    if not isinstance(value, (tuple, list)):
        raise AssertionError("value must be a tuple or a list.")
    for i, v in enumerate(value):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "value" + str(i)
    if pse_shift is not None:
        op.input.append(pse_shift.tensor)
        op.input_desc.add().CopyFrom(pse_shift.desc)
        op.input_desc[-1].name = "pse_shift"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pse_shift"
    if atten_mask is not None:
        op.input.append(atten_mask.tensor)
        op.input_desc.add().CopyFrom(atten_mask.desc)
        op.input_desc[-1].name = "atten_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "atten_mask"
    if actual_seq_lengths is not None:
        op.input.append(actual_seq_lengths.tensor)
        op.input_desc.add().CopyFrom(actual_seq_lengths.desc)
        op.input_desc[-1].name = "actual_seq_lengths"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_lengths"
    if dequant_scale1 is not None:
        op.input.append(dequant_scale1.tensor)
        op.input_desc.add().CopyFrom(dequant_scale1.desc)
        op.input_desc[-1].name = "dequant_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dequant_scale1"
    if quant_scale1 is not None:
        op.input.append(quant_scale1.tensor)
        op.input_desc.add().CopyFrom(quant_scale1.desc)
        op.input_desc[-1].name = "quant_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale1"
    if dequant_scale2 is not None:
        op.input.append(dequant_scale2.tensor)
        op.input_desc.add().CopyFrom(dequant_scale2.desc)
        op.input_desc[-1].name = "dequant_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dequant_scale2"
    if quant_scale2 is not None:
        op.input.append(quant_scale2.tensor)
        op.input_desc.add().CopyFrom(quant_scale2.desc)
        op.input_desc[-1].name = "quant_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale2"
    if quant_offset2 is not None:
        op.input.append(quant_offset2.tensor)
        op.input_desc.add().CopyFrom(quant_offset2.desc)
        op.input_desc[-1].name = "quant_offset2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_offset2"
    if antiquant_scale is not None:
        op.input.append(antiquant_scale.tensor)
        op.input_desc.add().CopyFrom(antiquant_scale.desc)
        op.input_desc[-1].name = "antiquant_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_scale"
    if antiquant_offset is not None:
        op.input.append(antiquant_offset.tensor)
        op.input_desc.add().CopyFrom(antiquant_offset.desc)
        op.input_desc[-1].name = "antiquant_offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_offset"
    if block_table is not None:
        op.input.append(block_table.tensor)
        op.input_desc.add().CopyFrom(block_table.desc)
        op.input_desc[-1].name = "block_table"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "block_table"
    if kv_padding_size is not None:
        op.input.append(kv_padding_size.tensor)
        op.input_desc.add().CopyFrom(kv_padding_size.desc)
        op.input_desc[-1].name = "kv_padding_size"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "kv_padding_size"

    # process attrs
    op.attr["num_heads"].i = num_heads
    op.attr["scale_value"].f = scale_value
    op.attr["input_layout"].s = compat_as_bytes(input_layout)
    op.attr["num_key_value_heads"].i = num_key_value_heads
    op.attr["block_size"].i = block_size
    op.attr["inner_precise"].i = inner_precise

    # process outputs
    output_index = 0
    op.output_desc.add().name = "attention_out"
    attention_out = Tensor(op, output_index)
    output_index += 1


    return attention_out


# This api is auto-generated from IR PromptFlashAttention
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True, True, True, True, True])
def PromptFlashAttention(query: Tensor, key: Tensor, value: Tensor, pse_shift: Optional[Tensor], atten_mask: Optional[Tensor], actual_seq_lengths: Optional[Tensor], actual_seq_lengths_kv: Optional[Tensor], deq_scale1: Optional[Tensor], quant_scale1: Optional[Tensor], deq_scale2: Optional[Tensor], quant_scale2: Optional[Tensor], quant_offset2: Optional[Tensor], *, num_heads: int, scale_value: float=1.000000, pre_tokens: int=214748647, next_tokens: int=0, input_layout: str="BSH", num_key_value_heads: int=1, sparse_mode: int=0, inner_precise: int=1, dependencies=[], node_name=None):
    """REG_OP(PromptFlashAttention)\n
.INPUT(query, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.INPUT(key, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.INPUT(value, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(pse_shift, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(atten_mask, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(actual_seq_lengths, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(actual_seq_lengths_kv, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(deq_scale1, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(quant_scale1, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(deq_scale2, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(quant_scale2, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(quant_offset2, TensorType({DT_FLOAT32}))\n
.OUTPUT(attention_out, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.REQUIRED_ATTR(num_heads, Int)\n
.ATTR(scale_value, Float, 1.0)\n
.ATTR(pre_tokens, Int, 214748647)\n
.ATTR(next_tokens, Int, 0)\n
.ATTR(input_layout, String, "BSH")\n
.ATTR(num_key_value_heads, Int, 1)\n
.ATTR(sparse_mode, Int, 0)\n
.ATTR(inner_precise, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PromptFlashAttention"
    op.name = next_unique_name(node_name, "PromptFlashAttention")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    if pse_shift is not None:
        op.input.append(pse_shift.tensor)
        op.input_desc.add().CopyFrom(pse_shift.desc)
        op.input_desc[-1].name = "pse_shift"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pse_shift"
    if atten_mask is not None:
        op.input.append(atten_mask.tensor)
        op.input_desc.add().CopyFrom(atten_mask.desc)
        op.input_desc[-1].name = "atten_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "atten_mask"
    if actual_seq_lengths is not None:
        op.input.append(actual_seq_lengths.tensor)
        op.input_desc.add().CopyFrom(actual_seq_lengths.desc)
        op.input_desc[-1].name = "actual_seq_lengths"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_lengths"
    if actual_seq_lengths_kv is not None:
        op.input.append(actual_seq_lengths_kv.tensor)
        op.input_desc.add().CopyFrom(actual_seq_lengths_kv.desc)
        op.input_desc[-1].name = "actual_seq_lengths_kv"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_lengths_kv"
    if deq_scale1 is not None:
        op.input.append(deq_scale1.tensor)
        op.input_desc.add().CopyFrom(deq_scale1.desc)
        op.input_desc[-1].name = "deq_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "deq_scale1"
    if quant_scale1 is not None:
        op.input.append(quant_scale1.tensor)
        op.input_desc.add().CopyFrom(quant_scale1.desc)
        op.input_desc[-1].name = "quant_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale1"
    if deq_scale2 is not None:
        op.input.append(deq_scale2.tensor)
        op.input_desc.add().CopyFrom(deq_scale2.desc)
        op.input_desc[-1].name = "deq_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "deq_scale2"
    if quant_scale2 is not None:
        op.input.append(quant_scale2.tensor)
        op.input_desc.add().CopyFrom(quant_scale2.desc)
        op.input_desc[-1].name = "quant_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale2"
    if quant_offset2 is not None:
        op.input.append(quant_offset2.tensor)
        op.input_desc.add().CopyFrom(quant_offset2.desc)
        op.input_desc[-1].name = "quant_offset2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_offset2"

    # process attrs
    op.attr["num_heads"].i = num_heads
    op.attr["scale_value"].f = scale_value
    op.attr["pre_tokens"].i = pre_tokens
    op.attr["next_tokens"].i = next_tokens
    op.attr["input_layout"].s = compat_as_bytes(input_layout)
    op.attr["num_key_value_heads"].i = num_key_value_heads
    op.attr["sparse_mode"].i = sparse_mode
    op.attr["inner_precise"].i = inner_precise

    # process outputs
    output_index = 0
    op.output_desc.add().name = "attention_out"
    attention_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return attention_out


# This api is auto-generated from IR PasteSubImg
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def PasteSubImg(patch_img: Tensor, patch_coord: Tensor, core_area_coord: Tensor, combine_img: Tensor, *, scale: float, dependencies=[], node_name=None):
    """REG_OP(PasteSubImg)\n
.INPUT(patch_img, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT32}))\n
.INPUT(patch_coord, TensorType({DT_INT32}))\n
.INPUT(core_area_coord, TensorType({DT_INT32}))\n
.INPUT(combine_img, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT32}))\n
.OUTPUT(combine_img, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT32}))\n
.REQUIRED_ATTR(scale, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PasteSubImg"
    op.name = next_unique_name(node_name, "PasteSubImg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(patch_img.tensor)
    op.input_desc.add().CopyFrom(patch_img.desc)
    op.input_desc[-1].name = "patch_img"
    op.input.append(patch_coord.tensor)
    op.input_desc.add().CopyFrom(patch_coord.desc)
    op.input_desc[-1].name = "patch_coord"
    op.input.append(core_area_coord.tensor)
    op.input_desc.add().CopyFrom(core_area_coord.desc)
    op.input_desc[-1].name = "core_area_coord"
    op.input.append(combine_img.tensor)
    op.input_desc.add().CopyFrom(combine_img.desc)
    op.input_desc[-1].name = "combine_img"

    # process attrs
    op.attr["scale"].f = scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "combine_img"
    combine_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return combine_img


# This api is auto-generated from IR RotatedFeatureAlignGrad
@auto_convert_to_tensor([False, False], [False, False])
def RotatedFeatureAlignGrad(dy: Tensor, bboxes: Tensor, *, spatial_scale: float, points: int=1, dependencies=[], node_name=None):
    """REG_OP(RotatedFeatureAlignGrad)\n
.INPUT(dy, TensorType({DT_FLOAT}))\n
.INPUT(bboxes, TensorType({DT_FLOAT}))\n
.OUTPUT(dx, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.ATTR(points, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotatedFeatureAlignGrad"
    op.name = next_unique_name(node_name, "RotatedFeatureAlignGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"

    # process attrs
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["points"].i = points

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx


# This api is auto-generated from IR Conv2DTransposeDCompress
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def Conv2DTransposeDCompress(x: Tensor, filter_compress: Tensor, compress_index: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, input_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", output_padding: List[int]=[0, 0, 0, 0], offset_x: int=0, alg: str="weight_sparse_4_2", dependencies=[], node_name=None):
    """REG_OP(Conv2DTransposeDCompress)\n
.INPUT(x, TensorType({DT_INT8}))\n
.INPUT(filter_compress, TensorType({DT_INT8}))\n
.INPUT(compress_index, TensorType({DT_INT8}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(output_padding, ListInt, {0, 0, 0, 0})\n
.ATTR(offset_x, Int, 0)\n
.ATTR(alg, String, "weight_sparse_4_2")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DTransposeDCompress"
    op.name = next_unique_name(node_name, "Conv2DTransposeDCompress")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter_compress.tensor)
    op.input_desc.add().CopyFrom(filter_compress.desc)
    op.input_desc[-1].name = "filter_compress"
    op.input.append(compress_index.tensor)
    op.input_desc.add().CopyFrom(compress_index.desc)
    op.input_desc[-1].name = "compress_index"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["output_padding"].list.val_type = 2
    op.attr["output_padding"].list.i.extend(output_padding)
    op.attr["offset_x"].i = offset_x
    op.attr["alg"].s = compat_as_bytes(alg)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ForeachNonFiniteCheckAndUnscale
@auto_convert_to_tensor([True, False, False], [False, False, False])
def ForeachNonFiniteCheckAndUnscale(scaled_grads: List[Tensor], found_inf: Tensor, inv_scale: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachNonFiniteCheckAndUnscale)\n
.DYNAMIC_INPUT(scaled_grads, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(found_inf, TensorType({DT_FLOAT}))\n
.INPUT(inv_scale, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachNonFiniteCheckAndUnscale"
    op.name = next_unique_name(node_name, "ForeachNonFiniteCheckAndUnscale")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(scaled_grads, (tuple, list)):
        raise AssertionError("scaled_grads must be a tuple or a list.")
    for i, v in enumerate(scaled_grads):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "scaled_grads" + str(i)
    op.input.append(found_inf.tensor)
    op.input_desc.add().CopyFrom(found_inf.desc)
    op.input_desc[-1].name = "found_inf"
    op.input.append(inv_scale.tensor)
    op.input_desc.add().CopyFrom(inv_scale.desc)
    op.input_desc[-1].name = "inv_scale"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMulScalarInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachMulScalarInplace(x: List[Tensor], scalar: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachMulScalarInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMulScalarInplace"
    op.name = next_unique_name(node_name, "ForeachMulScalarInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR SwitchByIndex
@auto_convert_to_tensor([False], [False])
def SwitchByIndex(x: Tensor, *, indices: int, dependencies=[], node_name=None):
    """REG_OP(SwitchByIndex)\n
.INPUT(x, TensorType({DT_UINT64}))\n
.REQUIRED_ATTR(indices, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SwitchByIndex"
    op.name = next_unique_name(node_name, "SwitchByIndex")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["indices"].i = indices

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR QuantBatchMatmul
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def QuantBatchMatmul(x1: Tensor, x2: Tensor, bias: Tensor, deq_scale: Tensor, *, adj_x1: bool=False, adj_x2: bool=False, dependencies=[], node_name=None):
    """REG_OP(QuantBatchMatmul)\n
.INPUT(x1, TensorType({DT_INT8}))\n
.INPUT(x2, TensorType({DT_INT8}))\n
.INPUT(bias, TensorType({DT_INT32}))\n
.INPUT(deq_scale, TensorType({DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.ATTR(adj_x1, Bool, false)\n
.ATTR(adj_x2, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QuantBatchMatmul"
    op.name = next_unique_name(node_name, "QuantBatchMatmul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"
    op.input.append(deq_scale.tensor)
    op.input_desc.add().CopyFrom(deq_scale.desc)
    op.input_desc[-1].name = "deq_scale"

    # process attrs
    op.attr["adj_x1"].b = adj_x1
    op.attr["adj_x2"].b = adj_x2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MoeFFN
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True, True, True])
def MoeFFN(x: Tensor, expert_tokens: Tensor, weight1: Tensor, bias1: Optional[Tensor], weight2: Optional[Tensor], bias2: Optional[Tensor], scale: Optional[Tensor], offset: Optional[Tensor], deq_scale1: Optional[Tensor], deq_scale2: Optional[Tensor], *, activation: str="gelu", dependencies=[], node_name=None):
    """REG_OP(MoeFFN)\n
.INPUT(x, TensorType({DT_INT8, DT_FLOAT16}))\n
.INPUT(expert_tokens, TensorType({DT_INT64}))\n
.INPUT(weight1, TensorType({DT_INT8, DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias1, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(weight2, TensorType({DT_INT8, DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias2, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(scale, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(offset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(deq_scale1, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(deq_scale2, TensorType({DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_FLOAT16}))\n
.ATTR(activation, String, "gelu")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MoeFFN"
    op.name = next_unique_name(node_name, "MoeFFN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(expert_tokens.tensor)
    op.input_desc.add().CopyFrom(expert_tokens.desc)
    op.input_desc[-1].name = "expert_tokens"
    op.input.append(weight1.tensor)
    op.input_desc.add().CopyFrom(weight1.desc)
    op.input_desc[-1].name = "weight1"
    if bias1 is not None:
        op.input.append(bias1.tensor)
        op.input_desc.add().CopyFrom(bias1.desc)
        op.input_desc[-1].name = "bias1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias1"
    if weight2 is not None:
        op.input.append(weight2.tensor)
        op.input_desc.add().CopyFrom(weight2.desc)
        op.input_desc[-1].name = "weight2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight2"
    if bias2 is not None:
        op.input.append(bias2.tensor)
        op.input_desc.add().CopyFrom(bias2.desc)
        op.input_desc[-1].name = "bias2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias2"
    if scale is not None:
        op.input.append(scale.tensor)
        op.input_desc.add().CopyFrom(scale.desc)
        op.input_desc[-1].name = "scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"
    if deq_scale1 is not None:
        op.input.append(deq_scale1.tensor)
        op.input_desc.add().CopyFrom(deq_scale1.desc)
        op.input_desc[-1].name = "deq_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "deq_scale1"
    if deq_scale2 is not None:
        op.input.append(deq_scale2.tensor)
        op.input_desc.add().CopyFrom(deq_scale2.desc)
        op.input_desc[-1].name = "deq_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "deq_scale2"

    # process attrs
    op.attr["activation"].s = compat_as_bytes(activation)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MoeFinalizeRouting
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, True, False, False, False, False])
def MoeFinalizeRouting(expanded_x: Tensor, x1: Tensor, x2: Optional[Tensor], bias: Tensor, scales: Tensor, expanded_row_idx: Tensor, expanded_expert_idx: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MoeFinalizeRouting)\n
.INPUT(expanded_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(scales, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(expanded_row_idx, TensorType({DT_INT32, DT_INT32, DT_INT32}))\n
.INPUT(expanded_expert_idx, TensorType({DT_INT32, DT_INT32, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MoeFinalizeRouting"
    op.name = next_unique_name(node_name, "MoeFinalizeRouting")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(expanded_x.tensor)
    op.input_desc.add().CopyFrom(expanded_x.desc)
    op.input_desc[-1].name = "expanded_x"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    if x2 is not None:
        op.input.append(x2.tensor)
        op.input_desc.add().CopyFrom(x2.desc)
        op.input_desc[-1].name = "x2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x2"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"
    op.input.append(scales.tensor)
    op.input_desc.add().CopyFrom(scales.desc)
    op.input_desc[-1].name = "scales"
    op.input.append(expanded_row_idx.tensor)
    op.input_desc.add().CopyFrom(expanded_row_idx.desc)
    op.input_desc[-1].name = "expanded_row_idx"
    op.input.append(expanded_expert_idx.tensor)
    op.input_desc.add().CopyFrom(expanded_expert_idx.desc)
    op.input_desc[-1].name = "expanded_expert_idx"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ForeachAddScalarInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachAddScalarInplace(x: List[Tensor], scalar: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachAddScalarInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAddScalarInplace"
    op.name = next_unique_name(node_name, "ForeachAddScalarInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachAddScalar
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachAddScalar(x: List[Tensor], scalar: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachAddScalar)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAddScalar"
    op.name = next_unique_name(node_name, "ForeachAddScalar")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachAddScalarListInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachAddScalarListInplace(x: List[Tensor], scalars: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachAddScalarListInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAddScalarListInplace"
    op.name = next_unique_name(node_name, "ForeachAddScalarListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachAddScalarList
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachAddScalarList(x: List[Tensor], scalars: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachAddScalarList)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAddScalarList"
    op.name = next_unique_name(node_name, "ForeachAddScalarList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachAddListInplace
@auto_convert_to_tensor([True, True, False], [False, False, False])
def ForeachAddListInplace(x1: List[Tensor], x2: List[Tensor], alpha: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachAddListInplace)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAddListInplace"
    op.name = next_unique_name(node_name, "ForeachAddListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachAddList
@auto_convert_to_tensor([True, True, False], [False, False, False])
def _ForeachAddList(x1: List[Tensor], x2: List[Tensor], alpha: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachAddList)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAddList"
    op.name = next_unique_name(node_name, "ForeachAddList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachSubScalarInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachSubScalarInplace(x: List[Tensor], scalar: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachSubScalarInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSubScalarInplace"
    op.name = next_unique_name(node_name, "ForeachSubScalarInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSubScalar
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachSubScalar(x: List[Tensor], scalar: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachSubScalar)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSubScalar"
    op.name = next_unique_name(node_name, "ForeachSubScalar")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachSubScalarListInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachSubScalarListInplace(x: List[Tensor], scalars: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachSubScalarListInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSubScalarListInplace"
    op.name = next_unique_name(node_name, "ForeachSubScalarListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSubScalarList
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachSubScalarList(x: List[Tensor], scalars: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachSubScalarList)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSubScalarList"
    op.name = next_unique_name(node_name, "ForeachSubScalarList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachSubListInplace
@auto_convert_to_tensor([True, True, False], [False, False, False])
def ForeachSubListInplace(x1: List[Tensor], x2: List[Tensor], alpha: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachSubListInplace)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSubListInplace"
    op.name = next_unique_name(node_name, "ForeachSubListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSubList
@auto_convert_to_tensor([True, True, False], [False, False, False])
def _ForeachSubList(x1: List[Tensor], x2: List[Tensor], alpha: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachSubList)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSubList"
    op.name = next_unique_name(node_name, "ForeachSubList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMulScalar
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachMulScalar(x: List[Tensor], scalar: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMulScalar)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMulScalar"
    op.name = next_unique_name(node_name, "ForeachMulScalar")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMulScalarListInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachMulScalarListInplace(x: List[Tensor], scalars: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachMulScalarListInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMulScalarListInplace"
    op.name = next_unique_name(node_name, "ForeachMulScalarListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMulScalarList
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachMulScalarList(x: List[Tensor], scalars: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMulScalarList)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMulScalarList"
    op.name = next_unique_name(node_name, "ForeachMulScalarList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMulListInplace
@auto_convert_to_tensor([True, True], [False, False])
def ForeachMulListInplace(x1: List[Tensor], x2: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachMulListInplace)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMulListInplace"
    op.name = next_unique_name(node_name, "ForeachMulListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMulList
@auto_convert_to_tensor([True, True], [False, False])
def _ForeachMulList(x1: List[Tensor], x2: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMulList)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMulList"
    op.name = next_unique_name(node_name, "ForeachMulList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachDivScalarInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachDivScalarInplace(x: List[Tensor], scalar: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachDivScalarInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachDivScalarInplace"
    op.name = next_unique_name(node_name, "ForeachDivScalarInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachDivScalar
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachDivScalar(x: List[Tensor], scalar: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachDivScalar)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachDivScalar"
    op.name = next_unique_name(node_name, "ForeachDivScalar")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachDivScalarListInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachDivScalarListInplace(x: List[Tensor], scalars: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachDivScalarListInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachDivScalarListInplace"
    op.name = next_unique_name(node_name, "ForeachDivScalarListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachDivScalarList
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachDivScalarList(x: List[Tensor], scalars: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachDivScalarList)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachDivScalarList"
    op.name = next_unique_name(node_name, "ForeachDivScalarList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachDivListInplace
@auto_convert_to_tensor([True, True], [False, False])
def ForeachDivListInplace(x1: List[Tensor], x2: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachDivListInplace)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachDivListInplace"
    op.name = next_unique_name(node_name, "ForeachDivListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachDivList
@auto_convert_to_tensor([True, True], [False, False])
def _ForeachDivList(x1: List[Tensor], x2: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachDivList)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachDivList"
    op.name = next_unique_name(node_name, "ForeachDivList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMaximumScalarInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachMaximumScalarInplace(x: List[Tensor], scalar: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachMaximumScalarInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMaximumScalarInplace"
    op.name = next_unique_name(node_name, "ForeachMaximumScalarInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMaximumScalar
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachMaximumScalar(x: List[Tensor], scalar: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMaximumScalar)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMaximumScalar"
    op.name = next_unique_name(node_name, "ForeachMaximumScalar")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMaximumScalarListInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachMaximumScalarListInplace(x: List[Tensor], scalars: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachMaximumScalarListInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMaximumScalarListInplace"
    op.name = next_unique_name(node_name, "ForeachMaximumScalarListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMaximumScalarList
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachMaximumScalarList(x: List[Tensor], scalars: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMaximumScalarList)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMaximumScalarList"
    op.name = next_unique_name(node_name, "ForeachMaximumScalarList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMaximumListInplace
@auto_convert_to_tensor([True, True], [False, False])
def ForeachMaximumListInplace(x1: List[Tensor], x2: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachMaximumListInplace)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMaximumListInplace"
    op.name = next_unique_name(node_name, "ForeachMaximumListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMaximumList
@auto_convert_to_tensor([True, True], [False, False])
def _ForeachMaximumList(x1: List[Tensor], x2: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMaximumList)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMaximumList"
    op.name = next_unique_name(node_name, "ForeachMaximumList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMinimumScalarInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachMinimumScalarInplace(x: List[Tensor], scalar: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachMinimumScalarInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMinimumScalarInplace"
    op.name = next_unique_name(node_name, "ForeachMinimumScalarInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMinimumScalar
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachMinimumScalar(x: List[Tensor], scalar: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMinimumScalar)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMinimumScalar"
    op.name = next_unique_name(node_name, "ForeachMinimumScalar")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMinimumScalarListInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachMinimumScalarListInplace(x: List[Tensor], scalars: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachMinimumScalarListInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMinimumScalarListInplace"
    op.name = next_unique_name(node_name, "ForeachMinimumScalarListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMinimumScalarList
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachMinimumScalarList(x: List[Tensor], scalars: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMinimumScalarList)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMinimumScalarList"
    op.name = next_unique_name(node_name, "ForeachMinimumScalarList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachMinimumListInplace
@auto_convert_to_tensor([True, True], [False, False])
def ForeachMinimumListInplace(x1: List[Tensor], x2: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachMinimumListInplace)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMinimumListInplace"
    op.name = next_unique_name(node_name, "ForeachMinimumListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachMinimumList
@auto_convert_to_tensor([True, True], [False, False])
def _ForeachMinimumList(x1: List[Tensor], x2: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachMinimumList)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachMinimumList"
    op.name = next_unique_name(node_name, "ForeachMinimumList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachPowScalarInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachPowScalarInplace(x: List[Tensor], scalar: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachPowScalarInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachPowScalarInplace"
    op.name = next_unique_name(node_name, "ForeachPowScalarInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachPowScalar
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachPowScalar(x: List[Tensor], scalar: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachPowScalar)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalar, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachPowScalar"
    op.name = next_unique_name(node_name, "ForeachPowScalar")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalar.tensor)
    op.input_desc.add().CopyFrom(scalar.desc)
    op.input_desc[-1].name = "scalar"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachPowScalarListInplace
@auto_convert_to_tensor([True, False], [False, False])
def ForeachPowScalarListInplace(x: List[Tensor], scalars: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ForeachPowScalarListInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachPowScalarListInplace"
    op.name = next_unique_name(node_name, "ForeachPowScalarListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachPowScalarList
@auto_convert_to_tensor([True, False], [False, False])
def _ForeachPowScalarList(x: List[Tensor], scalars: Tensor, *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachPowScalarList)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(scalars, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachPowScalarList"
    op.name = next_unique_name(node_name, "ForeachPowScalarList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(scalars.tensor)
    op.input_desc.add().CopyFrom(scalars.desc)
    op.input_desc[-1].name = "scalars"

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachPowListInplace
@auto_convert_to_tensor([True, True], [False, False])
def ForeachPowListInplace(x1: List[Tensor], x2: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachPowListInplace)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachPowListInplace"
    op.name = next_unique_name(node_name, "ForeachPowListInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachPowList
@auto_convert_to_tensor([True, True], [False, False])
def _ForeachPowList(x1: List[Tensor], x2: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachPowList)\n
.DYNAMIC_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachPowList"
    op.name = next_unique_name(node_name, "ForeachPowList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x1, (tuple, list)):
        raise AssertionError("x1 must be a tuple or a list.")
    for i, v in enumerate(x1):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x1" + str(i)
    if not isinstance(x2, (tuple, list)):
        raise AssertionError("x2 must be a tuple or a list.")
    for i, v in enumerate(x2):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x2" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachAbsInplace
@auto_convert_to_tensor([True], [False])
def ForeachAbsInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachAbsInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAbsInplace"
    op.name = next_unique_name(node_name, "ForeachAbsInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachAbs
@auto_convert_to_tensor([True], [False])
def _ForeachAbs(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachAbs)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachAbs"
    op.name = next_unique_name(node_name, "ForeachAbs")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachACosInplace
@auto_convert_to_tensor([True], [False])
def ForeachACosInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachACosInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachACosInplace"
    op.name = next_unique_name(node_name, "ForeachACosInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachACos
@auto_convert_to_tensor([True], [False])
def _ForeachACos(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachACos)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachACos"
    op.name = next_unique_name(node_name, "ForeachACos")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachASinInplace
@auto_convert_to_tensor([True], [False])
def ForeachASinInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachASinInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachASinInplace"
    op.name = next_unique_name(node_name, "ForeachASinInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachASin
@auto_convert_to_tensor([True], [False])
def _ForeachASin(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachASin)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachASin"
    op.name = next_unique_name(node_name, "ForeachASin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachATanInplace
@auto_convert_to_tensor([True], [False])
def ForeachATanInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachATanInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachATanInplace"
    op.name = next_unique_name(node_name, "ForeachATanInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachATan
@auto_convert_to_tensor([True], [False])
def _ForeachATan(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachATan)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachATan"
    op.name = next_unique_name(node_name, "ForeachATan")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachCosInplace
@auto_convert_to_tensor([True], [False])
def ForeachCosInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachCosInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachCosInplace"
    op.name = next_unique_name(node_name, "ForeachCosInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachCos
@auto_convert_to_tensor([True], [False])
def _ForeachCos(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachCos)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachCos"
    op.name = next_unique_name(node_name, "ForeachCos")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachSinInplace
@auto_convert_to_tensor([True], [False])
def ForeachSinInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachSinInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSinInplace"
    op.name = next_unique_name(node_name, "ForeachSinInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSin
@auto_convert_to_tensor([True], [False])
def _ForeachSin(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachSin)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSin"
    op.name = next_unique_name(node_name, "ForeachSin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachTanInplace
@auto_convert_to_tensor([True], [False])
def ForeachTanInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachTanInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachTanInplace"
    op.name = next_unique_name(node_name, "ForeachTanInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachTan
@auto_convert_to_tensor([True], [False])
def _ForeachTan(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachTan)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachTan"
    op.name = next_unique_name(node_name, "ForeachTan")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachCoshInplace
@auto_convert_to_tensor([True], [False])
def ForeachCoshInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachCoshInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachCoshInplace"
    op.name = next_unique_name(node_name, "ForeachCoshInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachCosh
@auto_convert_to_tensor([True], [False])
def _ForeachCosh(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachCosh)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachCosh"
    op.name = next_unique_name(node_name, "ForeachCosh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachSinhInplace
@auto_convert_to_tensor([True], [False])
def ForeachSinhInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachSinhInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSinhInplace"
    op.name = next_unique_name(node_name, "ForeachSinhInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSinh
@auto_convert_to_tensor([True], [False])
def _ForeachSinh(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachSinh)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSinh"
    op.name = next_unique_name(node_name, "ForeachSinh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachTanhInplace
@auto_convert_to_tensor([True], [False])
def ForeachTanhInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachTanhInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachTanhInplace"
    op.name = next_unique_name(node_name, "ForeachTanhInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachTanh
@auto_convert_to_tensor([True], [False])
def _ForeachTanh(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachTanh)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachTanh"
    op.name = next_unique_name(node_name, "ForeachTanh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachSqrtInplace
@auto_convert_to_tensor([True], [False])
def ForeachSqrtInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachSqrtInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSqrtInplace"
    op.name = next_unique_name(node_name, "ForeachSqrtInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSqrt
@auto_convert_to_tensor([True], [False])
def _ForeachSqrt(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachSqrt)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSqrt"
    op.name = next_unique_name(node_name, "ForeachSqrt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachNegInplace
@auto_convert_to_tensor([True], [False])
def ForeachNegInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachNegInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachNegInplace"
    op.name = next_unique_name(node_name, "ForeachNegInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachNeg
@auto_convert_to_tensor([True], [False])
def _ForeachNeg(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachNeg)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachNeg"
    op.name = next_unique_name(node_name, "ForeachNeg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachExpInplace
@auto_convert_to_tensor([True], [False])
def ForeachExpInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachExpInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachExpInplace"
    op.name = next_unique_name(node_name, "ForeachExpInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachExp
@auto_convert_to_tensor([True], [False])
def _ForeachExp(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachExp)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachExp"
    op.name = next_unique_name(node_name, "ForeachExp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachExpm1Inplace
@auto_convert_to_tensor([True], [False])
def ForeachExpm1Inplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachExpm1Inplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachExpm1Inplace"
    op.name = next_unique_name(node_name, "ForeachExpm1Inplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachExpm1
@auto_convert_to_tensor([True], [False])
def _ForeachExpm1(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachExpm1)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachExpm1"
    op.name = next_unique_name(node_name, "ForeachExpm1")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachLogInplace
@auto_convert_to_tensor([True], [False])
def ForeachLogInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachLogInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLogInplace"
    op.name = next_unique_name(node_name, "ForeachLogInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachLog
@auto_convert_to_tensor([True], [False])
def _ForeachLog(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachLog)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLog"
    op.name = next_unique_name(node_name, "ForeachLog")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachLog2Inplace
@auto_convert_to_tensor([True], [False])
def ForeachLog2Inplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachLog2Inplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLog2Inplace"
    op.name = next_unique_name(node_name, "ForeachLog2Inplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachLog2
@auto_convert_to_tensor([True], [False])
def _ForeachLog2(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachLog2)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLog2"
    op.name = next_unique_name(node_name, "ForeachLog2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachLog10Inplace
@auto_convert_to_tensor([True], [False])
def ForeachLog10Inplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachLog10Inplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLog10Inplace"
    op.name = next_unique_name(node_name, "ForeachLog10Inplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachLog10
@auto_convert_to_tensor([True], [False])
def _ForeachLog10(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachLog10)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLog10"
    op.name = next_unique_name(node_name, "ForeachLog10")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachLog1pInplace
@auto_convert_to_tensor([True], [False])
def ForeachLog1pInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachLog1pInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLog1pInplace"
    op.name = next_unique_name(node_name, "ForeachLog1pInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachLog1p
@auto_convert_to_tensor([True], [False])
def _ForeachLog1p(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachLog1p)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachLog1p"
    op.name = next_unique_name(node_name, "ForeachLog1p")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachReciprocalInplace
@auto_convert_to_tensor([True], [False])
def ForeachReciprocalInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachReciprocalInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachReciprocalInplace"
    op.name = next_unique_name(node_name, "ForeachReciprocalInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachReciprocal
@auto_convert_to_tensor([True], [False])
def _ForeachReciprocal(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachReciprocal)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachReciprocal"
    op.name = next_unique_name(node_name, "ForeachReciprocal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ForeachZeroInplace
@auto_convert_to_tensor([True], [False])
def ForeachZeroInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachZeroInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachZeroInplace"
    op.name = next_unique_name(node_name, "ForeachZeroInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSigmoidInplace
@auto_convert_to_tensor([True], [False])
def ForeachSigmoidInplace(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ForeachSigmoidInplace)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSigmoidInplace"
    op.name = next_unique_name(node_name, "ForeachSigmoidInplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR ForeachSigmoid
@auto_convert_to_tensor([True], [False])
def _ForeachSigmoid(x: List[Tensor], *, size_of_y: int, dependencies=[], node_name=None):
    """REG_OP(ForeachSigmoid)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ForeachSigmoid"
    op.name = next_unique_name(node_name, "ForeachSigmoid")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR RoiAlignRotatedGrad
@auto_convert_to_tensor([False, False], [False, False])
def RoiAlignRotatedGrad(x_grad: Tensor, rois: Tensor, *, y_grad_shape: List[int], pooled_h: int, pooled_w: int, spatial_scale: float, sampling_ratio: int=0, aligned: bool=True, clockwise: bool=False, dependencies=[], node_name=None):
    """REG_OP(RoiAlignRotatedGrad)\n
.INPUT(x_grad, TensorType({DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(y_grad_shape, ListInt)\n
.REQUIRED_ATTR(pooled_h, Int)\n
.REQUIRED_ATTR(pooled_w, Int)\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.ATTR(sampling_ratio, Int, 0)\n
.ATTR(aligned, Bool, true)\n
.ATTR(clockwise, Bool, false)\n
.OUTPUT(y_grad, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RoiAlignRotatedGrad"
    op.name = next_unique_name(node_name, "RoiAlignRotatedGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_grad.tensor)
    op.input_desc.add().CopyFrom(x_grad.desc)
    op.input_desc[-1].name = "x_grad"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"

    # process attrs
    op.attr["y_grad_shape"].list.val_type = 2
    op.attr["y_grad_shape"].list.i.extend(y_grad_shape)
    op.attr["pooled_h"].i = pooled_h
    op.attr["pooled_w"].i = pooled_w
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["sampling_ratio"].i = sampling_ratio
    op.attr["aligned"].b = aligned
    op.attr["clockwise"].b = clockwise

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_grad"
    y_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_grad


# This api is auto-generated from IR ToBool
@auto_convert_to_tensor([False], [False])
def ToBool(input: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ToBool)\n
.INPUT(input, TensorType({DT_INT64, DT_INT32, DT_INT16, DT_INT8, DT_UINT8, DT_FLOAT, DT_DOUBLE, DT_STRING, DT_BOOL}))\n
.OUTPUT(output, DT_BOOL)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ToBool"
    op.name = next_unique_name(node_name, "ToBool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR OpTiling
@auto_convert_to_tensor([True, True], [False, False], inputs_tensor_type=[TensorType.TT_ALL, TensorType.TT_ALL])
def OpTiling(x: List[Tensor], output_shape: List[Tensor], *, tiling_node: str, op_type: str, dependencies=[], node_name=None):
    """REG_OP(OpTiling)\n
.DYNAMIC_INPUT(x, TensorType::ALL())\n
.DYNAMIC_INPUT(output_shape, TensorType::ALL())\n
.OUTPUT(tiling_data, TensorType({DT_UINT8}))\n
.OUTPUT(tiling_key, TensorType({DT_UINT64}))\n
.OUTPUT(block_dim, TensorType({DT_INT32}))\n
.OUTPUT(tiling_cond, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(tiling_node, String)\n
.REQUIRED_ATTR(op_type, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OpTiling"
    op.name = next_unique_name(node_name, "OpTiling")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    if not isinstance(output_shape, (tuple, list)):
        raise AssertionError("output_shape must be a tuple or a list.")
    for i, v in enumerate(output_shape):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "output_shape" + str(i)

    # process attrs
    op.attr["tiling_node"].s = compat_as_bytes(tiling_node)
    op.attr["op_type"].s = compat_as_bytes(op_type)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "tiling_data"
    tiling_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tiling_key"
    tiling_key = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "block_dim"
    block_dim = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tiling_cond"
    tiling_cond = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return tiling_data, tiling_key, block_dim, tiling_cond


# This api is auto-generated from IR ConditionCalc
@auto_convert_to_tensor([True], [False], inputs_tensor_type=[TensorType.TT_ALL])
def ConditionCalc(x: List[Tensor], *, cond_func: str, x_dependency: List[int], dependencies=[], node_name=None):
    """REG_OP(ConditionCalc)\n
.DYNAMIC_INPUT(x, TensorType::ALL())\n
.OUTPUT(cond, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(cond_func, String)\n
.REQUIRED_ATTR(x_dependency, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConditionCalc"
    op.name = next_unique_name(node_name, "ConditionCalc")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["cond_func"].s = compat_as_bytes(cond_func)
    op.attr["x_dependency"].list.val_type = 2
    op.attr["x_dependency"].list.i.extend(x_dependency)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "cond"
    cond = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return cond


# This api is auto-generated from IR MakeIterator
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_ALL, TensorType.TT_ALL])
def MakeIterator(x: Tensor, x1: Tensor, *, _kernel: str="dp", dependencies=[], node_name=None):
    """REG_OP(MakeIterator)\n
.INPUT(x, TensorType::ALL())\n
.INPUT(x1, TensorType::ALL())\n
.ATTR(_kernel, String, "dp")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MakeIterator"
    op.name = next_unique_name(node_name, "MakeIterator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"

    # process attrs
    op.attr["_kernel"].s = compat_as_bytes(_kernel)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR IteratorV2
@auto_convert_to_tensor([], [])
def IteratorV2(*, output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(IteratorV2)\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListInt, {})\n
.ATTR(output_shapes, ListListInt, {{}, {}})\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IteratorV2"
    op.name = next_unique_name(node_name, "IteratorV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["output_types"].list.val_type = 2
    op.attr["output_types"].list.i.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IteratorGetNext
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def _IteratorGetNext(x: Tensor, *, size_of_y: int, output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], output_num: int=1, _kernel: str="dp", dependencies=[], node_name=None):
    """REG_OP(IteratorGetNext)\n
.INPUT(x, TensorType::ALL())\n
.DYNAMIC_OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListInt, {})\n
.ATTR(output_shapes, ListListInt, {{},{}})\n
.ATTR(output_num, Int, 1)\n
.ATTR(_kernel, String, "dp")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IteratorGetNext"
    op.name = next_unique_name(node_name, "IteratorGetNext")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_types"].list.val_type = 2
    op.attr["output_types"].list.i.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["output_num"].i = output_num
    op.attr["_kernel"].s = compat_as_bytes(_kernel)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR DeviceQueueDataset
@auto_convert_to_tensor([], [])
def DeviceQueueDataset(*, output_types: List[int]=[], output_shapes: List[List[int]]=[[], []], channel_name: str="", _iterator_name: str="IteratorV2", dependencies=[], node_name=None):
    """REG_OP(DeviceQueueDataset)\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(output_types, ListInt, {})\n
.ATTR(output_shapes, ListListInt, {{},{}})\n
.ATTR(channel_name, String, "")\n
.ATTR(_iterator_name, String, "IteratorV2")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeviceQueueDataset"
    op.name = next_unique_name(node_name, "DeviceQueueDataset")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["output_types"].list.val_type = 2
    op.attr["output_types"].list.i.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["channel_name"].s = compat_as_bytes(channel_name)
    op.attr["_iterator_name"].s = compat_as_bytes(_iterator_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HcomAllGather
@auto_convert_to_tensor([False], [False])
def HcomAllGather(x: Tensor, *, rank_size: int, group: str, fusion: int=0, fusion_id: int=-1, dependencies=[], node_name=None):
    """REG_OP(HcomAllGather)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(rank_size, Int)\n
.REQUIRED_ATTR(group, String)\n
.ATTR(fusion, Int, 0)\n
.ATTR(fusion_id, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomAllGather"
    op.name = next_unique_name(node_name, "HcomAllGather")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["rank_size"].i = rank_size
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["fusion"].i = fusion
    op.attr["fusion_id"].i = fusion_id

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HcomAllReduce
@auto_convert_to_tensor([False], [False])
def HcomAllReduce(x: Tensor, *, reduction: str, group: str, fusion: int=1, fusion_id: int=-1, dependencies=[], node_name=None):
    """REG_OP(HcomAllReduce)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16}))\n
.REQUIRED_ATTR(reduction, String)\n
.REQUIRED_ATTR(group, String)\n
.ATTR(fusion, Int, 1)\n
.ATTR(fusion_id, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomAllReduce"
    op.name = next_unique_name(node_name, "HcomAllReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["fusion"].i = fusion
    op.attr["fusion_id"].i = fusion_id

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HcomBroadcast
@auto_convert_to_tensor([True], [False])
def _HcomBroadcast(x: List[Tensor], *, size_of_y: int, root_rank: int, group: str, fusion: int=0, fusion_id: int=-1, dependencies=[], node_name=None):
    """REG_OP(HcomBroadcast)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(root_rank, Int)\n
.REQUIRED_ATTR(group, String)\n
.ATTR(fusion, Int, 0)\n
.ATTR(fusion_id, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomBroadcast"
    op.name = next_unique_name(node_name, "HcomBroadcast")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["root_rank"].i = root_rank
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["fusion"].i = fusion
    op.attr["fusion_id"].i = fusion_id

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR HcomReduce
@auto_convert_to_tensor([False], [False])
def HcomReduce(x: Tensor, *, root_rank: int, reduction: str, group: str, fusion: int=0, fusion_id: int=-1, dependencies=[], node_name=None):
    """REG_OP(HcomReduce)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16}))\n
.REQUIRED_ATTR(root_rank, Int)\n
.REQUIRED_ATTR(reduction, String)\n
.REQUIRED_ATTR(group, String)\n
.ATTR(fusion, Int, 0)\n
.ATTR(fusion_id, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomReduce"
    op.name = next_unique_name(node_name, "HcomReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["root_rank"].i = root_rank
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["fusion"].i = fusion
    op.attr["fusion_id"].i = fusion_id

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HcomReduceScatter
@auto_convert_to_tensor([False], [False])
def HcomReduceScatter(x: Tensor, *, reduction: str, group: str, rank_size: int, fusion: int=0, fusion_id: int=-1, dependencies=[], node_name=None):
    """REG_OP(HcomReduceScatter)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16}))\n
.REQUIRED_ATTR(reduction, String)\n
.ATTR(fusion, Int, 0)\n
.ATTR(fusion_id, Int, -1)\n
.REQUIRED_ATTR(group, String)\n
.REQUIRED_ATTR(rank_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomReduceScatter"
    op.name = next_unique_name(node_name, "HcomReduceScatter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["rank_size"].i = rank_size
    op.attr["fusion"].i = fusion
    op.attr["fusion_id"].i = fusion_id

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HcomSend
@auto_convert_to_tensor([False], [False])
def HcomSend(x: Tensor, *, group: str, sr_tag: int, dest_rank: int, dependencies=[], node_name=None):
    """REG_OP(HcomSend)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(group, String)\n
.REQUIRED_ATTR(sr_tag, Int)\n
.REQUIRED_ATTR(dest_rank, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomSend"
    op.name = next_unique_name(node_name, "HcomSend")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["sr_tag"].i = sr_tag
    op.attr["dest_rank"].i = dest_rank

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR HcomReceive
@auto_convert_to_tensor([], [])
def HcomReceive(*, group: str, sr_tag: int, src_rank: int, shape: List[int], dtype: int, dependencies=[], node_name=None):
    """REG_OP(HcomReceive)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(group, String)\n
.REQUIRED_ATTR(sr_tag, Int)\n
.REQUIRED_ATTR(src_rank, Int)\n
.REQUIRED_ATTR(shape, ListInt)\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomReceive"
    op.name = next_unique_name(node_name, "HcomReceive")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["sr_tag"].i = sr_tag
    op.attr["src_rank"].i = src_rank
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HcomRemoteRead
@auto_convert_to_tensor([False], [False])
def HcomRemoteRead(remote: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(HcomRemoteRead)\n
.INPUT(remote, TensorType({DT_INT64, DT_UINT64}))\n
.OUTPUT(local, TensorType::ALL())\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomRemoteRead"
    op.name = next_unique_name(node_name, "HcomRemoteRead")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(remote.tensor)
    op.input_desc.add().CopyFrom(remote.desc)
    op.input_desc[-1].name = "remote"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "local"
    local = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return local


# This api is auto-generated from IR HcomRemoteRefRead
@auto_convert_to_tensor([False, False, False], [False, False, False])
def HcomRemoteRefRead(remote: Tensor, cache_var: Tensor, local_offset: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(HcomRemoteRefRead)\n
.INPUT(remote, TensorType({DT_UINT64}))\n
.INPUT(cache_var, TensorType({DT_UINT64}))\n
.INPUT(local_offset, TensorType({DT_UINT64}))\n
.OUTPUT(cache_var, TensorType({DT_UINT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomRemoteRefRead"
    op.name = next_unique_name(node_name, "HcomRemoteRefRead")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(remote.tensor)
    op.input_desc.add().CopyFrom(remote.desc)
    op.input_desc[-1].name = "remote"
    op.input.append(cache_var.tensor)
    op.input_desc.add().CopyFrom(cache_var.desc)
    op.input_desc[-1].name = "cache_var"
    op.input.append(local_offset.tensor)
    op.input_desc.add().CopyFrom(local_offset.desc)
    op.input_desc[-1].name = "local_offset"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "cache_var"
    cache_var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return cache_var


# This api is auto-generated from IR HcomRemoteWrite
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_ALL])
def HcomRemoteWrite(remote: Tensor, local: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(HcomRemoteWrite)\n
.INPUT(remote, TensorType({DT_INT64, DT_UINT64}))\n
.INPUT(local, TensorType::ALL())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomRemoteWrite"
    op.name = next_unique_name(node_name, "HcomRemoteWrite")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(remote.tensor)
    op.input_desc.add().CopyFrom(remote.desc)
    op.input_desc[-1].name = "remote"
    op.input.append(local.tensor)
    op.input_desc.add().CopyFrom(local.desc)
    op.input_desc[-1].name = "local"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR HcomRemoteScatterWrite
@auto_convert_to_tensor([False, False, False], [False, False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_ALL, TensorType.TT_UNKNOWN])
def HcomRemoteScatterWrite(remote: Tensor, local: Tensor, local_offset: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(HcomRemoteScatterWrite)\n
.INPUT(remote, TensorType({DT_INT64, DT_UINT64}))\n
.INPUT(local, TensorType::ALL())\n
.OPTIONAL_INPUT(local_offset, TensorType({DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomRemoteScatterWrite"
    op.name = next_unique_name(node_name, "HcomRemoteScatterWrite")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(remote.tensor)
    op.input_desc.add().CopyFrom(remote.desc)
    op.input_desc[-1].name = "remote"
    op.input.append(local.tensor)
    op.input_desc.add().CopyFrom(local.desc)
    op.input_desc[-1].name = "local"
    if local_offset is not None:
        op.input.append(local_offset.tensor)
        op.input_desc.add().CopyFrom(local_offset.desc)
        op.input_desc[-1].name = "local_offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "local_offset"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR HcomAllToAllV
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def HcomAllToAllV(send_data: Tensor, send_counts: Tensor, send_displacements: Tensor, recv_counts: Tensor, recv_displacements: Tensor, *, group: str, dependencies=[], node_name=None):
    """REG_OP(HcomAllToAllV)\n
.INPUT(send_data, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.INPUT(send_counts, TensorType({DT_INT64}))\n
.INPUT(send_displacements, TensorType({DT_INT64}))\n
.INPUT(recv_counts, TensorType({DT_INT64}))\n
.INPUT(recv_displacements, TensorType({DT_INT64}))\n
.OUTPUT(recv_data, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(group, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomAllToAllV"
    op.name = next_unique_name(node_name, "HcomAllToAllV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(send_data.tensor)
    op.input_desc.add().CopyFrom(send_data.desc)
    op.input_desc[-1].name = "send_data"
    op.input.append(send_counts.tensor)
    op.input_desc.add().CopyFrom(send_counts.desc)
    op.input_desc[-1].name = "send_counts"
    op.input.append(send_displacements.tensor)
    op.input_desc.add().CopyFrom(send_displacements.desc)
    op.input_desc[-1].name = "send_displacements"
    op.input.append(recv_counts.tensor)
    op.input_desc.add().CopyFrom(recv_counts.desc)
    op.input_desc[-1].name = "recv_counts"
    op.input.append(recv_displacements.tensor)
    op.input_desc.add().CopyFrom(recv_displacements.desc)
    op.input_desc[-1].name = "recv_displacements"

    # process attrs
    op.attr["group"].s = compat_as_bytes(group)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "recv_data"
    recv_data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return recv_data


# This api is auto-generated from IR HcomAllToAll
@auto_convert_to_tensor([False], [False])
def HcomAllToAll(x: Tensor, *, group: str, dependencies=[], node_name=None):
    """REG_OP(HcomAllToAll)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(group, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomAllToAll"
    op.name = next_unique_name(node_name, "HcomAllToAll")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["group"].s = compat_as_bytes(group)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HcomAllToAllVC
@auto_convert_to_tensor([False, False], [False, False])
def HcomAllToAllVC(send_data: Tensor, send_count_matrix: Tensor, *, rank: int, group: str, fusion: int=0, fusion_id: int=-1, dependencies=[], node_name=None):
    """REG_OP(HcomAllToAllVC)\n
.INPUT(send_data, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.INPUT(send_count_matrix, TensorType({DT_INT64}))\n
.OUTPUT(recv_data, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(rank, Int)\n
.REQUIRED_ATTR(group, String)\n
.ATTR(fusion, Int, 0)\n
.ATTR(fusion_id, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomAllToAllVC"
    op.name = next_unique_name(node_name, "HcomAllToAllVC")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(send_data.tensor)
    op.input_desc.add().CopyFrom(send_data.desc)
    op.input_desc[-1].name = "send_data"
    op.input.append(send_count_matrix.tensor)
    op.input_desc.add().CopyFrom(send_count_matrix.desc)
    op.input_desc[-1].name = "send_count_matrix"

    # process attrs
    op.attr["rank"].i = rank
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["fusion"].i = fusion
    op.attr["fusion_id"].i = fusion_id

    # process outputs
    output_index = 0
    op.output_desc.add().name = "recv_data"
    recv_data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return recv_data


# This api is auto-generated from IR HcomGatherAllToAllV
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def HcomGatherAllToAllV(addrinfo: Tensor, addrinfo_count_per_rank: Tensor, recv_counts: Tensor, recv_displacements: Tensor, *, group: str, dtype: int, addr_length: int, dependencies=[], node_name=None):
    """REG_OP(HcomGatherAllToAllV)\n
.INPUT(addrinfo, TensorType({DT_UINT64}))\n
.INPUT(addrinfo_count_per_rank, TensorType({DT_INT64}))\n
.INPUT(recv_counts, TensorType({DT_INT64}))\n
.INPUT(recv_displacements, TensorType({DT_INT64}))\n
.OUTPUT(recv_data, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.OUTPUT(gathered, TensorType({DT_FLOAT, DT_INT32, DT_INT8, DT_INT16, DT_FLOAT16, DT_INT64, DT_UINT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_FLOAT64}))\n
.REQUIRED_ATTR(group, String)\n
.REQUIRED_ATTR(dtype, Type)\n
.REQUIRED_ATTR(addr_length, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HcomGatherAllToAllV"
    op.name = next_unique_name(node_name, "HcomGatherAllToAllV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(addrinfo.tensor)
    op.input_desc.add().CopyFrom(addrinfo.desc)
    op.input_desc[-1].name = "addrinfo"
    op.input.append(addrinfo_count_per_rank.tensor)
    op.input_desc.add().CopyFrom(addrinfo_count_per_rank.desc)
    op.input_desc[-1].name = "addrinfo_count_per_rank"
    op.input.append(recv_counts.tensor)
    op.input_desc.add().CopyFrom(recv_counts.desc)
    op.input_desc[-1].name = "recv_counts"
    op.input.append(recv_displacements.tensor)
    op.input_desc.add().CopyFrom(recv_displacements.desc)
    op.input_desc[-1].name = "recv_displacements"

    # process attrs
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["dtype"].dt = dtype
    op.attr["addr_length"].i = addr_length

    # process outputs
    output_index = 0
    op.output_desc.add().name = "recv_data"
    recv_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "gathered"
    gathered = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return recv_data, gathered


# This api is auto-generated from IR HorovodAllgather
@auto_convert_to_tensor([False], [False])
def HorovodAllgather(x: Tensor, *, rank_size: int, dependencies=[], node_name=None):
    """REG_OP(HorovodAllgather)\n
.INPUT(x, TensorType({DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_BOOL}))\n
.REQUIRED_ATTR(rank_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HorovodAllgather"
    op.name = next_unique_name(node_name, "HorovodAllgather")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["rank_size"].i = rank_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HorovodAllreduce
@auto_convert_to_tensor([False], [False])
def HorovodAllreduce(x: Tensor, *, reduce_op: int, dependencies=[], node_name=None):
    """REG_OP(HorovodAllreduce)\n
.INPUT(x, TensorType({DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(reduce_op, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HorovodAllreduce"
    op.name = next_unique_name(node_name, "HorovodAllreduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["reduce_op"].i = reduce_op

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HorovodBroadcast
@auto_convert_to_tensor([False], [False])
def HorovodBroadcast(x: Tensor, *, root_rank: int, dependencies=[], node_name=None):
    """REG_OP(HorovodBroadcast)\n
.INPUT(x, TensorType({DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_BOOL}))\n
.REQUIRED_ATTR(root_rank, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HorovodBroadcast"
    op.name = next_unique_name(node_name, "HorovodBroadcast")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["root_rank"].i = root_rank

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LUT3D
@auto_convert_to_tensor([False, False], [False, False])
def LUT3D(img: Tensor, lut_table: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LUT3D)\n
.INPUT(img, TensorType({DT_UINT8, DT_FLOAT}))\n
.INPUT(lut_table, TensorType({DT_UINT8, DT_FLOAT}))\n
.OUTPUT(lut_img, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LUT3D"
    op.name = next_unique_name(node_name, "LUT3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"
    op.input.append(lut_table.tensor)
    op.input_desc.add().CopyFrom(lut_table.desc)
    op.input_desc[-1].name = "lut_table"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "lut_img"
    lut_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return lut_img


# This api is auto-generated from IR AdjustBrightness
@auto_convert_to_tensor([False, False], [False, False])
def AdjustBrightness(images: Tensor, delta: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdjustBrightness)\n
.INPUT(images, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(delta, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdjustBrightness"
    op.name = next_unique_name(node_name, "AdjustBrightness")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(delta.tensor)
    op.input_desc.add().CopyFrom(delta.desc)
    op.input_desc[-1].name = "delta"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdjustBrightnessV2
@auto_convert_to_tensor([False, False], [False, False])
def AdjustBrightnessV2(images: Tensor, factor: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdjustBrightnessV2)\n
.INPUT(images, TensorType({DT_UINT8}))\n
.INPUT(factor, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdjustBrightnessV2"
    op.name = next_unique_name(node_name, "AdjustBrightnessV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(factor.tensor)
    op.input_desc.add().CopyFrom(factor.desc)
    op.input_desc[-1].name = "factor"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdjustContrastWithMean
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AdjustContrastWithMean(images: Tensor, mean: Tensor, contrast_factor: Tensor, *, data_format: str="HWC", dependencies=[], node_name=None):
    """REG_OP(AdjustContrastWithMean)\n
.INPUT(images, TensorType({DT_FLOAT16,DT_FLOAT,DT_UINT8}))\n
.INPUT(mean, TensorType({DT_FLOAT16,DT_FLOAT,DT_UINT8}))\n
.INPUT(contrast_factor, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_UINT8}))\n
.ATTR(data_format, String, "HWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdjustContrastWithMean"
    op.name = next_unique_name(node_name, "AdjustContrastWithMean")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(contrast_factor.tensor)
    op.input_desc.add().CopyFrom(contrast_factor.desc)
    op.input_desc[-1].name = "contrast_factor"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RgbToGrayscale
@auto_convert_to_tensor([False], [False])
def RgbToGrayscale(images: Tensor, *, data_format: str="HWC", output_channels: int=1, dependencies=[], node_name=None):
    """REG_OP(RgbToGrayscale)\n
.INPUT(images, "T")\n
.ATTR(data_format, String, "HWC")\n
.ATTR(output_channels, Int, 1)\n
.OUTPUT(y, "T")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RgbToGrayscale"
    op.name = next_unique_name(node_name, "RgbToGrayscale")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["output_channels"].i = output_channels

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdjustSaturationV2
@auto_convert_to_tensor([False, False], [False, False])
def AdjustSaturationV2(images: Tensor, scale: Tensor, *, data_format: str="CHW", dependencies=[], node_name=None):
    """REG_OP(AdjustSaturationV2)\n
.INPUT(images, TensorType({DT_UINT8}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
.ATTR(data_format, String, "CHW")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdjustSaturationV2"
    op.name = next_unique_name(node_name, "AdjustSaturationV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeTrilinear
@auto_convert_to_tensor([False, False], [False, False])
def ResizeTrilinear(x: Tensor, size: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeTrilinear)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeTrilinear"
    op.name = next_unique_name(node_name, "ResizeTrilinear")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR WarpAffineV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def WarpAffineV2(x: Tensor, matrix: Tensor, dst_size: Tensor, *, interpolation: str="bilinear", border_type: str="constant", border_value: float=0.000000, data_format: str="HWC", dependencies=[], node_name=None):
    """REG_OP(WarpAffineV2)\n
.INPUT(x, TensorType({DT_FLOAT, DT_UINT8}))\n
.INPUT(matrix, TensorType({DT_FLOAT}))\n
.INPUT(dst_size, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_UINT8}))\n
.ATTR(interpolation, String, "bilinear")\n
.ATTR(border_type, String, "constant")\n
.ATTR(border_value, Float, 0.0)\n
.ATTR(data_format, String, "HWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "WarpAffineV2"
    op.name = next_unique_name(node_name, "WarpAffineV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(matrix.tensor)
    op.input_desc.add().CopyFrom(matrix.desc)
    op.input_desc[-1].name = "matrix"
    op.input.append(dst_size.tensor)
    op.input_desc.add().CopyFrom(dst_size.desc)
    op.input_desc[-1].name = "dst_size"

    # process attrs
    op.attr["interpolation"].s = compat_as_bytes(interpolation)
    op.attr["border_type"].s = compat_as_bytes(border_type)
    op.attr["border_value"].f = border_value
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeV2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ResizeV2(x: Tensor, dst_size: Tensor, *, interpolation: str="nearest", data_format: str="HWC", dependencies=[], node_name=None):
    """REG_OP(ResizeV2)\n
.INPUT(x, TensorType({DT_FLOAT, DT_UINT8}))\n
.INPUT(dst_size, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_UINT8}))\n
.ATTR(interpolation, String, "nearest")\n
.ATTR(data_format, String, "HWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeV2"
    op.name = next_unique_name(node_name, "ResizeV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(dst_size.tensor)
    op.input_desc.add().CopyFrom(dst_size.desc)
    op.input_desc[-1].name = "dst_size"

    # process attrs
    op.attr["interpolation"].s = compat_as_bytes(interpolation)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GaussianBlur
@auto_convert_to_tensor([False], [False])
def GaussianBlur(x: Tensor, *, kernel_size: List[int], sigma: List[float], padding_mode: str="constant", dependencies=[], node_name=None):
    """REG_OP(GaussianBlur)\n
.INPUT(x, "T")\n
.OUTPUT(y, "T")\n
.REQUIRED_ATTR(kernel_size, ListInt)\n
.REQUIRED_ATTR(sigma, ListFloat)\n
.ATTR(padding_mode, String, "constant")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GaussianBlur"
    op.name = next_unique_name(node_name, "GaussianBlur")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["kernel_size"].list.val_type = 2
    op.attr["kernel_size"].list.i.extend(kernel_size)
    op.attr["sigma"].list.val_type = 3
    op.attr["sigma"].list.f.extend(sigma)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Rotate
@auto_convert_to_tensor([False], [False])
def Rotate(x: Tensor, *, angle: float, center: List[int]=[], expand: bool=False, interpolation: str="nearest", padding_mode: str="constant", padding_value: float=0.000000, data_format: str="HWC", dependencies=[], node_name=None):
    """REG_OP(Rotate)\n
.INPUT(x, "T")\n
.OUTPUT(y, "T")\n
.REQUIRED_ATTR(angle, Float)\n
.ATTR(center, ListInt, {})\n
.ATTR(expand, Bool, false)\n
.ATTR(interpolation, String, "nearest")\n
.ATTR(padding_mode, String, "constant")\n
.ATTR(padding_value, Float, 0.0)\n
.ATTR(data_format, String, "HWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Rotate"
    op.name = next_unique_name(node_name, "Rotate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["angle"].f = angle
    op.attr["center"].list.val_type = 2
    op.attr["center"].list.i.extend(center)
    op.attr["expand"].b = expand
    op.attr["interpolation"].s = compat_as_bytes(interpolation)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["padding_value"].f = padding_value
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ImgCrop
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ImgCrop(x: Tensor, boxes: Tensor, box_index: Tensor, *, data_format: str="CHW", dependencies=[], node_name=None):
    """REG_OP(ImgCrop)\n
.INPUT(x, TensorType({DT_FLOAT, DT_UINT8}))\n
.INPUT(boxes, TensorType({DT_UINT32, DT_INT32}))\n
.INPUT(box_index, TensorType({DT_UINT32, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_UINT8}))\n
.ATTR(data_format, String, "CHW")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ImgCrop"
    op.name = next_unique_name(node_name, "ImgCrop")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(box_index.tensor)
    op.input_desc.add().CopyFrom(box_index.desc)
    op.input_desc[-1].name = "box_index"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DecodeGif
@auto_convert_to_tensor([False], [False])
def DecodeGif(contents: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DecodeGif)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(image, TensorType({DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeGif"
    op.name = next_unique_name(node_name, "DecodeGif")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "image"
    image = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return image


# This api is auto-generated from IR AdjustHue
@auto_convert_to_tensor([False, False], [False, False])
def AdjustHue(images: Tensor, delta: Tensor, *, data_format: str="HWC", dependencies=[], node_name=None):
    """REG_OP(AdjustHue)\n
.INPUT(images, TensorType({DT_FLOAT16,DT_FLOAT,DT_UINT8}))\n
.INPUT(delta, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_UINT8}))\n
.ATTR(data_format, String, "HWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdjustHue"
    op.name = next_unique_name(node_name, "AdjustHue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(delta.tensor)
    op.input_desc.add().CopyFrom(delta.desc)
    op.input_desc[-1].name = "delta"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdjustSaturation
@auto_convert_to_tensor([False, False], [False, False])
def AdjustSaturation(images: Tensor, scale: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AdjustSaturation)\n
.INPUT(images, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdjustSaturation"
    op.name = next_unique_name(node_name, "AdjustSaturation")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdjustContrast
@auto_convert_to_tensor([False, False], [False, False])
def AdjustContrast(images: Tensor, contrast_factor: Tensor, *, data_format: str="HWC", mean_mode: str="chn_wise", dependencies=[], node_name=None):
    """REG_OP(AdjustContrast)\n
.INPUT(images, TensorType({DT_FLOAT16,DT_FLOAT,DT_UINT8}))\n
.INPUT(contrast_factor, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_UINT8}))\n
.ATTR(data_format, String, "HWC")\n
.ATTR(mean_mode, String, "chn_wise")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdjustContrast"
    op.name = next_unique_name(node_name, "AdjustContrast")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(contrast_factor.tensor)
    op.input_desc.add().CopyFrom(contrast_factor.desc)
    op.input_desc[-1].name = "contrast_factor"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["mean_mode"].s = compat_as_bytes(mean_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CropAndResize
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def CropAndResize(x: Tensor, boxes: Tensor, box_index: Tensor, crop_size: Tensor, *, extrapolation_value: float=0.000000, method: str="bilinear", dependencies=[], node_name=None):
    """REG_OP(CropAndResize)\n
.INPUT(x, TensorType({DT_UINT8, DT_UINT16, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(box_index, TensorType({DT_INT32}))\n
.INPUT(crop_size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(extrapolation_value, Float, 0)\n
.ATTR(method, String, "bilinear")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CropAndResize"
    op.name = next_unique_name(node_name, "CropAndResize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(box_index.tensor)
    op.input_desc.add().CopyFrom(box_index.desc)
    op.input_desc[-1].name = "box_index"
    op.input.append(crop_size.tensor)
    op.input_desc.add().CopyFrom(crop_size.desc)
    op.input_desc[-1].name = "crop_size"

    # process attrs
    op.attr["extrapolation_value"].f = extrapolation_value
    op.attr["method"].s = compat_as_bytes(method)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CropAndResizeV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def CropAndResizeV2(x: Tensor, boxes: Tensor, box_index: Tensor, crop_size: Tensor, *, extrapolation_value: float=0.000000, method: str="bilinear", dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(CropAndResizeV2)\n
.INPUT(x, TensorType({DT_UINT8, DT_UINT16, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(box_index, TensorType({DT_INT32}))\n
.INPUT(crop_size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT}))\n
.ATTR(extrapolation_value, Float, 0)\n
.ATTR(method, String, "bilinear")\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CropAndResizeV2"
    op.name = next_unique_name(node_name, "CropAndResizeV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(box_index.tensor)
    op.input_desc.add().CopyFrom(box_index.desc)
    op.input_desc[-1].name = "box_index"
    op.input.append(crop_size.tensor)
    op.input_desc.add().CopyFrom(crop_size.desc)
    op.input_desc[-1].name = "crop_size"

    # process attrs
    op.attr["extrapolation_value"].f = extrapolation_value
    op.attr["method"].s = compat_as_bytes(method)
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CropAndResizeD
@auto_convert_to_tensor([False, False, False], [False, False, False])
def CropAndResizeD(x: Tensor, boxes: Tensor, box_index: Tensor, *, crop_size: List[int], extrapolation_value: float=0.000000, method: str="bilinear", dependencies=[], node_name=None):
    """REG_OP(CropAndResizeD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(box_index, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(crop_size, ListInt)\n
.ATTR(extrapolation_value, Float, 0)\n
.ATTR(method, String, "bilinear")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CropAndResizeD"
    op.name = next_unique_name(node_name, "CropAndResizeD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(box_index.tensor)
    op.input_desc.add().CopyFrom(box_index.desc)
    op.input_desc[-1].name = "box_index"

    # process attrs
    op.attr["crop_size"].list.val_type = 2
    op.attr["crop_size"].list.i.extend(crop_size)
    op.attr["extrapolation_value"].f = extrapolation_value
    op.attr["method"].s = compat_as_bytes(method)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CropAndResizeGradBoxes
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def CropAndResizeGradBoxes(grads: Tensor, images: Tensor, boxes: Tensor, box_index: Tensor, *, method: str="bilinear", dependencies=[], node_name=None):
    """REG_OP(CropAndResizeGradBoxes)\n
.INPUT(grads, TensorType({DT_FLOAT}))\n
.INPUT(images, TensorType({DT_UINT8, DT_UINT16, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(box_index, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(method, String, "bilinear")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CropAndResizeGradBoxes"
    op.name = next_unique_name(node_name, "CropAndResizeGradBoxes")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(box_index.tensor)
    op.input_desc.add().CopyFrom(box_index.desc)
    op.input_desc[-1].name = "box_index"

    # process attrs
    op.attr["method"].s = compat_as_bytes(method)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CropAndResizeGradImage
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def CropAndResizeGradImage(grads: Tensor, boxes: Tensor, box_index: Tensor, image_size: Tensor, *, T: int, method: str="bilinear", dependencies=[], node_name=None):
    """REG_OP(CropAndResizeGradImage)\n
.INPUT(grads, TensorType({DT_FLOAT}))\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(box_index, TensorType({DT_INT32}))\n
.INPUT(image_size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(method, String, "bilinear")\n
.REQUIRED_ATTR(T, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CropAndResizeGradImage"
    op.name = next_unique_name(node_name, "CropAndResizeGradImage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(box_index.tensor)
    op.input_desc.add().CopyFrom(box_index.desc)
    op.input_desc[-1].name = "box_index"
    op.input.append(image_size.tensor)
    op.input_desc.add().CopyFrom(image_size.desc)
    op.input_desc[-1].name = "image_size"

    # process attrs
    op.attr["T"].dt = T
    op.attr["method"].s = compat_as_bytes(method)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ExtractGlimpse
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ExtractGlimpse(x: Tensor, size: Tensor, offsets: Tensor, *, centered: bool=True, normalized: bool=True, uniform_noise: bool=True, noise: str="uniform", dependencies=[], node_name=None):
    """REG_OP(ExtractGlimpse)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.INPUT(offsets, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(centered, Bool, true)\n
.ATTR(normalized, Bool, true)\n
.ATTR(uniform_noise, Bool, true)\n
.ATTR(noise, String, "uniform")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ExtractGlimpse"
    op.name = next_unique_name(node_name, "ExtractGlimpse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"

    # process attrs
    op.attr["centered"].b = centered
    op.attr["normalized"].b = normalized
    op.attr["uniform_noise"].b = uniform_noise
    op.attr["noise"].s = compat_as_bytes(noise)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HSVToRGB
@auto_convert_to_tensor([False], [False])
def HSVToRGB(images: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(HSVToRGB)\n
.INPUT(images, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HSVToRGB"
    op.name = next_unique_name(node_name, "HSVToRGB")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR QuantizedResizeBilinear
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def QuantizedResizeBilinear(images: Tensor, size: Tensor, min: Tensor, max: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(QuantizedResizeBilinear)\n
.INPUT(images, TensorType({DT_QUINT8,DT_QINT32,DT_FLOAT}))\n
.INPUT(size, TensorType({ DT_INT32 }))\n
.INPUT(min, TensorType({ DT_FLOAT }))\n
.INPUT(max, TensorType({ DT_FLOAT }))\n
.OUTPUT(resized_images, TensorType({DT_QUINT8,DT_QINT32,DT_FLOAT }))\n
.OUTPUT(y_min, TensorType({ DT_FLOAT }))\n
.OUTPUT(y_max, TensorType({ DT_FLOAT }))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QuantizedResizeBilinear"
    op.name = next_unique_name(node_name, "QuantizedResizeBilinear")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "resized_images"
    resized_images = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_min"
    y_min = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_max"
    y_max = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return resized_images, y_min, y_max


# This api is auto-generated from IR ResizeArea
@auto_convert_to_tensor([False, False], [False, False])
def ResizeArea(images: Tensor, size: Tensor, *, align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeArea)\n
.INPUT(images, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeArea"
    op.name = next_unique_name(node_name, "ResizeArea")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeBicubicGrad
@auto_convert_to_tensor([False, False], [False, False])
def ResizeBicubicGrad(grads: Tensor, original_image: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeBicubicGrad)\n
.INPUT(grads, TensorType({DT_FLOAT}))\n
.INPUT(original_image, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeBicubicGrad"
    op.name = next_unique_name(node_name, "ResizeBicubicGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(original_image.tensor)
    op.input_desc.add().CopyFrom(original_image.desc)
    op.input_desc[-1].name = "original_image"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeBicubic
@auto_convert_to_tensor([False, False], [False, False])
def ResizeBicubic(images: Tensor, size: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(ResizeBicubic)\n
.INPUT(images, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_UINT8, DT_FLOAT}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeBicubic"
    op.name = next_unique_name(node_name, "ResizeBicubic")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeNearestNeighborV2Grad
@auto_convert_to_tensor([False, False], [False, False])
def ResizeNearestNeighborV2Grad(grads: Tensor, size: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeNearestNeighborV2Grad)\n
.INPUT(grads, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeNearestNeighborV2Grad"
    op.name = next_unique_name(node_name, "ResizeNearestNeighborV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeNearestNeighborV2GradD
@auto_convert_to_tensor([False], [False])
def ResizeNearestNeighborV2GradD(grads: Tensor, *, size: List[int], align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeNearestNeighborV2GradD)\n
.INPUT(grads, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(size, ListInt)\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeNearestNeighborV2GradD"
    op.name = next_unique_name(node_name, "ResizeNearestNeighborV2GradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"

    # process attrs
    op.attr["size"].list.val_type = 2
    op.attr["size"].list.i.extend(size)
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeBilinearV2Grad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_FLOATING])
def ResizeBilinearV2Grad(grads: Tensor, original_image: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeBilinearV2Grad)\n
.INPUT(grads, TensorType({DT_FLOAT}))\n
.INPUT(original_image, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeBilinearV2Grad"
    op.name = next_unique_name(node_name, "ResizeBilinearV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(original_image.tensor)
    op.input_desc.add().CopyFrom(original_image.desc)
    op.input_desc[-1].name = "original_image"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SyncResizeBilinearV2Grad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_FLOATING])
def SyncResizeBilinearV2Grad(grads: Tensor, original_image: Tensor, *, size: List[int]=[], ori_image_size: List[int]=[], src_start_w: int=0, dst_start_w: int=0, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(SyncResizeBilinearV2Grad)\n
.INPUT(grads, TensorType({DT_FLOAT}))\n
.INPUT(original_image, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(size, ListInt, {})\n
.ATTR(ori_image_size, ListInt, {})\n
.ATTR(src_start_w, Int, 0)\n
.ATTR(dst_start_w, Int, 0)\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SyncResizeBilinearV2Grad"
    op.name = next_unique_name(node_name, "SyncResizeBilinearV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(original_image.tensor)
    op.input_desc.add().CopyFrom(original_image.desc)
    op.input_desc[-1].name = "original_image"

    # process attrs
    op.attr["size"].list.val_type = 2
    op.attr["size"].list.i.extend(size)
    op.attr["ori_image_size"].list.val_type = 2
    op.attr["ori_image_size"].list.i.extend(ori_image_size)
    op.attr["src_start_w"].i = src_start_w
    op.attr["dst_start_w"].i = dst_start_w
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeBilinearV2
@auto_convert_to_tensor([False, False], [False, False])
def ResizeBilinearV2(x: Tensor, size: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(ResizeBilinearV2)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_UINT8, DT_FLOAT}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeBilinearV2"
    op.name = next_unique_name(node_name, "ResizeBilinearV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SyncResizeBilinearV2
@auto_convert_to_tensor([False, False], [False, False])
def SyncResizeBilinearV2(x: Tensor, size: Tensor, *, ori_image_size: List[int]=[], split_size: List[int]=[], src_start_w: int=0, dst_start_w: int=0, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(SyncResizeBilinearV2)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(ori_image_size, ListInt, {})\n
.ATTR(split_size, ListInt, {})\n
.ATTR(src_start_w, Int, 0)\n
.ATTR(dst_start_w, Int, 0)\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SyncResizeBilinearV2"
    op.name = next_unique_name(node_name, "SyncResizeBilinearV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["ori_image_size"].list.val_type = 2
    op.attr["ori_image_size"].list.i.extend(ori_image_size)
    op.attr["split_size"].list.val_type = 2
    op.attr["split_size"].list.i.extend(split_size)
    op.attr["src_start_w"].i = src_start_w
    op.attr["dst_start_w"].i = dst_start_w
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RGBToHSV
@auto_convert_to_tensor([False], [False])
def RGBToHSV(images: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RGBToHSV)\n
.INPUT(images, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE }))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RGBToHSV"
    op.name = next_unique_name(node_name, "RGBToHSV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SampleDistortedBoundingBox
@auto_convert_to_tensor([False, False], [False, False])
def SampleDistortedBoundingBox(image_size: Tensor, bounding_boxes: Tensor, *, seed: int=0, seed2: int=0, min_object_covered: float=0.100000, aspect_ratio_range: List[float]=[0.750000, 1.330000], area_range: List[float]=[0.050000, 1.000000], max_attempts: int=100, use_image_if_no_bounding_boxes: bool=False, dependencies=[], node_name=None):
    """REG_OP(SampleDistortedBoundingBox)\n
.INPUT(image_size, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.INPUT(bounding_boxes, TensorType({ DT_FLOAT }))\n
.OUTPUT(begin, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.OUTPUT(size, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.OUTPUT(bboxes, TensorType({ DT_FLOAT }))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
.ATTR(min_object_covered, Float, 0.1f)\n
.ATTR(aspect_ratio_range, ListFloat, { 0.75f, 1.33f })\n
.ATTR(area_range, ListFloat, { 0.05f, 1.0f })\n
.ATTR(max_attempts, Int, 100)\n
.ATTR(use_image_if_no_bounding_boxes, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SampleDistortedBoundingBox"
    op.name = next_unique_name(node_name, "SampleDistortedBoundingBox")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(image_size.tensor)
    op.input_desc.add().CopyFrom(image_size.desc)
    op.input_desc[-1].name = "image_size"
    op.input.append(bounding_boxes.tensor)
    op.input_desc.add().CopyFrom(bounding_boxes.desc)
    op.input_desc[-1].name = "bounding_boxes"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2
    op.attr["min_object_covered"].f = min_object_covered
    op.attr["aspect_ratio_range"].list.val_type = 3
    op.attr["aspect_ratio_range"].list.f.extend(aspect_ratio_range)
    op.attr["area_range"].list.val_type = 3
    op.attr["area_range"].list.f.extend(area_range)
    op.attr["max_attempts"].i = max_attempts
    op.attr["use_image_if_no_bounding_boxes"].b = use_image_if_no_bounding_boxes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "begin"
    begin = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "bboxes"
    bboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return begin, size, bboxes


# This api is auto-generated from IR SampleDistortedBoundingBoxExt2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SampleDistortedBoundingBoxExt2(image_size: Tensor, bounding_boxes: Tensor, min_object_covered: Tensor, *, seed: int=0, seed2: int=0, aspect_ratio_range: List[float]=[0.750000, 1.330000], area_range: List[float]=[0.050000, 1.000000], max_attempts: int=100, use_image_if_no_bounding_boxes: bool=False, dependencies=[], node_name=None):
    """REG_OP(SampleDistortedBoundingBoxExt2)\n
.INPUT(image_size, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.INPUT(bounding_boxes, TensorType({ DT_FLOAT }))\n
.INPUT(min_object_covered, TensorType({ DT_FLOAT }))\n
.OUTPUT(begin, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.OUTPUT(size, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.OUTPUT(bboxes, TensorType({ DT_FLOAT }))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
.ATTR(aspect_ratio_range, ListFloat, { 0.75f, 1.33f })\n
.ATTR(area_range, ListFloat, { 0.05f, 1.0f })\n
.ATTR(max_attempts, Int, 100)\n
.ATTR(use_image_if_no_bounding_boxes, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SampleDistortedBoundingBoxExt2"
    op.name = next_unique_name(node_name, "SampleDistortedBoundingBoxExt2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(image_size.tensor)
    op.input_desc.add().CopyFrom(image_size.desc)
    op.input_desc[-1].name = "image_size"
    op.input.append(bounding_boxes.tensor)
    op.input_desc.add().CopyFrom(bounding_boxes.desc)
    op.input_desc[-1].name = "bounding_boxes"
    op.input.append(min_object_covered.tensor)
    op.input_desc.add().CopyFrom(min_object_covered.desc)
    op.input_desc[-1].name = "min_object_covered"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2
    op.attr["aspect_ratio_range"].list.val_type = 3
    op.attr["aspect_ratio_range"].list.f.extend(aspect_ratio_range)
    op.attr["area_range"].list.val_type = 3
    op.attr["area_range"].list.f.extend(area_range)
    op.attr["max_attempts"].i = max_attempts
    op.attr["use_image_if_no_bounding_boxes"].b = use_image_if_no_bounding_boxes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "begin"
    begin = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "bboxes"
    bboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return begin, size, bboxes


# This api is auto-generated from IR ResizeNearestNeighborV2
@auto_convert_to_tensor([False, False], [False, False])
def ResizeNearestNeighborV2(x: Tensor, size: Tensor, *, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeNearestNeighborV2)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeNearestNeighborV2"
    op.name = next_unique_name(node_name, "ResizeNearestNeighborV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DrawBoundingBoxes
@auto_convert_to_tensor([False, False], [False, False])
def DrawBoundingBoxes(images: Tensor, boxes: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DrawBoundingBoxes)\n
.INPUT(images, TensorType({DT_FLOAT}))\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DrawBoundingBoxes"
    op.name = next_unique_name(node_name, "DrawBoundingBoxes")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NonMaxSuppression
@auto_convert_to_tensor([False, False, False], [False, False, False])
def NonMaxSuppression(boxes: Tensor, scores: Tensor, max_output_size: Tensor, *, iou_threshold: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppression)\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT}))\n
.INPUT(max_output_size, TensorType({DT_INT32}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
.ATTR(iou_threshold, Float, 0.5f)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppression"
    op.name = next_unique_name(node_name, "NonMaxSuppression")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(max_output_size.tensor)
    op.input_desc.add().CopyFrom(max_output_size.desc)
    op.input_desc[-1].name = "max_output_size"

    # process attrs
    op.attr["iou_threshold"].f = iou_threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices


# This api is auto-generated from IR NonMaxSuppressionV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def NonMaxSuppressionV2(boxes: Tensor, scores: Tensor, max_output_size: Tensor, iou_threshold: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionV2)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(max_output_size, TensorType({DT_INT32}))\n
.INPUT(iou_threshold, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionV2"
    op.name = next_unique_name(node_name, "NonMaxSuppressionV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(max_output_size.tensor)
    op.input_desc.add().CopyFrom(max_output_size.desc)
    op.input_desc[-1].name = "max_output_size"
    op.input.append(iou_threshold.tensor)
    op.input_desc.add().CopyFrom(iou_threshold.desc)
    op.input_desc[-1].name = "iou_threshold"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices


# This api is auto-generated from IR NonMaxSuppressionV3
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def NonMaxSuppressionV3(boxes: Tensor, scores: Tensor, max_output_size: Tensor, iou_threshold: Tensor, score_threshold: Tensor, *, offset: int=0, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionV3)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(max_output_size, TensorType({DT_INT32}))\n
.INPUT(iou_threshold, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(score_threshold, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
.ATTR(offset, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionV3"
    op.name = next_unique_name(node_name, "NonMaxSuppressionV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(max_output_size.tensor)
    op.input_desc.add().CopyFrom(max_output_size.desc)
    op.input_desc[-1].name = "max_output_size"
    op.input.append(iou_threshold.tensor)
    op.input_desc.add().CopyFrom(iou_threshold.desc)
    op.input_desc[-1].name = "iou_threshold"
    op.input.append(score_threshold.tensor)
    op.input_desc.add().CopyFrom(score_threshold.desc)
    op.input_desc[-1].name = "score_threshold"

    # process attrs
    op.attr["offset"].i = offset

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices


# This api is auto-generated from IR NonMaxSuppressionV4
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def NonMaxSuppressionV4(boxes: Tensor, scores: Tensor, max_output_size: Tensor, iou_threshold: Tensor, score_threshold: Tensor, *, pad_to_max_output_size: bool=False, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionV4)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(max_output_size, TensorType({DT_INT32}))\n
.INPUT(iou_threshold, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(score_threshold, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
.OUTPUT(valid_outputs, TensorType({DT_INT32}))\n
.ATTR(pad_to_max_output_size, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionV4"
    op.name = next_unique_name(node_name, "NonMaxSuppressionV4")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(max_output_size.tensor)
    op.input_desc.add().CopyFrom(max_output_size.desc)
    op.input_desc[-1].name = "max_output_size"
    op.input.append(iou_threshold.tensor)
    op.input_desc.add().CopyFrom(iou_threshold.desc)
    op.input_desc[-1].name = "iou_threshold"
    op.input.append(score_threshold.tensor)
    op.input_desc.add().CopyFrom(score_threshold.desc)
    op.input_desc[-1].name = "score_threshold"

    # process attrs
    op.attr["pad_to_max_output_size"].b = pad_to_max_output_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "valid_outputs"
    valid_outputs = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices, valid_outputs


# This api is auto-generated from IR NonMaxSuppressionWithOverlaps
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def NonMaxSuppressionWithOverlaps(overlaps: Tensor, scores: Tensor, max_output_size: Tensor, overlap_threshold: Tensor, score_threshold: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionWithOverlaps)\n
.INPUT(overlaps, TensorType({DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT}))\n
.INPUT(max_output_size, TensorType({DT_INT32}))\n
.INPUT(overlap_threshold, TensorType({DT_FLOAT}))\n
.INPUT(score_threshold, TensorType({DT_FLOAT}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionWithOverlaps"
    op.name = next_unique_name(node_name, "NonMaxSuppressionWithOverlaps")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(overlaps.tensor)
    op.input_desc.add().CopyFrom(overlaps.desc)
    op.input_desc[-1].name = "overlaps"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(max_output_size.tensor)
    op.input_desc.add().CopyFrom(max_output_size.desc)
    op.input_desc[-1].name = "max_output_size"
    op.input.append(overlap_threshold.tensor)
    op.input_desc.add().CopyFrom(overlap_threshold.desc)
    op.input_desc[-1].name = "overlap_threshold"
    op.input.append(score_threshold.tensor)
    op.input_desc.add().CopyFrom(score_threshold.desc)
    op.input_desc[-1].name = "score_threshold"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices


# This api is auto-generated from IR EncodeJpeg
@auto_convert_to_tensor([False], [False])
def EncodeJpeg(image: Tensor, *, format: str="", quality: int=95, progressive: bool=False, optimize_size: bool=False, chroma_downsampling: bool=True, density_unit: str="in", x_density: int=300, y_density: int=300, xmp_metadata: str="", dependencies=[], node_name=None):
    """REG_OP(EncodeJpeg)\n
.INPUT(image, TensorType({DT_UINT8}))\n
.OUTPUT(contents, TensorType({DT_STRING}))\n
.ATTR(format, String, "")\n
.ATTR(quality, Int, 95)\n
.ATTR(progressive, Bool, false)\n
.ATTR(optimize_size, Bool, false)\n
.ATTR(chroma_downsampling, Bool, true)\n
.ATTR(density_unit, String, "in")\n
.ATTR(x_density, Int, 300)\n
.ATTR(y_density, Int, 300)\n
.ATTR(xmp_metadata, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EncodeJpeg"
    op.name = next_unique_name(node_name, "EncodeJpeg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(image.tensor)
    op.input_desc.add().CopyFrom(image.desc)
    op.input_desc[-1].name = "image"

    # process attrs
    op.attr["format"].s = compat_as_bytes(format)
    op.attr["quality"].i = quality
    op.attr["progressive"].b = progressive
    op.attr["optimize_size"].b = optimize_size
    op.attr["chroma_downsampling"].b = chroma_downsampling
    op.attr["density_unit"].s = compat_as_bytes(density_unit)
    op.attr["x_density"].i = x_density
    op.attr["y_density"].i = y_density
    op.attr["xmp_metadata"].s = compat_as_bytes(xmp_metadata)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "contents"
    contents = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return contents


# This api is auto-generated from IR EncodePng
@auto_convert_to_tensor([False], [False])
def EncodePng(image: Tensor, *, compression: int=-1, dependencies=[], node_name=None):
    """REG_OP(EncodePng)\n
.INPUT(image, TensorType({DT_UINT8, DT_UINT16}))\n
.OUTPUT(contents, TensorType({DT_STRING}))\n
.ATTR(compression, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EncodePng"
    op.name = next_unique_name(node_name, "EncodePng")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(image.tensor)
    op.input_desc.add().CopyFrom(image.desc)
    op.input_desc[-1].name = "image"

    # process attrs
    op.attr["compression"].i = compression

    # process outputs
    output_index = 0
    op.output_desc.add().name = "contents"
    contents = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return contents


# This api is auto-generated from IR DecodePng
@auto_convert_to_tensor([False], [False])
def DecodePng(contents: Tensor, *, dtype: int=DataType.DT_UINT8, channels: int=0, dependencies=[], node_name=None):
    """REG_OP(DecodePng)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(image, TensorType({DT_UINT8, DT_UINT16}))\n
.ATTR(dtype, Type, DT_UINT8)\n
.ATTR(channels, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodePng"
    op.name = next_unique_name(node_name, "DecodePng")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["channels"].i = channels

    # process outputs
    output_index = 0
    op.output_desc.add().name = "image"
    image = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return image


# This api is auto-generated from IR DecodeBmp
@auto_convert_to_tensor([False], [False])
def DecodeBmp(contents: Tensor, *, channels: int=0, dependencies=[], node_name=None):
    """REG_OP(DecodeBmp)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(image, TensorType({DT_UINT8}))\n
.ATTR(channels, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeBmp"
    op.name = next_unique_name(node_name, "DecodeBmp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs
    op.attr["channels"].i = channels

    # process outputs
    output_index = 0
    op.output_desc.add().name = "image"
    image = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return image


# This api is auto-generated from IR DecodeAndCropJpeg
@auto_convert_to_tensor([False, False], [False, False])
def DecodeAndCropJpeg(contents: Tensor, crop_window: Tensor, *, channels: int=0, ratio: int=1, fancy_upscaling: bool=True, try_recover_truncated: bool=False, acceptable_fraction: float=1.000000, dct_method: str="", dst_img_format: str="HWC", dependencies=[], node_name=None):
    """REG_OP(DecodeAndCropJpeg)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.INPUT(crop_window, TensorType({DT_INT32}))\n
.OUTPUT(image, TensorType({DT_UINT8}))\n
.ATTR(channels, Int, 0)\n
.ATTR(ratio, Int, 1)\n
.ATTR(fancy_upscaling, Bool, true)\n
.ATTR(try_recover_truncated, Bool, false)\n
.ATTR(acceptable_fraction, Float, 1.0)\n
.ATTR(dct_method, String, "")\n
.ATTR(dst_img_format, String, "HWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeAndCropJpeg"
    op.name = next_unique_name(node_name, "DecodeAndCropJpeg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"
    op.input.append(crop_window.tensor)
    op.input_desc.add().CopyFrom(crop_window.desc)
    op.input_desc[-1].name = "crop_window"

    # process attrs
    op.attr["channels"].i = channels
    op.attr["ratio"].i = ratio
    op.attr["fancy_upscaling"].b = fancy_upscaling
    op.attr["try_recover_truncated"].b = try_recover_truncated
    op.attr["acceptable_fraction"].f = acceptable_fraction
    op.attr["dct_method"].s = compat_as_bytes(dct_method)
    op.attr["dst_img_format"].s = compat_as_bytes(dst_img_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "image"
    image = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return image


# This api is auto-generated from IR ResizeBilinearV2D
@auto_convert_to_tensor([False], [False])
def ResizeBilinearV2D(x: Tensor, *, size: List[int], align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeBilinearV2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
.REQUIRED_ATTR(size, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeBilinearV2D"
    op.name = next_unique_name(node_name, "ResizeBilinearV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["size"].list.val_type = 2
    op.attr["size"].list.i.extend(size)
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR KeepRatioResizeBilinear
@auto_convert_to_tensor([False], [False])
def KeepRatioResizeBilinear(images: Tensor, *, min_dimension: int, max_dimension: int, align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(KeepRatioResizeBilinear)\n
.INPUT(images, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(min_dimension, Int)\n
.REQUIRED_ATTR(max_dimension, Int)\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "KeepRatioResizeBilinear"
    op.name = next_unique_name(node_name, "KeepRatioResizeBilinear")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"

    # process attrs
    op.attr["min_dimension"].i = min_dimension
    op.attr["max_dimension"].i = max_dimension
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeNearestNeighborV2D
@auto_convert_to_tensor([False], [False])
def ResizeNearestNeighborV2D(x: Tensor, *, size: List[int], align_corners: bool=False, half_pixel_centers: bool=False, dependencies=[], node_name=None):
    """REG_OP(ResizeNearestNeighborV2D)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.REQUIRED_ATTR(size, ListInt)\n
.ATTR(align_corners, Bool, false)\n
.ATTR(half_pixel_centers, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeNearestNeighborV2D"
    op.name = next_unique_name(node_name, "ResizeNearestNeighborV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["size"].list.val_type = 2
    op.attr["size"].list.i.extend(size)
    op.attr["align_corners"].b = align_corners
    op.attr["half_pixel_centers"].b = half_pixel_centers

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ExtractJpegShape
@auto_convert_to_tensor([False], [False])
def ExtractJpegShape(contents: Tensor, *, output_type: int, dependencies=[], node_name=None):
    """REG_OP(ExtractJpegShape)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(image_shape, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(output_type, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ExtractJpegShape"
    op.name = next_unique_name(node_name, "ExtractJpegShape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs
    op.attr["output_type"].dt = output_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "image_shape"
    image_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return image_shape


# This api is auto-generated from IR DrawBoundingBoxesV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DrawBoundingBoxesV2(images: Tensor, boxes: Tensor, colors: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DrawBoundingBoxesV2)\n
.INPUT(images, TensorType({DT_FLOAT}))\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(colors, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DrawBoundingBoxesV2"
    op.name = next_unique_name(node_name, "DrawBoundingBoxesV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(colors.tensor)
    op.input_desc.add().CopyFrom(colors.desc)
    op.input_desc[-1].name = "colors"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NonMaxSuppressionV5
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def NonMaxSuppressionV5(boxes: Tensor, scores: Tensor, max_output_size: Tensor, iou_threshold: Tensor, score_threshold: Tensor, soft_nms_sigma: Tensor, *, T: int, pad_to_max_output_size: bool=False, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionV5)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(max_output_size, TensorType({DT_INT32}))\n
.INPUT(iou_threshold, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(score_threshold, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(soft_nms_sigma, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
.OUTPUT(selected_scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(valid_outputs, TensorType({DT_INT32}))\n
.ATTR(pad_to_max_output_size, Bool, false)\n
.REQUIRED_ATTR(T, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionV5"
    op.name = next_unique_name(node_name, "NonMaxSuppressionV5")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(max_output_size.tensor)
    op.input_desc.add().CopyFrom(max_output_size.desc)
    op.input_desc[-1].name = "max_output_size"
    op.input.append(iou_threshold.tensor)
    op.input_desc.add().CopyFrom(iou_threshold.desc)
    op.input_desc[-1].name = "iou_threshold"
    op.input.append(score_threshold.tensor)
    op.input_desc.add().CopyFrom(score_threshold.desc)
    op.input_desc[-1].name = "score_threshold"
    op.input.append(soft_nms_sigma.tensor)
    op.input_desc.add().CopyFrom(soft_nms_sigma.desc)
    op.input_desc[-1].name = "soft_nms_sigma"

    # process attrs
    op.attr["T"].dt = T
    op.attr["pad_to_max_output_size"].b = pad_to_max_output_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "selected_scores"
    selected_scores = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "valid_outputs"
    valid_outputs = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices, selected_scores, valid_outputs


# This api is auto-generated from IR ScaleAndTranslate
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def ScaleAndTranslate(images: Tensor, size: Tensor, scale: Tensor, translation: Tensor, *, kernel_type: str="lanczos3", antialias: bool=True, dependencies=[], node_name=None):
    """REG_OP(ScaleAndTranslate)\n
.INPUT(images, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(translation, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(kernel_type, String, "lanczos3")\n
.ATTR(antialias, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScaleAndTranslate"
    op.name = next_unique_name(node_name, "ScaleAndTranslate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(translation.tensor)
    op.input_desc.add().CopyFrom(translation.desc)
    op.input_desc[-1].name = "translation"

    # process attrs
    op.attr["kernel_type"].s = compat_as_bytes(kernel_type)
    op.attr["antialias"].b = antialias

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScaleAndTranslateGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def ScaleAndTranslateGrad(grads: Tensor, original_image: Tensor, scale: Tensor, translation: Tensor, *, kernel_type: str="lanczos3", antialias: bool=True, dependencies=[], node_name=None):
    """REG_OP(ScaleAndTranslateGrad)\n
.INPUT(grads, TensorType({DT_FLOAT}))\n
.INPUT(original_image, TensorType({DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(translation, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(kernel_type, String, "lanczos3")\n
.ATTR(antialias, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScaleAndTranslateGrad"
    op.name = next_unique_name(node_name, "ScaleAndTranslateGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(original_image.tensor)
    op.input_desc.add().CopyFrom(original_image.desc)
    op.input_desc[-1].name = "original_image"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(translation.tensor)
    op.input_desc.add().CopyFrom(translation.desc)
    op.input_desc[-1].name = "translation"

    # process attrs
    op.attr["kernel_type"].s = compat_as_bytes(kernel_type)
    op.attr["antialias"].b = antialias

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CombinedNonMaxSuppression
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def CombinedNonMaxSuppression(boxes: Tensor, scores: Tensor, max_output_size_per_class: Tensor, max_total_size: Tensor, iou_threshold: Tensor, score_threshold: Tensor, *, pad_per_class: bool=False, clip_boxes: bool=True, dependencies=[], node_name=None):
    """REG_OP(CombinedNonMaxSuppression)\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT}))\n
.INPUT(max_output_size_per_class, TensorType({DT_INT32}))\n
.INPUT(max_total_size, TensorType({DT_INT32}))\n
.INPUT(iou_threshold, TensorType({DT_FLOAT}))\n
.INPUT(score_threshold, TensorType({DT_FLOAT}))\n
.OUTPUT(nmsed_boxes, TensorType({DT_FLOAT}))\n
.OUTPUT(nmsed_scores, TensorType({DT_FLOAT}))\n
.OUTPUT(nmsed_classes, TensorType({DT_FLOAT}))\n
.OUTPUT(valid_detections, TensorType({DT_INT32}))\n
.ATTR(pad_per_class, Bool, false)\n
.ATTR(clip_boxes, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CombinedNonMaxSuppression"
    op.name = next_unique_name(node_name, "CombinedNonMaxSuppression")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(max_output_size_per_class.tensor)
    op.input_desc.add().CopyFrom(max_output_size_per_class.desc)
    op.input_desc[-1].name = "max_output_size_per_class"
    op.input.append(max_total_size.tensor)
    op.input_desc.add().CopyFrom(max_total_size.desc)
    op.input_desc[-1].name = "max_total_size"
    op.input.append(iou_threshold.tensor)
    op.input_desc.add().CopyFrom(iou_threshold.desc)
    op.input_desc[-1].name = "iou_threshold"
    op.input.append(score_threshold.tensor)
    op.input_desc.add().CopyFrom(score_threshold.desc)
    op.input_desc[-1].name = "score_threshold"

    # process attrs
    op.attr["pad_per_class"].b = pad_per_class
    op.attr["clip_boxes"].b = clip_boxes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "nmsed_boxes"
    nmsed_boxes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nmsed_scores"
    nmsed_scores = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nmsed_classes"
    nmsed_classes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "valid_detections"
    valid_detections = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections


# This api is auto-generated from IR IMGWarp
@auto_convert_to_tensor([False, False], [False, False])
def IMGWarp(img: Tensor, warp_offset: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IMGWarp)\n
.INPUT(img, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT32}))\n
.INPUT(warp_offset, TensorType({DT_FLOAT32}))\n
.OUTPUT(warp_img, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IMGWarp"
    op.name = next_unique_name(node_name, "IMGWarp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"
    op.input.append(warp_offset.tensor)
    op.input_desc.add().CopyFrom(warp_offset.desc)
    op.input_desc[-1].name = "warp_offset"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "warp_img"
    warp_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return warp_img


# This api is auto-generated from IR Remap
@auto_convert_to_tensor([False, False], [False, False])
def Remap(img: Tensor, map_offset: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Remap)\n
.INPUT(img, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT32}))\n
.INPUT(map_offset, TensorType({DT_FLOAT32}))\n
.OUTPUT(map_img, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Remap"
    op.name = next_unique_name(node_name, "Remap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"
    op.input.append(map_offset.tensor)
    op.input_desc.add().CopyFrom(map_offset.desc)
    op.input_desc[-1].name = "map_offset"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "map_img"
    map_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return map_img


# This api is auto-generated from IR IMGWarpResize
@auto_convert_to_tensor([False, False], [False, False])
def IMGWarpResize(img: Tensor, warp_index: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IMGWarpResize)\n
.INPUT(img, TensorType({DT_FLOAT32}))\n
.INPUT(warp_index, TensorType({DT_FLOAT32}))\n
.OUTPUT(warp_img, TensorType({DT_FLOAT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IMGWarpResize"
    op.name = next_unique_name(node_name, "IMGWarpResize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"
    op.input.append(warp_index.tensor)
    op.input_desc.add().CopyFrom(warp_index.desc)
    op.input_desc[-1].name = "warp_index"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "warp_img"
    warp_img = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return warp_img


# This api is auto-generated from IR SpatialTransformerD
@auto_convert_to_tensor([False, False], [False, True])
def SpatialTransformerD(x: Tensor, theta: Optional[Tensor], *, output_size: List[int]=[-1, -1], default_theta: List[float]=[], align_corners: bool=False, use_default_theta: List[bool]=[], dependencies=[], node_name=None):
    """REG_OP(SpatialTransformerD)\n
.INPUT(x, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.OPTIONAL_INPUT(theta, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.ATTR(output_size, ListInt, {-1, -1})\n
.ATTR(default_theta, ListFloat, {})\n
.ATTR(align_corners, Bool, false)\n
.ATTR(use_default_theta, ListBool, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SpatialTransformerD"
    op.name = next_unique_name(node_name, "SpatialTransformerD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if theta is not None:
        op.input.append(theta.tensor)
        op.input_desc.add().CopyFrom(theta.desc)
        op.input_desc[-1].name = "theta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "theta"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["default_theta"].list.val_type = 3
    op.attr["default_theta"].list.f.extend(default_theta)
    op.attr["align_corners"].b = align_corners
    op.attr["use_default_theta"].list.val_type = 4
    op.attr["use_default_theta"].list.b.extend(use_default_theta)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SpatialTransformer
@auto_convert_to_tensor([False, False], [False, True])
def SpatialTransformer(x: Tensor, theta: Optional[Tensor], *, output_size: List[int]=[-1, -1], default_theta: List[float]=[], align_corners: bool=False, use_default_theta: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(SpatialTransformer)\n
.INPUT(x, TensorType({DT_FLOAT,DT_FLOAT16,DT_DOUBLE,DT_UINT8,DT_INT8,DT_UINT16, DT_INT16,DT_INT32,DT_UINT32,DT_UINT64,DT_INT64}))\n
.OPTIONAL_INPUT(theta, TensorType({DT_FLOAT,DT_FLOAT16,DT_DOUBLE,DT_UINT8,DT_INT8, DT_UINT16,DT_INT16,DT_INT32,DT_UINT32,DT_UINT64,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT,DT_FLOAT16,DT_DOUBLE,DT_UINT8,DT_INT8,DT_UINT16, DT_INT16,DT_INT32,DT_UINT32,DT_UINT64,DT_INT64}))\n
.ATTR(output_size, ListInt, {-1, -1})\n
.ATTR(default_theta, ListFloat, {})\n
.ATTR(align_corners, Bool, false)\n
.ATTR(use_default_theta, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SpatialTransformer"
    op.name = next_unique_name(node_name, "SpatialTransformer")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if theta is not None:
        op.input.append(theta.tensor)
        op.input_desc.add().CopyFrom(theta.desc)
        op.input_desc[-1].name = "theta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "theta"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["default_theta"].list.val_type = 3
    op.attr["default_theta"].list.f.extend(default_theta)
    op.attr["align_corners"].b = align_corners
    op.attr["use_default_theta"].list.val_type = 2
    op.attr["use_default_theta"].list.i.extend(use_default_theta)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Resize
@auto_convert_to_tensor([False, False, False, False], [False, True, True, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def Resize(x: Tensor, roi: Optional[Tensor], scales: Optional[Tensor], sizes: Optional[Tensor], *, coordinate_transformation_mode: str="half_pixel", cubic_coeff_a: float=-0.750000, exclude_outside: int=0, extrapolation_value: float=0.000000, mode: str="nearest", nearest_mode: str="round_prefer_floor", dependencies=[], node_name=None):
    """REG_OP(Resize)\n
.INPUT(x, TensorType({DT_INT8,DT_UINT8,DT_INT16,DT_UINT16,DT_INT32, DT_INT64,DT_FLOAT16,DT_FLOAT,DT_DOUBLE}))\n
.OPTIONAL_INPUT(roi, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE}))\n
.OPTIONAL_INPUT(scales, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(sizes, TensorType({DT_INT64,DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT8,DT_UINT8,DT_INT16,DT_UINT16,DT_INT32, DT_INT64,DT_FLOAT16,DT_FLOAT,DT_DOUBLE}))\n
.ATTR(coordinate_transformation_mode, String, "half_pixel")\n
.ATTR(cubic_coeff_a, Float, -0.75)\n
.ATTR(exclude_outside, Int, 0)\n
.ATTR(extrapolation_value, Float, 0.0)\n
.ATTR(mode, String, "nearest")\n
.ATTR(nearest_mode, String, "round_prefer_floor")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Resize"
    op.name = next_unique_name(node_name, "Resize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if roi is not None:
        op.input.append(roi.tensor)
        op.input_desc.add().CopyFrom(roi.desc)
        op.input_desc[-1].name = "roi"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "roi"
    if scales is not None:
        op.input.append(scales.tensor)
        op.input_desc.add().CopyFrom(scales.desc)
        op.input_desc[-1].name = "scales"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scales"
    if sizes is not None:
        op.input.append(sizes.tensor)
        op.input_desc.add().CopyFrom(sizes.desc)
        op.input_desc[-1].name = "sizes"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "sizes"

    # process attrs
    op.attr["coordinate_transformation_mode"].s = compat_as_bytes(coordinate_transformation_mode)
    op.attr["cubic_coeff_a"].f = cubic_coeff_a
    op.attr["exclude_outside"].i = exclude_outside
    op.attr["extrapolation_value"].f = extrapolation_value
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["nearest_mode"].s = compat_as_bytes(nearest_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeGrad
@auto_convert_to_tensor([False, False, False, False], [False, True, True, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ResizeGrad(grads: Tensor, roi: Optional[Tensor], scales: Optional[Tensor], original_size: Tensor, *, coordinate_transformation_mode: str="half_pixel", cubic_coeff_a: float=-0.750000, exclude_outside: int=0, extrapolation_value: float=0.000000, mode: str="nearest", nearest_mode: str="round_prefer_floor", dependencies=[], node_name=None):
    """REG_OP(ResizeGrad)\n
.INPUT(grads, TensorType({OrdinaryType, DT_STRING}))\n
.OPTIONAL_INPUT(roi, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OPTIONAL_INPUT(scales, TensorType({DT_FLOAT}))\n
.INPUT(original_size, TensorType({DT_INT64, DT_INT32}))\n
.OUTPUT(y, TensorType({OrdinaryType, DT_STRING}))\n
.ATTR(coordinate_transformation_mode, String, "half_pixel")\n
.ATTR(cubic_coeff_a, Float, -0.75)\n
.ATTR(exclude_outside, Int, 0)\n
.ATTR(extrapolation_value, Float, 0.0)\n
.ATTR(mode, String, "nearest")\n
.ATTR(nearest_mode, String, "round_prefer_floor")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeGrad"
    op.name = next_unique_name(node_name, "ResizeGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    if roi is not None:
        op.input.append(roi.tensor)
        op.input_desc.add().CopyFrom(roi.desc)
        op.input_desc[-1].name = "roi"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "roi"
    if scales is not None:
        op.input.append(scales.tensor)
        op.input_desc.add().CopyFrom(scales.desc)
        op.input_desc[-1].name = "scales"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scales"
    op.input.append(original_size.tensor)
    op.input_desc.add().CopyFrom(original_size.desc)
    op.input_desc[-1].name = "original_size"

    # process attrs
    op.attr["coordinate_transformation_mode"].s = compat_as_bytes(coordinate_transformation_mode)
    op.attr["cubic_coeff_a"].f = cubic_coeff_a
    op.attr["exclude_outside"].i = exclude_outside
    op.attr["extrapolation_value"].f = extrapolation_value
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["nearest_mode"].s = compat_as_bytes(nearest_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DecodeJpeg
@auto_convert_to_tensor([False], [False])
def DecodeJpeg(contents: Tensor, *, channels: int=0, ratio: int=1, fancy_upscaling: bool=True, try_recover_truncated: bool=False, acceptable_fraction: float=1.000000, dct_method: str="", dst_img_format: str="HWC", dependencies=[], node_name=None):
    """REG_OP(DecodeJpeg)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(image, TensorType({DT_UINT8}))\n
.ATTR(channels, Int, 0)\n
.ATTR(ratio, Int, 1)\n
.ATTR(fancy_upscaling, Bool, true)\n
.ATTR(try_recover_truncated, Bool, false)\n
.ATTR(acceptable_fraction, Float, 1.0)\n
.ATTR(dct_method, String, "")\n
.ATTR(dst_img_format, String, "HWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeJpeg"
    op.name = next_unique_name(node_name, "DecodeJpeg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs
    op.attr["channels"].i = channels
    op.attr["ratio"].i = ratio
    op.attr["fancy_upscaling"].b = fancy_upscaling
    op.attr["try_recover_truncated"].b = try_recover_truncated
    op.attr["acceptable_fraction"].f = acceptable_fraction
    op.attr["dct_method"].s = compat_as_bytes(dct_method)
    op.attr["dst_img_format"].s = compat_as_bytes(dst_img_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "image"
    image = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return image


# This api is auto-generated from IR DenseImageWarp
@auto_convert_to_tensor([False, False], [False, False])
def DenseImageWarp(image: Tensor, flow: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DenseImageWarp)\n
.INPUT(image, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(flow, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DenseImageWarp"
    op.name = next_unique_name(node_name, "DenseImageWarp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(image.tensor)
    op.input_desc.add().CopyFrom(image.desc)
    op.input_desc[-1].name = "image"
    op.input.append(flow.tensor)
    op.input_desc.add().CopyFrom(flow.desc)
    op.input_desc[-1].name = "flow"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeD
@auto_convert_to_tensor([False], [False])
def ResizeD(x: Tensor, *, sizes: List[int], scales: List[float]=[], roi: List[int]=[], coordinate_transformation_mode: str="half_pixel", cubic_coeff_a: float=-0.750000, exclude_outside: int=0, extrapolation_value: float=0.000000, mode: str="nearest", nearest_mode: str="round_prefer_floor", dependencies=[], node_name=None):
    """REG_OP(ResizeD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(sizes, ListInt)\n
.ATTR(scales, ListFloat, {})\n
.ATTR(roi, ListInt, {})\n
.ATTR(coordinate_transformation_mode, String, "half_pixel")\n
.ATTR(cubic_coeff_a, Float, -0.75)\n
.ATTR(exclude_outside, Int, 0)\n
.ATTR(extrapolation_value, Float, 0.0)\n
.ATTR(mode, String, "nearest")\n
.ATTR(nearest_mode, String, "round_prefer_floor")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeD"
    op.name = next_unique_name(node_name, "ResizeD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["sizes"].list.val_type = 2
    op.attr["sizes"].list.i.extend(sizes)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)
    op.attr["roi"].list.val_type = 2
    op.attr["roi"].list.i.extend(roi)
    op.attr["coordinate_transformation_mode"].s = compat_as_bytes(coordinate_transformation_mode)
    op.attr["cubic_coeff_a"].f = cubic_coeff_a
    op.attr["exclude_outside"].i = exclude_outside
    op.attr["extrapolation_value"].f = extrapolation_value
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["nearest_mode"].s = compat_as_bytes(nearest_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ResizeGradD
@auto_convert_to_tensor([False], [False])
def ResizeGradD(grads: Tensor, *, original_size: List[int], roi: List[int]=[], scales: List[float]=[], coordinate_transformation_mode: str="half_pixel", cubic_coeff_a: float=-0.750000, exclude_outside: int=0, extrapolation_value: float=0.000000, mode: str="nearest", nearest_mode: str="round_prefer_floor", dependencies=[], node_name=None):
    """REG_OP(ResizeGradD)\n
.INPUT(grads, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(original_size, ListInt)\n
.ATTR(roi, ListInt, {})\n
.ATTR(scales, ListFloat, {})\n
.ATTR(coordinate_transformation_mode, String, "half_pixel")\n
.ATTR(cubic_coeff_a, Float, -0.75)\n
.ATTR(exclude_outside, Int, 0)\n
.ATTR(extrapolation_value, Float, 0.0)\n
.ATTR(mode, String, "nearest")\n
.ATTR(nearest_mode, String, "round_prefer_floor")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeGradD"
    op.name = next_unique_name(node_name, "ResizeGradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"

    # process attrs
    op.attr["original_size"].list.val_type = 2
    op.attr["original_size"].list.i.extend(original_size)
    op.attr["roi"].list.val_type = 2
    op.attr["roi"].list.i.extend(roi)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)
    op.attr["coordinate_transformation_mode"].s = compat_as_bytes(coordinate_transformation_mode)
    op.attr["cubic_coeff_a"].f = cubic_coeff_a
    op.attr["exclude_outside"].i = exclude_outside
    op.attr["extrapolation_value"].f = extrapolation_value
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["nearest_mode"].s = compat_as_bytes(nearest_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DenseImageWarpGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DenseImageWarpGrad(grad: Tensor, image: Tensor, flow: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DenseImageWarpGrad)\n
.INPUT(grad, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(image, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(flow, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(grad_image, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(grad_flow, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DenseImageWarpGrad"
    op.name = next_unique_name(node_name, "DenseImageWarpGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(image.tensor)
    op.input_desc.add().CopyFrom(image.desc)
    op.input_desc[-1].name = "image"
    op.input.append(flow.tensor)
    op.input_desc.add().CopyFrom(flow.desc)
    op.input_desc[-1].name = "flow"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_image"
    grad_image = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "grad_flow"
    grad_flow = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_image, grad_flow


# This api is auto-generated from IR GridSampler2D
@auto_convert_to_tensor([False, False], [False, False])
def GridSampler2D(x: Tensor, grid: Tensor, *, interpolation_mode: str="bilinear", padding_mode: str="zeros", align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(GridSampler2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(grid, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(interpolation_mode, String, "bilinear")\n
.ATTR(padding_mode, String, "zeros")\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GridSampler2D"
    op.name = next_unique_name(node_name, "GridSampler2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grid.tensor)
    op.input_desc.add().CopyFrom(grid.desc)
    op.input_desc[-1].name = "grid"

    # process attrs
    op.attr["interpolation_mode"].s = compat_as_bytes(interpolation_mode)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GridSampler2DGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def GridSampler2DGrad(grad: Tensor, x: Tensor, grid: Tensor, *, interpolation_mode: str="bilinear", padding_mode: str="zeros", align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(GridSampler2DGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(grid, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(dgrid, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(interpolation_mode, String, "bilinear")\n
.ATTR(padding_mode, String, "zeros")\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GridSampler2DGrad"
    op.name = next_unique_name(node_name, "GridSampler2DGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grid.tensor)
    op.input_desc.add().CopyFrom(grid.desc)
    op.input_desc[-1].name = "grid"

    # process attrs
    op.attr["interpolation_mode"].s = compat_as_bytes(interpolation_mode)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgrid"
    dgrid = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx, dgrid


# This api is auto-generated from IR GridUnnormal
@auto_convert_to_tensor([False, False], [False, False])
def GridUnnormal(grid: Tensor, assist: Tensor, *, align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(GridUnnormal)\n
.INPUT(grid, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(assist, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(diff, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(position, TensorType({DT_INT32}))\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GridUnnormal"
    op.name = next_unique_name(node_name, "GridUnnormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grid.tensor)
    op.input_desc.add().CopyFrom(grid.desc)
    op.input_desc[-1].name = "grid"
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "diff"
    diff = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "position"
    position = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return diff, position


# This api is auto-generated from IR ImageUnfold
@auto_convert_to_tensor([False, False], [False, False])
def ImageUnfold(x: Tensor, position: Tensor, *, padding_mode: str="zeros", dependencies=[], node_name=None):
    """REG_OP(ImageUnfold)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(position, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(padding_mode, String, "zeros")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ImageUnfold"
    op.name = next_unique_name(node_name, "ImageUnfold")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(position.tensor)
    op.input_desc.add().CopyFrom(position.desc)
    op.input_desc[-1].name = "position"

    # process attrs
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IMGWarpOffsets
@auto_convert_to_tensor([False, False], [False, False])
def IMGWarpOffsets(images: Tensor, offsets: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IMGWarpOffsets)\n
.INPUT(images, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT}))\n
.INPUT(offsets, TensorType({DT_FLOAT, DT_INT32}))\n
.OUTPUT(warp_images, TensorType({DT_UINT8, DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IMGWarpOffsets"
    op.name = next_unique_name(node_name, "IMGWarpOffsets")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "warp_images"
    warp_images = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return warp_images


# This api is auto-generated from IR GridSampler3D
@auto_convert_to_tensor([False, False], [False, False])
def GridSampler3D(x: Tensor, grid: Tensor, *, interpolation_mode: str="bilinear", padding_mode: str="zeros", data_format: str="NCDHW", align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(GridSampler3D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(grid, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(interpolation_mode, String, "bilinear")\n
.ATTR(padding_mode, String, "zeros")\n
.ATTR(data_format, String, "NCDHW")\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GridSampler3D"
    op.name = next_unique_name(node_name, "GridSampler3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grid.tensor)
    op.input_desc.add().CopyFrom(grid.desc)
    op.input_desc[-1].name = "grid"

    # process attrs
    op.attr["interpolation_mode"].s = compat_as_bytes(interpolation_mode)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GridSampler3DGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def GridSampler3DGrad(grad: Tensor, x: Tensor, grid: Tensor, *, interpolation_mode: str="bilinear", padding_mode: str="zeros", align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(GridSampler3DGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(grid, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(dgrid, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(interpolation_mode, String, "bilinear")\n
.ATTR(padding_mode, String, "zeros")\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GridSampler3DGrad"
    op.name = next_unique_name(node_name, "GridSampler3DGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grid.tensor)
    op.input_desc.add().CopyFrom(grid.desc)
    op.input_desc[-1].name = "grid"

    # process attrs
    op.attr["interpolation_mode"].s = compat_as_bytes(interpolation_mode)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgrid"
    dgrid = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx, dgrid


# This api is auto-generated from IR UpsampleNearest3d
@auto_convert_to_tensor([False], [False])
def UpsampleNearest3d(x: Tensor, *, output_size: List[int]=[], scales: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(UpsampleNearest3d)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(output_size, ListInt, {})\n
.ATTR(scales, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpsampleNearest3d"
    op.name = next_unique_name(node_name, "UpsampleNearest3d")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UpsampleTrilinear3d
@auto_convert_to_tensor([False], [False])
def UpsampleTrilinear3d(x: Tensor, *, output_size: List[int]=[], scales: List[float]=[], align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(UpsampleTrilinear3d)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(output_size, ListInt, {})\n
.ATTR(scales, ListFloat, {})\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpsampleTrilinear3d"
    op.name = next_unique_name(node_name, "UpsampleTrilinear3d")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UpsampleNearest3dGrad
@auto_convert_to_tensor([False], [False])
def UpsampleNearest3dGrad(grad_output: Tensor, *, input_size: List[int], output_size: List[int]=[], scales: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(UpsampleNearest3dGrad)\n
.INPUT(grad_output, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.ATTR(output_size, ListInt, {})\n
.ATTR(scales, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpsampleNearest3dGrad"
    op.name = next_unique_name(node_name, "UpsampleNearest3dGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad_output.tensor)
    op.input_desc.add().CopyFrom(grad_output.desc)
    op.input_desc[-1].name = "grad_output"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UpsampleTrilinear3dGrad
@auto_convert_to_tensor([False], [False])
def UpsampleTrilinear3dGrad(grad_output: Tensor, *, input_size: List[int], output_size: List[int]=[], scales: List[float]=[], align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(UpsampleTrilinear3dGrad)\n
.INPUT(grad_output, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.ATTR(output_size, ListInt, {})\n
.ATTR(scales, ListFloat, {})\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpsampleTrilinear3dGrad"
    op.name = next_unique_name(node_name, "UpsampleTrilinear3dGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad_output.tensor)
    op.input_desc.add().CopyFrom(grad_output.desc)
    op.input_desc[-1].name = "grad_output"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UpsampleNearest1d
@auto_convert_to_tensor([False], [False])
def UpsampleNearest1d(x: Tensor, *, output_size: List[int], scales: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(UpsampleNearest1d)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(output_size, ListInt)\n
.ATTR(scales, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpsampleNearest1d"
    op.name = next_unique_name(node_name, "UpsampleNearest1d")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UpsampleNearest1dGrad
@auto_convert_to_tensor([False], [False])
def UpsampleNearest1dGrad(grad_output: Tensor, *, input_size: List[int], output_size: List[int], scales: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(UpsampleNearest1dGrad)\n
.INPUT(grad_output, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.REQUIRED_ATTR(output_size, ListInt)\n
.ATTR(scales, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UpsampleNearest1dGrad"
    op.name = next_unique_name(node_name, "UpsampleNearest1dGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad_output.tensor)
    op.input_desc.add().CopyFrom(grad_output.desc)
    op.input_desc[-1].name = "grad_output"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DecodeImage
@auto_convert_to_tensor([False], [False])
def DecodeImage(contents: Tensor, *, channels: int=0, dtype: int=DataType.DT_UINT8, expand_animations: bool=True, dependencies=[], node_name=None):
    """REG_OP(DecodeImage)\n
.INPUT(contents, TensorType({DT_STRING}))\n
.OUTPUT(image, TensorType({DT_UINT8, DT_UINT16, DT_FLOAT}))\n
.ATTR(channels, Int, 0)\n
.ATTR(dtype, Type, DT_UINT8)\n
.ATTR(expand_animations, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeImage"
    op.name = next_unique_name(node_name, "DecodeImage")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(contents.tensor)
    op.input_desc.add().CopyFrom(contents.desc)
    op.input_desc[-1].name = "contents"

    # process attrs
    op.attr["channels"].i = channels
    op.attr["dtype"].dt = dtype
    op.attr["expand_animations"].b = expand_animations

    # process outputs
    output_index = 0
    op.output_desc.add().name = "image"
    image = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return image


# This api is auto-generated from IR EncodeJpegVariableQuality
@auto_convert_to_tensor([False, False], [False, False])
def EncodeJpegVariableQuality(images: Tensor, quality: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(EncodeJpegVariableQuality)\n
.INPUT(images, TensorType({DT_UINT8}))\n
.INPUT(quality, TensorType({DT_INT32}))\n
.OUTPUT(contents, TensorType({DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EncodeJpegVariableQuality"
    op.name = next_unique_name(node_name, "EncodeJpegVariableQuality")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(quality.tensor)
    op.input_desc.add().CopyFrom(quality.desc)
    op.input_desc[-1].name = "quality"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "contents"
    contents = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return contents


# This api is auto-generated from IR GenerateBoundingBoxProposals
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def GenerateBoundingBoxProposals(scores: Tensor, bbox_deltas: Tensor, image_info: Tensor, anchors: Tensor, nms_threshold: Tensor, pre_nms_topn: Tensor, min_size: Tensor, *, post_nms_topn: int=300, dependencies=[], node_name=None):
    """REG_OP(GenerateBoundingBoxProposals)\n
.INPUT(scores, TensorType({DT_FLOAT}))\n
.INPUT(bbox_deltas, TensorType({DT_FLOAT}))\n
.INPUT(image_info, TensorType({DT_FLOAT}))\n
.INPUT(anchors, TensorType({DT_FLOAT}))\n
.INPUT(nms_threshold, TensorType({DT_FLOAT}))\n
.INPUT(pre_nms_topn, TensorType({DT_INT32}))\n
.INPUT(min_size, TensorType({DT_FLOAT}))\n
.OUTPUT(rois, TensorType({DT_FLOAT}))\n
.OUTPUT(rois_probabilities, TensorType({DT_FLOAT}))\n
.ATTR(post_nms_topn, Int, 300)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GenerateBoundingBoxProposals"
    op.name = next_unique_name(node_name, "GenerateBoundingBoxProposals")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(bbox_deltas.tensor)
    op.input_desc.add().CopyFrom(bbox_deltas.desc)
    op.input_desc[-1].name = "bbox_deltas"
    op.input.append(image_info.tensor)
    op.input_desc.add().CopyFrom(image_info.desc)
    op.input_desc[-1].name = "image_info"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"
    op.input.append(nms_threshold.tensor)
    op.input_desc.add().CopyFrom(nms_threshold.desc)
    op.input_desc[-1].name = "nms_threshold"
    op.input.append(pre_nms_topn.tensor)
    op.input_desc.add().CopyFrom(pre_nms_topn.desc)
    op.input_desc[-1].name = "pre_nms_topn"
    op.input.append(min_size.tensor)
    op.input_desc.add().CopyFrom(min_size.desc)
    op.input_desc[-1].name = "min_size"

    # process attrs
    op.attr["post_nms_topn"].i = post_nms_topn

    # process outputs
    output_index = 0
    op.output_desc.add().name = "rois"
    rois = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rois_probabilities"
    rois_probabilities = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return rois, rois_probabilities


# This api is auto-generated from IR ImageProjectiveTransform
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ImageProjectiveTransform(images: Tensor, transforms: Tensor, output_shape: Tensor, *, interpolation: str, fill_mode: str="CONSTANT", dependencies=[], node_name=None):
    """REG_OP(ImageProjectiveTransform)\n
.INPUT(images, TensorType({DT_UINT8, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(transforms, TensorType({DT_FLOAT}))\n
.INPUT(output_shape, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(interpolation, String)\n
.ATTR(fill_mode, String, "CONSTANT")\n
.OUTPUT(transformed_images, TensorType({DT_UINT8, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ImageProjectiveTransform"
    op.name = next_unique_name(node_name, "ImageProjectiveTransform")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(transforms.tensor)
    op.input_desc.add().CopyFrom(transforms.desc)
    op.input_desc[-1].name = "transforms"
    op.input.append(output_shape.tensor)
    op.input_desc.add().CopyFrom(output_shape.desc)
    op.input_desc[-1].name = "output_shape"

    # process attrs
    op.attr["interpolation"].s = compat_as_bytes(interpolation)
    op.attr["fill_mode"].s = compat_as_bytes(fill_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "transformed_images"
    transformed_images = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return transformed_images


# This api is auto-generated from IR ImageProjectiveTransformV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True])
def ImageProjectiveTransformV2(images: Tensor, transforms: Tensor, output_shape: Tensor, fill_value: Optional[Tensor], *, interpolation: str, fill_mode: str="CONSTANT", dependencies=[], node_name=None):
    """REG_OP(ImageProjectiveTransformV2)\n
.INPUT(images, TensorType({DT_UINT8, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(transforms, TensorType({DT_FLOAT}))\n
.INPUT(output_shape, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(fill_value, TensorType({DT_UINT8, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(interpolation, String)\n
.ATTR(fill_mode, String, "CONSTANT")\n
.OUTPUT(transformed_images, TensorType({DT_UINT8, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ImageProjectiveTransformV2"
    op.name = next_unique_name(node_name, "ImageProjectiveTransformV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(images.tensor)
    op.input_desc.add().CopyFrom(images.desc)
    op.input_desc[-1].name = "images"
    op.input.append(transforms.tensor)
    op.input_desc.add().CopyFrom(transforms.desc)
    op.input_desc[-1].name = "transforms"
    op.input.append(output_shape.tensor)
    op.input_desc.add().CopyFrom(output_shape.desc)
    op.input_desc[-1].name = "output_shape"
    if fill_value is not None:
        op.input.append(fill_value.tensor)
        op.input_desc.add().CopyFrom(fill_value.desc)
        op.input_desc[-1].name = "fill_value"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "fill_value"

    # process attrs
    op.attr["interpolation"].s = compat_as_bytes(interpolation)
    op.attr["fill_mode"].s = compat_as_bytes(fill_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "transformed_images"
    transformed_images = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return transformed_images


# This api is auto-generated from IR ExtractGlimpseV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ExtractGlimpseV2(input: Tensor, size: Tensor, offsets: Tensor, *, centered: bool=True, normalized: bool=True, uniform_noise: bool=True, noise: str="uniform", dependencies=[], node_name=None):
    """REG_OP(ExtractGlimpseV2)\n
.INPUT(input, TensorType({DT_FLOAT}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.INPUT(offsets, TensorType({DT_FLOAT}))\n
.OUTPUT(glimpse, TensorType({DT_FLOAT}))\n
.ATTR(centered, Bool, true)\n
.ATTR(normalized, Bool, true)\n
.ATTR(uniform_noise, Bool, true)\n
.ATTR(noise, String, "uniform")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ExtractGlimpseV2"
    op.name = next_unique_name(node_name, "ExtractGlimpseV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"

    # process attrs
    op.attr["centered"].b = centered
    op.attr["normalized"].b = normalized
    op.attr["uniform_noise"].b = uniform_noise
    op.attr["noise"].s = compat_as_bytes(noise)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "glimpse"
    glimpse = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return glimpse


# This api is auto-generated from IR ImgToTensor
@auto_convert_to_tensor([False], [False])
def ImgToTensor(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ImgToTensor)\n
.INPUT(x, TensorType({DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ImgToTensor"
    op.name = next_unique_name(node_name, "ImgToTensor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NormalizeV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def NormalizeV2(x: Tensor, mean: Tensor, variance: Tensor, *, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(NormalizeV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NormalizeV2"
    op.name = next_unique_name(node_name, "NormalizeV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR WarpAffine
@auto_convert_to_tensor([False, False], [False, False])
def WarpAffine(x: Tensor, matrix: Tensor, *, out_height: int, out_width: int, interpolation_mode: str="bilinear", padding_mode: str="const", padding_value: int=0, dependencies=[], node_name=None):
    """REG_OP(WarpAffine)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(matrix, TensorType({ DT_FLOAT }))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(out_height, Int)\n
.REQUIRED_ATTR(out_width, Int)\n
.ATTR(interpolation_mode, String, "bilinear")\n
.ATTR(padding_mode, String, "const")\n
.ATTR(padding_value, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "WarpAffine"
    op.name = next_unique_name(node_name, "WarpAffine")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(matrix.tensor)
    op.input_desc.add().CopyFrom(matrix.desc)
    op.input_desc[-1].name = "matrix"

    # process attrs
    op.attr["out_height"].i = out_height
    op.attr["out_width"].i = out_width
    op.attr["interpolation_mode"].s = compat_as_bytes(interpolation_mode)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["padding_value"].i = padding_value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AssistHelp
@auto_convert_to_tensor([True], [False])
def _AssistHelp(x: List[Tensor], *, size_of_y: int, func_name: str, dependencies=[], node_name=None):
    """REG_OP(AssistHelp)\n
.DYNAMIC_INPUT(x, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE }))\n
.DYNAMIC_OUTPUT(y, TensorType({ DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.REQUIRED_ATTR(func_name, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AssistHelp"
    op.name = next_unique_name(node_name, "AssistHelp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["func_name"].s = compat_as_bytes(func_name)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR CacheUpdate
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def CacheUpdate(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(CacheUpdate)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(x, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CacheUpdate"
    op.name = next_unique_name(node_name, "CacheUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x"
    x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x


# This api is auto-generated from IR InternalDataMove
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def InternalDataMove(x: Tensor, *, src_buf: str, dst_buf: str, dependencies=[], node_name=None):
    """REG_OP(InternalDataMove)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.REQUIRED_ATTR(src_buf, String)\n
.REQUIRED_ATTR(dst_buf, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InternalDataMove"
    op.name = next_unique_name(node_name, "InternalDataMove")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["src_buf"].s = compat_as_bytes(src_buf)
    op.attr["dst_buf"].s = compat_as_bytes(dst_buf)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CholeskyGrad
@auto_convert_to_tensor([False, False], [False, False])
def CholeskyGrad(x: Tensor, grad: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(CholeskyGrad)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(grad, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CholeskyGrad"
    op.name = next_unique_name(node_name, "CholeskyGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Cholesky
@auto_convert_to_tensor([False], [False])
def Cholesky(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Cholesky)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cholesky"
    op.name = next_unique_name(node_name, "Cholesky")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Ger
@auto_convert_to_tensor([False, False], [False, False])
def Ger(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Ger)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Ger"
    op.name = next_unique_name(node_name, "Ger")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LogMatrixDeterminant
@auto_convert_to_tensor([False], [False])
def LogMatrixDeterminant(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LogMatrixDeterminant)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(sign, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogMatrixDeterminant"
    op.name = next_unique_name(node_name, "LogMatrixDeterminant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sign"
    sign = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sign, y


# This api is auto-generated from IR MatrixDeterminant
@auto_convert_to_tensor([False], [False])
def MatrixDeterminant(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixDeterminant)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDeterminant"
    op.name = next_unique_name(node_name, "MatrixDeterminant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixInverse
@auto_convert_to_tensor([False], [False])
def MatrixInverse(x: Tensor, *, adjoint: bool=False, dependencies=[], node_name=None):
    """REG_OP(MatrixInverse)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(adjoint, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixInverse"
    op.name = next_unique_name(node_name, "MatrixInverse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["adjoint"].b = adjoint

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixSolve
@auto_convert_to_tensor([False, False], [False, False])
def MatrixSolve(matrix: Tensor, rhs: Tensor, *, adjoint: bool=False, dependencies=[], node_name=None):
    """REG_OP(MatrixSolve)\n
.INPUT(matrix, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(rhs, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(adjoint, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixSolve"
    op.name = next_unique_name(node_name, "MatrixSolve")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(matrix.tensor)
    op.input_desc.add().CopyFrom(matrix.desc)
    op.input_desc[-1].name = "matrix"
    op.input.append(rhs.tensor)
    op.input_desc.add().CopyFrom(rhs.desc)
    op.input_desc[-1].name = "rhs"

    # process attrs
    op.attr["adjoint"].b = adjoint

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixSolveLs
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MatrixSolveLs(matrix: Tensor, rhs: Tensor, l2: Tensor, *, fast: bool=True, dependencies=[], node_name=None):
    """REG_OP(MatrixSolveLs)\n
.INPUT(matrix, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(rhs, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(l2, TensorType({DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(fast, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixSolveLs"
    op.name = next_unique_name(node_name, "MatrixSolveLs")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(matrix.tensor)
    op.input_desc.add().CopyFrom(matrix.desc)
    op.input_desc[-1].name = "matrix"
    op.input.append(rhs.tensor)
    op.input_desc.add().CopyFrom(rhs.desc)
    op.input_desc[-1].name = "rhs"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"

    # process attrs
    op.attr["fast"].b = fast

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixTriangularSolve
@auto_convert_to_tensor([False, False], [False, False])
def MatrixTriangularSolve(matrix: Tensor, rhs: Tensor, *, lower: bool=True, adjoint: bool=False, dependencies=[], node_name=None):
    """REG_OP(MatrixTriangularSolve)\n
.INPUT(matrix, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(rhs, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(lower, Bool, true)\n
.ATTR(adjoint, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixTriangularSolve"
    op.name = next_unique_name(node_name, "MatrixTriangularSolve")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(matrix.tensor)
    op.input_desc.add().CopyFrom(matrix.desc)
    op.input_desc[-1].name = "matrix"
    op.input.append(rhs.tensor)
    op.input_desc.add().CopyFrom(rhs.desc)
    op.input_desc[-1].name = "rhs"

    # process attrs
    op.attr["lower"].b = lower
    op.attr["adjoint"].b = adjoint

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Qr
@auto_convert_to_tensor([False], [False])
def Qr(x: Tensor, *, full_matrices: bool=False, dependencies=[], node_name=None):
    """REG_OP(Qr)\n
.INPUT(x, TensorType({ DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128 }))\n
.OUTPUT(q, TensorType({ DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128 }))\n
.OUTPUT(r, TensorType({ DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128 }))\n
.ATTR(full_matrices, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Qr"
    op.name = next_unique_name(node_name, "Qr")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["full_matrices"].b = full_matrices

    # process outputs
    output_index = 0
    op.output_desc.add().name = "q"
    q = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "r"
    r = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return q, r


# This api is auto-generated from IR SelfAdjointEig
@auto_convert_to_tensor([False], [False])
def SelfAdjointEig(x: Tensor, *, compute_v: bool=True, dependencies=[], node_name=None):
    """REG_OP(SelfAdjointEig)\n
.INPUT(x, TensorType({ DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128 }))\n
.OUTPUT(eigen_value, TensorType({ DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128 }))\n
.OUTPUT(eigen_vector, TensorType({ DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128 }))\n
.ATTR(compute_v, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SelfAdjointEig"
    op.name = next_unique_name(node_name, "SelfAdjointEig")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["compute_v"].b = compute_v

    # process outputs
    output_index = 0
    op.output_desc.add().name = "eigen_value"
    eigen_value = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "eigen_vector"
    eigen_vector = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return eigen_value, eigen_vector


# This api is auto-generated from IR Slogdet
@auto_convert_to_tensor([False], [False])
def Slogdet(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Slogdet)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(sign, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Slogdet"
    op.name = next_unique_name(node_name, "Slogdet")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sign"
    sign = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sign, y


# This api is auto-generated from IR Svd
@auto_convert_to_tensor([False], [False])
def Svd(x: Tensor, *, compute_uv: bool=True, full_matrices: bool=False, dependencies=[], node_name=None):
    """REG_OP(Svd)\n
.INPUT(x, TensorType({ DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128 }))\n
.OUTPUT(sigma, TensorType({ DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128 }))\n
.OUTPUT(u, TensorType({ DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128 }))\n
.OUTPUT(v, TensorType({ DT_DOUBLE, DT_FLOAT, DT_COMPLEX64, DT_COMPLEX128 }))\n
.ATTR(compute_uv, Bool, true)\n
.ATTR(full_matrices, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Svd"
    op.name = next_unique_name(node_name, "Svd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["compute_uv"].b = compute_uv
    op.attr["full_matrices"].b = full_matrices

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sigma"
    sigma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "u"
    u = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "v"
    v = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sigma, u, v


# This api is auto-generated from IR Lu
@auto_convert_to_tensor([False], [False])
def Lu(input: Tensor, *, output_idx_type: int, dependencies=[], node_name=None):
    """REG_OP(Lu)\n
.INPUT(input, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(lu, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(p, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(output_idx_type, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Lu"
    op.name = next_unique_name(node_name, "Lu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["output_idx_type"].dt = output_idx_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "lu"
    lu = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "p"
    p = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return lu, p


# This api is auto-generated from IR MatrixSquareRoot
@auto_convert_to_tensor([False], [False])
def MatrixSquareRoot(input: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixSquareRoot)\n
.INPUT(input, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixSquareRoot"
    op.name = next_unique_name(node_name, "MatrixSquareRoot")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TridiagonalSolve
@auto_convert_to_tensor([False, False], [False, False])
def TridiagonalSolve(diagonals: Tensor, rhs: Tensor, *, partial_pivoting: bool=True, dependencies=[], node_name=None):
    """REG_OP(TridiagonalSolve)\n
.INPUT(diagonals, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(rhs, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(partial_pivoting, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TridiagonalSolve"
    op.name = next_unique_name(node_name, "TridiagonalSolve")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(diagonals.tensor)
    op.input_desc.add().CopyFrom(diagonals.desc)
    op.input_desc[-1].name = "diagonals"
    op.input.append(rhs.tensor)
    op.input_desc.add().CopyFrom(rhs.desc)
    op.input_desc[-1].name = "rhs"

    # process attrs
    op.attr["partial_pivoting"].b = partial_pivoting

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BandedTriangularSolve
@auto_convert_to_tensor([False, False], [False, False])
def BandedTriangularSolve(bands: Tensor, rhs: Tensor, *, lower: bool=True, adjoint: bool=False, dependencies=[], node_name=None):
    """REG_OP(BandedTriangularSolve)\n
.INPUT(bands, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(rhs, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(lower, Bool, true)\n
.ATTR(adjoint, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BandedTriangularSolve"
    op.name = next_unique_name(node_name, "BandedTriangularSolve")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bands.tensor)
    op.input_desc.add().CopyFrom(bands.desc)
    op.input_desc[-1].name = "bands"
    op.input.append(rhs.tensor)
    op.input_desc.add().CopyFrom(rhs.desc)
    op.input_desc[-1].name = "rhs"

    # process attrs
    op.attr["lower"].b = lower
    op.attr["adjoint"].b = adjoint

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR ConjugateTranspose
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def ConjugateTranspose(x: Tensor, perm: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ConjugateTranspose)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(perm, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConjugateTranspose"
    op.name = next_unique_name(node_name, "ConjugateTranspose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(perm.tensor)
    op.input_desc.add().CopyFrom(perm.desc)
    op.input_desc[-1].name = "perm"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EmptyTensorList
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def EmptyTensorList(element_shape: Tensor, max_num_elements: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(EmptyTensorList)\n
.INPUT(element_shape, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(max_num_elements, TensorType({DT_INT32}))\n
.OUTPUT(handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmptyTensorList"
    op.name = next_unique_name(node_name, "EmptyTensorList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"
    op.input.append(max_num_elements.tensor)
    op.input_desc.add().CopyFrom(max_num_elements.desc)
    op.input_desc[-1].name = "max_num_elements"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR TensorListPushBack
@auto_convert_to_tensor([False, False], [False, False])
def TensorListPushBack(input_handle: Tensor, tensor: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListPushBack)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL,DT_RESOURCE, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListPushBack"
    op.name = next_unique_name(node_name, "TensorListPushBack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(tensor.tensor)
    op.input_desc.add().CopyFrom(tensor.desc)
    op.input_desc[-1].name = "tensor"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorListPopBack
@auto_convert_to_tensor([False, False], [False, False])
def TensorListPopBack(input_handle: Tensor, element_shape: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListPopBack)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(element_shape, TensorType({DT_INT32}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
.OUTPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL,DT_RESOURCE, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListPopBack"
    op.name = next_unique_name(node_name, "TensorListPopBack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tensor"
    tensor = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle, tensor


# This api is auto-generated from IR TensorListLength
@auto_convert_to_tensor([False], [False])
def TensorListLength(input_handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorListLength)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.OUTPUT(length, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListLength"
    op.name = next_unique_name(node_name, "TensorListLength")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "length"
    length = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return length


# This api is auto-generated from IR TensorListElementShape
@auto_convert_to_tensor([False], [False])
def TensorListElementShape(input_handle: Tensor, *, shape_type: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListElementShape)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.OUTPUT(element_shape, TensorType({DT_INT32,DT_INT64}))\n
.ATTR(shape_type, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListElementShape"
    op.name = next_unique_name(node_name, "TensorListElementShape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"

    # process attrs
    op.attr["shape_type"].dt = shape_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "element_shape"
    element_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return element_shape


# This api is auto-generated from IR TensorListReserve
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def TensorListReserve(element_shape: Tensor, num_elements: Tensor, *, element_dtype: int=DataType.DT_INT32, shape_type: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListReserve)\n
.INPUT(element_shape, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(num_elements, TensorType({DT_INT32}))\n
.OUTPUT(handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
.ATTR(shape_type, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListReserve"
    op.name = next_unique_name(node_name, "TensorListReserve")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"
    op.input.append(num_elements.tensor)
    op.input_desc.add().CopyFrom(num_elements.desc)
    op.input_desc[-1].name = "num_elements"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype
    op.attr["shape_type"].dt = shape_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR TensorListGetItem
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TensorListGetItem(input_handle: Tensor, index: Tensor, element_shape: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListGetItem)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(index, TensorType({DT_INT32}))\n
.INPUT(element_shape, TensorType({DT_INT32}))\n
.OUTPUT(item, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListGetItem"
    op.name = next_unique_name(node_name, "TensorListGetItem")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "item"
    item = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return item


# This api is auto-generated from IR TensorListSetItem
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TensorListSetItem(input_handle: Tensor, index: Tensor, item: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListSetItem)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(index, TensorType({DT_INT32}))\n
.INPUT(item, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL,DT_RESOURCE, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListSetItem"
    op.name = next_unique_name(node_name, "TensorListSetItem")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"
    op.input.append(item.tensor)
    op.input_desc.add().CopyFrom(item.desc)
    op.input_desc[-1].name = "item"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorListPushBackBatch
@auto_convert_to_tensor([False, False], [False, False])
def TensorListPushBackBatch(input_handles: Tensor, tensor: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListPushBackBatch)\n
.INPUT(input_handles, TensorType({DT_VARIANT}))\n
.INPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(output_handles, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListPushBackBatch"
    op.name = next_unique_name(node_name, "TensorListPushBackBatch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handles.tensor)
    op.input_desc.add().CopyFrom(input_handles.desc)
    op.input_desc[-1].name = "input_handles"
    op.input.append(tensor.tensor)
    op.input_desc.add().CopyFrom(tensor.desc)
    op.input_desc[-1].name = "tensor"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handles"
    output_handles = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handles


# This api is auto-generated from IR TensorListStack
@auto_convert_to_tensor([False, False], [False, False])
def TensorListStack(input_handle: Tensor, element_shape: Tensor, *, element_dtype: int=DataType.DT_INT32, num_elements: int=-1, dependencies=[], node_name=None):
    """REG_OP(TensorListStack)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(element_shape, TensorType({DT_INT32}))\n
.OUTPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
.ATTR(num_elements, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListStack"
    op.name = next_unique_name(node_name, "TensorListStack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype
    op.attr["num_elements"].i = num_elements

    # process outputs
    output_index = 0
    op.output_desc.add().name = "tensor"
    tensor = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return tensor


# This api is auto-generated from IR TensorListConcatV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def TensorListConcatV2(input_handle: Tensor, element_shape: Tensor, leading_dims: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListConcatV2)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(element_shape, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(leading_dims, TensorType({DT_INT64}))\n
.OUTPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(lengths, TensorType({DT_INT64}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListConcatV2"
    op.name = next_unique_name(node_name, "TensorListConcatV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"
    op.input.append(leading_dims.tensor)
    op.input_desc.add().CopyFrom(leading_dims.desc)
    op.input_desc[-1].name = "leading_dims"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "tensor"
    tensor = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "lengths"
    lengths = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return tensor, lengths


# This api is auto-generated from IR TensorListSplit
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def TensorListSplit(tensor: Tensor, element_shape: Tensor, lengths: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListSplit)\n
.INPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.INPUT(element_shape, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(lengths, TensorType({DT_INT64}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListSplit"
    op.name = next_unique_name(node_name, "TensorListSplit")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(tensor.tensor)
    op.input_desc.add().CopyFrom(tensor.desc)
    op.input_desc[-1].name = "tensor"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"
    op.input.append(lengths.tensor)
    op.input_desc.add().CopyFrom(lengths.desc)
    op.input_desc[-1].name = "lengths"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorListFromTensor
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def TensorListFromTensor(tensor: Tensor, element_shape: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListFromTensor)\n
.INPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.INPUT(element_shape, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListFromTensor"
    op.name = next_unique_name(node_name, "TensorListFromTensor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(tensor.tensor)
    op.input_desc.add().CopyFrom(tensor.desc)
    op.input_desc[-1].name = "tensor"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorListResize
@auto_convert_to_tensor([False, False], [False, False])
def TensorListResize(input_handle: Tensor, size: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorListResize)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(size, TensorType({DT_INT32}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListResize"
    op.name = next_unique_name(node_name, "TensorListResize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorListGather
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TensorListGather(input_handle: Tensor, indices: Tensor, element_shape: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListGather)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(element_shape, TensorType({DT_INT32}))\n
.OUTPUT(values, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListGather"
    op.name = next_unique_name(node_name, "TensorListGather")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values


# This api is auto-generated from IR TensorListScatterV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def TensorListScatterV2(tensor: Tensor, indices: Tensor, element_shape: Tensor, num_elements: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListScatterV2)\n
.INPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(element_shape, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(num_elements, TensorType({DT_INT32}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListScatterV2"
    op.name = next_unique_name(node_name, "TensorListScatterV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(tensor.tensor)
    op.input_desc.add().CopyFrom(tensor.desc)
    op.input_desc[-1].name = "tensor"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(element_shape.tensor)
    op.input_desc.add().CopyFrom(element_shape.desc)
    op.input_desc[-1].name = "element_shape"
    op.input.append(num_elements.tensor)
    op.input_desc.add().CopyFrom(num_elements.desc)
    op.input_desc[-1].name = "num_elements"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorListScatterIntoExistingList
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TensorListScatterIntoExistingList(input_handle: Tensor, tensor: Tensor, indices: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListScatterIntoExistingList)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(tensor, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE,DT_INT8, DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_QINT8,DT_QUINT8, DT_QINT16,DT_QUINT16,DT_QINT32,DT_BOOL, DT_STRING,DT_COMPLEX64,DT_COMPLEX128}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListScatterIntoExistingList"
    op.name = next_unique_name(node_name, "TensorListScatterIntoExistingList")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(tensor.tensor)
    op.input_desc.add().CopyFrom(tensor.desc)
    op.input_desc[-1].name = "tensor"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorListConcatLists
@auto_convert_to_tensor([False, False], [False, False])
def TensorListConcatLists(input_a: Tensor, input_b: Tensor, *, element_dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(TensorListConcatLists)\n
.INPUT(input_a, TensorType({DT_VARIANT}))\n
.INPUT(input_b, TensorType({DT_VARIANT}))\n
.OUTPUT(output, TensorType({DT_VARIANT}))\n
.ATTR(element_dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorListConcatLists"
    op.name = next_unique_name(node_name, "TensorListConcatLists")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_a.tensor)
    op.input_desc.add().CopyFrom(input_a.desc)
    op.input_desc[-1].name = "input_a"
    op.input.append(input_b.tensor)
    op.input_desc.add().CopyFrom(input_b.desc)
    op.input_desc[-1].name = "input_b"

    # process attrs
    op.attr["element_dtype"].dt = element_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR Timestamp
@auto_convert_to_tensor([], [])
def Timestamp(*, dependencies=[], node_name=None):
    """REG_OP(Timestamp)\n
.OUTPUT(y, TensorType({DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Timestamp"
    op.name = next_unique_name(node_name, "Timestamp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Assert
@auto_convert_to_tensor([False, True], [False, False])
def Assert(input_condition: Tensor, input_data: List[Tensor], *, summarize: int=3, dependencies=[], node_name=None):
    """REG_OP(Assert)\n
.INPUT(input_condition, TensorType{DT_BOOL})\n
.DYNAMIC_INPUT(input_data, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING}))\n
.ATTR(summarize, Int, 3)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Assert"
    op.name = next_unique_name(node_name, "Assert")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_condition.tensor)
    op.input_desc.add().CopyFrom(input_condition.desc)
    op.input_desc[-1].name = "input_condition"
    if not isinstance(input_data, (tuple, list)):
        raise AssertionError("input_data must be a tuple or a list.")
    for i, v in enumerate(input_data):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "input_data" + str(i)

    # process attrs
    op.attr["summarize"].i = summarize

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR Print
@auto_convert_to_tensor([True], [False])
def Print(x: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(Print)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_DOUBLE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Print"
    op.name = next_unique_name(node_name, "Print")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR PrintV2
@auto_convert_to_tensor([False], [False])
def PrintV2(x: Tensor, *, output_stream: str="stderr", dependencies=[], node_name=None):
    """REG_OP(PrintV2)\n
.INPUT(x, TensorType({DT_STRING}))\n
.ATTR(output_stream, String, "stderr")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PrintV2"
    op.name = next_unique_name(node_name, "PrintV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_stream"].s = compat_as_bytes(output_stream)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR PrintV3
@auto_convert_to_tensor([False, True], [False, False])
def PrintV3(x: Tensor, data: List[Tensor], *, message: str="", first_n: int=-1, summarize: int=3, dependencies=[], node_name=None):
    """REG_OP(PrintV3)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_DOUBLE, DT_STRING}))\n
.DYNAMIC_INPUT(data, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_DOUBLE, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_DOUBLE, DT_STRING}))\n
.ATTR(message, String, "")\n
.ATTR(first_n, Int, -1)\n
.ATTR(summarize, Int, 3)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PrintV3"
    op.name = next_unique_name(node_name, "PrintV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if not isinstance(data, (tuple, list)):
        raise AssertionError("data must be a tuple or a list.")
    for i, v in enumerate(data):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "data" + str(i)

    # process attrs
    op.attr["message"].s = compat_as_bytes(message)
    op.attr["first_n"].i = first_n
    op.attr["summarize"].i = summarize

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LookupTableImport
@auto_convert_to_tensor([False, False, False], [False, False, False])
def LookupTableImport(handle: Tensor, keys: Tensor, values: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LookupTableImport)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(keys, TensorType({DT_STRING, DT_INT32, DT_INT64}))\n
.INPUT(values, TensorType({DT_BOOL, DT_DOUBLE, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LookupTableImport"
    op.name = next_unique_name(node_name, "LookupTableImport")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR LookupTableInsert
@auto_convert_to_tensor([False, False, False], [False, False, False])
def LookupTableInsert(handle: Tensor, keys: Tensor, values: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LookupTableInsert)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(keys, TensorType({DT_STRING, DT_INT32, DT_INT64}))\n
.INPUT(values, TensorType({DT_BOOL, DT_DOUBLE, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LookupTableInsert"
    op.name = next_unique_name(node_name, "LookupTableInsert")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR LookupTableExport
@auto_convert_to_tensor([False], [False])
def LookupTableExport(handle: Tensor, *, Tkeys: int, Tvalues: int, dependencies=[], node_name=None):
    """REG_OP(LookupTableExport)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(keys, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(values, TensorType({DT_BOOL, DT_DOUBLE, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING}))\n
.REQUIRED_ATTR(Tkeys, Type)\n
.REQUIRED_ATTR(Tvalues, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LookupTableExport"
    op.name = next_unique_name(node_name, "LookupTableExport")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs
    op.attr["Tkeys"].dt = Tkeys
    op.attr["Tvalues"].dt = Tvalues

    # process outputs
    output_index = 0
    op.output_desc.add().name = "keys"
    keys = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return keys, values


# This api is auto-generated from IR LookupTableSize
@auto_convert_to_tensor([False], [False])
def LookupTableSize(handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LookupTableSize)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.OUTPUT(size, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LookupTableSize"
    op.name = next_unique_name(node_name, "LookupTableSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR LookupTableFind
@auto_convert_to_tensor([False, False, False], [False, False, False])
def LookupTableFind(handle: Tensor, keys: Tensor, default_value: Tensor, *, Tout: int, dependencies=[], node_name=None):
    """REG_OP(LookupTableFind)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(keys, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(default_value, TensorType({DT_DOUBLE, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING, DT_BOOL}))\n
.OUTPUT(values, TensorType({DT_DOUBLE, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING, DT_BOOL}))\n
.REQUIRED_ATTR(Tout, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LookupTableFind"
    op.name = next_unique_name(node_name, "LookupTableFind")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(default_value.tensor)
    op.input_desc.add().CopyFrom(default_value.desc)
    op.input_desc[-1].name = "default_value"

    # process attrs
    op.attr["Tout"].dt = Tout

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values


# This api is auto-generated from IR HashTable
@auto_convert_to_tensor([], [])
def HashTable(*, key_dtype: int, value_dtype: int, container: str="", shared_name: str="", use_node_name_sharing: bool=False, dependencies=[], node_name=None):
    """REG_OP(HashTable)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(use_node_name_sharing, Bool, false)\n
.REQUIRED_ATTR(key_dtype, Type)\n
.REQUIRED_ATTR(value_dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HashTable"
    op.name = next_unique_name(node_name, "HashTable")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["key_dtype"].dt = key_dtype
    op.attr["value_dtype"].dt = value_dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["use_node_name_sharing"].b = use_node_name_sharing

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR InitializeTable
@auto_convert_to_tensor([False, False, False], [False, False, False])
def InitializeTable(handle: Tensor, keys: Tensor, values: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InitializeTable)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(keys, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(values, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InitializeTable"
    op.name = next_unique_name(node_name, "InitializeTable")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR MutableDenseHashTable
@auto_convert_to_tensor([False, False], [False, False])
def MutableDenseHashTable(empty_key: Tensor, deleted_key: Tensor, *, value_dtype: int, container: str="", shared_name: str="", use_node_name_sharing: bool=False, value_shape: List[int]=[], initial_num_buckets: int=131072, max_load_factor: float=0.800000, dependencies=[], node_name=None):
    """REG_OP(MutableDenseHashTable)\n
.INPUT(empty_key, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(deleted_key, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(use_node_name_sharing, Bool, false)\n
.REQUIRED_ATTR(value_dtype, Type)\n
.ATTR(value_shape, ListInt, {})\n
.ATTR(initial_num_buckets, Int, 131072)\n
.ATTR(max_load_factor, Float, 0.8)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MutableDenseHashTable"
    op.name = next_unique_name(node_name, "MutableDenseHashTable")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(empty_key.tensor)
    op.input_desc.add().CopyFrom(empty_key.desc)
    op.input_desc[-1].name = "empty_key"
    op.input.append(deleted_key.tensor)
    op.input_desc.add().CopyFrom(deleted_key.desc)
    op.input_desc[-1].name = "deleted_key"

    # process attrs
    op.attr["value_dtype"].dt = value_dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["use_node_name_sharing"].b = use_node_name_sharing
    op.attr["value_shape"].list.val_type = 2
    op.attr["value_shape"].list.i.extend(value_shape)
    op.attr["initial_num_buckets"].i = initial_num_buckets
    op.attr["max_load_factor"].f = max_load_factor

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR MutableHashTableOfTensors
@auto_convert_to_tensor([], [])
def MutableHashTableOfTensors(*, key_dtype: int, value_dtype: int, container: str="", shared_name: str="", use_node_name_sharing: bool=False, value_shape: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(MutableHashTableOfTensors)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(use_node_name_sharing, Bool, false)\n
.REQUIRED_ATTR(key_dtype, Type)\n
.REQUIRED_ATTR(value_dtype, Type)\n
.ATTR(value_shape, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MutableHashTableOfTensors"
    op.name = next_unique_name(node_name, "MutableHashTableOfTensors")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["key_dtype"].dt = key_dtype
    op.attr["value_dtype"].dt = value_dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["use_node_name_sharing"].b = use_node_name_sharing
    op.attr["value_shape"].list.val_type = 2
    op.attr["value_shape"].list.i.extend(value_shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR MutableHashTable
@auto_convert_to_tensor([], [])
def MutableHashTable(*, key_dtype: int, value_dtype: int, container: str="", shared_name: str="", use_node_name_sharing: bool=False, dependencies=[], node_name=None):
    """REG_OP(MutableHashTable)\n
.OUTPUT(handle, TensorType({DT_RESOURCE}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.ATTR(use_node_name_sharing, Bool, false)\n
.REQUIRED_ATTR(key_dtype, Type)\n
.REQUIRED_ATTR(value_dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MutableHashTable"
    op.name = next_unique_name(node_name, "MutableHashTable")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["key_dtype"].dt = key_dtype
    op.attr["value_dtype"].dt = value_dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["use_node_name_sharing"].b = use_node_name_sharing

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR LookupTableRemove
@auto_convert_to_tensor([False, False], [False, False])
def LookupTableRemove(table_handle: Tensor, keys: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LookupTableRemove)\n
.INPUT(table_handle, TensorType({DT_RESOURCE}))\n
.INPUT(keys, TensorType({RealNumberType, DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LookupTableRemove"
    op.name = next_unique_name(node_name, "LookupTableRemove")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(table_handle.tensor)
    op.input_desc.add().CopyFrom(table_handle.desc)
    op.input_desc[-1].name = "table_handle"
    op.input.append(keys.tensor)
    op.input_desc.add().CopyFrom(keys.desc)
    op.input_desc[-1].name = "keys"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR TensorMapHasKey
@auto_convert_to_tensor([False, False], [False, False])
def TensorMapHasKey(input_handle: Tensor, key: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorMapHasKey)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(key, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(has_key, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorMapHasKey"
    op.name = next_unique_name(node_name, "TensorMapHasKey")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "has_key"
    has_key = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return has_key


# This api is auto-generated from IR TensorMapErase
@auto_convert_to_tensor([False, False], [False, False])
def TensorMapErase(input_handle: Tensor, key: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorMapErase)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(key, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorMapErase"
    op.name = next_unique_name(node_name, "TensorMapErase")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorMapInsert
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_BASIC])
def TensorMapInsert(input_handle: Tensor, key: Tensor, value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorMapInsert)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(key, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(value, BasicType)\n
.OUTPUT(output_handle, TensorType({DT_VARIANT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorMapInsert"
    op.name = next_unique_name(node_name, "TensorMapInsert")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_handle"
    output_handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_handle


# This api is auto-generated from IR TensorMapLookup
@auto_convert_to_tensor([False, False], [False, False])
def TensorMapLookup(input_handle: Tensor, key: Tensor, *, value_dtype: int, dependencies=[], node_name=None):
    """REG_OP(TensorMapLookup)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.INPUT(key, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(value, BasicType)\n
.REQUIRED_ATTR(value_dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorMapLookup"
    op.name = next_unique_name(node_name, "TensorMapLookup")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"

    # process attrs
    op.attr["value_dtype"].dt = value_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "value"
    value = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return value


# This api is auto-generated from IR TensorMapSize
@auto_convert_to_tensor([False], [False])
def TensorMapSize(input_handle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorMapSize)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.OUTPUT(size, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorMapSize"
    op.name = next_unique_name(node_name, "TensorMapSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR TensorMapStackKeys
@auto_convert_to_tensor([False], [False])
def TensorMapStackKeys(input_handle: Tensor, *, key_dtype: int, dependencies=[], node_name=None):
    """REG_OP(TensorMapStackKeys)\n
.INPUT(input_handle, TensorType({DT_VARIANT}))\n
.OUTPUT(keys, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.REQUIRED_ATTR(key_dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorMapStackKeys"
    op.name = next_unique_name(node_name, "TensorMapStackKeys")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_handle.tensor)
    op.input_desc.add().CopyFrom(input_handle.desc)
    op.input_desc[-1].name = "input_handle"

    # process attrs
    op.attr["key_dtype"].dt = key_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "keys"
    keys = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return keys


# This api is auto-generated from IR EmptyTensorMap
@auto_convert_to_tensor([], [])
def EmptyTensorMap(*, dependencies=[], node_name=None):
    """REG_OP(EmptyTensorMap)\n
.OUTPUT(handle, TensorType({DT_VARIANT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmptyTensorMap"
    op.name = next_unique_name(node_name, "EmptyTensorMap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR Power
@auto_convert_to_tensor([False], [False])
def Power(x: Tensor, *, power: float=1.000000, scale: float=1.000000, shift: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Power)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(power, Float, 1.0)\n
.ATTR(scale, Float, 1.0)\n
.ATTR(shift, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Power"
    op.name = next_unique_name(node_name, "Power")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["power"].f = power
    op.attr["scale"].f = scale
    op.attr["shift"].f = shift

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Igamma
@auto_convert_to_tensor([False, False], [False, False])
def Igamma(a: Tensor, x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Igamma)\n
.INPUT(a, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(z, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Igamma"
    op.name = next_unique_name(node_name, "Igamma")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(a.tensor)
    op.input_desc.add().CopyFrom(a.desc)
    op.input_desc[-1].name = "a"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Igammac
@auto_convert_to_tensor([False, False], [False, False])
def Igammac(a: Tensor, x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Igammac)\n
.INPUT(a, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(z, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Igammac"
    op.name = next_unique_name(node_name, "Igammac")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(a.tensor)
    op.input_desc.add().CopyFrom(a.desc)
    op.input_desc[-1].name = "a"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Histogram
@auto_convert_to_tensor([False], [False])
def Histogram(x: Tensor, *, bins: int=100, min: float=0.000000, max: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Histogram)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32}))\n
.ATTR(bins, Int, 100)\n
.ATTR(min, Float, 0.0)\n
.ATTR(max, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Histogram"
    op.name = next_unique_name(node_name, "Histogram")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["bins"].i = bins
    op.attr["min"].f = min
    op.attr["max"].f = max

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CompareAndBitpack
@auto_convert_to_tensor([False, False], [False, False])
def CompareAndBitpack(x: Tensor, threshold: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(CompareAndBitpack)\n
.INPUT(x, TensorType({ DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_BOOL }))\n
.INPUT(threshold, TensorType({ DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_BOOL }))\n
.OUTPUT(y, TensorType(DT_UINT8))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CompareAndBitpack"
    op.name = next_unique_name(node_name, "CompareAndBitpack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(threshold.tensor)
    op.input_desc.add().CopyFrom(threshold.desc)
    op.input_desc[-1].name = "threshold"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Bincount
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Bincount(array: Tensor, size: Tensor, weights: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Bincount)\n
.INPUT(array, TensorType(DT_INT32))\n
.INPUT(size, TensorType(DT_INT32))\n
.INPUT(weights, TensorType({ DT_FLOAT, DT_INT32, DT_INT64, DT_DOUBLE }))\n
.OUTPUT(bins, TensorType({ DT_FLOAT, DT_INT32, DT_INT64, DT_DOUBLE }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Bincount"
    op.name = next_unique_name(node_name, "Bincount")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(array.tensor)
    op.input_desc.add().CopyFrom(array.desc)
    op.input_desc[-1].name = "array"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "bins"
    bins = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return bins


# This api is auto-generated from IR Betainc
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Betainc(a: Tensor, b: Tensor, x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Betainc)\n
.INPUT(a, TensorType({DT_DOUBLE, DT_FLOAT}))\n
.INPUT(b, TensorType({DT_DOUBLE, DT_FLOAT}))\n
.INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_DOUBLE, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Betainc"
    op.name = next_unique_name(node_name, "Betainc")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(a.tensor)
    op.input_desc.add().CopyFrom(a.desc)
    op.input_desc[-1].name = "a"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Zeta
@auto_convert_to_tensor([False, False], [False, False])
def Zeta(x: Tensor, q: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Zeta)\n
.INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT}))\n
.INPUT(q, TensorType({DT_DOUBLE, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_DOUBLE, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Zeta"
    op.name = next_unique_name(node_name, "Zeta")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(q.tensor)
    op.input_desc.add().CopyFrom(q.desc)
    op.input_desc[-1].name = "q"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Bucketize
@auto_convert_to_tensor([False], [False])
def Bucketize(x: Tensor, *, boundaries: List[float], dtype: int=DataType.DT_INT32, right: bool=True, dependencies=[], node_name=None):
    """REG_OP(Bucketize)\n
.INPUT(x, TensorType({DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(boundaries, ListFloat)\n
.ATTR(dtype, Type, DT_INT32)\n
.ATTR(right, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Bucketize"
    op.name = next_unique_name(node_name, "Bucketize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["boundaries"].list.val_type = 3
    op.attr["boundaries"].list.f.extend(boundaries)
    op.attr["dtype"].dt = dtype
    op.attr["right"].b = right

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Trunc
@auto_convert_to_tensor([False], [False])
def Trunc(input_x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Trunc)\n
.INPUT(input_x, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Trunc"
    op.name = next_unique_name(node_name, "Trunc")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR SparseSegmentSum
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseSegmentSum(x: Tensor, indices: Tensor, segment_ids: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSegmentSum)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(segment_ids, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSegmentSum"
    op.name = next_unique_name(node_name, "SparseSegmentSum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseSegmentMean
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseSegmentMean(x: Tensor, indices: Tensor, segment_ids: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSegmentMean)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16}))\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(segment_ids, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSegmentMean"
    op.name = next_unique_name(node_name, "SparseSegmentMean")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseSegmentMeanGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def SparseSegmentMeanGrad(x: Tensor, indices: Tensor, segment_ids: Tensor, output_dim0: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSegmentMeanGrad)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16}))\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(segment_ids, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(output_dim0, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSegmentMeanGrad"
    op.name = next_unique_name(node_name, "SparseSegmentMeanGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"
    op.input.append(output_dim0.tensor)
    op.input_desc.add().CopyFrom(output_dim0.desc)
    op.input_desc[-1].name = "output_dim0"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IgammaGradA
@auto_convert_to_tensor([False, False], [False, False])
def IgammaGradA(a: Tensor, x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IgammaGradA)\n
.INPUT(a, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(z, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IgammaGradA"
    op.name = next_unique_name(node_name, "IgammaGradA")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(a.tensor)
    op.input_desc.add().CopyFrom(a.desc)
    op.input_desc[-1].name = "a"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR InitData
@auto_convert_to_tensor([], [])
def InitData(*, channel_name: str="", dependencies=[], node_name=None):
    """REG_OP(InitData)\n
.ATTR(channel_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InitData"
    op.name = next_unique_name(node_name, "InitData")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["channel_name"].s = compat_as_bytes(channel_name)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR GetNext
@auto_convert_to_tensor([], [])
def _GetNext(*, size_of_y: int, output_types: List[int]=[], output_shapes: List[List[int]]=[], output_num: int=1, channel_name: str="", dependencies=[], node_name=None):
    """REG_OP(GetNext)\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL}))\n
.ATTR(output_types, ListType, {})\n
.ATTR(output_shapes, ListListInt, {})\n
.ATTR(output_num, Int, 1)\n
.ATTR(channel_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GetNext"
    op.name = next_unique_name(node_name, "GetNext")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["output_types"].list.val_type = 10
    op.attr["output_types"].list.dt.extend(output_types)
    op.attr["output_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(output_shapes))
    op.attr["output_num"].i = output_num
    op.attr["channel_name"].s = compat_as_bytes(channel_name)

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR GetDynamicDims
@auto_convert_to_tensor([True], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def GetDynamicDims(input: List[Tensor], *, shape_info: List[int], N: int, dependencies=[], node_name=None):
    """REG_OP(GetDynamicDims)\n
.DYNAMIC_INPUT(input, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(dims, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(shape_info, ListInt)\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GetDynamicDims"
    op.name = next_unique_name(node_name, "GetDynamicDims")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(input, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(input):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "input" + str(i)

    # process attrs
    op.attr["shape_info"].list.val_type = 2
    op.attr["shape_info"].list.i.extend(shape_info)
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dims"
    dims = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dims


# This api is auto-generated from IR EndOfSequence
@auto_convert_to_tensor([False], [False])
def EndOfSequence(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(EndOfSequence)\n
.INPUT(x, TensorType({DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EndOfSequence"
    op.name = next_unique_name(node_name, "EndOfSequence")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Erf
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def Erf(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Erf)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Erf"
    op.name = next_unique_name(node_name, "Erf")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Erfc
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def Erfc(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Erfc)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Erfc"
    op.name = next_unique_name(node_name, "Erfc")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HistogramFixedWidth
@auto_convert_to_tensor([False, False, False], [False, False, False])
def HistogramFixedWidth(x: Tensor, range: Tensor, nbins: Tensor, *, dtype: int=3, dependencies=[], node_name=None):
    """REG_OP(HistogramFixedWidth)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64}))\n
.INPUT(range, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64}))\n
.INPUT(nbins, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.ATTR(dtype, Int, 3)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HistogramFixedWidth"
    op.name = next_unique_name(node_name, "HistogramFixedWidth")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(range.tensor)
    op.input_desc.add().CopyFrom(range.desc)
    op.input_desc[-1].name = "range"
    op.input.append(nbins.tensor)
    op.input_desc.add().CopyFrom(nbins.desc)
    op.input_desc[-1].name = "nbins"

    # process attrs
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HistogramFixedWidthD
@auto_convert_to_tensor([False, False], [False, False])
def HistogramFixedWidthD(x: Tensor, range: Tensor, *, nbins: int, dtype: int=3, dependencies=[], node_name=None):
    """REG_OP(HistogramFixedWidthD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64}))\n
.INPUT(range, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(nbins, Int)\n
.ATTR(dtype, Int, 3)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HistogramFixedWidthD"
    op.name = next_unique_name(node_name, "HistogramFixedWidthD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(range.tensor)
    op.input_desc.add().CopyFrom(range.desc)
    op.input_desc[-1].name = "range"

    # process attrs
    op.attr["nbins"].i = nbins
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NextAfter
@auto_convert_to_tensor([False, False], [False, False])
def NextAfter(x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NextAfter)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NextAfter"
    op.name = next_unique_name(node_name, "NextAfter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR Pdist
@auto_convert_to_tensor([False], [False])
def Pdist(x: Tensor, *, p: float=2.000000, dependencies=[], node_name=None):
    """REG_OP(Pdist)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(p, Float, 2.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Pdist"
    op.name = next_unique_name(node_name, "Pdist")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].f = p

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IsFinite
@auto_convert_to_tensor([False], [False])
def IsFinite(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IsFinite)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsFinite"
    op.name = next_unique_name(node_name, "IsFinite")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IsPosInf
@auto_convert_to_tensor([False], [False])
def IsPosInf(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IsPosInf)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BFLOAT16}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsPosInf"
    op.name = next_unique_name(node_name, "IsPosInf")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IsInf
@auto_convert_to_tensor([False], [False])
def IsInf(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IsInf)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsInf"
    op.name = next_unique_name(node_name, "IsInf")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IsNegInf
@auto_convert_to_tensor([False], [False])
def IsNegInf(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IsNegInf)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BFLOAT16}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsNegInf"
    op.name = next_unique_name(node_name, "IsNegInf")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ComplexAbs
@auto_convert_to_tensor([False], [False])
def ComplexAbs(x: Tensor, *, Tout: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(ComplexAbs)\n
.INPUT(x, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(Tout, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ComplexAbs"
    op.name = next_unique_name(node_name, "ComplexAbs")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["Tout"].dt = Tout

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IsNan
@auto_convert_to_tensor([False], [False])
def IsNan(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IsNan)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsNan"
    op.name = next_unique_name(node_name, "IsNan")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Real
@auto_convert_to_tensor([False], [False])
def Real(input: Tensor, *, Tout: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(Real)\n
.INPUT(input, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(Tout, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Real"
    op.name = next_unique_name(node_name, "Real")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["Tout"].dt = Tout

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR Conj
@auto_convert_to_tensor([False], [False])
def Conj(input: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Conj)\n
.INPUT(input, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(output, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conj"
    op.name = next_unique_name(node_name, "Conj")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR NLLLoss
@auto_convert_to_tensor([False, False, False], [False, False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def NLLLoss(x: Tensor, target: Tensor, weight: Optional[Tensor], *, reduction: str="mean", ignore_index: int=-100, dependencies=[], node_name=None):
    """REG_OP(NLLLoss)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(target, TensorType({DT_INT32, DT_INT64}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.OUTPUT(total_weight, TensorType({DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
.ATTR(ignore_index, Int, -100)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NLLLoss"
    op.name = next_unique_name(node_name, "NLLLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["ignore_index"].i = ignore_index

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "total_weight"
    total_weight = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, total_weight


# This api is auto-generated from IR NLLLossGrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def NLLLossGrad(x: Tensor, y_grad: Tensor, target: Tensor, weight: Tensor, total_weight: Tensor, *, reduction: str="mean", ignore_index: int=-100, dependencies=[], node_name=None):
    """REG_OP(NLLLossGrad)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(y_grad, TensorType({DT_FLOAT}))\n
.INPUT(target, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(weight, TensorType({DT_FLOAT}))\n
.INPUT(total_weight, TensorType({DT_FLOAT}))\n
.OUTPUT(x_grad, TensorType({DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
.ATTR(ignore_index, Int, -100)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NLLLossGrad"
    op.name = next_unique_name(node_name, "NLLLossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"
    op.input.append(total_weight.tensor)
    op.input_desc.add().CopyFrom(total_weight.desc)
    op.input_desc[-1].name = "total_weight"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["ignore_index"].i = ignore_index

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR IFMR
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def IFMR(data: Tensor, data_min: Tensor, data_max: Tensor, cumsum: Tensor, *, min_percentile: float, max_percentile: float, search_range: List[float], search_step: float, with_offset: bool, dependencies=[], node_name=None):
    """REG_OP(IFMR)\n
.INPUT(data, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(data_min, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(data_max, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(cumsum, TensorType({DT_INT32}))\n
.OUTPUT(scale, TensorType({DT_FLOAT}))\n
.OUTPUT(offset, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(min_percentile, Float)\n
.REQUIRED_ATTR(max_percentile, Float)\n
.REQUIRED_ATTR(search_range, ListFloat)\n
.REQUIRED_ATTR(search_step, Float)\n
.REQUIRED_ATTR(with_offset, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IFMR"
    op.name = next_unique_name(node_name, "IFMR")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(data_min.tensor)
    op.input_desc.add().CopyFrom(data_min.desc)
    op.input_desc[-1].name = "data_min"
    op.input.append(data_max.tensor)
    op.input_desc.add().CopyFrom(data_max.desc)
    op.input_desc[-1].name = "data_max"
    op.input.append(cumsum.tensor)
    op.input_desc.add().CopyFrom(cumsum.desc)
    op.input_desc[-1].name = "cumsum"

    # process attrs
    op.attr["min_percentile"].f = min_percentile
    op.attr["max_percentile"].f = max_percentile
    op.attr["search_range"].list.val_type = 3
    op.attr["search_range"].list.f.extend(search_range)
    op.attr["search_step"].f = search_step
    op.attr["with_offset"].b = with_offset

    # process outputs
    output_index = 0
    op.output_desc.add().name = "scale"
    scale = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "offset"
    offset = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return scale, offset


# This api is auto-generated from IR WtsARQ
@auto_convert_to_tensor([False, False, False], [False, False, False])
def WtsARQ(w: Tensor, w_min: Tensor, w_max: Tensor, *, num_bits: int=8, offset_flag: bool=False, dependencies=[], node_name=None):
    """REG_OP(WtsARQ)\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w_min, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w_max, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(num_bits, Int, 8)\n
.ATTR(offset_flag, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "WtsARQ"
    op.name = next_unique_name(node_name, "WtsARQ")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(w_min.tensor)
    op.input_desc.add().CopyFrom(w_min.desc)
    op.input_desc[-1].name = "w_min"
    op.input.append(w_max.tensor)
    op.input_desc.add().CopyFrom(w_max.desc)
    op.input_desc[-1].name = "w_max"

    # process attrs
    op.attr["num_bits"].i = num_bits
    op.attr["offset_flag"].b = offset_flag

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ActsULQ
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ActsULQ(x: Tensor, clamp_min: Tensor, clamp_max: Tensor, *, fixed_min: bool=False, num_bits: int=8, dependencies=[], node_name=None):
    """REG_OP(ActsULQ)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(clamp_min, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(clamp_max, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(clamp_min_mask, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(clamp_max_mask, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(x_clamped_loss, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(fixed_min, Bool, false)\n
.ATTR(num_bits, Int, 8)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ActsULQ"
    op.name = next_unique_name(node_name, "ActsULQ")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(clamp_min.tensor)
    op.input_desc.add().CopyFrom(clamp_min.desc)
    op.input_desc[-1].name = "clamp_min"
    op.input.append(clamp_max.tensor)
    op.input_desc.add().CopyFrom(clamp_max.desc)
    op.input_desc[-1].name = "clamp_max"

    # process attrs
    op.attr["fixed_min"].b = fixed_min
    op.attr["num_bits"].i = num_bits

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "clamp_min_mask"
    clamp_min_mask = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "clamp_max_mask"
    clamp_max_mask = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "x_clamped_loss"
    x_clamped_loss = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, clamp_min_mask, clamp_max_mask, x_clamped_loss


# This api is auto-generated from IR ActsULQInputGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ActsULQInputGrad(y_grad: Tensor, clamp_min_mask: Tensor, clamp_max_mask: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ActsULQInputGrad)\n
.INPUT(y_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(clamp_min_mask, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT}))\n
.INPUT(clamp_max_mask, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(x_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ActsULQInputGrad"
    op.name = next_unique_name(node_name, "ActsULQInputGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(clamp_min_mask.tensor)
    op.input_desc.add().CopyFrom(clamp_min_mask.desc)
    op.input_desc[-1].name = "clamp_min_mask"
    op.input.append(clamp_max_mask.tensor)
    op.input_desc.add().CopyFrom(clamp_max_mask.desc)
    op.input_desc[-1].name = "clamp_max_mask"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR ActULQClampMaxGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ActULQClampMaxGrad(y_grad: Tensor, clamp_max_mask: Tensor, x_clamped_loss: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ActULQClampMaxGrad)\n
.INPUT(y_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(clamp_max_mask, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x_clamped_loss, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(clamp_max_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ActULQClampMaxGrad"
    op.name = next_unique_name(node_name, "ActULQClampMaxGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(clamp_max_mask.tensor)
    op.input_desc.add().CopyFrom(clamp_max_mask.desc)
    op.input_desc[-1].name = "clamp_max_mask"
    op.input.append(x_clamped_loss.tensor)
    op.input_desc.add().CopyFrom(x_clamped_loss.desc)
    op.input_desc[-1].name = "x_clamped_loss"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "clamp_max_grad"
    clamp_max_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return clamp_max_grad


# This api is auto-generated from IR ActULQClampMinGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ActULQClampMinGrad(y_grad: Tensor, clamp_min_mask: Tensor, x_clamped_loss: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ActULQClampMinGrad)\n
.INPUT(y_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(clamp_min_mask, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x_clamped_loss, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(clamp_min_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ActULQClampMinGrad"
    op.name = next_unique_name(node_name, "ActULQClampMinGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(clamp_min_mask.tensor)
    op.input_desc.add().CopyFrom(clamp_min_mask.desc)
    op.input_desc[-1].name = "clamp_min_mask"
    op.input.append(x_clamped_loss.tensor)
    op.input_desc.add().CopyFrom(x_clamped_loss.desc)
    op.input_desc[-1].name = "x_clamped_loss"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "clamp_min_grad"
    clamp_min_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return clamp_min_grad


# This api is auto-generated from IR LpNorm
@auto_convert_to_tensor([False], [False])
def LpNorm(x: Tensor, *, p: int=2, axes: List[int]=[], keepdim: bool=False, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LpNorm)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(p, Int, 2)\n
.ATTR(axes, ListInt, {})\n
.ATTR(keepdim, Bool, false)\n
.ATTR(epsilon, Float, 1e-12)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LpNorm"
    op.name = next_unique_name(node_name, "LpNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].i = p
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keepdim"].b = keepdim
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LpNormV2
@auto_convert_to_tensor([False], [False])
def LpNormV2(x: Tensor, *, p: float=2.000000, axes: List[int]=[], keepdim: bool=False, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LpNormV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(p, Float, 2.0)\n
.ATTR(axes, ListInt, {})\n
.ATTR(keepdim, Bool, false)\n
.ATTR(epsilon, Float, 1e-12)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LpNormV2"
    op.name = next_unique_name(node_name, "LpNormV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].f = p
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keepdim"].b = keepdim
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LpNormReduce
@auto_convert_to_tensor([False], [False])
def LpNormReduce(x: Tensor, *, p: int=2, axes: List[int]=[], keepdim: bool=False, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LpNormReduce)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(p, Int, 2)\n
.ATTR(axes, ListInt, {})\n
.ATTR(keepdim, Bool, false)\n
.ATTR(epsilon, Float, 1e-12)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LpNormReduce"
    op.name = next_unique_name(node_name, "LpNormReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].i = p
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keepdim"].b = keepdim
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LpNormReduceV2
@auto_convert_to_tensor([False], [False])
def LpNormReduceV2(x: Tensor, *, p: float=2.000000, axes: List[int]=[], keepdim: bool=False, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LpNormReduceV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(p, Float, 2.0)\n
.ATTR(axes, ListInt, {})\n
.ATTR(keepdim, Bool, false)\n
.ATTR(epsilon, Float, 1e-12)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LpNormReduceV2"
    op.name = next_unique_name(node_name, "LpNormReduceV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].f = p
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keepdim"].b = keepdim
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LpNormUpdate
@auto_convert_to_tensor([False], [False])
def LpNormUpdate(x: Tensor, *, p: int=2, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LpNormUpdate)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(p, Int, 2)\n
.ATTR(epsilon, Float, 1e-12)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LpNormUpdate"
    op.name = next_unique_name(node_name, "LpNormUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].i = p
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LpNormUpdateV2
@auto_convert_to_tensor([False], [False])
def LpNormUpdateV2(x: Tensor, *, p: float=2.000000, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LpNormUpdateV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(p, Float, 2.0)\n
.ATTR(epsilon, Float, 1e-12)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LpNormUpdateV2"
    op.name = next_unique_name(node_name, "LpNormUpdateV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].f = p
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Complex
@auto_convert_to_tensor([False, False], [False, False])
def Complex(real: Tensor, imag: Tensor, *, Tout: int=16, dependencies=[], node_name=None):
    """REG_OP(Complex)\n
.INPUT(real, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(imag, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(out, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(Tout, Type, DT_COMPLEX64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Complex"
    op.name = next_unique_name(node_name, "Complex")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(real.tensor)
    op.input_desc.add().CopyFrom(real.desc)
    op.input_desc[-1].name = "real"
    op.input.append(imag.tensor)
    op.input_desc.add().CopyFrom(imag.desc)
    op.input_desc[-1].name = "imag"

    # process attrs
    op.attr["Tout"].dt = Tout

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR SparseBincount
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def SparseBincount(indices: Tensor, values: Tensor, dense_shape: Tensor, size: Tensor, weights: Tensor, *, binary_output: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseBincount)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(dense_shape, TensorType({DT_INT64}))\n
.INPUT(size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(weights, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(binary_output, Bool, false)\n
.OUTPUT(output, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseBincount"
    op.name = next_unique_name(node_name, "SparseBincount")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(dense_shape.tensor)
    op.input_desc.add().CopyFrom(dense_shape.desc)
    op.input_desc[-1].name = "dense_shape"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["binary_output"].b = binary_output

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR Imag
@auto_convert_to_tensor([False], [False])
def Imag(input: Tensor, *, Tout: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(Imag)\n
.INPUT(input, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(Tout, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Imag"
    op.name = next_unique_name(node_name, "Imag")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["Tout"].dt = Tout

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR Angle
@auto_convert_to_tensor([False], [False])
def Angle(input: Tensor, *, Tout: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(Angle)\n
.INPUT(input, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.ATTR(Tout, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Angle"
    op.name = next_unique_name(node_name, "Angle")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["Tout"].dt = Tout

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR SoftMarginLossGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SoftMarginLossGrad(predict: Tensor, label: Tensor, dout: Tensor, *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SoftMarginLossGrad)\n
.INPUT(predict, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(label, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(dout, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(gradient, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftMarginLossGrad"
    op.name = next_unique_name(node_name, "SoftMarginLossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "gradient"
    gradient = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return gradient


# This api is auto-generated from IR Cross
@auto_convert_to_tensor([False, False], [False, False])
def Cross(x1: Tensor, x2: Tensor, *, dim: int=-65530, dependencies=[], node_name=None):
    """REG_OP(Cross)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_INT16}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_INT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8, DT_INT16}))\n
.ATTR(dim, Int, -65530)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cross"
    op.name = next_unique_name(node_name, "Cross")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["dim"].i = dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Cdist
@auto_convert_to_tensor([False, False], [False, False])
def Cdist(x1: Tensor, x2: Tensor, *, p: float=2.000000, dependencies=[], node_name=None):
    """REG_OP(Cdist)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(p, Float, 2.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cdist"
    op.name = next_unique_name(node_name, "Cdist")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["p"].f = p

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CdistGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def CdistGrad(grad: Tensor, x1: Tensor, x2: Tensor, cdist: Tensor, *, p: float=2.000000, dependencies=[], node_name=None):
    """REG_OP(CdistGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(cdist, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(p, Float, 2.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CdistGrad"
    op.name = next_unique_name(node_name, "CdistGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(cdist.tensor)
    op.input_desc.add().CopyFrom(cdist.desc)
    op.input_desc[-1].name = "cdist"

    # process attrs
    op.attr["p"].f = p

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RaggedBincount
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def RaggedBincount(splits: Tensor, values: Tensor, size: Tensor, weights: Tensor, *, binary_output: bool=False, dependencies=[], node_name=None):
    """REG_OP(RaggedBincount)\n
.INPUT(splits, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(weights, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(output, TensorType({DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(binary_output, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedBincount"
    op.name = next_unique_name(node_name, "RaggedBincount")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(splits.tensor)
    op.input_desc.add().CopyFrom(splits.desc)
    op.input_desc[-1].name = "splits"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["binary_output"].b = binary_output

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR DenseCountSparseOutput
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def DenseCountSparseOutput(values: Tensor, weights: Tensor, *, binary_output: bool, minlength: int=-1, maxlength: int=-1, dependencies=[], node_name=None):
    """REG_OP(DenseCountSparseOutput)\n
.INPUT(values, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(weights, TensorType({DT_INT32,DT_INT64,DT_FLOAT,DT_DOUBLE}))\n
.OUTPUT(output_indices, TensorType({DT_INT64}))\n
.OUTPUT(output_values, TensorType({DT_INT32,DT_INT64,DT_FLOAT,DT_DOUBLE}))\n
.OUTPUT(output_dense_shape, TensorType({DT_INT64}))\n
.ATTR(minlength, Int, -1)\n
.ATTR(maxlength, Int, -1)\n
.REQUIRED_ATTR(binary_output, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DenseCountSparseOutput"
    op.name = next_unique_name(node_name, "DenseCountSparseOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["binary_output"].b = binary_output
    op.attr["minlength"].i = minlength
    op.attr["maxlength"].i = maxlength

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_indices"
    output_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_values"
    output_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_dense_shape"
    output_dense_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_indices, output_values, output_dense_shape


# This api is auto-generated from IR SparseSegmentSumGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def SparseSegmentSumGrad(grad: Tensor, indices: Tensor, segment_ids: Tensor, output_dim0: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSegmentSumGrad)\n
.INPUT(grad, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(segment_ids, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(output_dim0, TensorType({DT_INT32}))\n
.OUTPUT(output, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSegmentSumGrad"
    op.name = next_unique_name(node_name, "SparseSegmentSumGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"
    op.input.append(output_dim0.tensor)
    op.input_desc.add().CopyFrom(output_dim0.desc)
    op.input_desc[-1].name = "output_dim0"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR RaggedCountSparseOutput
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def RaggedCountSparseOutput(splits: Tensor, values: Tensor, weights: Tensor, *, binary_output: bool, minlength: int=-1, maxlength: int=-1, dependencies=[], node_name=None):
    """REG_OP(RaggedCountSparseOutput)\n
.INPUT(splits, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(weights, TensorType({DT_INT32,DT_INT64,DT_FLOAT,DT_DOUBLE}))\n
.OUTPUT(output_indices, TensorType({DT_INT64}))\n
.OUTPUT(output_values, TensorType({DT_INT32,DT_INT64,DT_FLOAT,DT_DOUBLE}))\n
.OUTPUT(output_dense_shape, TensorType({DT_INT64}))\n
.ATTR(minlength, Int, -1)\n
.ATTR(maxlength, Int, -1)\n
.REQUIRED_ATTR(binary_output, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedCountSparseOutput"
    op.name = next_unique_name(node_name, "RaggedCountSparseOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(splits.tensor)
    op.input_desc.add().CopyFrom(splits.desc)
    op.input_desc[-1].name = "splits"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["binary_output"].b = binary_output
    op.attr["minlength"].i = minlength
    op.attr["maxlength"].i = maxlength

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_indices"
    output_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_values"
    output_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_dense_shape"
    output_dense_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_indices, output_values, output_dense_shape


# This api is auto-generated from IR SignBitsUnpack
@auto_convert_to_tensor([False], [False])
def SignBitsUnpack(x: Tensor, *, size: int, dtype: int, dependencies=[], node_name=None):
    """REG_OP(SignBitsUnpack)\n
.INPUT(x, TensorType({DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(size, Int)\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SignBitsUnpack"
    op.name = next_unique_name(node_name, "SignBitsUnpack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["size"].i = size
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScaledMaskedSoftmax
@auto_convert_to_tensor([False, False], [False, True])
def ScaledMaskedSoftmax(x: Tensor, mask: Optional[Tensor], *, scale: float=1.000000, fixed_triu_mask: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScaledMaskedSoftmax)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_BOOL, DT_UINT1}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16}))\n
.ATTR(scale, Float, 1.0)\n
.ATTR(fixed_triu_mask, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScaledMaskedSoftmax"
    op.name = next_unique_name(node_name, "ScaledMaskedSoftmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["scale"].f = scale
    op.attr["fixed_triu_mask"].b = fixed_triu_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScaledMaskedSoftmaxGrad
@auto_convert_to_tensor([False, False, False], [False, False, True])
def ScaledMaskedSoftmaxGrad(y_grad: Tensor, y: Tensor, mask: Optional[Tensor], *, scale: float=1.000000, fixed_triu_mask: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScaledMaskedSoftmaxGrad)\n
.INPUT(y_grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_BOOL, DT_UINT1}))\n
.OUTPUT(x_grad, TensorType({DT_FLOAT16, DT_BF16}))\n
.ATTR(scale, Float, 1.0)\n
.ATTR(fixed_triu_mask, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScaledMaskedSoftmaxGrad"
    op.name = next_unique_name(node_name, "ScaledMaskedSoftmaxGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["scale"].f = scale
    op.attr["fixed_triu_mask"].b = fixed_triu_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR SignBitsPack
@auto_convert_to_tensor([False], [False])
def SignBitsPack(x: Tensor, *, size: int, dependencies=[], node_name=None):
    """REG_OP(SignBitsPack)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
.REQUIRED_ATTR(size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SignBitsPack"
    op.name = next_unique_name(node_name, "SignBitsPack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["size"].i = size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SobolSample
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SobolSample(dim: Tensor, num_results: Tensor, skip: Tensor, *, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(SobolSample)\n
.INPUT(dim, TensorType({DT_INT32}))\n
.INPUT(num_results, TensorType({DT_INT32}))\n
.INPUT(skip, TensorType({DT_INT32}))\n
.OUTPUT(samples, TensorType({DT_FLOAT,DT_DOUBLE}))\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SobolSample"
    op.name = next_unique_name(node_name, "SobolSample")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dim.tensor)
    op.input_desc.add().CopyFrom(dim.desc)
    op.input_desc[-1].name = "dim"
    op.input.append(num_results.tensor)
    op.input_desc.add().CopyFrom(num_results.desc)
    op.input_desc[-1].name = "num_results"
    op.input.append(skip.tensor)
    op.input_desc.add().CopyFrom(skip.desc)
    op.input_desc[-1].name = "skip"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "samples"
    samples = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return samples


# This api is auto-generated from IR SparseCountSparseOutput
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def SparseCountSparseOutput(indices: Tensor, values: Tensor, dense_shape: Tensor, weights: Tensor, *, binary_output: bool, minlength: int=-1, maxlength: int=-1, dependencies=[], node_name=None):
    """REG_OP(SparseCountSparseOutput)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(dense_shape, TensorType({DT_INT64}))\n
.INPUT(weights, TensorType({DT_INT32,DT_INT64,DT_FLOAT,DT_DOUBLE}))\n
.OUTPUT(output_indices, TensorType({DT_INT64}))\n
.OUTPUT(output_values, TensorType({DT_INT32,DT_INT64,DT_FLOAT,DT_DOUBLE}))\n
.OUTPUT(output_dense_shape, TensorType({DT_INT64}))\n
.ATTR(minlength, Int, -1)\n
.ATTR(maxlength, Int, -1)\n
.REQUIRED_ATTR(binary_output, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseCountSparseOutput"
    op.name = next_unique_name(node_name, "SparseCountSparseOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(dense_shape.tensor)
    op.input_desc.add().CopyFrom(dense_shape.desc)
    op.input_desc[-1].name = "dense_shape"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["binary_output"].b = binary_output
    op.attr["minlength"].i = minlength
    op.attr["maxlength"].i = maxlength

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_indices"
    output_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_values"
    output_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_dense_shape"
    output_dense_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_indices, output_values, output_dense_shape


# This api is auto-generated from IR RaggedBinCount
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def RaggedBinCount(splits: Tensor, values: Tensor, size: Tensor, weights: Tensor, *, binary_output: bool=False, dependencies=[], node_name=None):
    """REG_OP(RaggedBinCount)\n
.INPUT(splits, TensorType(DT_INT64))\n
.INPUT(values, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(weights, TensorType(DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE))\n
.OUTPUT(output, TensorType(DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE))\n
.ATTR(binary_output, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedBinCount"
    op.name = next_unique_name(node_name, "RaggedBinCount")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(splits.tensor)
    op.input_desc.add().CopyFrom(splits.desc)
    op.input_desc[-1].name = "splits"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["binary_output"].b = binary_output

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR DenseBincount
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def DenseBincount(input: Tensor, size: Tensor, weights: Tensor, *, binary_output: bool=False, dependencies=[], node_name=None):
    """REG_OP(DenseBincount)\n
.INPUT(input, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(weights, TensorType(DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE))\n
.OUTPUT(output, TensorType(DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE))\n
.ATTR(binary_output, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DenseBincount"
    op.name = next_unique_name(node_name, "DenseBincount")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["binary_output"].b = binary_output

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR AttentionQKVGradW
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def AttentionQKVGradW(x: Tensor, query_dx: Tensor, key_dw: Optional[Tensor], value_dw: Optional[Tensor], *, trans_a: bool=True, trans_b: bool=False, trans_dw: bool=False, dependencies=[], node_name=None):
    """REG_OP(AttentionQKVGradW)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(query_dx, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(key_dw, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(value_dw, TensorType({DT_FLOAT16}))\n
.OUTPUT(dw_query, TensorType({DT_FLOAT16}))\n
.OUTPUT(dw_key, TensorType({DT_FLOAT16}))\n
.OUTPUT(dw_value, TensorType({DT_FLOAT16}))\n
.OUTPUT(dbias_query, TensorType({DT_FLOAT16}))\n
.OUTPUT(dbias_key, TensorType({DT_FLOAT16}))\n
.OUTPUT(dbias_value, TensorType({DT_FLOAT16}))\n
.ATTR(trans_a, Bool, true)\n
.ATTR(trans_b, Bool, false)\n
.ATTR(trans_dw, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AttentionQKVGradW"
    op.name = next_unique_name(node_name, "AttentionQKVGradW")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(query_dx.tensor)
    op.input_desc.add().CopyFrom(query_dx.desc)
    op.input_desc[-1].name = "query_dx"
    if key_dw is not None:
        op.input.append(key_dw.tensor)
        op.input_desc.add().CopyFrom(key_dw.desc)
        op.input_desc[-1].name = "key_dw"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "key_dw"
    if value_dw is not None:
        op.input.append(value_dw.tensor)
        op.input_desc.add().CopyFrom(value_dw.desc)
        op.input_desc[-1].name = "value_dw"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "value_dw"

    # process attrs
    op.attr["trans_a"].b = trans_a
    op.attr["trans_b"].b = trans_b
    op.attr["trans_dw"].b = trans_dw

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dw_query"
    dw_query = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dw_key"
    dw_key = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dw_value"
    dw_value = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dbias_query"
    dbias_query = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dbias_key"
    dbias_key = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dbias_value"
    dbias_value = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dw_query, dw_key, dw_value, dbias_query, dbias_key, dbias_value


# This api is auto-generated from IR AttentionQKVGradX
@auto_convert_to_tensor([False, False, False, False, False, False, False], [True, False, False, False, False, False, False])
def AttentionQKVGradX(ln_dx: Optional[Tensor], query_dx: Tensor, key_dw: Tensor, value_dw: Tensor, kernel_query: Tensor, kernel_key: Tensor, kernel_value: Tensor, *, trans_a: bool=False, trans_b: bool=True, dependencies=[], node_name=None):
    """REG_OP(AttentionQKVGradX)\n
.OPTIONAL_INPUT(ln_dx, TensorType({DT_FLOAT16}))\n
.INPUT(query_dx, TensorType({DT_FLOAT16}))\n
.INPUT(key_dw, TensorType({DT_FLOAT16}))\n
.INPUT(value_dw, TensorType({DT_FLOAT16}))\n
.INPUT(kernel_query, TensorType({DT_FLOAT16}))\n
.INPUT(kernel_key, TensorType({DT_FLOAT16}))\n
.INPUT(kernel_value, TensorType({DT_FLOAT16}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16}))\n
.ATTR(trans_a, Bool, false)\n
.ATTR(trans_b, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AttentionQKVGradX"
    op.name = next_unique_name(node_name, "AttentionQKVGradX")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if ln_dx is not None:
        op.input.append(ln_dx.tensor)
        op.input_desc.add().CopyFrom(ln_dx.desc)
        op.input_desc[-1].name = "ln_dx"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "ln_dx"
    op.input.append(query_dx.tensor)
    op.input_desc.add().CopyFrom(query_dx.desc)
    op.input_desc[-1].name = "query_dx"
    op.input.append(key_dw.tensor)
    op.input_desc.add().CopyFrom(key_dw.desc)
    op.input_desc[-1].name = "key_dw"
    op.input.append(value_dw.tensor)
    op.input_desc.add().CopyFrom(value_dw.desc)
    op.input_desc[-1].name = "value_dw"
    op.input.append(kernel_query.tensor)
    op.input_desc.add().CopyFrom(kernel_query.desc)
    op.input_desc[-1].name = "kernel_query"
    op.input.append(kernel_key.tensor)
    op.input_desc.add().CopyFrom(kernel_key.desc)
    op.input_desc[-1].name = "kernel_key"
    op.input.append(kernel_value.tensor)
    op.input_desc.add().CopyFrom(kernel_value.desc)
    op.input_desc[-1].name = "kernel_value"

    # process attrs
    op.attr["trans_a"].b = trans_a
    op.attr["trans_b"].b = trans_b

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx


# This api is auto-generated from IR AttentionLnQKV
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, True, True, True])
def AttentionLnQKV(x: Tensor, kernel_query: Tensor, kernel_key: Tensor, kernel_value: Tensor, gamma: Tensor, beta: Tensor, bias_query: Optional[Tensor], bias_key: Optional[Tensor], bias_value: Optional[Tensor], *, epsilon: float=0.000000, trans_a: bool=False, trans_b: bool=False, dependencies=[], node_name=None):
    """REG_OP(AttentionLnQKV)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(kernel_query, TensorType({DT_FLOAT16}))\n
.INPUT(kernel_key, TensorType({DT_FLOAT16}))\n
.INPUT(kernel_value, TensorType({DT_FLOAT16}))\n
.INPUT(gamma, TensorType({DT_FLOAT16}))\n
.INPUT(beta, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias_query, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias_key, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias_value, TensorType({DT_FLOAT16}))\n
.OUTPUT(norm, TensorType({DT_FLOAT16}))\n
.OUTPUT(query_output, TensorType({DT_FLOAT16}))\n
.OUTPUT(key_output, TensorType({DT_FLOAT16}))\n
.OUTPUT(value_output, TensorType({DT_FLOAT16}))\n
.OUTPUT(mean, TensorType({DT_FLOAT16}))\n
.OUTPUT(variance, TensorType({DT_FLOAT16}))\n
.ATTR(epsilon, Float, 0.0000001)\n
.ATTR(trans_a, Bool, false)\n
.ATTR(trans_b, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AttentionLnQKV"
    op.name = next_unique_name(node_name, "AttentionLnQKV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(kernel_query.tensor)
    op.input_desc.add().CopyFrom(kernel_query.desc)
    op.input_desc[-1].name = "kernel_query"
    op.input.append(kernel_key.tensor)
    op.input_desc.add().CopyFrom(kernel_key.desc)
    op.input_desc[-1].name = "kernel_key"
    op.input.append(kernel_value.tensor)
    op.input_desc.add().CopyFrom(kernel_value.desc)
    op.input_desc[-1].name = "kernel_value"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"
    if bias_query is not None:
        op.input.append(bias_query.tensor)
        op.input_desc.add().CopyFrom(bias_query.desc)
        op.input_desc[-1].name = "bias_query"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_query"
    if bias_key is not None:
        op.input.append(bias_key.tensor)
        op.input_desc.add().CopyFrom(bias_key.desc)
        op.input_desc[-1].name = "bias_key"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_key"
    if bias_value is not None:
        op.input.append(bias_value.tensor)
        op.input_desc.add().CopyFrom(bias_value.desc)
        op.input_desc[-1].name = "bias_value"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_value"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["trans_a"].b = trans_a
    op.attr["trans_b"].b = trans_b

    # process outputs
    output_index = 0
    op.output_desc.add().name = "norm"
    norm = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "query_output"
    query_output = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "key_output"
    key_output = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_output"
    value_output = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return norm, query_output, key_output, value_output, mean, variance


# This api is auto-generated from IR SwinTransformerLnQKV
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def SwinTransformerLnQKV(x: Tensor, gamma: Tensor, beta: Tensor, weight: Tensor, bias: Tensor, *, head_num: int, head_dim: int, seq_length: int, shifts: List[int]=[], epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(SwinTransformerLnQKV)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(gamma, TensorType({DT_FLOAT16}))\n
.INPUT(beta, TensorType({DT_FLOAT16}))\n
.INPUT(weight, TensorType({DT_FLOAT16}))\n
.INPUT(bias, TensorType({DT_FLOAT16}))\n
.OUTPUT(query_output, TensorType({DT_FLOAT16}))\n
.OUTPUT(key_output, TensorType({DT_FLOAT16}))\n
.OUTPUT(value_output, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(head_num, Int)\n
.REQUIRED_ATTR(head_dim, Int)\n
.REQUIRED_ATTR(seq_length, Int)\n
.ATTR(shifts, ListInt, {})\n
.ATTR(epsilon, Float, 0.0000001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SwinTransformerLnQKV"
    op.name = next_unique_name(node_name, "SwinTransformerLnQKV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["head_num"].i = head_num
    op.attr["head_dim"].i = head_dim
    op.attr["seq_length"].i = seq_length
    op.attr["shifts"].list.val_type = 2
    op.attr["shifts"].list.i.extend(shifts)
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "query_output"
    query_output = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "key_output"
    key_output = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_output"
    value_output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return query_output, key_output, value_output


# This api is auto-generated from IR MatMul
@auto_convert_to_tensor([False, False, False], [False, False, True])
def MatMul(x1: Tensor, x2: Tensor, bias: Optional[Tensor], *, transpose_x1: bool=False, transpose_x2: bool=False, dependencies=[], node_name=None):
    """REG_OP(MatMul)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.ATTR(transpose_x1, Bool, false)\n
.ATTR(transpose_x2, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatMul"
    op.name = next_unique_name(node_name, "MatMul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["transpose_x1"].b = transpose_x1
    op.attr["transpose_x2"].b = transpose_x2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatMulV2
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def MatMulV2(x1: Tensor, x2: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, transpose_x1: bool=False, transpose_x2: bool=False, offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(MatMulV2)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8, DT_INT4}))\n
.ATTR(transpose_x1, Bool, false)\n
.ATTR(transpose_x2, Bool, false)\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatMulV2"
    op.name = next_unique_name(node_name, "MatMulV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["transpose_x1"].b = transpose_x1
    op.attr["transpose_x2"].b = transpose_x2
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatMulV2Compress
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def MatMulV2Compress(x1: Tensor, x2: Tensor, compress_index: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, transpose_x1: bool=False, transpose_x2: bool=False, offset_x: int=0, alg: str="weight_unzip", dependencies=[], node_name=None):
    """REG_OP(MatMulV2Compress)\n
.INPUT(x1, TensorType({DT_INT8}))\n
.INPUT(x2, TensorType({DT_INT8}))\n
.INPUT(compress_index, TensorType({DT_INT8}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_INT32, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.ATTR(transpose_x1, Bool, false)\n
.ATTR(transpose_x2, Bool, false)\n
.ATTR(offset_x, Int, 0)\n
.ATTR(alg, String, "weight_unzip")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatMulV2Compress"
    op.name = next_unique_name(node_name, "MatMulV2Compress")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(compress_index.tensor)
    op.input_desc.add().CopyFrom(compress_index.desc)
    op.input_desc[-1].name = "compress_index"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["transpose_x1"].b = transpose_x1
    op.attr["transpose_x2"].b = transpose_x2
    op.attr["offset_x"].i = offset_x
    op.attr["alg"].s = compat_as_bytes(alg)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GEMM
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def GEMM(a: Tensor, b: Tensor, c: Tensor, alpha: Tensor, beta: Tensor, *, transpose_a: bool=False, transpose_b: bool=False, dependencies=[], node_name=None):
    """REG_OP(GEMM)\n
.INPUT(a, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT32}))\n
.INPUT(b, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT32}))\n
.INPUT(c, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT32}))\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT32}))\n
.INPUT(beta, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT32}))\n
.ATTR(transpose_a, Bool, false)\n
.ATTR(transpose_b, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GEMM"
    op.name = next_unique_name(node_name, "GEMM")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(a.tensor)
    op.input_desc.add().CopyFrom(a.desc)
    op.input_desc[-1].name = "a"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["transpose_a"].b = transpose_a
    op.attr["transpose_b"].b = transpose_b

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BatchMatMul
@auto_convert_to_tensor([False, False], [False, False])
def BatchMatMul(x1: Tensor, x2: Tensor, *, adj_x1: bool=False, adj_x2: bool=False, dependencies=[], node_name=None):
    """REG_OP(BatchMatMul)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.ATTR(adj_x1, Bool, false)\n
.ATTR(adj_x2, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchMatMul"
    op.name = next_unique_name(node_name, "BatchMatMul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["adj_x1"].b = adj_x1
    op.attr["adj_x2"].b = adj_x2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BatchMatMulV2
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def BatchMatMulV2(x1: Tensor, x2: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, adj_x1: bool=False, adj_x2: bool=False, offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(BatchMatMulV2)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_INT4, DT_BF16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8, DT_INT4}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_BF16}))\n
.ATTR(adj_x1, Bool, false)\n
.ATTR(adj_x2, Bool, false)\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchMatMulV2"
    op.name = next_unique_name(node_name, "BatchMatMulV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["adj_x1"].b = adj_x1
    op.attr["adj_x2"].b = adj_x2
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR L2Loss
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def L2Loss(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(L2Loss)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "L2Loss"
    op.name = next_unique_name(node_name, "L2Loss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixDiag
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def MatrixDiag(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixDiag)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiag"
    op.name = next_unique_name(node_name, "MatrixDiag")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixDiagD
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC])
def MatrixDiagD(x: Tensor, assist: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixDiagD)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(assist, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiagD"
    op.name = next_unique_name(node_name, "MatrixDiagD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixDiagPart
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def MatrixDiagPart(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixDiagPart)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiagPart"
    op.name = next_unique_name(node_name, "MatrixDiagPart")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixDiagPartD
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC])
def MatrixDiagPartD(x: Tensor, assist: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixDiagPartD)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(assist, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiagPartD"
    op.name = next_unique_name(node_name, "MatrixDiagPartD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixSetDiag
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC])
def MatrixSetDiag(x: Tensor, diagonal: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixSetDiag)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(diagonal, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixSetDiag"
    op.name = next_unique_name(node_name, "MatrixSetDiag")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(diagonal.tensor)
    op.input_desc.add().CopyFrom(diagonal.desc)
    op.input_desc[-1].name = "diagonal"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixSetDiagD
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_BASIC])
def MatrixSetDiagD(x: Tensor, diagonal: Tensor, assist: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixSetDiagD)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(diagonal, TensorType::BasicType())\n
.INPUT(assist, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixSetDiagD"
    op.name = next_unique_name(node_name, "MatrixSetDiagD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(diagonal.tensor)
    op.input_desc.add().CopyFrom(diagonal.desc)
    op.input_desc[-1].name = "diagonal"
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AttentionScore
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, True])
def AttentionScore(query: Tensor, key: Tensor, value: Tensor, padding_mask: Tensor, scale: Tensor, drop_mask: Optional[Tensor], *, keep_prob: float=1.000000, query_transpose: bool=False, key_transpose: bool=False, bmm_score_transpose_a: bool=False, bmm_score_transpose_b: bool=False, softmax_axes: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(AttentionScore)\n
.INPUT(query, TensorType({DT_FLOAT16}))\n
.INPUT(key, TensorType({DT_FLOAT16}))\n
.INPUT(value, TensorType({DT_FLOAT16}))\n
.INPUT(padding_mask, TensorType({DT_FLOAT16}))\n
.INPUT(scale, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(drop_mask, TensorType({DT_INT8}))\n
.OUTPUT(attention_score, TensorType({DT_FLOAT16}))\n
.OUTPUT(softmax_output, TensorType({DT_FLOAT16}))\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(query_transpose, Bool, false)\n
.ATTR(key_transpose, Bool, false)\n
.ATTR(bmm_score_transpose_a, Bool, false)\n
.ATTR(bmm_score_transpose_b, Bool, false)\n
.ATTR(softmax_axes, ListInt, {-1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AttentionScore"
    op.name = next_unique_name(node_name, "AttentionScore")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(padding_mask.tensor)
    op.input_desc.add().CopyFrom(padding_mask.desc)
    op.input_desc[-1].name = "padding_mask"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if drop_mask is not None:
        op.input.append(drop_mask.tensor)
        op.input_desc.add().CopyFrom(drop_mask.desc)
        op.input_desc[-1].name = "drop_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "drop_mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob
    op.attr["query_transpose"].b = query_transpose
    op.attr["key_transpose"].b = key_transpose
    op.attr["bmm_score_transpose_a"].b = bmm_score_transpose_a
    op.attr["bmm_score_transpose_b"].b = bmm_score_transpose_b
    op.attr["softmax_axes"].list.val_type = 2
    op.attr["softmax_axes"].list.i.extend(softmax_axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "attention_score"
    attention_score = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "softmax_output"
    softmax_output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return attention_score, softmax_output


# This api is auto-generated from IR AttentionScoreGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, True])
def AttentionScoreGrad(attention_score: Tensor, dx: Tensor, query: Tensor, key: Tensor, value: Tensor, scale: Tensor, drop_mask: Optional[Tensor], *, keep_prob: float=1.000000, query_transpose: bool=False, key_transpose: bool=False, value_transpose: bool=False, dx_transpose: bool=False, softmax_axes: int=-1, dependencies=[], node_name=None):
    """REG_OP(AttentionScoreGrad)\n
.INPUT(attention_score, TensorType({DT_FLOAT16}))\n
.INPUT(dx, TensorType({DT_FLOAT16}))\n
.INPUT(query, TensorType({DT_FLOAT16}))\n
.INPUT(key, TensorType({DT_FLOAT16}))\n
.INPUT(value, TensorType({DT_FLOAT16}))\n
.INPUT(scale, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(drop_mask, TensorType({DT_INT8}))\n
.OUTPUT(value_dw, TensorType({DT_FLOAT16}))\n
.OUTPUT(query_dx, TensorType({DT_FLOAT16}))\n
.OUTPUT(key_dw, TensorType({DT_FLOAT16}))\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(query_transpose, Bool, false)\n
.ATTR(key_transpose, Bool, false)\n
.ATTR(value_transpose, Bool, false)\n
.ATTR(dx_transpose, Bool, false)\n
.ATTR(softmax_axes, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AttentionScoreGrad"
    op.name = next_unique_name(node_name, "AttentionScoreGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(attention_score.tensor)
    op.input_desc.add().CopyFrom(attention_score.desc)
    op.input_desc[-1].name = "attention_score"
    op.input.append(dx.tensor)
    op.input_desc.add().CopyFrom(dx.desc)
    op.input_desc[-1].name = "dx"
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if drop_mask is not None:
        op.input.append(drop_mask.tensor)
        op.input_desc.add().CopyFrom(drop_mask.desc)
        op.input_desc[-1].name = "drop_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "drop_mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob
    op.attr["query_transpose"].b = query_transpose
    op.attr["key_transpose"].b = key_transpose
    op.attr["value_transpose"].b = value_transpose
    op.attr["dx_transpose"].b = dx_transpose
    op.attr["softmax_axes"].i = softmax_axes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "value_dw"
    value_dw = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "query_dx"
    query_dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "key_dw"
    key_dw = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return value_dw, query_dx, key_dw


# This api is auto-generated from IR ScatterNdUpdate
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def ScatterNdUpdate(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterNdUpdate)\n
.INPUT(var, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::BasicType())\n
.OUTPUT(var, TensorType::BasicType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNdUpdate"
    op.name = next_unique_name(node_name, "ScatterNdUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR TensorScatterUpdate
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def TensorScatterUpdate(x: Tensor, indices: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorScatterUpdate)\n
.INPUT(x, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorScatterUpdate"
    op.name = next_unique_name(node_name, "TensorScatterUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScatterElements
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_NUMBER])
def ScatterElements(data: Tensor, indices: Tensor, updates: Tensor, *, axis: int=0, reduction: str="none", dependencies=[], node_name=None):
    """REG_OP(ScatterElements)\n
.INPUT(data, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(axis, Int, 0)\n
.ATTR(reduction, String, "none")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterElements"
    op.name = next_unique_name(node_name, "ScatterElements")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is generated from IR ScatterElementsV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_NUMBER])
def ScatterElementsV2(data: Tensor, indices: Tensor, updates: Tensor, *, axis: int=0, reduction: str="none", dependencies=[], node_name=None):
    """REG_OP(ScatterElementsV2)\n
.INPUT(data, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(axis, Int, 0)\n
.ATTR(reduction, String, "none")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterElementsV2"
    op.name = next_unique_name(node_name, "ScatterElementsV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScatterNdMax
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def ScatterNdMax(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterNdMax)\n
.INPUT(var, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::BasicType())\n
.OUTPUT(var, TensorType::BasicType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNdMax"
    op.name = next_unique_name(node_name, "ScatterNdMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterAdd
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterAdd(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterAdd)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterAdd"
    op.name = next_unique_name(node_name, "ScatterAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterAddWithAxis
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterAddWithAxis(var: Tensor, indices: Tensor, updates: Tensor, *, axis: int, dependencies=[], node_name=None):
    """REG_OP(ScatterAddWithAxis)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.REQUIRED_ATTR(axis, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterAddWithAxis"
    op.name = next_unique_name(node_name, "ScatterAddWithAxis")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterDiv
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterDiv(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterDiv)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterDiv"
    op.name = next_unique_name(node_name, "ScatterDiv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterNdAdd
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterNdAdd(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterNdAdd)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNdAdd"
    op.name = next_unique_name(node_name, "ScatterNdAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR TensorScatterAdd
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def TensorScatterAdd(x: Tensor, indices: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorScatterAdd)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorScatterAdd"
    op.name = next_unique_name(node_name, "TensorScatterAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScatterNdSub
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterNdSub(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterNdSub)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNdSub"
    op.name = next_unique_name(node_name, "ScatterNdSub")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterNdMin
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def ScatterNdMin(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterNdMin)\n
.INPUT(var, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::BasicType())\n
.OUTPUT(var, TensorType::BasicType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNdMin"
    op.name = next_unique_name(node_name, "ScatterNdMin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR TensorScatterSub
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def TensorScatterSub(x: Tensor, indices: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorScatterSub)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorScatterSub"
    op.name = next_unique_name(node_name, "TensorScatterSub")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScatterSub
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterSub(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterSub)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterSub"
    op.name = next_unique_name(node_name, "ScatterSub")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR DiagPartD
@auto_convert_to_tensor([False, False], [False, False])
def DiagPartD(x: Tensor, assist: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DiagPartD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(assist, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DiagPartD"
    op.name = next_unique_name(node_name, "DiagPartD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DiagPart
@auto_convert_to_tensor([False], [False])
def DiagPart(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DiagPart)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DiagPart"
    op.name = next_unique_name(node_name, "DiagPart")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FullyConnection
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def FullyConnection(x: Tensor, w: Tensor, b: Optional[Tensor], offset_w: Optional[Tensor], *, num_output: int, transpose: bool=False, axis: int=1, offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(FullyConnection)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_INT8, DT_INT4, DT_FLOAT, DT_BF16}))\n
.INPUT(w, TensorType({DT_FLOAT16, DT_INT8, DT_INT4, DT_FLOAT, DT_BF16}))\n
.OPTIONAL_INPUT(b, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT, DT_BF16}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8, DT_INT4}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT, DT_BF16}))\n
.REQUIRED_ATTR(num_output, Int)\n
.ATTR(transpose, Bool, false)\n
.ATTR(axis, Int, 1)\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FullyConnection"
    op.name = next_unique_name(node_name, "FullyConnection")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    if b is not None:
        op.input.append(b.tensor)
        op.input_desc.add().CopyFrom(b.desc)
        op.input_desc[-1].name = "b"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "b"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["num_output"].i = num_output
    op.attr["transpose"].b = transpose
    op.attr["axis"].i = axis
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FullyConnectionCompress
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def FullyConnectionCompress(x: Tensor, w: Tensor, comress_index: Tensor, b: Optional[Tensor], offset_w: Optional[Tensor], *, num_output: int, transpose: bool=False, axis: int=1, offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(FullyConnectionCompress)\n
.INPUT(x, TensorType({DT_UINT8, DT_INT8}))\n
.INPUT(w, TensorType({DT_INT8}))\n
.INPUT(comress_index, TensorType({DT_INT8}))\n
.OPTIONAL_INPUT(b, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(num_output, Int)\n
.ATTR(transpose, Bool, false)\n
.ATTR(axis, Int, 1)\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FullyConnectionCompress"
    op.name = next_unique_name(node_name, "FullyConnectionCompress")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(comress_index.tensor)
    op.input_desc.add().CopyFrom(comress_index.desc)
    op.input_desc[-1].name = "comress_index"
    if b is not None:
        op.input.append(b.tensor)
        op.input_desc.add().CopyFrom(b.desc)
        op.input_desc[-1].name = "b"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "b"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["num_output"].i = num_output
    op.attr["transpose"].b = transpose
    op.attr["axis"].i = axis
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConfusionMatrix
@auto_convert_to_tensor([False, False, False], [False, False, True])
def ConfusionMatrix(labels: Tensor, predictions: Tensor, weights: Optional[Tensor], *, num_classes: int, dtype: str, dependencies=[], node_name=None):
    """REG_OP(ConfusionMatrix)\n
.INPUT(labels, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16, DT_INT8, DT_UINT8}))\n
.INPUT(predictions, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16, DT_INT8, DT_UINT8}))\n
.OPTIONAL_INPUT(weights, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16, DT_INT8, DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16, DT_INT8, DT_UINT8}))\n
.REQUIRED_ATTR(num_classes, Int)\n
.REQUIRED_ATTR(dtype, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConfusionMatrix"
    op.name = next_unique_name(node_name, "ConfusionMatrix")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"
    op.input.append(predictions.tensor)
    op.input_desc.add().CopyFrom(predictions.desc)
    op.input_desc[-1].name = "predictions"
    if weights is not None:
        op.input.append(weights.tensor)
        op.input_desc.add().CopyFrom(weights.desc)
        op.input_desc[-1].name = "weights"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["num_classes"].i = num_classes
    op.attr["dtype"].s = compat_as_bytes(dtype)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Scatter
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def Scatter(var: Tensor, indices: Tensor, updates: Tensor, *, reduce: str, axis: int=0, dependencies=[], node_name=None):
    """REG_OP(Scatter)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.REQUIRED_ATTR(reduce, String)\n
.ATTR(axis, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Scatter"
    op.name = next_unique_name(node_name, "Scatter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["reduce"].s = compat_as_bytes(reduce)
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR QuantUpdateScatter
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, True])
def QuantUpdateScatter(var: Tensor,
                       indices: Tensor,
                       updates: Tensor,
                       quant_scales: Tensor,
                       quant_zero_points: Optional[Tensor],
                       *,
                       reduce: str,
                       axis: int = 0,
                       quant_axis: int = 1,
                       dependencies=[],
                       node_name=None):
    """REG_OP(QuantUpdateScatter)\n
.INPUT(var, TensorType({DT_INT32, DT_INT8, DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(quant_scales, TensorType({DT_FLOAT, DT_BF16}))\n
.OPTIONAL_INPUT(quant_zero_points, TensorType({DT_INT8, DT_UINT8, DT_INT32, DT_BF16}))\n
.OUTPUT(var, TensorType({DT_INT32, DT_INT8, DT_UINT8}))\n
.REQUIRED_ATTR(reduce, String)\n
.ATTR(axis, Int, 0)\n
.ATTR(quant_axis, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QuantUpdateScatter"
    op.name = next_unique_name(node_name, "QuantUpdateScatter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"
    op.input.append(quant_scales.tensor)
    op.input_desc.add().CopyFrom(quant_scales.desc)
    op.input_desc[-1].name = "quant_scales"
    if quant_zero_points is not None:
        op.input.append(quant_zero_points.tensor)
        op.input_desc.add().CopyFrom(quant_zero_points.desc)
        op.input_desc[-1].name = "quant_zero_points"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_zero_points"

    # process attrs
    op.attr["reduce"].s = compat_as_bytes(reduce)
    op.attr["axis"].i = axis
    op.attr["quant_axis"].i = quant_axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    return var


# This api is auto-generated from IR ScatterMul
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterMul(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterMul)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterMul"
    op.name = next_unique_name(node_name, "ScatterMul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterMin
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterMin(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterMin)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterMin"
    op.name = next_unique_name(node_name, "ScatterMin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterMax
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterMax(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterMax)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterMax"
    op.name = next_unique_name(node_name, "ScatterMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ScatterMaxWithArgmax
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ScatterMaxWithArgmax(x: Tensor, indices: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ScatterMaxWithArgmax)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(updates, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.OUTPUT(argmax, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterMaxWithArgmax"
    op.name = next_unique_name(node_name, "ScatterMaxWithArgmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, argmax


# This api is auto-generated from IR ScatterUpdate
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterUpdate(var: Tensor, indices: Tensor, updates: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ScatterUpdate)\n
.INPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.OUTPUT(var, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT32,DT_INT8,DT_UINT8}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterUpdate"
    op.name = next_unique_name(node_name, "ScatterUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR MatrixDiagPartV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_BASIC])
def MatrixDiagPartV2(input: Tensor, k: Tensor, padding_value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixDiagPartV2)\n
.INPUT(input, TensorType::BasicType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.INPUT(padding_value, TensorType::BasicType())\n
.OUTPUT(diagonal, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiagPartV2"
    op.name = next_unique_name(node_name, "MatrixDiagPartV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"
    op.input.append(padding_value.tensor)
    op.input_desc.add().CopyFrom(padding_value.desc)
    op.input_desc[-1].name = "padding_value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "diagonal"
    diagonal = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return diagonal


# This api is auto-generated from IR MatrixSetDiagV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_UNKNOWN])
def MatrixSetDiagV2(input: Tensor, diagonal: Tensor, k: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixSetDiagV2)\n
.INPUT(input, TensorType::BasicType())\n
.INPUT(diagonal, TensorType::BasicType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.OUTPUT(output, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixSetDiagV2"
    op.name = next_unique_name(node_name, "MatrixSetDiagV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(diagonal.tensor)
    op.input_desc.add().CopyFrom(diagonal.desc)
    op.input_desc[-1].name = "diagonal"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR MatrixSetDiagV3
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_UNKNOWN])
def MatrixSetDiagV3(input: Tensor, diagonal: Tensor, k: Tensor, *, align: str="RIGHT_LEFT", dependencies=[], node_name=None):
    """REG_OP(MatrixSetDiagV3)\n
.INPUT(input, TensorType::BasicType())\n
.INPUT(diagonal, TensorType::BasicType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.OUTPUT(output, TensorType::BasicType())\n
.ATTR(align, String, "RIGHT_LEFT")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixSetDiagV3"
    op.name = next_unique_name(node_name, "MatrixSetDiagV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(diagonal.tensor)
    op.input_desc.add().CopyFrom(diagonal.desc)
    op.input_desc[-1].name = "diagonal"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"

    # process attrs
    op.attr["align"].s = compat_as_bytes(align)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR MatrixDiagV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_BASIC])
def MatrixDiagV2(diagonal: Tensor, k: Tensor, num_rows: Tensor, num_cols: Tensor, padding_value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MatrixDiagV2)\n
.INPUT(diagonal, TensorType::BasicType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.INPUT(num_rows, TensorType({DT_INT32}))\n
.INPUT(num_cols, TensorType({DT_INT32}))\n
.INPUT(padding_value, TensorType::BasicType())\n
.OUTPUT(output, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiagV2"
    op.name = next_unique_name(node_name, "MatrixDiagV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(diagonal.tensor)
    op.input_desc.add().CopyFrom(diagonal.desc)
    op.input_desc[-1].name = "diagonal"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"
    op.input.append(num_rows.tensor)
    op.input_desc.add().CopyFrom(num_rows.desc)
    op.input_desc[-1].name = "num_rows"
    op.input.append(num_cols.tensor)
    op.input_desc.add().CopyFrom(num_cols.desc)
    op.input_desc[-1].name = "num_cols"
    op.input.append(padding_value.tensor)
    op.input_desc.add().CopyFrom(padding_value.desc)
    op.input_desc[-1].name = "padding_value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR IndexAdd
@auto_convert_to_tensor([False, False, False], [False, False, False])
def IndexAdd(var: Tensor, indices: Tensor, updates: Tensor, *, axis: int=0, dependencies=[], node_name=None):
    """REG_OP(IndexAdd)\n
.INPUT(var, TensorType({DT_INT32, DT_INT8, DT_UINT8, DT_FLOAT32, DT_FLOAT16}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(updates, TensorType({DT_INT32, DT_INT8, DT_UINT8, DT_FLOAT32, DT_FLOAT16}))\n
.OUTPUT(var_out, TensorType({DT_INT32, DT_INT8, DT_UINT8, DT_FLOAT32, DT_FLOAT16}))\n
.ATTR(axis, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IndexAdd"
    op.name = next_unique_name(node_name, "IndexAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var_out"
    var_out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var_out


# This api is auto-generated from IR IndexPut
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC])
def IndexPut(x1: Tensor, x2: Tensor, *, indices: List[int], accumulate: int=0, dependencies=[], node_name=None):
    """REG_OP(IndexPut)\n
.INPUT(x1, TensorType::BasicType())\n
.INPUT(x2, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(indices, ListInt)\n
.ATTR(accumulate, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IndexPut"
    op.name = next_unique_name(node_name, "IndexPut")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["indices"].list.val_type = 2
    op.attr["indices"].list.i.extend(indices)
    op.attr["accumulate"].i = accumulate

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Triu
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def Triu(x: Tensor, *, diagonal: int=0, dependencies=[], node_name=None):
    """REG_OP(Triu)\n
.INPUT(x, TensorType::BasicType())\n
.ATTR(diagonal, Int, 0)\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Triu"
    op.name = next_unique_name(node_name, "Triu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["diagonal"].i = diagonal

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Tril
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def Tril(x: Tensor, *, diagonal: int=0, dependencies=[], node_name=None):
    """REG_OP(Tril)\n
.INPUT(x, TensorType::BasicType())\n
.ATTR(diagonal, Int, 0)\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Tril"
    op.name = next_unique_name(node_name, "Tril")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["diagonal"].i = diagonal

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Einsum
@auto_convert_to_tensor([True], [False])
def Einsum(x: List[Tensor], *, equation: str, N: int, dependencies=[], node_name=None):
    """REG_OP(Einsum)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.REQUIRED_ATTR(equation, String)\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Einsum"
    op.name = next_unique_name(node_name, "Einsum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["equation"].s = compat_as_bytes(equation)
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Eye
@auto_convert_to_tensor([], [])
def Eye(*, num_rows: int, num_columns: int=0, batch_shape: List[int]=[], dtype: int=0, dependencies=[], node_name=None):
    """REG_OP(Eye)\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(num_rows, Int)\n
.ATTR(num_columns, Int, 0)\n
.ATTR(batch_shape, ListInt, {})\n
.ATTR(dtype, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Eye"
    op.name = next_unique_name(node_name, "Eye")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["num_rows"].i = num_rows
    op.attr["num_columns"].i = num_columns
    op.attr["batch_shape"].list.val_type = 2
    op.attr["batch_shape"].list.i.extend(batch_shape)
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FillDiagonal
@auto_convert_to_tensor([False], [False])
def FillDiagonal(x: Tensor, *, fill_value: float, wrap: bool=False, dependencies=[], node_name=None):
    """REG_OP(FillDiagonal)\n
.INPUT(x, TensorType({BasicType(), DT_BOOL}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL}))\n
.REQUIRED_ATTR(fill_value, Float)\n
.ATTR(wrap, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FillDiagonal"
    op.name = next_unique_name(node_name, "FillDiagonal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["fill_value"].f = fill_value
    op.attr["wrap"].b = wrap

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Trace
@auto_convert_to_tensor([False], [False])
def Trace(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Trace)\n
.INPUT(x, TensorType({DT_COMPLEX128, DT_COMPLEX64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_COMPLEX128, DT_COMPLEX64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT64, DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Trace"
    op.name = next_unique_name(node_name, "Trace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Pinverse
@auto_convert_to_tensor([False], [False])
def Pinverse(x: Tensor, *, rcond: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Pinverse)\n
.INPUT(x, TensorType({ DT_FLOAT, DT_DOUBLE }))\n
.OUTPUT(y, TensorType({ DT_FLOAT, DT_DOUBLE }))\n
.ATTR(rcond, Float, 1e-15)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Pinverse"
    op.name = next_unique_name(node_name, "Pinverse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["rcond"].f = rcond

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TensorScatterMax
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def TensorScatterMax(input: Tensor, indices: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorScatterMax)\n
.INPUT(input, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::BasicType())\n
.OUTPUT(output, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorScatterMax"
    op.name = next_unique_name(node_name, "TensorScatterMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR TensorScatterMin
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def TensorScatterMin(input: Tensor, indices: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(TensorScatterMin)\n
.INPUT(input, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::BasicType())\n
.OUTPUT(output, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TensorScatterMin"
    op.name = next_unique_name(node_name, "TensorScatterMin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR MatrixDiagPartV3
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_BASIC])
def MatrixDiagPartV3(x: Tensor, k: Tensor, padding_value: Tensor, *, align: str="RIGHT_LEFT", dependencies=[], node_name=None):
    """REG_OP(MatrixDiagPartV3)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.INPUT(padding_value, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(align, String, "RIGHT_LEFT")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiagPartV3"
    op.name = next_unique_name(node_name, "MatrixDiagPartV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"
    op.input.append(padding_value.tensor)
    op.input_desc.add().CopyFrom(padding_value.desc)
    op.input_desc[-1].name = "padding_value"

    # process attrs
    op.attr["align"].s = compat_as_bytes(align)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatrixDiagV3
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def MatrixDiagV3(x: Tensor, k: Tensor, num_rows: Tensor, num_cols: Tensor, padding_value: Tensor, *, align: str="RIGHT_LEFT", dependencies=[], node_name=None):
    """REG_OP(MatrixDiagV3)\n
.INPUT(x, TensorType({BasicType(), DT_BOOL}))\n
.INPUT(k, TensorType({DT_INT32}))\n
.INPUT(num_rows, TensorType({DT_INT32}))\n
.INPUT(num_cols, TensorType({DT_INT32}))\n
.INPUT(padding_value, TensorType({BasicType(), DT_BOOL}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL}))\n
.ATTR(align, String, "RIGHT_LEFT")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MatrixDiagV3"
    op.name = next_unique_name(node_name, "MatrixDiagV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"
    op.input.append(num_rows.tensor)
    op.input_desc.add().CopyFrom(num_rows.desc)
    op.input_desc[-1].name = "num_rows"
    op.input.append(num_cols.tensor)
    op.input_desc.add().CopyFrom(num_cols.desc)
    op.input_desc[-1].name = "num_cols"
    op.input.append(padding_value.tensor)
    op.input_desc.add().CopyFrom(padding_value.desc)
    op.input_desc[-1].name = "padding_value"

    # process attrs
    op.attr["align"].s = compat_as_bytes(align)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SwinAttentionScore
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, True, True, False, True])
def SwinAttentionScore(query: Tensor, key: Tensor, value: Tensor, padding_mask1: Optional[Tensor], padding_mask2: Optional[Tensor], scale: Tensor, drop_mask: Optional[Tensor], *, keep_prob: float=1.000000, query_transpose: bool=False, key_transpose: bool=False, bmm_score_transpose_a: bool=False, bmm_score_transpose_b: bool=False, softmax_axes: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(SwinAttentionScore)\n
.INPUT(query, TensorType({DT_FLOAT16}))\n
.INPUT(key, TensorType({DT_FLOAT16}))\n
.INPUT(value, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(padding_mask1, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(padding_mask2, TensorType({DT_FLOAT16}))\n
.INPUT(scale, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(drop_mask, TensorType({DT_INT8}))\n
.OUTPUT(attention_score, TensorType({DT_FLOAT16}))\n
.OUTPUT(softmax, TensorType({DT_FLOAT16}))\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(query_transpose, Bool, false)\n
.ATTR(key_transpose, Bool, false)\n
.ATTR(bmm_score_transpose_a, Bool, false)\n
.ATTR(bmm_score_transpose_b, Bool, false)\n
.ATTR(softmax_axes, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SwinAttentionScore"
    op.name = next_unique_name(node_name, "SwinAttentionScore")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    if padding_mask1 is not None:
        op.input.append(padding_mask1.tensor)
        op.input_desc.add().CopyFrom(padding_mask1.desc)
        op.input_desc[-1].name = "padding_mask1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "padding_mask1"
    if padding_mask2 is not None:
        op.input.append(padding_mask2.tensor)
        op.input_desc.add().CopyFrom(padding_mask2.desc)
        op.input_desc[-1].name = "padding_mask2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "padding_mask2"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if drop_mask is not None:
        op.input.append(drop_mask.tensor)
        op.input_desc.add().CopyFrom(drop_mask.desc)
        op.input_desc[-1].name = "drop_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "drop_mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob
    op.attr["query_transpose"].b = query_transpose
    op.attr["key_transpose"].b = key_transpose
    op.attr["bmm_score_transpose_a"].b = bmm_score_transpose_a
    op.attr["bmm_score_transpose_b"].b = bmm_score_transpose_b
    op.attr["softmax_axes"].list.val_type = 2
    op.attr["softmax_axes"].list.i.extend(softmax_axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "attention_score"
    attention_score = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "softmax"
    softmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return attention_score, softmax


# This api is auto-generated from IR SwinAttentionFFN
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SwinAttentionFFN(x1: Tensor, x2: Tensor, bias: Tensor, *, shifts: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(SwinAttentionFFN)\n
.INPUT(x1, TensorType({DT_FLOAT16}))\n
.INPUT(x2, TensorType({DT_FLOAT16}))\n
.INPUT(bias, TensorType({DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.ATTR(shifts, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SwinAttentionFFN"
    op.name = next_unique_name(node_name, "SwinAttentionFFN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["shifts"].list.val_type = 2
    op.attr["shifts"].list.i.extend(shifts)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR L2Normalize
@auto_convert_to_tensor([False], [False])
def L2Normalize(x: Tensor, *, axis: List[int]=[], eps: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(L2Normalize)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(axis, ListInt, {})\n
.ATTR(eps, Float, 1e-4)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "L2Normalize"
    op.name = next_unique_name(node_name, "L2Normalize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR L2NormalizeGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def L2NormalizeGrad(x: Tensor, y: Tensor, dy: Tensor, *, dim: List[int]=[], eps: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(L2NormalizeGrad)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(dx, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(dim, ListInt, {})\n
.ATTR(eps, Float, 0.0001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "L2NormalizeGrad"
    op.name = next_unique_name(node_name, "L2NormalizeGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs
    op.attr["dim"].list.val_type = 2
    op.attr["dim"].list.i.extend(dim)
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx


# This api is auto-generated from IR BatchNorm
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def BatchNorm(x: Tensor, scale: Tensor, offset: Tensor, mean: Optional[Tensor], variance: Optional[Tensor], *, epsilon: float=0.000100, data_format: str="NHWC", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(BatchNorm)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_1, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_2, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_3, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchNorm"
    op.name = next_unique_name(node_name, "BatchNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_1"
    reserve_space_1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_2"
    reserve_space_2 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_3"
    reserve_space_3 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance, reserve_space_1, reserve_space_2, reserve_space_3


# This api is auto-generated from IR SyncBatchNormGatherStatsWithCounts
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def SyncBatchNormGatherStatsWithCounts(mean_all: Tensor, invert_std_all: Tensor, count_all: Tensor, mean_broadcast: Tensor, count_sum: Tensor, running_var: Tensor, *, momentum: float=0.100000, epsilon: float=0.001000, dependencies=[], node_name=None):
    """REG_OP(SyncBatchNormGatherStatsWithCounts)\n
.INPUT(mean_all, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(invert_std_all, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(count_all, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mean_broadcast, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(count_sum, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(running_var, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(invert_std, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(running_var_update, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(momentum, Float, 0.1)\n
.ATTR(epsilon, Float, 0.001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SyncBatchNormGatherStatsWithCounts"
    op.name = next_unique_name(node_name, "SyncBatchNormGatherStatsWithCounts")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(mean_all.tensor)
    op.input_desc.add().CopyFrom(mean_all.desc)
    op.input_desc[-1].name = "mean_all"
    op.input.append(invert_std_all.tensor)
    op.input_desc.add().CopyFrom(invert_std_all.desc)
    op.input_desc[-1].name = "invert_std_all"
    op.input.append(count_all.tensor)
    op.input_desc.add().CopyFrom(count_all.desc)
    op.input_desc[-1].name = "count_all"
    op.input.append(mean_broadcast.tensor)
    op.input_desc.add().CopyFrom(mean_broadcast.desc)
    op.input_desc[-1].name = "mean_broadcast"
    op.input.append(count_sum.tensor)
    op.input_desc.add().CopyFrom(count_sum.desc)
    op.input_desc[-1].name = "count_sum"
    op.input.append(running_var.tensor)
    op.input_desc.add().CopyFrom(running_var.desc)
    op.input_desc[-1].name = "running_var"

    # process attrs
    op.attr["momentum"].f = momentum
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "invert_std"
    invert_std = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "running_var_update"
    running_var_update = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return invert_std, running_var_update


# This api is auto-generated from IR SyncBNTrainingUpdate
@auto_convert_to_tensor([False, False], [False, False])
def SyncBNTrainingUpdate(mean: Tensor, running_mean: Tensor, *, momentum: float=0.100000, dependencies=[], node_name=None):
    """REG_OP(SyncBNTrainingUpdate)\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(running_mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(running_mean_update, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(momentum, Float, 0.1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SyncBNTrainingUpdate"
    op.name = next_unique_name(node_name, "SyncBNTrainingUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(running_mean.tensor)
    op.input_desc.add().CopyFrom(running_mean.desc)
    op.input_desc[-1].name = "running_mean"

    # process attrs
    op.attr["momentum"].f = momentum

    # process outputs
    output_index = 0
    op.output_desc.add().name = "running_mean_update"
    running_mean_update = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return running_mean_update


# This api is auto-generated from IR SyncBatchNormBackwardReduce
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SyncBatchNormBackwardReduce(sum_dy: Tensor, sum_dy_dx_pad: Tensor, mean: Tensor, invert_std: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SyncBatchNormBackwardReduce)\n
.INPUT(sum_dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(sum_dy_dx_pad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(invert_std, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(sum_dy_xmu, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SyncBatchNormBackwardReduce"
    op.name = next_unique_name(node_name, "SyncBatchNormBackwardReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(sum_dy.tensor)
    op.input_desc.add().CopyFrom(sum_dy.desc)
    op.input_desc[-1].name = "sum_dy"
    op.input.append(sum_dy_dx_pad.tensor)
    op.input_desc.add().CopyFrom(sum_dy_dx_pad.desc)
    op.input_desc[-1].name = "sum_dy_dx_pad"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(invert_std.tensor)
    op.input_desc.add().CopyFrom(invert_std.desc)
    op.input_desc[-1].name = "invert_std"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sum_dy_xmu"
    sum_dy_xmu = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sum_dy_xmu, y


# This api is auto-generated from IR SyncBatchNormBackwardElemt
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def SyncBatchNormBackwardElemt(grad_output: Tensor, save_input: Tensor, mean: Tensor, invstd: Tensor, weight: Tensor, mean_dy: Tensor, mean_dy_xmu: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SyncBatchNormBackwardElemt)\n
.INPUT(grad_output, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.INPUT(save_input, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.INPUT(invstd, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.INPUT(weight, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.INPUT(mean_dy, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.INPUT(mean_dy_xmu, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.OUTPUT(grad_input, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SyncBatchNormBackwardElemt"
    op.name = next_unique_name(node_name, "SyncBatchNormBackwardElemt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad_output.tensor)
    op.input_desc.add().CopyFrom(grad_output.desc)
    op.input_desc[-1].name = "grad_output"
    op.input.append(save_input.tensor)
    op.input_desc.add().CopyFrom(save_input.desc)
    op.input_desc[-1].name = "save_input"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(invstd.tensor)
    op.input_desc.add().CopyFrom(invstd.desc)
    op.input_desc[-1].name = "invstd"
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"
    op.input.append(mean_dy.tensor)
    op.input_desc.add().CopyFrom(mean_dy.desc)
    op.input_desc[-1].name = "mean_dy"
    op.input.append(mean_dy_xmu.tensor)
    op.input_desc.add().CopyFrom(mean_dy_xmu.desc)
    op.input_desc[-1].name = "mean_dy_xmu"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_input"
    grad_input = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_input


# This api is auto-generated from IR SyncBatchNormGatherStats
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def SyncBatchNormGatherStats(total_sum: Tensor, total_square_sum: Tensor, sample_count: Tensor, mean: Tensor, variance: Tensor, *, momentum: float=0.100000, eps: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(SyncBatchNormGatherStats)\n
.INPUT(total_sum, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(total_square_sum, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(sample_count, TensorType({DT_INT32}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(batch_invstd, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(momentum, Float, 0.1)\n
.ATTR(eps, Float, 0.00001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SyncBatchNormGatherStats"
    op.name = next_unique_name(node_name, "SyncBatchNormGatherStats")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(total_sum.tensor)
    op.input_desc.add().CopyFrom(total_sum.desc)
    op.input_desc[-1].name = "total_sum"
    op.input.append(total_square_sum.tensor)
    op.input_desc.add().CopyFrom(total_square_sum.desc)
    op.input_desc[-1].name = "total_square_sum"
    op.input.append(sample_count.tensor)
    op.input_desc.add().CopyFrom(sample_count.desc)
    op.input_desc[-1].name = "sample_count"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["momentum"].f = momentum
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_invstd"
    batch_invstd = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return batch_mean, batch_invstd, mean, variance


# This api is auto-generated from IR BatchNorm3D
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def BatchNorm3D(x: Tensor, scale: Tensor, offset: Tensor, mean: Optional[Tensor], variance: Optional[Tensor], *, epsilon: float=0.000100, data_format: str="NCDHW", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(BatchNorm3D)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_1, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_2, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.ATTR(data_format, String, "NCDHW")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchNorm3D"
    op.name = next_unique_name(node_name, "BatchNorm3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_1"
    reserve_space_1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_2"
    reserve_space_2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance, reserve_space_1, reserve_space_2


# This api is auto-generated from IR BatchNormExt2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def BatchNormExt2(input_x: Tensor, input_scale: Tensor, input_offset: Tensor, input_mean: Optional[Tensor], input_variance: Optional[Tensor], *, epsilon: float=0.000100, data_format: str="NHWC", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(BatchNormExt2)\n
.INPUT(input_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_scale, TensorType({DT_FLOAT}))\n
.INPUT(input_offset, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(input_mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(input_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(output_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(output_reserve_space_1, TensorType({DT_FLOAT}))\n
.OUTPUT(output_reserve_space_2, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchNormExt2"
    op.name = next_unique_name(node_name, "BatchNormExt2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"
    op.input.append(input_scale.tensor)
    op.input_desc.add().CopyFrom(input_scale.desc)
    op.input_desc[-1].name = "input_scale"
    op.input.append(input_offset.tensor)
    op.input_desc.add().CopyFrom(input_offset.desc)
    op.input_desc[-1].name = "input_offset"
    if input_mean is not None:
        op.input.append(input_mean.tensor)
        op.input_desc.add().CopyFrom(input_mean.desc)
        op.input_desc[-1].name = "input_mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "input_mean"
    if input_variance is not None:
        op.input.append(input_variance.tensor)
        op.input_desc.add().CopyFrom(input_variance.desc)
        op.input_desc[-1].name = "input_variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "input_variance"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_mean"
    output_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_variance"
    output_variance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_reserve_space_1"
    output_reserve_space_1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_reserve_space_2"
    output_reserve_space_2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y, output_mean, output_variance, output_reserve_space_1, output_reserve_space_2


# This api is auto-generated from IR BatchNormGrad
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, True])
def BatchNormGrad(y_backprop: Tensor, x: Tensor, scale: Tensor, reserve_space_1: Tensor, reserve_space_2: Tensor, reserve_space_3: Optional[Tensor], *, epsilon: float=0.000100, data_format: str="NHWC", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(BatchNormGrad)\n
.INPUT(y_backprop, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(reserve_space_1, TensorType({DT_FLOAT}))\n
.INPUT(reserve_space_2, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(reserve_space_3, TensorType({DT_FLOAT}))\n
.OUTPUT(x_backprop, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(scale_backprop, TensorType({DT_FLOAT}))\n
.OUTPUT(offset_backprop, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_4, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_5, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchNormGrad"
    op.name = next_unique_name(node_name, "BatchNormGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_backprop.tensor)
    op.input_desc.add().CopyFrom(y_backprop.desc)
    op.input_desc[-1].name = "y_backprop"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(reserve_space_1.tensor)
    op.input_desc.add().CopyFrom(reserve_space_1.desc)
    op.input_desc[-1].name = "reserve_space_1"
    op.input.append(reserve_space_2.tensor)
    op.input_desc.add().CopyFrom(reserve_space_2.desc)
    op.input_desc[-1].name = "reserve_space_2"
    if reserve_space_3 is not None:
        op.input.append(reserve_space_3.tensor)
        op.input_desc.add().CopyFrom(reserve_space_3.desc)
        op.input_desc[-1].name = "reserve_space_3"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "reserve_space_3"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_backprop"
    x_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "scale_backprop"
    scale_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "offset_backprop"
    offset_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_4"
    reserve_space_4 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_5"
    reserve_space_5 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_backprop, scale_backprop, offset_backprop, reserve_space_4, reserve_space_5


# This api is auto-generated from IR BatchNorm3DGrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def BatchNorm3DGrad(y_backprop: Tensor, x: Tensor, scale: Tensor, reserve_space_1: Tensor, reserve_space_2: Tensor, *, epsilon: float=0.000100, data_format: str="NCDHW", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(BatchNorm3DGrad)\n
.INPUT(y_backprop, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(reserve_space_1, TensorType({DT_FLOAT}))\n
.INPUT(reserve_space_2, TensorType({DT_FLOAT}))\n
.OUTPUT(x_backprop, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(scale_backprop, TensorType({DT_FLOAT}))\n
.OUTPUT(offset_backprop, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_4, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_5, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.ATTR(data_format, String, "NCDHW")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchNorm3DGrad"
    op.name = next_unique_name(node_name, "BatchNorm3DGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_backprop.tensor)
    op.input_desc.add().CopyFrom(y_backprop.desc)
    op.input_desc[-1].name = "y_backprop"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(reserve_space_1.tensor)
    op.input_desc.add().CopyFrom(reserve_space_1.desc)
    op.input_desc[-1].name = "reserve_space_1"
    op.input.append(reserve_space_2.tensor)
    op.input_desc.add().CopyFrom(reserve_space_2.desc)
    op.input_desc[-1].name = "reserve_space_2"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_backprop"
    x_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "scale_backprop"
    scale_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "offset_backprop"
    offset_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_4"
    reserve_space_4 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_5"
    reserve_space_5 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_backprop, scale_backprop, offset_backprop, reserve_space_4, reserve_space_5


# This api is auto-generated from IR BatchNormGradExt2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def BatchNormGradExt2(y_backprop: Tensor, x: Tensor, scale: Tensor, reserve_space_1: Tensor, reserve_space_2: Tensor, *, epsilon: float=0.000100, data_format: str="NHWC", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(BatchNormGradExt2)\n
.INPUT(y_backprop, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(reserve_space_1, TensorType({DT_FLOAT}))\n
.INPUT(reserve_space_2, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(is_training, Bool, true)\n
.OUTPUT(x_backprop, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(scale_backprop, TensorType({DT_FLOAT}))\n
.OUTPUT(offset_backprop, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_3, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_4, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchNormGradExt2"
    op.name = next_unique_name(node_name, "BatchNormGradExt2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_backprop.tensor)
    op.input_desc.add().CopyFrom(y_backprop.desc)
    op.input_desc[-1].name = "y_backprop"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(reserve_space_1.tensor)
    op.input_desc.add().CopyFrom(reserve_space_1.desc)
    op.input_desc[-1].name = "reserve_space_1"
    op.input.append(reserve_space_2.tensor)
    op.input_desc.add().CopyFrom(reserve_space_2.desc)
    op.input_desc[-1].name = "reserve_space_2"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_backprop"
    x_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "scale_backprop"
    scale_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "offset_backprop"
    offset_backprop = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_3"
    reserve_space_3 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_4"
    reserve_space_4 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_backprop, scale_backprop, offset_backprop, reserve_space_3, reserve_space_4


# This api is auto-generated from IR BNInference
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, True, True])
def BNInference(x: Tensor, mean: Tensor, variance: Tensor, momentum: Tensor, scale: Optional[Tensor], offset: Optional[Tensor], *, epsilon: float=0.000010, use_global_stats: bool=True, mode: int=1, dependencies=[], node_name=None):
    """REG_OP(BNInference)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(momentum, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OPTIONAL_INPUT(scale, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OPTIONAL_INPUT(offset, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(epsilon, Float, 1e-5f)\n
.ATTR(use_global_stats, Bool, true)\n
.ATTR(mode, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNInference"
    op.name = next_unique_name(node_name, "BNInference")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    if scale is not None:
        op.input.append(scale.tensor)
        op.input_desc.add().CopyFrom(scale.desc)
        op.input_desc[-1].name = "scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["use_global_stats"].b = use_global_stats
    op.attr["mode"].i = mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BNInferenceD
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def BNInferenceD(x: Tensor, mean: Tensor, variance: Tensor, scale: Optional[Tensor], b: Optional[Tensor], *, momentum: float=0.900000, epsilon: float=0.000010, use_global_stats: bool=True, mode: int=1, dependencies=[], node_name=None):
    """REG_OP(BNInferenceD)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OPTIONAL_INPUT(scale, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OPTIONAL_INPUT(b, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(momentum, Float, 0.9)\n
.ATTR(epsilon, Float, 1e-5f)\n
.ATTR(use_global_stats, Bool, true)\n
.ATTR(mode, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNInferenceD"
    op.name = next_unique_name(node_name, "BNInferenceD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    if scale is not None:
        op.input.append(scale.tensor)
        op.input_desc.add().CopyFrom(scale.desc)
        op.input_desc[-1].name = "scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scale"
    if b is not None:
        op.input.append(b.tensor)
        op.input_desc.add().CopyFrom(b.desc)
        op.input_desc[-1].name = "b"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "b"

    # process attrs
    op.attr["momentum"].f = momentum
    op.attr["epsilon"].f = epsilon
    op.attr["use_global_stats"].b = use_global_stats
    op.attr["mode"].i = mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DepthwiseConv2DBackpropFilter
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def DepthwiseConv2DBackpropFilter(input: Tensor, filter_size: Tensor, out_backprop: Tensor, *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(DepthwiseConv2DBackpropFilter)\n
.INPUT(input, TensorType({float16}))\n
.INPUT(filter_size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(out_backprop, TensorType({float16}))\n
.OUTPUT(filter_grad, TensorType({float32}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthwiseConv2DBackpropFilter"
    op.name = next_unique_name(node_name, "DepthwiseConv2DBackpropFilter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(filter_size.tensor)
    op.input_desc.add().CopyFrom(filter_size.desc)
    op.input_desc[-1].name = "filter_size"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "filter_grad"
    filter_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return filter_grad


# This api is auto-generated from IR DepthwiseConv2DBackpropFilterD
@auto_convert_to_tensor([False, False], [False, False])
def DepthwiseConv2DBackpropFilterD(input: Tensor, out_backprop: Tensor, *, filter_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(DepthwiseConv2DBackpropFilterD)\n
.INPUT(input, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(filter_grad, TensorType({DT_FLOAT32}))\n
.REQUIRED_ATTR(filter_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthwiseConv2DBackpropFilterD"
    op.name = next_unique_name(node_name, "DepthwiseConv2DBackpropFilterD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["filter_size"].list.val_type = 2
    op.attr["filter_size"].list.i.extend(filter_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "filter_grad"
    filter_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return filter_grad


# This api is auto-generated from IR DepthwiseConv2DBackpropInput
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def DepthwiseConv2DBackpropInput(input_size: Tensor, filter: Tensor, out_backprop: Tensor, *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(DepthwiseConv2DBackpropInput)\n
.INPUT(input_size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(filter, TensorType({DT_FLOAT16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16}))\n
.OUTPUT(input_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthwiseConv2DBackpropInput"
    op.name = next_unique_name(node_name, "DepthwiseConv2DBackpropInput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_size.tensor)
    op.input_desc.add().CopyFrom(input_size.desc)
    op.input_desc[-1].name = "input_size"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "input_grad"
    input_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return input_grad


# This api is auto-generated from IR DepthwiseConv2DBackpropInputD
@auto_convert_to_tensor([False, False], [False, False])
def DepthwiseConv2DBackpropInputD(filter: Tensor, out_backprop: Tensor, *, input_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(DepthwiseConv2DBackpropInputD)\n
.INPUT(filter, TensorType({DT_FLOAT16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16}))\n
.OUTPUT(input_grad, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthwiseConv2DBackpropInputD"
    op.name = next_unique_name(node_name, "DepthwiseConv2DBackpropInputD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "input_grad"
    input_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return input_grad


# This api is auto-generated from IR DepthwiseConv2D
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def DepthwiseConv2D(x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], data_format: str="NHWC", offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(DepthwiseConv2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_INT8, DT_INT4}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_INT8, DT_INT4}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_FLOAT16, DT_INT8, DT_INT4}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthwiseConv2D"
    op.name = next_unique_name(node_name, "DepthwiseConv2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BiasAddGrad
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_NUMBER])
def BiasAddGrad(x: Tensor, *, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(BiasAddGrad)\n
.INPUT(x, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BiasAddGrad"
    op.name = next_unique_name(node_name, "BiasAddGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2DBackpropInput
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Conv2DBackpropInput(input_size: Tensor, filter: Tensor, out_backprop: Tensor, *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(Conv2DBackpropInput)\n
.INPUT(input_size, TensorType({DT_INT32}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DBackpropInput"
    op.name = next_unique_name(node_name, "Conv2DBackpropInput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_size.tensor)
    op.input_desc.add().CopyFrom(input_size.desc)
    op.input_desc[-1].name = "input_size"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2DBackpropInputD
@auto_convert_to_tensor([False, False], [False, False])
def Conv2DBackpropInputD(filter: Tensor, out_backprop: Tensor, *, input_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(Conv2DBackpropInputD)\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_INT8, DT_BF16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_INT8, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT32, DT_BF16}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DBackpropInputD"
    op.name = next_unique_name(node_name, "Conv2DBackpropInputD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Deconvolution
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def Deconvolution(x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, strides: List[int]=[1, 1], pads: List[int]=[0, 0, 0, 0], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NCHW", offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(Deconvolution)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_INT8}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_INT8}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_INT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_INT32}))\n
.ATTR(strides, ListInt, {1, 1})\n
.ATTR(pads, ListInt, {0, 0, 0, 0})\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Deconvolution"
    op.name = next_unique_name(node_name, "Deconvolution")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2DBackpropFilter
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Conv2DBackpropFilter(x: Tensor, filter_size: Tensor, out_backprop: Tensor, *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(Conv2DBackpropFilter)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(filter_size, TensorType({DT_INT32}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DBackpropFilter"
    op.name = next_unique_name(node_name, "Conv2DBackpropFilter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter_size.tensor)
    op.input_desc.add().CopyFrom(filter_size.desc)
    op.input_desc[-1].name = "filter_size"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2DBackpropFilterD
@auto_convert_to_tensor([False, False], [False, False])
def Conv2DBackpropFilterD(x: Tensor, out_backprop: Tensor, *, filter_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(Conv2DBackpropFilterD)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(filter_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DBackpropFilterD"
    op.name = next_unique_name(node_name, "Conv2DBackpropFilterD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["filter_size"].list.val_type = 2
    op.attr["filter_size"].list.i.extend(filter_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2D
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def Conv2D(x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(Conv2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_BF16}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_BF16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_BF16}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2D"
    op.name = next_unique_name(node_name, "Conv2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2DCompress
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def Conv2DCompress(x: Tensor, filter_compress: Tensor, compress_index: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", offset_x: int=0, alg: str="weight_unzip", dependencies=[], node_name=None):
    """REG_OP(Conv2DCompress)\n
.INPUT(x, TensorType({DT_INT8}))\n
.INPUT(filter_compress, TensorType({DT_INT8}))\n
.INPUT(compress_index, TensorType({DT_INT8}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(offset_x, Int, 0)\n
.ATTR(alg, String, "weight_unzip")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DCompress"
    op.name = next_unique_name(node_name, "Conv2DCompress")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter_compress.tensor)
    op.input_desc.add().CopyFrom(filter_compress.desc)
    op.input_desc[-1].name = "filter_compress"
    op.input.append(compress_index.tensor)
    op.input_desc.add().CopyFrom(compress_index.desc)
    op.input_desc[-1].name = "compress_index"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["offset_x"].i = offset_x
    op.attr["alg"].s = compat_as_bytes(alg)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DeformableConv2D
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True])
def DeformableConv2D(x: Tensor, filter: Tensor, offsets: Tensor, bias: Optional[Tensor], *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", deformable_groups: int=1, modulated: bool=True, dependencies=[], node_name=None):
    """REG_OP(DeformableConv2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(offsets, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(deformable_groups, Int, 1)\n
.ATTR(modulated, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeformableConv2D"
    op.name = next_unique_name(node_name, "DeformableConv2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["deformable_groups"].i = deformable_groups
    op.attr["modulated"].b = modulated

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR QuantConv2D
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def QuantConv2D(x: Tensor, filter: Tensor, scale: Tensor, bias: Optional[Tensor], offset: Optional[Tensor], *, dtype: int, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", offset_x: int=0, round_mode: str="rint", dependencies=[], node_name=None):
    """REG_OP(QuantConv2D)\n
.INPUT(x, TensorType({DT_INT8}))\n
.INPUT(filter, TensorType({DT_INT8}))\n
.INPUT(scale, TensorType({DT_UINT64, DT_INT64}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(offset, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(dtype, Int)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(offset_x, Int, 0)\n
.ATTR(round_mode, String, "rint")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "QuantConv2D"
    op.name = next_unique_name(node_name, "QuantConv2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["dtype"].i = dtype
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["offset_x"].i = offset_x
    op.attr["round_mode"].s = compat_as_bytes(round_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv3D
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def Conv3D(x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1, 1], groups: int=1, data_format: str="NDHWC", offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(Conv3D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_INT8}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_INT8}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT32}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NDHWC")\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv3D"
    op.name = next_unique_name(node_name, "Conv3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv3DBackpropInput
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def Conv3DBackpropInput(input_size: Tensor, filter: Tensor, out_backprop: Tensor, *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1, 1], groups: int=1, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(Conv3DBackpropInput)\n
.INPUT(input_size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv3DBackpropInput"
    op.name = next_unique_name(node_name, "Conv3DBackpropInput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_size.tensor)
    op.input_desc.add().CopyFrom(input_size.desc)
    op.input_desc[-1].name = "input_size"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv3DBackpropInputD
@auto_convert_to_tensor([False, False], [False, False])
def Conv3DBackpropInputD(filter: Tensor, out_backprop: Tensor, *, input_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1, 1], groups: int=1, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(Conv3DBackpropInputD)\n
.INPUT(filter, TensorType({DT_FLOAT16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv3DBackpropInputD"
    op.name = next_unique_name(node_name, "Conv3DBackpropInputD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LSTM
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, True, True, True, True])
def LSTM(x: Tensor, cont: Tensor, w_x: Tensor, bias: Tensor, w_h: Tensor, x_static: Optional[Tensor], h_0: Optional[Tensor], c_0: Optional[Tensor], w_x_static: Optional[Tensor], *, num_output: int=0, expose_hidden: bool=False, dependencies=[], node_name=None):
    """REG_OP(LSTM)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(cont, TensorType({DT_FLOAT32,DT_FLOAT16}))\n
.INPUT(w_x, TensorType({DT_FLOAT16}))\n
.INPUT(bias, TensorType({DT_FLOAT16,DT_FLOAT32,DT_INT16,DT_INT32}))\n
.INPUT(w_h, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(x_static, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(h_0, TensorType({DT_FLOAT16,DT_FLOAT32}))\n
.OPTIONAL_INPUT(c_0, TensorType({DT_FLOAT16,DT_FLOAT32}))\n
.OPTIONAL_INPUT(w_x_static, TensorType({DT_FLOAT16}))\n
.OUTPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(h_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(c_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(num_output, Int, 0)\n
.ATTR(expose_hidden, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LSTM"
    op.name = next_unique_name(node_name, "LSTM")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(cont.tensor)
    op.input_desc.add().CopyFrom(cont.desc)
    op.input_desc[-1].name = "cont"
    op.input.append(w_x.tensor)
    op.input_desc.add().CopyFrom(w_x.desc)
    op.input_desc[-1].name = "w_x"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"
    op.input.append(w_h.tensor)
    op.input_desc.add().CopyFrom(w_h.desc)
    op.input_desc[-1].name = "w_h"
    if x_static is not None:
        op.input.append(x_static.tensor)
        op.input_desc.add().CopyFrom(x_static.desc)
        op.input_desc[-1].name = "x_static"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x_static"
    if h_0 is not None:
        op.input.append(h_0.tensor)
        op.input_desc.add().CopyFrom(h_0.desc)
        op.input_desc[-1].name = "h_0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "h_0"
    if c_0 is not None:
        op.input.append(c_0.tensor)
        op.input_desc.add().CopyFrom(c_0.desc)
        op.input_desc[-1].name = "c_0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "c_0"
    if w_x_static is not None:
        op.input.append(w_x_static.tensor)
        op.input_desc.add().CopyFrom(w_x_static.desc)
        op.input_desc[-1].name = "w_x_static"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "w_x_static"

    # process attrs
    op.attr["num_output"].i = num_output
    op.attr["expose_hidden"].b = expose_hidden

    # process outputs
    output_index = 0
    op.output_desc.add().name = "h"
    h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "h_t"
    h_t = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "c_t"
    c_t = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return h, h_t, c_t


# This api is auto-generated from IR Conv3DBackpropFilter
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Conv3DBackpropFilter(x: Tensor, filter_size: Tensor, out_backprop: Tensor, *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1, 1], groups: int=1, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(Conv3DBackpropFilter)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(filter_size, TensorType({DT_INT32}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv3DBackpropFilter"
    op.name = next_unique_name(node_name, "Conv3DBackpropFilter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter_size.tensor)
    op.input_desc.add().CopyFrom(filter_size.desc)
    op.input_desc[-1].name = "filter_size"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv3DBackpropFilterD
@auto_convert_to_tensor([False, False], [False, False])
def Conv3DBackpropFilterD(x: Tensor, out_backprop: Tensor, *, filter_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1, 1], groups: int=1, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(Conv3DBackpropFilterD)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(filter_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv3DBackpropFilterD"
    op.name = next_unique_name(node_name, "Conv3DBackpropFilterD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["filter_size"].list.val_type = 2
    op.attr["filter_size"].list.i.extend(filter_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv3DTranspose
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def Conv3DTranspose(input_size: Tensor, x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1, 1], groups: int=1, data_format: str="NDHWC", output_padding: List[int]=[0, 0, 0, 0, 0], offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(Conv3DTranspose)\n
.INPUT(input_size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(filter, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NDHWC")\n
.ATTR(output_padding, ListInt, {0, 0, 0, 0, 0})\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv3DTranspose"
    op.name = next_unique_name(node_name, "Conv3DTranspose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_size.tensor)
    op.input_desc.add().CopyFrom(input_size.desc)
    op.input_desc[-1].name = "input_size"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["output_padding"].list.val_type = 2
    op.attr["output_padding"].list.i.extend(output_padding)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv3DTransposeD
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def Conv3DTransposeD(x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, input_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1, 1], groups: int=1, data_format: str="NDHWC", output_padding: List[int]=[0, 0, 0, 0, 0], offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(Conv3DTransposeD)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(filter, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NDHWC")\n
.ATTR(output_padding, ListInt, {0, 0, 0, 0, 0})\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv3DTransposeD"
    op.name = next_unique_name(node_name, "Conv3DTransposeD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["output_padding"].list.val_type = 2
    op.attr["output_padding"].list.i.extend(output_padding)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2DTranspose
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def Conv2DTranspose(input_size: Tensor, x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", output_padding: List[int]=[0, 0, 0, 0], offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(Conv2DTranspose)\n
.INPUT(input_size, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_INT8}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_INT8}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(output_padding, ListInt, {0, 0, 0, 0})\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DTranspose"
    op.name = next_unique_name(node_name, "Conv2DTranspose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_size.tensor)
    op.input_desc.add().CopyFrom(input_size.desc)
    op.input_desc[-1].name = "input_size"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["output_padding"].list.val_type = 2
    op.attr["output_padding"].list.i.extend(output_padding)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Conv2DTransposeD
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def Conv2DTransposeD(x: Tensor, filter: Tensor, bias: Optional[Tensor], offset_w: Optional[Tensor], *, input_size: List[int], strides: List[int], pads: List[int], dilations: List[int]=[1, 1, 1, 1], groups: int=1, data_format: str="NHWC", output_padding: List[int]=[0, 0, 0, 0], offset_x: int=0, dependencies=[], node_name=None):
    """REG_OP(Conv2DTransposeD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_INT8, DT_FLOAT, DT_INT4}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_INT8, DT_FLOAT, DT_INT4}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT}))\n
.OPTIONAL_INPUT(offset_w, TensorType({DT_INT8, DT_INT4}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_INT32, DT_FLOAT}))\n
.REQUIRED_ATTR(input_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(groups, Int, 1)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(output_padding, ListInt, {0, 0, 0, 0})\n
.ATTR(offset_x, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Conv2DTransposeD"
    op.name = next_unique_name(node_name, "Conv2DTransposeD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if offset_w is not None:
        op.input.append(offset_w.tensor)
        op.input_desc.add().CopyFrom(offset_w.desc)
        op.input_desc[-1].name = "offset_w"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset_w"

    # process attrs
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["groups"].i = groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["output_padding"].list.val_type = 2
    op.attr["output_padding"].list.i.extend(output_padding)
    op.attr["offset_x"].i = offset_x

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DeformableOffsets
@auto_convert_to_tensor([False, False], [False, False])
def DeformableOffsets(x: Tensor, offsets: Tensor, *, strides: List[int], pads: List[int], ksize: List[int], dilations: List[int]=[1, 1, 1, 1], data_format: str="NCHW", deformable_groups: int=1, modulated: bool=True, dependencies=[], node_name=None):
    """REG_OP(DeformableOffsets)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(offsets, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.REQUIRED_ATTR(ksize, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(deformable_groups, Int, 1)\n
.ATTR(modulated, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeformableOffsets"
    op.name = next_unique_name(node_name, "DeformableOffsets")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["deformable_groups"].i = deformable_groups
    op.attr["modulated"].b = modulated

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DeformableOffsetsGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DeformableOffsetsGrad(grad: Tensor, x: Tensor, offsets: Tensor, *, strides: List[int], pads: List[int], ksize: List[int], dilations: List[int]=[1, 1, 1, 1], data_format: str="NCHW", deformable_groups: int=1, modulated: bool=True, dependencies=[], node_name=None):
    """REG_OP(DeformableOffsetsGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(offsets, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(grad_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(grad_offsets, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.REQUIRED_ATTR(ksize, ListInt)\n
.ATTR(dilations, ListInt, {1, 1, 1, 1})\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(deformable_groups, Int, 1)\n
.ATTR(modulated, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeformableOffsetsGrad"
    op.name = next_unique_name(node_name, "DeformableOffsetsGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["deformable_groups"].i = deformable_groups
    op.attr["modulated"].b = modulated

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_x"
    grad_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "grad_offsets"
    grad_offsets = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_x, grad_offsets


# This api is auto-generated from IR Dilation
@auto_convert_to_tensor([False], [False])
def Dilation(x: Tensor, *, dilations: List[int], pads: List[int]=[], padding_value: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Dilation)\n
.INPUT(x, TensorType({DT_INT8, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(dilations, ListInt)\n
.ATTR(pads, ListInt, {})\n
.ATTR(padding_value, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dilation"
    op.name = next_unique_name(node_name, "Dilation")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["padding_value"].f = padding_value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FixPipe
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, True, True, True, True, True, True, True, True, True])
def FixPipe(x1: Tensor, x2: Optional[Tensor], quant_scale_0: Optional[Tensor], relu_weight_0: Optional[Tensor], clip_value_0: Optional[Tensor], quant_scale_1: Optional[Tensor], relu_weight_1: Optional[Tensor], clip_value_1: Optional[Tensor], anti_quant_scale: Optional[Tensor], anti_quant_offset: Optional[Tensor], *, fusion_op_list: List[str], unit_list: List[str], eltwise_mode: str="", dependencies=[], node_name=None):
    """REG_OP(FixPipe)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT32}))\n
.OPTIONAL_INPUT(x2, TensorType({DT_FLOAT16, DT_INT8, DT_INT4}))\n
.OPTIONAL_INPUT(quant_scale_0, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(relu_weight_0, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(clip_value_0, TensorType({DT_FLOAT16, DT_INT8, DT_INT4}))\n
.OPTIONAL_INPUT(quant_scale_1, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(relu_weight_1, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(clip_value_1, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(anti_quant_scale, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(anti_quant_offset, TensorType({DT_INT8, DT_INT4}))\n
.OUTPUT(output, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT32, DT_INT8, DT_INT4}))\n
.REQUIRED_ATTR(fusion_op_list, ListString)\n
.REQUIRED_ATTR(unit_list, ListString)\n
.ATTR(eltwise_mode, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FixPipe"
    op.name = next_unique_name(node_name, "FixPipe")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    if x2 is not None:
        op.input.append(x2.tensor)
        op.input_desc.add().CopyFrom(x2.desc)
        op.input_desc[-1].name = "x2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x2"
    if quant_scale_0 is not None:
        op.input.append(quant_scale_0.tensor)
        op.input_desc.add().CopyFrom(quant_scale_0.desc)
        op.input_desc[-1].name = "quant_scale_0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale_0"
    if relu_weight_0 is not None:
        op.input.append(relu_weight_0.tensor)
        op.input_desc.add().CopyFrom(relu_weight_0.desc)
        op.input_desc[-1].name = "relu_weight_0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "relu_weight_0"
    if clip_value_0 is not None:
        op.input.append(clip_value_0.tensor)
        op.input_desc.add().CopyFrom(clip_value_0.desc)
        op.input_desc[-1].name = "clip_value_0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "clip_value_0"
    if quant_scale_1 is not None:
        op.input.append(quant_scale_1.tensor)
        op.input_desc.add().CopyFrom(quant_scale_1.desc)
        op.input_desc[-1].name = "quant_scale_1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale_1"
    if relu_weight_1 is not None:
        op.input.append(relu_weight_1.tensor)
        op.input_desc.add().CopyFrom(relu_weight_1.desc)
        op.input_desc[-1].name = "relu_weight_1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "relu_weight_1"
    if clip_value_1 is not None:
        op.input.append(clip_value_1.tensor)
        op.input_desc.add().CopyFrom(clip_value_1.desc)
        op.input_desc[-1].name = "clip_value_1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "clip_value_1"
    if anti_quant_scale is not None:
        op.input.append(anti_quant_scale.tensor)
        op.input_desc.add().CopyFrom(anti_quant_scale.desc)
        op.input_desc[-1].name = "anti_quant_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "anti_quant_scale"
    if anti_quant_offset is not None:
        op.input.append(anti_quant_offset.tensor)
        op.input_desc.add().CopyFrom(anti_quant_offset.desc)
        op.input_desc[-1].name = "anti_quant_offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "anti_quant_offset"

    # process attrs
    op.attr["fusion_op_list"].list.val_type = 0
    op.attr["fusion_op_list"].list.s.extend(compat_as_bytes_list(fusion_op_list))
    op.attr["unit_list"].list.val_type = 0
    op.attr["unit_list"].list.s.extend(compat_as_bytes_list(unit_list))
    op.attr["eltwise_mode"].s = compat_as_bytes(eltwise_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR IsotonicRegression
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def IsotonicRegression(input: Tensor, *, output_dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(IsotonicRegression)\n
.INPUT(input, TensorType::RealNumberType())\n
.OUTPUT(output, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(segments, TensorType({DT_INT32}))\n
.ATTR(output_dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsotonicRegression"
    op.name = next_unique_name(node_name, "IsotonicRegression")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["output_dtype"].dt = output_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "segments"
    segments = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output, segments


# This api is auto-generated from IR RoiAlignRotated
@auto_convert_to_tensor([False, False], [False, False])
def RoiAlignRotated(x: Tensor, rois: Tensor, *, pooled_h: int, pooled_w: int, spatial_scale: float, sampling_ratio: int=0, aligned: bool=True, clockwise: bool=False, dependencies=[], node_name=None):
    """REG_OP(RoiAlignRotated)\n
.INPUT(x, TensorType({DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(pooled_h, Int)\n
.REQUIRED_ATTR(pooled_w, Int)\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.ATTR(sampling_ratio, Int, 0)\n
.ATTR(aligned, Bool, true)\n
.ATTR(clockwise, Bool, false)\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RoiAlignRotated"
    op.name = next_unique_name(node_name, "RoiAlignRotated")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"

    # process attrs
    op.attr["pooled_h"].i = pooled_h
    op.attr["pooled_w"].i = pooled_w
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["sampling_ratio"].i = sampling_ratio
    op.attr["aligned"].b = aligned
    op.attr["clockwise"].b = clockwise

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BoundingBoxDecode
@auto_convert_to_tensor([False, False], [False, False])
def BoundingBoxDecode(rois: Tensor, deltas: Tensor, *, max_shape: List[int], means: List[float]=[0.000000, 0.000000, 0.000000, 0.000000], stds: List[float]=[1.000000, 1.000000, 1.000000, 1.000000], wh_ratio_clip: float=0.016000, dependencies=[], node_name=None):
    """REG_OP(BoundingBoxDecode)\n
.INPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(deltas, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(means, ListFloat, {0.0, 0.0, 0.0, 0.0})\n
.ATTR(stds, ListFloat, {1.0, 1.0, 1.0, 1.0})\n
.REQUIRED_ATTR(max_shape, ListInt)\n
.ATTR(wh_ratio_clip, Float, 0.016)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BoundingBoxDecode"
    op.name = next_unique_name(node_name, "BoundingBoxDecode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    op.input.append(deltas.tensor)
    op.input_desc.add().CopyFrom(deltas.desc)
    op.input_desc[-1].name = "deltas"

    # process attrs
    op.attr["max_shape"].list.val_type = 2
    op.attr["max_shape"].list.i.extend(max_shape)
    op.attr["means"].list.val_type = 3
    op.attr["means"].list.f.extend(means)
    op.attr["stds"].list.val_type = 3
    op.attr["stds"].list.f.extend(stds)
    op.attr["wh_ratio_clip"].f = wh_ratio_clip

    # process outputs
    output_index = 0
    op.output_desc.add().name = "bboxes"
    bboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return bboxes


# This api is auto-generated from IR BoundingBoxEncode
@auto_convert_to_tensor([False, False], [False, False])
def BoundingBoxEncode(anchor_box: Tensor, ground_truth_box: Tensor, *, means: List[float]=[0.000000, 0.000000, 0.000000, 0.000000], stds: List[float]=[1.000000, 1.000000, 1.000000, 1.000000], dependencies=[], node_name=None):
    """REG_OP(BoundingBoxEncode)\n
.INPUT(anchor_box, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(ground_truth_box, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(delats, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(means, ListFloat, {0.0, 0.0, 0.0, 0.0})\n
.ATTR(stds, ListFloat, {1.0, 1.0, 1.0, 1.0})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BoundingBoxEncode"
    op.name = next_unique_name(node_name, "BoundingBoxEncode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(anchor_box.tensor)
    op.input_desc.add().CopyFrom(anchor_box.desc)
    op.input_desc[-1].name = "anchor_box"
    op.input.append(ground_truth_box.tensor)
    op.input_desc.add().CopyFrom(ground_truth_box.desc)
    op.input_desc[-1].name = "ground_truth_box"

    # process attrs
    op.attr["means"].list.val_type = 3
    op.attr["means"].list.f.extend(means)
    op.attr["stds"].list.val_type = 3
    op.attr["stds"].list.f.extend(stds)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "delats"
    delats = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return delats


# This api is auto-generated from IR CheckValid
@auto_convert_to_tensor([False, False], [False, False])
def CheckValid(bbox_tensor: Tensor, img_metas: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(CheckValid)\n
.INPUT(bbox_tensor, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img_metas, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(valid_tensor, TensorType({DT_INT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CheckValid"
    op.name = next_unique_name(node_name, "CheckValid")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bbox_tensor.tensor)
    op.input_desc.add().CopyFrom(bbox_tensor.desc)
    op.input_desc[-1].name = "bbox_tensor"
    op.input.append(img_metas.tensor)
    op.input_desc.add().CopyFrom(img_metas.desc)
    op.input_desc[-1].name = "img_metas"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "valid_tensor"
    valid_tensor = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return valid_tensor


# This api is auto-generated from IR Iou
@auto_convert_to_tensor([False, False], [False, False])
def Iou(bboxes: Tensor, gtboxes: Tensor, *, mode: str="iou", eps: float=1.000000, aligned: bool=False, dependencies=[], node_name=None):
    """REG_OP(Iou)\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(overlap, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(mode, String, "iou")\n
.ATTR(eps, Float, 1.0)\n
.ATTR(aligned, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Iou"
    op.name = next_unique_name(node_name, "Iou")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["eps"].f = eps
    op.attr["aligned"].b = aligned

    # process outputs
    output_index = 0
    op.output_desc.add().name = "overlap"
    overlap = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return overlap


# This api is auto-generated from IR GIoU
@auto_convert_to_tensor([False, False], [False, False])
def GIoU(bboxes: Tensor, gtboxes: Tensor, *, trans: bool=False, is_cross: bool=True, mode: str="iou", dependencies=[], node_name=None):
    """REG_OP(GIoU)\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(overlap, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
.ATTR(is_cross, Bool, true)\n
.ATTR(mode, String, "iou")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GIoU"
    op.name = next_unique_name(node_name, "GIoU")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs
    op.attr["trans"].b = trans
    op.attr["is_cross"].b = is_cross
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "overlap"
    overlap = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return overlap


# This api is auto-generated from IR ROIAlignGrad
@auto_convert_to_tensor([False, False, False], [False, False, True])
def ROIAlignGrad(ydiff: Tensor, rois: Tensor, rois_n: Optional[Tensor], *, xdiff_shape: List[int], pooled_width: int, pooled_height: int, spatial_scale: float, sample_num: int=2, roi_end_mode: int=1, dependencies=[], node_name=None):
    """REG_OP(ROIAlignGrad)\n
.INPUT(ydiff, TensorType({DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(rois_n, TensorType({DT_INT32}))\n
.OUTPUT(xdiff, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(xdiff_shape, ListInt)\n
.REQUIRED_ATTR(pooled_width, Int)\n
.REQUIRED_ATTR(pooled_height, Int)\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.ATTR(sample_num, Int, 2)\n
.ATTR(roi_end_mode, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ROIAlignGrad"
    op.name = next_unique_name(node_name, "ROIAlignGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(ydiff.tensor)
    op.input_desc.add().CopyFrom(ydiff.desc)
    op.input_desc[-1].name = "ydiff"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if rois_n is not None:
        op.input.append(rois_n.tensor)
        op.input_desc.add().CopyFrom(rois_n.desc)
        op.input_desc[-1].name = "rois_n"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "rois_n"

    # process attrs
    op.attr["xdiff_shape"].list.val_type = 2
    op.attr["xdiff_shape"].list.i.extend(xdiff_shape)
    op.attr["pooled_width"].i = pooled_width
    op.attr["pooled_height"].i = pooled_height
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["sample_num"].i = sample_num
    op.attr["roi_end_mode"].i = roi_end_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "xdiff"
    xdiff = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return xdiff


# This api is auto-generated from IR ROIAlign
@auto_convert_to_tensor([False, False, False], [False, False, True])
def ROIAlign(features: Tensor, rois: Tensor, rois_n: Optional[Tensor], *, spatial_scale: float, pooled_height: int, pooled_width: int, sample_num: int=2, roi_end_mode: int=1, dependencies=[], node_name=None):
    """REG_OP(ROIAlign)\n
.INPUT(features, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(rois_n, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.REQUIRED_ATTR(pooled_height, Int)\n
.REQUIRED_ATTR(pooled_width, Int)\n
.ATTR(sample_num, Int, 2)\n
.ATTR(roi_end_mode, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ROIAlign"
    op.name = next_unique_name(node_name, "ROIAlign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if rois_n is not None:
        op.input.append(rois_n.tensor)
        op.input_desc.add().CopyFrom(rois_n.desc)
        op.input_desc[-1].name = "rois_n"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "rois_n"

    # process attrs
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["pooled_height"].i = pooled_height
    op.attr["pooled_width"].i = pooled_width
    op.attr["sample_num"].i = sample_num
    op.attr["roi_end_mode"].i = roi_end_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PriorBox
@auto_convert_to_tensor([False, False], [False, False])
def PriorBox(x: Tensor, img: Tensor, *, min_size: List[float], max_size: List[float], aspect_ratio: List[float], img_h: int=0, img_w: int=0, step_h: float=0.000000, step_w: float=0.000000, flip: bool=True, clip: bool=False, offset: float=0.500000, variance: List[float]=[0.100000], dependencies=[], node_name=None):
    """REG_OP(PriorBox)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(min_size, ListFloat)\n
.REQUIRED_ATTR(max_size, ListFloat)\n
.REQUIRED_ATTR(aspect_ratio, ListFloat)\n
.ATTR(img_h, Int, 0)\n
.ATTR(img_w, Int, 0)\n
.ATTR(step_h, Float, 0.0)\n
.ATTR(step_w, Float, 0.0)\n
.ATTR(flip, Bool, true)\n
.ATTR(clip, Bool, false)\n
.ATTR(offset, Float, 0.5)\n
.ATTR(variance, ListFloat, {0.1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PriorBox"
    op.name = next_unique_name(node_name, "PriorBox")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"

    # process attrs
    op.attr["min_size"].list.val_type = 3
    op.attr["min_size"].list.f.extend(min_size)
    op.attr["max_size"].list.val_type = 3
    op.attr["max_size"].list.f.extend(max_size)
    op.attr["aspect_ratio"].list.val_type = 3
    op.attr["aspect_ratio"].list.f.extend(aspect_ratio)
    op.attr["img_h"].i = img_h
    op.attr["img_w"].i = img_w
    op.attr["step_h"].f = step_h
    op.attr["step_w"].f = step_w
    op.attr["flip"].b = flip
    op.attr["clip"].b = clip
    op.attr["offset"].f = offset
    op.attr["variance"].list.val_type = 3
    op.attr["variance"].list.f.extend(variance)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PriorBoxD
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def PriorBoxD(x: Tensor, img: Tensor, data_h: Tensor, data_w: Tensor, box_height: Tensor, box_width: Tensor, *, min_size: List[float], max_size: List[float], img_h: int=0, img_w: int=0, step_h: float=0.000000, step_w: float=0.000000, flip: bool=True, clip: bool=False, offset: float=0.500000, variance: List[float]=[0.100000], dependencies=[], node_name=None):
    """REG_OP(PriorBoxD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(data_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(data_w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(box_height, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(box_width, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(min_size, ListFloat)\n
.REQUIRED_ATTR(max_size, ListFloat)\n
.ATTR(img_h, Int, 0)\n
.ATTR(img_w, Int, 0)\n
.ATTR(step_h, Float, 0.0)\n
.ATTR(step_w, Float, 0.0)\n
.ATTR(flip, Bool, true)\n
.ATTR(clip, Bool, false)\n
.ATTR(offset, Float, 0.5)\n
.ATTR(variance, ListFloat, {0.1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PriorBoxD"
    op.name = next_unique_name(node_name, "PriorBoxD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"
    op.input.append(data_h.tensor)
    op.input_desc.add().CopyFrom(data_h.desc)
    op.input_desc[-1].name = "data_h"
    op.input.append(data_w.tensor)
    op.input_desc.add().CopyFrom(data_w.desc)
    op.input_desc[-1].name = "data_w"
    op.input.append(box_height.tensor)
    op.input_desc.add().CopyFrom(box_height.desc)
    op.input_desc[-1].name = "box_height"
    op.input.append(box_width.tensor)
    op.input_desc.add().CopyFrom(box_width.desc)
    op.input_desc[-1].name = "box_width"

    # process attrs
    op.attr["min_size"].list.val_type = 3
    op.attr["min_size"].list.f.extend(min_size)
    op.attr["max_size"].list.val_type = 3
    op.attr["max_size"].list.f.extend(max_size)
    op.attr["img_h"].i = img_h
    op.attr["img_w"].i = img_w
    op.attr["step_h"].f = step_h
    op.attr["step_w"].f = step_w
    op.attr["flip"].b = flip
    op.attr["clip"].b = clip
    op.attr["offset"].f = offset
    op.attr["variance"].list.val_type = 3
    op.attr["variance"].list.f.extend(variance)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PriorBoxDV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def PriorBoxDV2(x: Tensor, img: Tensor, boxes: Tensor, *, min_size: List[float], max_size: List[float], img_h: int=0, img_w: int=0, step_h: float=0.000000, step_w: float=0.000000, flip: bool=True, clip: bool=False, offset: float=0.500000, variance: List[float]=[0.100000], dependencies=[], node_name=None):
    """REG_OP(PriorBoxDV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(min_size, ListFloat)\n
.REQUIRED_ATTR(max_size, ListFloat)\n
.ATTR(img_h, Int, 0)\n
.ATTR(img_w, Int, 0)\n
.ATTR(step_h, Float, 0.0)\n
.ATTR(step_w, Float, 0.0)\n
.ATTR(flip, Bool, true)\n
.ATTR(clip, Bool, false)\n
.ATTR(offset, Float, 0.5)\n
.ATTR(variance, ListFloat, {0.1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PriorBoxDV2"
    op.name = next_unique_name(node_name, "PriorBoxDV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"

    # process attrs
    op.attr["min_size"].list.val_type = 3
    op.attr["min_size"].list.f.extend(min_size)
    op.attr["max_size"].list.val_type = 3
    op.attr["max_size"].list.f.extend(max_size)
    op.attr["img_h"].i = img_h
    op.attr["img_w"].i = img_w
    op.attr["step_h"].f = step_h
    op.attr["step_w"].f = step_w
    op.attr["flip"].b = flip
    op.attr["clip"].b = clip
    op.attr["offset"].f = offset
    op.attr["variance"].list.val_type = 3
    op.attr["variance"].list.f.extend(variance)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PSROIPooling
@auto_convert_to_tensor([False, False], [False, False])
def PSROIPooling(x: Tensor, rois: Tensor, *, output_dim: int, group_size: int, spatial_scale: float, dependencies=[], node_name=None):
    """REG_OP(PSROIPooling)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(rois, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(output_dim, Int)\n
.REQUIRED_ATTR(group_size, Int)\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PSROIPooling"
    op.name = next_unique_name(node_name, "PSROIPooling")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"

    # process attrs
    op.attr["output_dim"].i = output_dim
    op.attr["group_size"].i = group_size
    op.attr["spatial_scale"].f = spatial_scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FSRDetectionOutput
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, True])
def FSRDetectionOutput(rois: Tensor, bbox_delta: Tensor, score: Tensor, im_info: Tensor, actual_rois_num: Optional[Tensor], *, num_classes: int, score_threshold: float, iou_threshold: float, batch_rois: int=1, dependencies=[], node_name=None):
    """REG_OP(FSRDetectionOutput)\n
.INPUT(rois, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(bbox_delta, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(score, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(im_info, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OPTIONAL_INPUT(actual_rois_num, TensorType({DT_INT32}))\n
.OUTPUT(actual_bbox_num, TensorType({DT_INT32}))\n
.OUTPUT(box, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(batch_rois, Int, 1)\n
.REQUIRED_ATTR(num_classes, Int)\n
.REQUIRED_ATTR(score_threshold, Float)\n
.REQUIRED_ATTR(iou_threshold, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FSRDetectionOutput"
    op.name = next_unique_name(node_name, "FSRDetectionOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    op.input.append(bbox_delta.tensor)
    op.input_desc.add().CopyFrom(bbox_delta.desc)
    op.input_desc[-1].name = "bbox_delta"
    op.input.append(score.tensor)
    op.input_desc.add().CopyFrom(score.desc)
    op.input_desc[-1].name = "score"
    op.input.append(im_info.tensor)
    op.input_desc.add().CopyFrom(im_info.desc)
    op.input_desc[-1].name = "im_info"
    if actual_rois_num is not None:
        op.input.append(actual_rois_num.tensor)
        op.input_desc.add().CopyFrom(actual_rois_num.desc)
        op.input_desc[-1].name = "actual_rois_num"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_rois_num"

    # process attrs
    op.attr["num_classes"].i = num_classes
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["batch_rois"].i = batch_rois

    # process outputs
    output_index = 0
    op.output_desc.add().name = "actual_bbox_num"
    actual_bbox_num = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box"
    box = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return actual_bbox_num, box


# This api is auto-generated from IR SSDDetectionOutput
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SSDDetectionOutput(bbox_delta: Tensor, score: Tensor, anchors: Tensor, *, num_classes: int=2, share_location: bool=True, background_label_id: int=0, iou_threshold: float=0.300000, top_k: int=200, eta: float=1.000000, variance_encoded_in_target: bool=False, code_type: int=1, keep_top_k: int=-1, confidence_threshold: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(SSDDetectionOutput)\n
.INPUT(bbox_delta, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(score, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(anchors, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(out_boxnum, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(num_classes, Int, 2)\n
.ATTR(share_location, Bool, true)\n
.ATTR(background_label_id, Int, 0)\n
.ATTR(iou_threshold, Float, 0.3)\n
.ATTR(top_k, Int, 200)\n
.ATTR(eta, Float, 1.0)\n
.ATTR(variance_encoded_in_target, Bool, false)\n
.ATTR(code_type, Int, 1)\n
.ATTR(keep_top_k, Int, -1)\n
.ATTR(confidence_threshold, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SSDDetectionOutput"
    op.name = next_unique_name(node_name, "SSDDetectionOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bbox_delta.tensor)
    op.input_desc.add().CopyFrom(bbox_delta.desc)
    op.input_desc[-1].name = "bbox_delta"
    op.input.append(score.tensor)
    op.input_desc.add().CopyFrom(score.desc)
    op.input_desc[-1].name = "score"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"

    # process attrs
    op.attr["num_classes"].i = num_classes
    op.attr["share_location"].b = share_location
    op.attr["background_label_id"].i = background_label_id
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["top_k"].i = top_k
    op.attr["eta"].f = eta
    op.attr["variance_encoded_in_target"].b = variance_encoded_in_target
    op.attr["code_type"].i = code_type
    op.attr["keep_top_k"].i = keep_top_k
    op.attr["confidence_threshold"].f = confidence_threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_boxnum"
    out_boxnum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_boxnum, y


# This api is auto-generated from IR Yolo
@auto_convert_to_tensor([False], [False])
def Yolo(x: Tensor, *, boxes: int=3, coords: int=4, classes: int=80, yolo_version: str="V3", softmax: bool=False, background: bool=False, softmaxtree: bool=False, dependencies=[], node_name=None):
    """REG_OP(Yolo)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(coord_data, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(obj_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(classes_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(yolo_version, String, "V3")\n
.ATTR(softmax, Bool, false)\n
.ATTR(background, Bool, false)\n
.ATTR(softmaxtree, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Yolo"
    op.name = next_unique_name(node_name, "Yolo")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["yolo_version"].s = compat_as_bytes(yolo_version)
    op.attr["softmax"].b = softmax
    op.attr["background"].b = background
    op.attr["softmaxtree"].b = softmaxtree

    # process outputs
    output_index = 0
    op.output_desc.add().name = "coord_data"
    coord_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "obj_prob"
    obj_prob = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "classes_prob"
    classes_prob = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return coord_data, obj_prob, classes_prob


# This api is auto-generated from IR YoloPreDetection
@auto_convert_to_tensor([False], [False])
def YoloPreDetection(x: Tensor, *, boxes: int=3, coords: int=4, classes: int=80, yolo_version: str="V5", softmax: bool=False, background: bool=False, softmaxtree: bool=False, dependencies=[], node_name=None):
    """REG_OP(YoloPreDetection)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(coord_data, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(obj_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(classes_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(yolo_version, String, "V5")\n
.ATTR(softmax, Bool, false)\n
.ATTR(background, Bool, false)\n
.ATTR(softmaxtree, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloPreDetection"
    op.name = next_unique_name(node_name, "YoloPreDetection")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["yolo_version"].s = compat_as_bytes(yolo_version)
    op.attr["softmax"].b = softmax
    op.attr["background"].b = background
    op.attr["softmaxtree"].b = softmaxtree

    # process outputs
    output_index = 0
    op.output_desc.add().name = "coord_data"
    coord_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "obj_prob"
    obj_prob = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "classes_prob"
    classes_prob = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return coord_data, obj_prob, classes_prob


# This api is auto-generated from IR YoloV5DetectionOutput
@auto_convert_to_tensor([True], [False])
def YoloV5DetectionOutput(x: List[Tensor], *, biases: List[float], boxes: int=3, coords: int=4, classes: int=80, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, N: int=10, resize_origin_img_to_net: bool=False, out_box_dim: int=3, alpha: float=2.000000, dependencies=[], node_name=None):
    """REG_OP(YoloV5DetectionOutput)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases, ListFloat)\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.ATTR(N, Int, 10)\n
.ATTR(resize_origin_img_to_net, Bool, false)\n
.ATTR(out_box_dim, Int, 3)\n
.ATTR(alpha, Float, 2.0)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV5DetectionOutput"
    op.name = next_unique_name(node_name, "YoloV5DetectionOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["biases"].list.val_type = 3
    op.attr["biases"].list.f.extend(biases)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn
    op.attr["N"].i = N
    op.attr["resize_origin_img_to_net"].b = resize_origin_img_to_net
    op.attr["out_box_dim"].i = out_box_dim
    op.attr["alpha"].f = alpha

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR YoloV5DetectionOutputD
@auto_convert_to_tensor([True, True, True], [False, False, False])
def YoloV5DetectionOutputD(x: List[Tensor], windex: List[Tensor], hindex: List[Tensor], *, biases: List[float], boxes: int=3, coords: int=4, classes: int=80, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, N: int=10, resize_origin_img_to_net: bool=False, out_box_dim: int=3, alpha: float=2.000000, dependencies=[], node_name=None):
    """REG_OP(YoloV5DetectionOutputD)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_INPUT(windex, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_INPUT(hindex, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases, ListFloat)\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.ATTR(N, Int, 10)\n
.ATTR(resize_origin_img_to_net, Bool, false)\n
.ATTR(out_box_dim, Int, 3)\n
.ATTR(alpha, Float, 2.0)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV5DetectionOutputD"
    op.name = next_unique_name(node_name, "YoloV5DetectionOutputD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    if not isinstance(windex, (tuple, list)):
        raise AssertionError("windex must be a tuple or a list.")
    for i, v in enumerate(windex):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "windex" + str(i)
    if not isinstance(hindex, (tuple, list)):
        raise AssertionError("hindex must be a tuple or a list.")
    for i, v in enumerate(hindex):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "hindex" + str(i)

    # process attrs
    op.attr["biases"].list.val_type = 3
    op.attr["biases"].list.f.extend(biases)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn
    op.attr["N"].i = N
    op.attr["resize_origin_img_to_net"].b = resize_origin_img_to_net
    op.attr["out_box_dim"].i = out_box_dim
    op.attr["alpha"].f = alpha

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR YoloV2DetectionOutput
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def YoloV2DetectionOutput(coord_data: Tensor, obj_prob: Tensor, classes_prob: Tensor, img_info: Tensor, *, biases: List[float], boxes: int=5, coords: int=4, classes: int=20, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, dependencies=[], node_name=None):
    """REG_OP(YoloV2DetectionOutput)\n
.INPUT(coord_data, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img_info, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases, ListFloat)\n
.ATTR(boxes, Int, 5)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 20)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV2DetectionOutput"
    op.name = next_unique_name(node_name, "YoloV2DetectionOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(coord_data.tensor)
    op.input_desc.add().CopyFrom(coord_data.desc)
    op.input_desc[-1].name = "coord_data"
    op.input.append(obj_prob.tensor)
    op.input_desc.add().CopyFrom(obj_prob.desc)
    op.input_desc[-1].name = "obj_prob"
    op.input.append(classes_prob.tensor)
    op.input_desc.add().CopyFrom(classes_prob.desc)
    op.input_desc[-1].name = "classes_prob"
    op.input.append(img_info.tensor)
    op.input_desc.add().CopyFrom(img_info.desc)
    op.input_desc[-1].name = "img_info"

    # process attrs
    op.attr["biases"].list.val_type = 3
    op.attr["biases"].list.f.extend(biases)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR YoloV2DetectionOutputD
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def YoloV2DetectionOutputD(coord_data: Tensor, obj_prob: Tensor, classes_prob: Tensor, img_info: Tensor, windex: Tensor, hindex: Tensor, *, biases: List[float], boxes: int=5, coords: int=4, classes: int=20, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, dependencies=[], node_name=None):
    """REG_OP(YoloV2DetectionOutputD)\n
.INPUT(coord_data, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img_info, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(windex, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hindex, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases, ListFloat)\n
.ATTR(boxes, Int, 5)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 20)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV2DetectionOutputD"
    op.name = next_unique_name(node_name, "YoloV2DetectionOutputD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(coord_data.tensor)
    op.input_desc.add().CopyFrom(coord_data.desc)
    op.input_desc[-1].name = "coord_data"
    op.input.append(obj_prob.tensor)
    op.input_desc.add().CopyFrom(obj_prob.desc)
    op.input_desc[-1].name = "obj_prob"
    op.input.append(classes_prob.tensor)
    op.input_desc.add().CopyFrom(classes_prob.desc)
    op.input_desc[-1].name = "classes_prob"
    op.input.append(img_info.tensor)
    op.input_desc.add().CopyFrom(img_info.desc)
    op.input_desc[-1].name = "img_info"
    op.input.append(windex.tensor)
    op.input_desc.add().CopyFrom(windex.desc)
    op.input_desc[-1].name = "windex"
    op.input.append(hindex.tensor)
    op.input_desc.add().CopyFrom(hindex.desc)
    op.input_desc[-1].name = "hindex"

    # process attrs
    op.attr["biases"].list.val_type = 3
    op.attr["biases"].list.f.extend(biases)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR YoloV3DetectionOutput
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False])
def YoloV3DetectionOutput(coord_data_low: Tensor, coord_data_mid: Tensor, coord_data_high: Tensor, obj_prob_low: Tensor, obj_prob_mid: Tensor, obj_prob_high: Tensor, classes_prob_low: Tensor, classes_prob_mid: Tensor, classes_prob_high: Tensor, img_info: Tensor, *, biases_low: List[float], biases_mid: List[float], biases_high: List[float], boxes: int=3, coords: int=4, classes: int=80, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, dependencies=[], node_name=None):
    """REG_OP(YoloV3DetectionOutput)\n
.INPUT(coord_data_low, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(coord_data_mid, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(coord_data_high, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob_low, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob_mid, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob_high, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob_low, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob_mid, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob_high, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img_info, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases_low, ListFloat)\n
.REQUIRED_ATTR(biases_mid, ListFloat)\n
.REQUIRED_ATTR(biases_high, ListFloat)\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV3DetectionOutput"
    op.name = next_unique_name(node_name, "YoloV3DetectionOutput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(coord_data_low.tensor)
    op.input_desc.add().CopyFrom(coord_data_low.desc)
    op.input_desc[-1].name = "coord_data_low"
    op.input.append(coord_data_mid.tensor)
    op.input_desc.add().CopyFrom(coord_data_mid.desc)
    op.input_desc[-1].name = "coord_data_mid"
    op.input.append(coord_data_high.tensor)
    op.input_desc.add().CopyFrom(coord_data_high.desc)
    op.input_desc[-1].name = "coord_data_high"
    op.input.append(obj_prob_low.tensor)
    op.input_desc.add().CopyFrom(obj_prob_low.desc)
    op.input_desc[-1].name = "obj_prob_low"
    op.input.append(obj_prob_mid.tensor)
    op.input_desc.add().CopyFrom(obj_prob_mid.desc)
    op.input_desc[-1].name = "obj_prob_mid"
    op.input.append(obj_prob_high.tensor)
    op.input_desc.add().CopyFrom(obj_prob_high.desc)
    op.input_desc[-1].name = "obj_prob_high"
    op.input.append(classes_prob_low.tensor)
    op.input_desc.add().CopyFrom(classes_prob_low.desc)
    op.input_desc[-1].name = "classes_prob_low"
    op.input.append(classes_prob_mid.tensor)
    op.input_desc.add().CopyFrom(classes_prob_mid.desc)
    op.input_desc[-1].name = "classes_prob_mid"
    op.input.append(classes_prob_high.tensor)
    op.input_desc.add().CopyFrom(classes_prob_high.desc)
    op.input_desc[-1].name = "classes_prob_high"
    op.input.append(img_info.tensor)
    op.input_desc.add().CopyFrom(img_info.desc)
    op.input_desc[-1].name = "img_info"

    # process attrs
    op.attr["biases_low"].list.val_type = 3
    op.attr["biases_low"].list.f.extend(biases_low)
    op.attr["biases_mid"].list.val_type = 3
    op.attr["biases_mid"].list.f.extend(biases_mid)
    op.attr["biases_high"].list.val_type = 3
    op.attr["biases_high"].list.f.extend(biases_high)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR YoloV3DetectionOutputD
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])
def YoloV3DetectionOutputD(coord_data_low: Tensor, coord_data_mid: Tensor, coord_data_high: Tensor, obj_prob_low: Tensor, obj_prob_mid: Tensor, obj_prob_high: Tensor, classes_prob_low: Tensor, classes_prob_mid: Tensor, classes_prob_high: Tensor, img_info: Tensor, windex1: Tensor, windex2: Tensor, windex3: Tensor, hindex1: Tensor, hindex2: Tensor, hindex3: Tensor, *, biases_low: List[float], biases_mid: List[float], biases_high: List[float], boxes: int=3, coords: int=4, classes: int=80, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, dependencies=[], node_name=None):
    """REG_OP(YoloV3DetectionOutputD)\n
.INPUT(coord_data_low, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(coord_data_mid, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(coord_data_high, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob_low, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob_mid, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(obj_prob_high, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob_low, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob_mid, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(classes_prob_high, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(img_info, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(windex1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(windex2, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(windex3, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hindex1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hindex2, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hindex3, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases_low, ListFloat)\n
.REQUIRED_ATTR(biases_mid, ListFloat)\n
.REQUIRED_ATTR(biases_high, ListFloat)\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV3DetectionOutputD"
    op.name = next_unique_name(node_name, "YoloV3DetectionOutputD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(coord_data_low.tensor)
    op.input_desc.add().CopyFrom(coord_data_low.desc)
    op.input_desc[-1].name = "coord_data_low"
    op.input.append(coord_data_mid.tensor)
    op.input_desc.add().CopyFrom(coord_data_mid.desc)
    op.input_desc[-1].name = "coord_data_mid"
    op.input.append(coord_data_high.tensor)
    op.input_desc.add().CopyFrom(coord_data_high.desc)
    op.input_desc[-1].name = "coord_data_high"
    op.input.append(obj_prob_low.tensor)
    op.input_desc.add().CopyFrom(obj_prob_low.desc)
    op.input_desc[-1].name = "obj_prob_low"
    op.input.append(obj_prob_mid.tensor)
    op.input_desc.add().CopyFrom(obj_prob_mid.desc)
    op.input_desc[-1].name = "obj_prob_mid"
    op.input.append(obj_prob_high.tensor)
    op.input_desc.add().CopyFrom(obj_prob_high.desc)
    op.input_desc[-1].name = "obj_prob_high"
    op.input.append(classes_prob_low.tensor)
    op.input_desc.add().CopyFrom(classes_prob_low.desc)
    op.input_desc[-1].name = "classes_prob_low"
    op.input.append(classes_prob_mid.tensor)
    op.input_desc.add().CopyFrom(classes_prob_mid.desc)
    op.input_desc[-1].name = "classes_prob_mid"
    op.input.append(classes_prob_high.tensor)
    op.input_desc.add().CopyFrom(classes_prob_high.desc)
    op.input_desc[-1].name = "classes_prob_high"
    op.input.append(img_info.tensor)
    op.input_desc.add().CopyFrom(img_info.desc)
    op.input_desc[-1].name = "img_info"
    op.input.append(windex1.tensor)
    op.input_desc.add().CopyFrom(windex1.desc)
    op.input_desc[-1].name = "windex1"
    op.input.append(windex2.tensor)
    op.input_desc.add().CopyFrom(windex2.desc)
    op.input_desc[-1].name = "windex2"
    op.input.append(windex3.tensor)
    op.input_desc.add().CopyFrom(windex3.desc)
    op.input_desc[-1].name = "windex3"
    op.input.append(hindex1.tensor)
    op.input_desc.add().CopyFrom(hindex1.desc)
    op.input_desc[-1].name = "hindex1"
    op.input.append(hindex2.tensor)
    op.input_desc.add().CopyFrom(hindex2.desc)
    op.input_desc[-1].name = "hindex2"
    op.input.append(hindex3.tensor)
    op.input_desc.add().CopyFrom(hindex3.desc)
    op.input_desc[-1].name = "hindex3"

    # process attrs
    op.attr["biases_low"].list.val_type = 3
    op.attr["biases_low"].list.f.extend(biases_low)
    op.attr["biases_mid"].list.val_type = 3
    op.attr["biases_mid"].list.f.extend(biases_mid)
    op.attr["biases_high"].list.val_type = 3
    op.attr["biases_high"].list.f.extend(biases_high)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR YoloV3DetectionOutputV2
@auto_convert_to_tensor([True], [False])
def YoloV3DetectionOutputV2(x: List[Tensor], *, biases: List[float], boxes: int=3, coords: int=4, classes: int=80, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, N: int=10, resize_origin_img_to_net: bool=False, out_box_dim: int=3, dependencies=[], node_name=None):
    """REG_OP(YoloV3DetectionOutputV2)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases, ListFloat)\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.ATTR(N, Int, 10)\n
.ATTR(resize_origin_img_to_net, Bool, false)\n
.ATTR(out_box_dim, Int, 3)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV3DetectionOutputV2"
    op.name = next_unique_name(node_name, "YoloV3DetectionOutputV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["biases"].list.val_type = 3
    op.attr["biases"].list.f.extend(biases)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn
    op.attr["N"].i = N
    op.attr["resize_origin_img_to_net"].b = resize_origin_img_to_net
    op.attr["out_box_dim"].i = out_box_dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR YoloV3DetectionOutputV2D
@auto_convert_to_tensor([True, True, True], [False, False, False])
def YoloV3DetectionOutputV2D(x: List[Tensor], windex: List[Tensor], hindex: List[Tensor], *, biases: List[float], boxes: int=3, coords: int=4, classes: int=80, relative: bool=True, obj_threshold: float=0.500000, post_nms_topn: int=512, score_threshold: float=0.500000, iou_threshold: float=0.450000, pre_nms_topn: int=512, N: int=10, resize_origin_img_to_net: bool=False, out_box_dim: int=3, dependencies=[], node_name=None):
    """REG_OP(YoloV3DetectionOutputV2D)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_INPUT(windex, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_INPUT(hindex, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(biases, ListFloat)\n
.ATTR(boxes, Int, 3)\n
.ATTR(coords, Int, 4)\n
.ATTR(classes, Int, 80)\n
.ATTR(relative, Bool, true)\n
.ATTR(obj_threshold, Float, 0.5)\n
.ATTR(post_nms_topn, Int, 512)\n
.ATTR(score_threshold, Float, 0.5)\n
.ATTR(iou_threshold, Float, 0.45)\n
.ATTR(pre_nms_topn, Int, 512)\n
.ATTR(N, Int, 10)\n
.ATTR(resize_origin_img_to_net, Bool, false)\n
.ATTR(out_box_dim, Int, 3)\n
.OUTPUT(box_out, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(box_out_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloV3DetectionOutputV2D"
    op.name = next_unique_name(node_name, "YoloV3DetectionOutputV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    if not isinstance(windex, (tuple, list)):
        raise AssertionError("windex must be a tuple or a list.")
    for i, v in enumerate(windex):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "windex" + str(i)
    if not isinstance(hindex, (tuple, list)):
        raise AssertionError("hindex must be a tuple or a list.")
    for i, v in enumerate(hindex):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "hindex" + str(i)

    # process attrs
    op.attr["biases"].list.val_type = 3
    op.attr["biases"].list.f.extend(biases)
    op.attr["boxes"].i = boxes
    op.attr["coords"].i = coords
    op.attr["classes"].i = classes
    op.attr["relative"].b = relative
    op.attr["obj_threshold"].f = obj_threshold
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["pre_nms_topn"].i = pre_nms_topn
    op.attr["N"].i = N
    op.attr["resize_origin_img_to_net"].b = resize_origin_img_to_net
    op.attr["out_box_dim"].i = out_box_dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "box_out"
    box_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "box_out_num"
    box_out_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return box_out, box_out_num


# This api is auto-generated from IR SPP
@auto_convert_to_tensor([False], [False])
def SPP(x: Tensor, *, pyramid_height: int, pool_method: int=0, dependencies=[], node_name=None):
    """REG_OP(SPP)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(pyramid_height, Int)\n
.ATTR(pool_method, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SPP"
    op.name = next_unique_name(node_name, "SPP")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["pyramid_height"].i = pyramid_height
    op.attr["pool_method"].i = pool_method

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ROIPooling
@auto_convert_to_tensor([False, False, False], [False, False, True])
def ROIPooling(x: Tensor, rois: Tensor, roi_actual_num: Optional[Tensor], *, pooled_h: int, pooled_w: int, spatial_scale_h: float, spatial_scale_w: float, dependencies=[], node_name=None):
    """REG_OP(ROIPooling)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(rois, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OPTIONAL_INPUT(roi_actual_num, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(pooled_h, Int)\n
.REQUIRED_ATTR(pooled_w, Int)\n
.REQUIRED_ATTR(spatial_scale_h, Float)\n
.REQUIRED_ATTR(spatial_scale_w, Float)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ROIPooling"
    op.name = next_unique_name(node_name, "ROIPooling")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if roi_actual_num is not None:
        op.input.append(roi_actual_num.tensor)
        op.input_desc.add().CopyFrom(roi_actual_num.desc)
        op.input_desc[-1].name = "roi_actual_num"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "roi_actual_num"

    # process attrs
    op.attr["pooled_h"].i = pooled_h
    op.attr["pooled_w"].i = pooled_w
    op.attr["spatial_scale_h"].f = spatial_scale_h
    op.attr["spatial_scale_w"].f = spatial_scale_w

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DecodeBbox
@auto_convert_to_tensor([False, False], [False, False])
def DecodeBbox(box_predictions: Tensor, anchors: Tensor, *, decode_clip: float, dependencies=[], node_name=None):
    """REG_OP(DecodeBbox)\n
.INPUT(box_predictions, TensorType{DT_FLOAT16})\n
.INPUT(anchors, TensorType{DT_FLOAT16})\n
.OUTPUT(decoded_boxes, TensorType{DT_FLOAT16})\n
.REQUIRED_ATTR(decode_clip, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeBbox"
    op.name = next_unique_name(node_name, "DecodeBbox")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(box_predictions.tensor)
    op.input_desc.add().CopyFrom(box_predictions.desc)
    op.input_desc[-1].name = "box_predictions"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"

    # process attrs
    op.attr["decode_clip"].f = decode_clip

    # process outputs
    output_index = 0
    op.output_desc.add().name = "decoded_boxes"
    decoded_boxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return decoded_boxes


# This api is auto-generated from IR ClipBoxes
@auto_convert_to_tensor([False, False], [False, False])
def ClipBoxes(boxes_input: Tensor, img_size: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ClipBoxes)\n
.INPUT(boxes_input, TensorType({DT_FLOAT16}))\n
.INPUT(img_size, TensorType({DT_INT32}))\n
.OUTPUT(boxes_output, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ClipBoxes"
    op.name = next_unique_name(node_name, "ClipBoxes")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes_input.tensor)
    op.input_desc.add().CopyFrom(boxes_input.desc)
    op.input_desc[-1].name = "boxes_input"
    op.input.append(img_size.tensor)
    op.input_desc.add().CopyFrom(img_size.desc)
    op.input_desc[-1].name = "img_size"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "boxes_output"
    boxes_output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return boxes_output


# This api is auto-generated from IR ClipBoxesD
@auto_convert_to_tensor([False], [False])
def ClipBoxesD(boxes_input: Tensor, *, img_size: List[int], dependencies=[], node_name=None):
    """REG_OP(ClipBoxesD)\n
.INPUT(boxes_input, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(img_size, ListInt)\n
.OUTPUT(boxes_output, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ClipBoxesD"
    op.name = next_unique_name(node_name, "ClipBoxesD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes_input.tensor)
    op.input_desc.add().CopyFrom(boxes_input.desc)
    op.input_desc[-1].name = "boxes_input"

    # process attrs
    op.attr["img_size"].list.val_type = 2
    op.attr["img_size"].list.i.extend(img_size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "boxes_output"
    boxes_output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return boxes_output


# This api is auto-generated from IR FastrcnnPredictions
@auto_convert_to_tensor([False, False], [False, False])
def FastrcnnPredictions(rois: Tensor, score: Tensor, *, nms_threshold: float, score_threshold: float, k: int, dependencies=[], node_name=None):
    """REG_OP(FastrcnnPredictions)\n
.INPUT(rois, TensorType({DT_FLOAT16}))\n
.INPUT(score, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(nms_threshold, Float)\n
.REQUIRED_ATTR(score_threshold, Float)\n
.REQUIRED_ATTR(k, Int)\n
.OUTPUT(sorted_rois, TensorType({DT_FLOAT16}))\n
.OUTPUT(sorted_scores, TensorType({DT_FLOAT16}))\n
.OUTPUT(sorted_classes, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FastrcnnPredictions"
    op.name = next_unique_name(node_name, "FastrcnnPredictions")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    op.input.append(score.tensor)
    op.input_desc.add().CopyFrom(score.desc)
    op.input_desc[-1].name = "score"

    # process attrs
    op.attr["nms_threshold"].f = nms_threshold
    op.attr["score_threshold"].f = score_threshold
    op.attr["k"].i = k

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sorted_rois"
    sorted_rois = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sorted_scores"
    sorted_scores = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sorted_classes"
    sorted_classes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sorted_rois, sorted_scores, sorted_classes


# This api is auto-generated from IR RpnProposals
@auto_convert_to_tensor([False, False, False], [False, False, False])
def RpnProposals(rois: Tensor, cls_bg_prob: Tensor, img_size: Tensor, *, score_threshold: float, k: int, min_size: float, nms_threshold: float, post_nms_num: int, score_filter: bool=True, box_filter: bool=True, score_sigmoid: bool=False, dependencies=[], node_name=None):
    """REG_OP(RpnProposals)\n
.INPUT(rois, TensorType({DT_FLOAT16}))\n
.INPUT(cls_bg_prob, TensorType({DT_FLOAT16}))\n
.INPUT(img_size, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(score_threshold, Float)\n
.REQUIRED_ATTR(k, Int)\n
.REQUIRED_ATTR(min_size, Float)\n
.REQUIRED_ATTR(nms_threshold, Float)\n
.REQUIRED_ATTR(post_nms_num, Int)\n
.ATTR(score_filter, Bool, true)\n
.ATTR(box_filter, Bool, true)\n
.ATTR(score_sigmoid, Bool, false)\n
.OUTPUT(sorted_box, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RpnProposals"
    op.name = next_unique_name(node_name, "RpnProposals")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    op.input.append(cls_bg_prob.tensor)
    op.input_desc.add().CopyFrom(cls_bg_prob.desc)
    op.input_desc[-1].name = "cls_bg_prob"
    op.input.append(img_size.tensor)
    op.input_desc.add().CopyFrom(img_size.desc)
    op.input_desc[-1].name = "img_size"

    # process attrs
    op.attr["score_threshold"].f = score_threshold
    op.attr["k"].i = k
    op.attr["min_size"].f = min_size
    op.attr["nms_threshold"].f = nms_threshold
    op.attr["post_nms_num"].i = post_nms_num
    op.attr["score_filter"].b = score_filter
    op.attr["box_filter"].b = box_filter
    op.attr["score_sigmoid"].b = score_sigmoid

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sorted_box"
    sorted_box = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sorted_box


# This api is auto-generated from IR RpnProposalsD
@auto_convert_to_tensor([False, False], [False, False])
def RpnProposalsD(rois: Tensor, cls_bg_prob: Tensor, *, img_size: List[int], score_threshold: float, k: int, min_size: float, nms_threshold: float, post_nms_num: int, score_filter: bool=True, box_filter: bool=True, score_sigmoid: bool=False, dependencies=[], node_name=None):
    """REG_OP(RpnProposalsD)\n
.INPUT(rois, TensorType({DT_FLOAT16}))\n
.INPUT(cls_bg_prob, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(img_size, ListInt)\n
.REQUIRED_ATTR(score_threshold, Float)\n
.REQUIRED_ATTR(k, Int)\n
.REQUIRED_ATTR(min_size, Float)\n
.REQUIRED_ATTR(nms_threshold, Float)\n
.REQUIRED_ATTR(post_nms_num, Int)\n
.ATTR(score_filter, Bool, true)\n
.ATTR(box_filter, Bool, true)\n
.ATTR(score_sigmoid, Bool, false)\n
.OUTPUT(sorted_box, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RpnProposalsD"
    op.name = next_unique_name(node_name, "RpnProposalsD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    op.input.append(cls_bg_prob.tensor)
    op.input_desc.add().CopyFrom(cls_bg_prob.desc)
    op.input_desc[-1].name = "cls_bg_prob"

    # process attrs
    op.attr["img_size"].list.val_type = 2
    op.attr["img_size"].list.i.extend(img_size)
    op.attr["score_threshold"].f = score_threshold
    op.attr["k"].i = k
    op.attr["min_size"].f = min_size
    op.attr["nms_threshold"].f = nms_threshold
    op.attr["post_nms_num"].i = post_nms_num
    op.attr["score_filter"].b = score_filter
    op.attr["box_filter"].b = box_filter
    op.attr["score_sigmoid"].b = score_sigmoid

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sorted_box"
    sorted_box = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sorted_box


# This api is auto-generated from IR RpnProposalPostProcessing
@auto_convert_to_tensor([False, False], [False, False])
def RpnProposalPostProcessing(sorted_proposal: Tensor, proposal_num: Tensor, *, img_size: List[int], score_threshold: float, k: int, min_size: float, nms_threshold: float, post_nms_num: int, box_filter: bool=True, core_max_num: int=8, dependencies=[], node_name=None):
    """REG_OP(RpnProposalPostProcessing)\n
.INPUT(sorted_proposal, TensorType({DT_FLOAT16}))\n
.INPUT(proposal_num, TensorType({DT_UINT32}))\n
.OUTPUT(sorted_box, TensorType({ DT_FLOAT16}))\n
.REQUIRED_ATTR(img_size, ListInt)\n
.REQUIRED_ATTR(score_threshold, Float)\n
.REQUIRED_ATTR(k, Int)\n
.REQUIRED_ATTR(min_size, Float)\n
.REQUIRED_ATTR(nms_threshold, Float)\n
.REQUIRED_ATTR(post_nms_num, Int)\n
.ATTR(box_filter, Bool, true)\n
.ATTR(core_max_num, Int, 8)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RpnProposalPostProcessing"
    op.name = next_unique_name(node_name, "RpnProposalPostProcessing")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(sorted_proposal.tensor)
    op.input_desc.add().CopyFrom(sorted_proposal.desc)
    op.input_desc[-1].name = "sorted_proposal"
    op.input.append(proposal_num.tensor)
    op.input_desc.add().CopyFrom(proposal_num.desc)
    op.input_desc[-1].name = "proposal_num"

    # process attrs
    op.attr["img_size"].list.val_type = 2
    op.attr["img_size"].list.i.extend(img_size)
    op.attr["score_threshold"].f = score_threshold
    op.attr["k"].i = k
    op.attr["min_size"].f = min_size
    op.attr["nms_threshold"].f = nms_threshold
    op.attr["post_nms_num"].i = post_nms_num
    op.attr["box_filter"].b = box_filter
    op.attr["core_max_num"].i = core_max_num

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sorted_box"
    sorted_box = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sorted_box


# This api is auto-generated from IR DecodeBoundariesTarget
@auto_convert_to_tensor([False, False], [False, False])
def DecodeBoundariesTarget(boundary_predictions: Tensor, anchors: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DecodeBoundariesTarget)\n
.INPUT(boundary_predictions, TensorType({DT_FLOAT16}))\n
.INPUT(anchors, TensorType({DT_FLOAT16}))\n
.OUTPUT(boundary_encoded, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeBoundariesTarget"
    op.name = next_unique_name(node_name, "DecodeBoundariesTarget")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boundary_predictions.tensor)
    op.input_desc.add().CopyFrom(boundary_predictions.desc)
    op.input_desc[-1].name = "boundary_predictions"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "boundary_encoded"
    boundary_encoded = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return boundary_encoded


# This api is auto-generated from IR DecodeCornerpointsTargetBG
@auto_convert_to_tensor([False, False], [False, False])
def DecodeCornerpointsTargetBG(keypoints_prediction: Tensor, anchors: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DecodeCornerpointsTargetBG)\n
.INPUT(keypoints_prediction, TensorType({DT_FLOAT16}))\n
.INPUT(anchors, TensorType({DT_FLOAT16}))\n
.OUTPUT(keypoints_decoded, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeCornerpointsTargetBG"
    op.name = next_unique_name(node_name, "DecodeCornerpointsTargetBG")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(keypoints_prediction.tensor)
    op.input_desc.add().CopyFrom(keypoints_prediction.desc)
    op.input_desc[-1].name = "keypoints_prediction"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "keypoints_decoded"
    keypoints_decoded = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return keypoints_decoded


# This api is auto-generated from IR DecodeCornerpointsTargetWrtCenterV1
@auto_convert_to_tensor([False, False], [False, False])
def DecodeCornerpointsTargetWrtCenterV1(keypoints_prediction: Tensor, anchors: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DecodeCornerpointsTargetWrtCenterV1)\n
.INPUT(keypoints_prediction, TensorType({DT_FLOAT16}))\n
.INPUT(anchors, TensorType({DT_FLOAT16}))\n
.OUTPUT(keypoints_decoded, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeCornerpointsTargetWrtCenterV1"
    op.name = next_unique_name(node_name, "DecodeCornerpointsTargetWrtCenterV1")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(keypoints_prediction.tensor)
    op.input_desc.add().CopyFrom(keypoints_prediction.desc)
    op.input_desc[-1].name = "keypoints_prediction"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "keypoints_decoded"
    keypoints_decoded = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return keypoints_decoded


# This api is auto-generated from IR DecodeWheelsTarget
@auto_convert_to_tensor([False, False], [False, False])
def DecodeWheelsTarget(boundary_predictions: Tensor, anchors: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DecodeWheelsTarget)\n
.INPUT(boundary_predictions, TensorType({DT_FLOAT16}))\n
.INPUT(anchors, TensorType({DT_FLOAT16}))\n
.OUTPUT(boundary_encoded, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeWheelsTarget"
    op.name = next_unique_name(node_name, "DecodeWheelsTarget")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boundary_predictions.tensor)
    op.input_desc.add().CopyFrom(boundary_predictions.desc)
    op.input_desc[-1].name = "boundary_predictions"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "boundary_encoded"
    boundary_encoded = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return boundary_encoded


# This api is auto-generated from IR BatchMultiClassNonMaxSuppression
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def BatchMultiClassNonMaxSuppression(boxes: Tensor, scores: Tensor, clip_window: Optional[Tensor], num_valid_boxes: Optional[Tensor], *, score_threshold: float, iou_threshold: float, max_size_per_class: int, max_total_size: int, change_coordinate_frame: bool=False, transpose_box: bool=False, image_size: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(BatchMultiClassNonMaxSuppression)\n
.INPUT(boxes, TensorType({DT_FLOAT16}))\n
.INPUT(scores, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(clip_window, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(num_valid_boxes, TensorType({DT_INT32}))\n
.OUTPUT(nmsed_boxes, TensorType({DT_FLOAT16}))\n
.OUTPUT(nmsed_scores, TensorType({DT_FLOAT16}))\n
.OUTPUT(nmsed_classes, TensorType({DT_FLOAT16}))\n
.OUTPUT(nmsed_num, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(score_threshold, Float)\n
.REQUIRED_ATTR(iou_threshold, Float)\n
.REQUIRED_ATTR(max_size_per_class, Int)\n
.REQUIRED_ATTR(max_total_size, Int)\n
.ATTR(change_coordinate_frame, Bool, false)\n
.ATTR(transpose_box, Bool, false)\n
.ATTR(image_size, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchMultiClassNonMaxSuppression"
    op.name = next_unique_name(node_name, "BatchMultiClassNonMaxSuppression")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    if clip_window is not None:
        op.input.append(clip_window.tensor)
        op.input_desc.add().CopyFrom(clip_window.desc)
        op.input_desc[-1].name = "clip_window"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "clip_window"
    if num_valid_boxes is not None:
        op.input.append(num_valid_boxes.tensor)
        op.input_desc.add().CopyFrom(num_valid_boxes.desc)
        op.input_desc[-1].name = "num_valid_boxes"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "num_valid_boxes"

    # process attrs
    op.attr["score_threshold"].f = score_threshold
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["max_size_per_class"].i = max_size_per_class
    op.attr["max_total_size"].i = max_total_size
    op.attr["change_coordinate_frame"].b = change_coordinate_frame
    op.attr["transpose_box"].b = transpose_box
    op.attr["image_size"].list.val_type = 2
    op.attr["image_size"].list.i.extend(image_size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "nmsed_boxes"
    nmsed_boxes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nmsed_scores"
    nmsed_scores = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nmsed_classes"
    nmsed_classes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nmsed_num"
    nmsed_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_num


# This api is auto-generated from IR ToAbsoluteBBox
@auto_convert_to_tensor([False, False], [False, False])
def ToAbsoluteBBox(normalized_boxes: Tensor, shape_hw: Tensor, *, reversed_box: bool=False, dependencies=[], node_name=None):
    """REG_OP(ToAbsoluteBBox)\n
.INPUT(normalized_boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(shape_hw, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(reversed_box, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ToAbsoluteBBox"
    op.name = next_unique_name(node_name, "ToAbsoluteBBox")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(normalized_boxes.tensor)
    op.input_desc.add().CopyFrom(normalized_boxes.desc)
    op.input_desc[-1].name = "normalized_boxes"
    op.input.append(shape_hw.tensor)
    op.input_desc.add().CopyFrom(shape_hw.desc)
    op.input_desc[-1].name = "shape_hw"

    # process attrs
    op.attr["reversed_box"].b = reversed_box

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NormalizeBBox
@auto_convert_to_tensor([False, False], [False, False])
def NormalizeBBox(boxes: Tensor, shape_hw: Tensor, *, reversed_box: bool=False, dependencies=[], node_name=None):
    """REG_OP(NormalizeBBox)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(shape_hw, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(reversed_box, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NormalizeBBox"
    op.name = next_unique_name(node_name, "NormalizeBBox")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(shape_hw.tensor)
    op.input_desc.add().CopyFrom(shape_hw.desc)
    op.input_desc[-1].name = "shape_hw"

    # process attrs
    op.attr["reversed_box"].b = reversed_box

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DecodeBboxV2
@auto_convert_to_tensor([False, False], [False, False])
def DecodeBboxV2(boxes: Tensor, anchors: Tensor, *, scales: List[float]=[1.000000, 1.000000, 1.000000, 1.000000], decode_clip: float=0.000000, reversed_box: bool=False, dependencies=[], node_name=None):
    """REG_OP(DecodeBboxV2)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(anchors, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(scales, ListFloat, {1.0, 1.0, 1.0, 1.0})\n
.ATTR(decode_clip, Float, 0.0)\n
.ATTR(reversed_box, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeBboxV2"
    op.name = next_unique_name(node_name, "DecodeBboxV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(anchors.tensor)
    op.input_desc.add().CopyFrom(anchors.desc)
    op.input_desc[-1].name = "anchors"

    # process attrs
    op.attr["scales"].list.val_type = 3
    op.attr["scales"].list.f.extend(scales)
    op.attr["decode_clip"].f = decode_clip
    op.attr["reversed_box"].b = reversed_box

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Sort
@auto_convert_to_tensor([False], [False])
def Sort(x: Tensor, *, axis: int=-1, descending: bool=False, stable: bool=False, dependencies=[], node_name=None):
    """REG_OP(Sort)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT16, DT_INT8, DT_UINT8, DT_INT32, DT_INT64}))\n
.OUTPUT(y1, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT16, DT_INT8, DT_UINT8, DT_INT32, DT_INT64}))\n
.OUTPUT(y2, TensorType({DT_INT32}))\n
.ATTR(axis, Int, -1)\n
.ATTR(descending, Bool, false)\n
.ATTR(stable, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Sort"
    op.name = next_unique_name(node_name, "Sort")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["descending"].b = descending
    op.attr["stable"].b = stable

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR PtIou
@auto_convert_to_tensor([False, False], [False, False])
def PtIou(bboxes: Tensor, gtboxes: Tensor, *, mode: str="iou", dependencies=[], node_name=None):
    """REG_OP(PtIou)\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(overlap, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(mode, String, "iou")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PtIou"
    op.name = next_unique_name(node_name, "PtIou")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "overlap"
    overlap = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return overlap


# This api is auto-generated from IR NonMaxSuppressionV6
@auto_convert_to_tensor([False, False, False, False, False], [False, False, True, True, True])
def NonMaxSuppressionV6(boxes: Tensor, scores: Tensor, max_output_size: Optional[Tensor], iou_threshold: Optional[Tensor], score_threshold: Optional[Tensor], *, center_point_box: int=0, max_boxes_size: int=0, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionV6)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(max_output_size, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(iou_threshold, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(score_threshold, TensorType({DT_FLOAT}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
.ATTR(center_point_box, Int, 0)\n
.ATTR(max_boxes_size, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionV6"
    op.name = next_unique_name(node_name, "NonMaxSuppressionV6")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    if max_output_size is not None:
        op.input.append(max_output_size.tensor)
        op.input_desc.add().CopyFrom(max_output_size.desc)
        op.input_desc[-1].name = "max_output_size"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "max_output_size"
    if iou_threshold is not None:
        op.input.append(iou_threshold.tensor)
        op.input_desc.add().CopyFrom(iou_threshold.desc)
        op.input_desc[-1].name = "iou_threshold"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "iou_threshold"
    if score_threshold is not None:
        op.input.append(score_threshold.tensor)
        op.input_desc.add().CopyFrom(score_threshold.desc)
        op.input_desc[-1].name = "score_threshold"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "score_threshold"

    # process attrs
    op.attr["center_point_box"].i = center_point_box
    op.attr["max_boxes_size"].i = max_boxes_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices


# This api is auto-generated from IR NonMaxSuppressionV7
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, True, True, True, True])
def NonMaxSuppressionV7(boxes: Tensor, scores: Tensor, max_output_size: Optional[Tensor], iou_threshold: Optional[Tensor], score_threshold: Optional[Tensor], index_id: Optional[Tensor], *, center_point_box: int=0, max_boxes_size: int=0, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionV7)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(max_output_size, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(iou_threshold, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(score_threshold, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(index_id, TensorType({DT_FLOAT16}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
.ATTR(center_point_box, Int, 0)\n
.ATTR(max_boxes_size, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionV7"
    op.name = next_unique_name(node_name, "NonMaxSuppressionV7")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    if max_output_size is not None:
        op.input.append(max_output_size.tensor)
        op.input_desc.add().CopyFrom(max_output_size.desc)
        op.input_desc[-1].name = "max_output_size"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "max_output_size"
    if iou_threshold is not None:
        op.input.append(iou_threshold.tensor)
        op.input_desc.add().CopyFrom(iou_threshold.desc)
        op.input_desc[-1].name = "iou_threshold"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "iou_threshold"
    if score_threshold is not None:
        op.input.append(score_threshold.tensor)
        op.input_desc.add().CopyFrom(score_threshold.desc)
        op.input_desc[-1].name = "score_threshold"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "score_threshold"
    if index_id is not None:
        op.input.append(index_id.tensor)
        op.input_desc.add().CopyFrom(index_id.desc)
        op.input_desc[-1].name = "index_id"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "index_id"

    # process attrs
    op.attr["center_point_box"].i = center_point_box
    op.attr["max_boxes_size"].i = max_boxes_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices


# This api is auto-generated from IR RoiExtractor
@auto_convert_to_tensor([True, False, False], [False, False, True])
def RoiExtractor(features: List[Tensor], rois: Tensor, index: Optional[Tensor], *, finest_scale: int=56, roi_scale_factor: float=0.000000, spatial_scale: List[float]=[0.250000, 0.125000, 0.062500, 0.031250], pooled_height: int=7, pooled_width: int=7, sample_num: int=0, pool_mode: str="avg", aligned: bool=True, dependencies=[], node_name=None):
    """REG_OP(RoiExtractor)\n
.DYNAMIC_INPUT(features, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(index, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(finest_scale, Int, 56)\n
.ATTR(roi_scale_factor, Float, 0)\n
.ATTR(spatial_scale, ListFloat, {1.f / 4, 1.f / 8, 1.f / 16, 1.f / 32})\n
.ATTR(pooled_height, Int, 7)\n
.ATTR(pooled_width, Int, 7)\n
.ATTR(sample_num, Int, 0)\n
.ATTR(pool_mode, String, "avg")\n
.ATTR(aligned, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RoiExtractor"
    op.name = next_unique_name(node_name, "RoiExtractor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(features, (tuple, list)):
        raise AssertionError("features must be a tuple or a list.")
    for i, v in enumerate(features):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "features" + str(i)
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if index is not None:
        op.input.append(index.tensor)
        op.input_desc.add().CopyFrom(index.desc)
        op.input_desc[-1].name = "index"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "index"

    # process attrs
    op.attr["finest_scale"].i = finest_scale
    op.attr["roi_scale_factor"].f = roi_scale_factor
    op.attr["spatial_scale"].list.val_type = 3
    op.attr["spatial_scale"].list.f.extend(spatial_scale)
    op.attr["pooled_height"].i = pooled_height
    op.attr["pooled_width"].i = pooled_width
    op.attr["sample_num"].i = sample_num
    op.attr["pool_mode"].s = compat_as_bytes(pool_mode)
    op.attr["aligned"].b = aligned

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PSROIPoolingV2
@auto_convert_to_tensor([False, False], [False, False])
def PSROIPoolingV2(x: Tensor, rois: Tensor, *, spatial_scale: float, output_dim: int, group_size: int, dependencies=[], node_name=None):
    """REG_OP(PSROIPoolingV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.REQUIRED_ATTR(output_dim, Int)\n
.REQUIRED_ATTR(group_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PSROIPoolingV2"
    op.name = next_unique_name(node_name, "PSROIPoolingV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"

    # process attrs
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["output_dim"].i = output_dim
    op.attr["group_size"].i = group_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PSROIPoolingGradV2D
@auto_convert_to_tensor([False, False], [False, False])
def PSROIPoolingGradV2D(x: Tensor, rois: Tensor, *, spatial_scale: float, output_dim: int, group_size: int, input_size: List[int], dependencies=[], node_name=None):
    """REG_OP(PSROIPoolingGradV2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(spatial_scale, Float)\n
.REQUIRED_ATTR(output_dim, Int)\n
.REQUIRED_ATTR(group_size, Int)\n
.REQUIRED_ATTR(input_size, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PSROIPoolingGradV2D"
    op.name = next_unique_name(node_name, "PSROIPoolingGradV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"

    # process attrs
    op.attr["spatial_scale"].f = spatial_scale
    op.attr["output_dim"].i = output_dim
    op.attr["group_size"].i = group_size
    op.attr["input_size"].list.val_type = 2
    op.attr["input_size"].list.i.extend(input_size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AnchorResponseFlags
@auto_convert_to_tensor([False], [False])
def AnchorResponseFlags(gt_bboxes: Tensor, *, featmap_size: List[int], strides: List[int], num_base_anchors: int, dependencies=[], node_name=None):
    """REG_OP(AnchorResponseFlags)\n
.INPUT(gt_bboxes, TensorType({DT_FLOAT}))\n
.OUTPUT(flags, TensorType({DT_UINT8}))\n
.REQUIRED_ATTR(featmap_size, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(num_base_anchors, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AnchorResponseFlags"
    op.name = next_unique_name(node_name, "AnchorResponseFlags")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gt_bboxes.tensor)
    op.input_desc.add().CopyFrom(gt_bboxes.desc)
    op.input_desc[-1].name = "gt_bboxes"

    # process attrs
    op.attr["featmap_size"].list.val_type = 2
    op.attr["featmap_size"].list.i.extend(featmap_size)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["num_base_anchors"].i = num_base_anchors

    # process outputs
    output_index = 0
    op.output_desc.add().name = "flags"
    flags = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return flags


# This api is auto-generated from IR YoloBoxesEncode
@auto_convert_to_tensor([False, False, False], [False, False, False])
def YoloBoxesEncode(anchor_boxes: Tensor, gt_bboxes: Tensor, stride: Tensor, *, performance_mode: str="high_precision", dependencies=[], node_name=None):
    """REG_OP(YoloBoxesEncode)\n
.INPUT(anchor_boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gt_bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(stride, TensorType({DT_INT32}))\n
.ATTR(performance_mode, String, "high_precision")\n
.OUTPUT(encoded_bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloBoxesEncode"
    op.name = next_unique_name(node_name, "YoloBoxesEncode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(anchor_boxes.tensor)
    op.input_desc.add().CopyFrom(anchor_boxes.desc)
    op.input_desc[-1].name = "anchor_boxes"
    op.input.append(gt_bboxes.tensor)
    op.input_desc.add().CopyFrom(gt_bboxes.desc)
    op.input_desc[-1].name = "gt_bboxes"
    op.input.append(stride.tensor)
    op.input_desc.add().CopyFrom(stride.desc)
    op.input_desc[-1].name = "stride"

    # process attrs
    op.attr["performance_mode"].s = compat_as_bytes(performance_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "encoded_bboxes"
    encoded_bboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return encoded_bboxes


# This api is auto-generated from IR GridAssignPositive
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False])
def GridAssignPositive(assigned_gt_inds: Tensor, overlaps: Tensor, box_responsible_flags: Tensor, max_overlaps: Tensor, argmax_overlaps: Tensor, gt_max_overlaps: Tensor, gt_argmax_overlaps: Tensor, num_gts: Tensor, *, pos_iou_thr: float, min_pos_iou: float, gt_max_assign_all: bool, dependencies=[], node_name=None):
    """REG_OP(GridAssignPositive)\n
.INPUT(assigned_gt_inds, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(overlaps, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(box_responsible_flags, TensorType({ DT_UINT8 }))\n
.INPUT(max_overlaps, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(argmax_overlaps, TensorType({ DT_INT32 }))\n
.INPUT(gt_max_overlaps, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(gt_argmax_overlaps, TensorType({ DT_INT32 }))\n
.INPUT(num_gts, TensorType({ DT_INT32 }))\n
.OUTPUT(assigned_gt_inds_pos, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(pos_iou_thr, Float)\n
.REQUIRED_ATTR(min_pos_iou, Float)\n
.REQUIRED_ATTR(gt_max_assign_all, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GridAssignPositive"
    op.name = next_unique_name(node_name, "GridAssignPositive")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(assigned_gt_inds.tensor)
    op.input_desc.add().CopyFrom(assigned_gt_inds.desc)
    op.input_desc[-1].name = "assigned_gt_inds"
    op.input.append(overlaps.tensor)
    op.input_desc.add().CopyFrom(overlaps.desc)
    op.input_desc[-1].name = "overlaps"
    op.input.append(box_responsible_flags.tensor)
    op.input_desc.add().CopyFrom(box_responsible_flags.desc)
    op.input_desc[-1].name = "box_responsible_flags"
    op.input.append(max_overlaps.tensor)
    op.input_desc.add().CopyFrom(max_overlaps.desc)
    op.input_desc[-1].name = "max_overlaps"
    op.input.append(argmax_overlaps.tensor)
    op.input_desc.add().CopyFrom(argmax_overlaps.desc)
    op.input_desc[-1].name = "argmax_overlaps"
    op.input.append(gt_max_overlaps.tensor)
    op.input_desc.add().CopyFrom(gt_max_overlaps.desc)
    op.input_desc[-1].name = "gt_max_overlaps"
    op.input.append(gt_argmax_overlaps.tensor)
    op.input_desc.add().CopyFrom(gt_argmax_overlaps.desc)
    op.input_desc[-1].name = "gt_argmax_overlaps"
    op.input.append(num_gts.tensor)
    op.input_desc.add().CopyFrom(num_gts.desc)
    op.input_desc[-1].name = "num_gts"

    # process attrs
    op.attr["pos_iou_thr"].f = pos_iou_thr
    op.attr["min_pos_iou"].f = min_pos_iou
    op.attr["gt_max_assign_all"].b = gt_max_assign_all

    # process outputs
    output_index = 0
    op.output_desc.add().name = "assigned_gt_inds_pos"
    assigned_gt_inds_pos = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return assigned_gt_inds_pos


# This api is auto-generated from IR GIoUGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def GIoUGrad(dy: Tensor, bboxes: Tensor, gtboxes: Tensor, *, trans: bool=False, is_cross: bool=True, mode: str="iou", dependencies=[], node_name=None):
    """REG_OP(GIoUGrad)\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dbboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
.ATTR(is_cross, Bool, true)\n
.ATTR(mode, String, "iou")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GIoUGrad"
    op.name = next_unique_name(node_name, "GIoUGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs
    op.attr["trans"].b = trans
    op.attr["is_cross"].b = is_cross
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dbboxes"
    dbboxes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgtboxes"
    dgtboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dbboxes, dgtboxes


# This api is auto-generated from IR DIoUGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DIoUGrad(dy: Tensor, bboxes: Tensor, gtboxes: Tensor, *, trans: bool=False, is_cross: bool=True, mode: str="iou", dependencies=[], node_name=None):
    """REG_OP(DIoUGrad)\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dbboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
.ATTR(is_cross, Bool, true)\n
.ATTR(mode, String, "iou")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DIoUGrad"
    op.name = next_unique_name(node_name, "DIoUGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs
    op.attr["trans"].b = trans
    op.attr["is_cross"].b = is_cross
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dbboxes"
    dbboxes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgtboxes"
    dgtboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dbboxes, dgtboxes


# This api is auto-generated from IR CIoUGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def CIoUGrad(dy: Tensor, bboxes: Tensor, gtboxes: Tensor, atan_sub: Tensor, *, trans: bool=False, is_cross: bool=True, mode: str="iou", dependencies=[], node_name=None):
    """REG_OP(CIoUGrad)\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(atan_sub, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dbboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
.ATTR(is_cross, Bool, true)\n
.ATTR(mode, String, "iou")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CIoUGrad"
    op.name = next_unique_name(node_name, "CIoUGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"
    op.input.append(atan_sub.tensor)
    op.input_desc.add().CopyFrom(atan_sub.desc)
    op.input_desc[-1].name = "atan_sub"

    # process attrs
    op.attr["trans"].b = trans
    op.attr["is_cross"].b = is_cross
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dbboxes"
    dbboxes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgtboxes"
    dgtboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dbboxes, dgtboxes


# This api is auto-generated from IR RotatedOverlaps
@auto_convert_to_tensor([False, False], [False, False])
def RotatedOverlaps(boxes: Tensor, query_boxes: Tensor, *, trans: bool=False, dependencies=[], node_name=None):
    """REG_OP(RotatedOverlaps)\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(query_boxes, TensorType({DT_FLOAT}))\n
.OUTPUT(overlaps, TensorType({DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotatedOverlaps"
    op.name = next_unique_name(node_name, "RotatedOverlaps")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(query_boxes.tensor)
    op.input_desc.add().CopyFrom(query_boxes.desc)
    op.input_desc[-1].name = "query_boxes"

    # process attrs
    op.attr["trans"].b = trans

    # process outputs
    output_index = 0
    op.output_desc.add().name = "overlaps"
    overlaps = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return overlaps


# This api is auto-generated from IR RotatedIou
@auto_convert_to_tensor([False, False], [False, False])
def RotatedIou(boxes: Tensor, query_boxes: Tensor, *, trans: bool=False, mode: str="iou", is_cross: bool=True, v_threshold: float=0.000000, e_threshold: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(RotatedIou)\n
.INPUT(boxes, TensorType({DT_FLOAT}))\n
.INPUT(query_boxes, TensorType({DT_FLOAT}))\n
.OUTPUT(iou, TensorType({DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
.ATTR(mode, String, "iou")\n
.ATTR(is_cross, Bool, true)\n
.ATTR(v_threshold, Float, 0)\n
.ATTR(e_threshold, Float, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotatedIou"
    op.name = next_unique_name(node_name, "RotatedIou")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(query_boxes.tensor)
    op.input_desc.add().CopyFrom(query_boxes.desc)
    op.input_desc[-1].name = "query_boxes"

    # process attrs
    op.attr["trans"].b = trans
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["is_cross"].b = is_cross
    op.attr["v_threshold"].f = v_threshold
    op.attr["e_threshold"].f = e_threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "iou"
    iou = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return iou


# This api is auto-generated from IR RotatedBoxEncode
@auto_convert_to_tensor([False, False], [False, False])
def RotatedBoxEncode(anchor_box: Tensor, gt_box: Tensor, *, weight: List[float]=[1.000000, 1.000000, 1.000000, 1.000000, 1.000000], dependencies=[], node_name=None):
    """REG_OP(RotatedBoxEncode)\n
.INPUT(anchor_box, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gt_box, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(weight, ListFloat, {1.0, 1.0, 1.0, 1.0, 1.0})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotatedBoxEncode"
    op.name = next_unique_name(node_name, "RotatedBoxEncode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(anchor_box.tensor)
    op.input_desc.add().CopyFrom(anchor_box.desc)
    op.input_desc[-1].name = "anchor_box"
    op.input.append(gt_box.tensor)
    op.input_desc.add().CopyFrom(gt_box.desc)
    op.input_desc[-1].name = "gt_box"

    # process attrs
    op.attr["weight"].list.val_type = 3
    op.attr["weight"].list.f.extend(weight)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RotatedBoxDecode
@auto_convert_to_tensor([False, False], [False, False])
def RotatedBoxDecode(anchor_box: Tensor, deltas: Tensor, *, weight: List[float]=[1.000000, 1.000000, 1.000000, 1.000000, 1.000000], dependencies=[], node_name=None):
    """REG_OP(RotatedBoxDecode)\n
.INPUT(anchor_box, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(deltas, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(weight, ListFloat, {1.0, 1.0, 1.0, 1.0, 1.0})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotatedBoxDecode"
    op.name = next_unique_name(node_name, "RotatedBoxDecode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(anchor_box.tensor)
    op.input_desc.add().CopyFrom(anchor_box.desc)
    op.input_desc[-1].name = "anchor_box"
    op.input.append(deltas.tensor)
    op.input_desc.add().CopyFrom(deltas.desc)
    op.input_desc[-1].name = "deltas"

    # process attrs
    op.attr["weight"].list.val_type = 3
    op.attr["weight"].list.f.extend(weight)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BalanceRois
@auto_convert_to_tensor([False], [False])
def BalanceRois(rois: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BalanceRois)\n
.INPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(balance_rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(index, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BalanceRois"
    op.name = next_unique_name(node_name, "BalanceRois")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "balance_rois"
    balance_rois = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "index"
    index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return balance_rois, index


# This api is auto-generated from IR CIoU
@auto_convert_to_tensor([False, False], [False, False])
def CIoU(bboxes: Tensor, gtboxes: Tensor, *, trans: bool=False, is_cross: bool=True, mode: str="iou", atan_sub_flag: bool=False, dependencies=[], node_name=None):
    """REG_OP(CIoU)\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(overlap, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(atan_sub, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
.ATTR(is_cross, Bool, true)\n
.ATTR(mode, String, "iou")\n
.ATTR(atan_sub_flag, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CIoU"
    op.name = next_unique_name(node_name, "CIoU")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs
    op.attr["trans"].b = trans
    op.attr["is_cross"].b = is_cross
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["atan_sub_flag"].b = atan_sub_flag

    # process outputs
    output_index = 0
    op.output_desc.add().name = "overlap"
    overlap = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "atan_sub"
    atan_sub = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return overlap, atan_sub


# This api is auto-generated from IR DIoU
@auto_convert_to_tensor([False, False], [False, False])
def DIoU(bboxes: Tensor, gtboxes: Tensor, *, trans: bool=False, is_cross: bool=True, mode: str="iou", dependencies=[], node_name=None):
    """REG_OP(DIoU)\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(overlap, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(trans, Bool, false)\n
.ATTR(is_cross, Bool, true)\n
.ATTR(mode, String, "iou")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DIoU"
    op.name = next_unique_name(node_name, "DIoU")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs
    op.attr["trans"].b = trans
    op.attr["is_cross"].b = is_cross
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "overlap"
    overlap = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return overlap


# This api is auto-generated from IR Iou3D
@auto_convert_to_tensor([False, False], [False, False])
def Iou3D(bboxes: Tensor, gtboxes: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Iou3D)\n
.INPUT(bboxes, TensorType({DT_FLOAT}))\n
.INPUT(gtboxes, TensorType({DT_FLOAT}))\n
.OUTPUT(iou, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Iou3D"
    op.name = next_unique_name(node_name, "Iou3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"
    op.input.append(gtboxes.tensor)
    op.input_desc.add().CopyFrom(gtboxes.desc)
    op.input_desc[-1].name = "gtboxes"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "iou"
    iou = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return iou


# This api is auto-generated from IR YoloxBoundingBoxDecode
@auto_convert_to_tensor([False, False], [False, False])
def YoloxBoundingBoxDecode(priors: Tensor, bboxes: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(YoloxBoundingBoxDecode)\n
.INPUT(priors, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(decoded_bboxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "YoloxBoundingBoxDecode"
    op.name = next_unique_name(node_name, "YoloxBoundingBoxDecode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(priors.tensor)
    op.input_desc.add().CopyFrom(priors.desc)
    op.input_desc[-1].name = "priors"
    op.input.append(bboxes.tensor)
    op.input_desc.add().CopyFrom(bboxes.desc)
    op.input_desc[-1].name = "bboxes"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "decoded_bboxes"
    decoded_bboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return decoded_bboxes


# This api is auto-generated from IR RoiPoolingWithArgMax
@auto_convert_to_tensor([False, False, False], [False, False, True])
def RoiPoolingWithArgMax(x: Tensor, rois: Tensor, roi_actual_num: Optional[Tensor], *, pooled_h: int, pooled_w: int, spatial_scale_h: float, spatial_scale_w: float, pool_channel: int, dependencies=[], node_name=None):
    """REG_OP(RoiPoolingWithArgMax)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(rois, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OPTIONAL_INPUT(roi_actual_num, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(pooled_h, Int)\n
.REQUIRED_ATTR(pooled_w, Int)\n
.REQUIRED_ATTR(spatial_scale_h, Float)\n
.REQUIRED_ATTR(spatial_scale_w, Float)\n
.REQUIRED_ATTR(pool_channel, Int)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(argmax, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RoiPoolingWithArgMax"
    op.name = next_unique_name(node_name, "RoiPoolingWithArgMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if roi_actual_num is not None:
        op.input.append(roi_actual_num.tensor)
        op.input_desc.add().CopyFrom(roi_actual_num.desc)
        op.input_desc[-1].name = "roi_actual_num"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "roi_actual_num"

    # process attrs
    op.attr["pooled_h"].i = pooled_h
    op.attr["pooled_w"].i = pooled_w
    op.attr["spatial_scale_h"].f = spatial_scale_h
    op.attr["spatial_scale_w"].f = spatial_scale_w
    op.attr["pool_channel"].i = pool_channel

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, argmax


# This api is auto-generated from IR RoiPoolingGradWithArgMax
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, False])
def RoiPoolingGradWithArgMax(grad: Tensor, x: Tensor, rois: Tensor, roi_actual_num: Optional[Tensor], argmax: Tensor, *, pooled_h: int, pooled_w: int, spatial_scale_h: float, spatial_scale_w: float, pool_channel: int, dependencies=[], node_name=None):
    """REG_OP(RoiPoolingGradWithArgMax)\n
.INPUT(grad, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(rois, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OPTIONAL_INPUT(roi_actual_num, TensorType({DT_INT32}))\n
.INPUT(argmax, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(pooled_h, Int)\n
.REQUIRED_ATTR(pooled_w, Int)\n
.REQUIRED_ATTR(spatial_scale_h, Float)\n
.REQUIRED_ATTR(spatial_scale_w, Float)\n
.REQUIRED_ATTR(pool_channel, Int)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RoiPoolingGradWithArgMax"
    op.name = next_unique_name(node_name, "RoiPoolingGradWithArgMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rois.tensor)
    op.input_desc.add().CopyFrom(rois.desc)
    op.input_desc[-1].name = "rois"
    if roi_actual_num is not None:
        op.input.append(roi_actual_num.tensor)
        op.input_desc.add().CopyFrom(roi_actual_num.desc)
        op.input_desc[-1].name = "roi_actual_num"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "roi_actual_num"
    op.input.append(argmax.tensor)
    op.input_desc.add().CopyFrom(argmax.desc)
    op.input_desc[-1].name = "argmax"

    # process attrs
    op.attr["pooled_h"].i = pooled_h
    op.attr["pooled_w"].i = pooled_w
    op.attr["spatial_scale_h"].f = spatial_scale_h
    op.attr["spatial_scale_w"].f = spatial_scale_w
    op.attr["pool_channel"].i = pool_channel

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RotaryMul
@auto_convert_to_tensor([False, False, False], [False, False, False])
def RotaryMul(x: Tensor, r1: Tensor, r2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RotaryMul)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BFLOAT16}))\n
.INPUT(r1, TensorType({DT_FLOAT16, DT_FLOAT, DT_BFLOAT16}))\n
.INPUT(r2, TensorType({DT_FLOAT16, DT_FLOAT, DT_BFLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BFLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotaryMul"
    op.name = next_unique_name(node_name, "RotaryMul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(r1.tensor)
    op.input_desc.add().CopyFrom(r1.desc)
    op.input_desc[-1].name = "r1"
    op.input.append(r2.tensor)
    op.input_desc.add().CopyFrom(r2.desc)
    op.input_desc[-1].name = "r2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RotaryMulGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def RotaryMulGrad(x: Tensor, r1: Tensor, r2: Tensor, dy:  Tensor, *, need_backward: bool=True, dependencies=[], node_name=None):
    """REG_OP(RotaryMulGrad)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BFLOAT16}))\n
.INPUT(r1, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BFLOAT16}))\n
.INPUT(r2, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BFLOAT16}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BFLOAT16}))\n
.ATTR(need_backward, Bool, true)\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BFLOAT16}))\n
.OUTPUT(dr1, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BFLOAT16}))\n
.OUTPUT(dr2, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BFLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RotaryMulGrad"
    op.name = next_unique_name(node_name, "RotaryMulGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(r1.tensor)
    op.input_desc.add().CopyFrom(r1.desc)
    op.input_desc[-1].name = "r1"
    op.input.append(r2.tensor)
    op.input_desc.add().CopyFrom(r2.desc)
    op.input_desc[-1].name = "r2"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs
    op.attr["need_backward"].b = need_backward
    
    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dr1"
    dr1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dr2"
    dr2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx, dr1, dr2


# This api is auto-generated from IR AddRmsNorm
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AddRmsNorm(x1: Tensor, x2: Tensor, gamma: Tensor, *, epsilon: float=0.000001, dependencies=[], node_name=None):
    """REG_OP(AddRmsNorm)\n
.INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(rstd, TensorType({DT_FLOAT, DT_FLOAT, DT_FLOAT}))\n
.OUTPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.ATTR(epsilon, Float, 1e-6)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AddRmsNorm"
    op.name = next_unique_name(node_name, "AddRmsNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rstd"
    rstd = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "x"
    x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, rstd, x


# This api is auto-generated from IR ApplyRotaryPosEmb
@auto_convert_to_tensor([False, False, False, False],
                        [False, False, False, False])
def ApplyRotaryPosEmb(q: Tensor,
                      k: Tensor,
                      cos: Tensor,
                      sin: Tensor,
                      *,
                      layout: int = 1,
                      dependencies = [],
                      node_name = None):
    """REG_OP(ApplyRotaryPosEmb)\n
.INPUT(q, TensorType({DT_FLOAT16, DT_BFLOAT16, DT_FLOAT}))\n
.INPUT(k, TensorType({DT_FLOAT16, DT_BFLOAT16, DT_FLOAT}))\n
.INPUT(cos, TensorType({DT_FLOAT16, DT_BFLOAT16, DT_FLOAT}))\n
.INPUT(sin, TensorType({DT_FLOAT16, DT_BFLOAT16, DT_FLOAT}))\n
.ATTR(layout, Int, 1)\n
.OUTPUT(q, TensorType({DT_FLOAT16, DT_BFLOAT16, DT_FLOAT}))\n
.OUTPUT(k, TensorType({DT_FLOAT16, DT_BFLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyRotaryPosEmb"
    op.name = next_unique_name(node_name, "ApplyRotaryPosEmb")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(q.tensor)
    op.input_desc.add().CopyFrom(q.desc)
    op.input_desc[-1].name = "q"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"
    op.input.append(cos.tensor)
    op.input_desc.add().CopyFrom(cos.desc)
    op.input_desc[-1].name = "cos"
    op.input.append(sin.tensor)
    op.input_desc.add().CopyFrom(sin.desc)
    op.input_desc[-1].name = "sin"

    # process attrs
    op.attr["layout"].i = layout

    # process outputs
    output_index = 0
    op.output_desc.add().name = "q"
    q = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "l"
    l = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return q, l


# This api is auto-generated from IR LogSoftmaxGrad
@auto_convert_to_tensor([False, False], [False, False])
def LogSoftmaxGrad(grad: Tensor, x: Tensor, *, axis: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(LogSoftmaxGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(axis, ListInt, {-1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogSoftmaxGrad"
    op.name = next_unique_name(node_name, "LogSoftmaxGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseSoftmaxCrossEntropyWithLogits
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def SparseSoftmaxCrossEntropyWithLogits(features: Tensor, labels: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSoftmaxCrossEntropyWithLogits)\n
.INPUT(features, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT}))\n
.INPUT(labels, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(loss, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(backprop, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSoftmaxCrossEntropyWithLogits"
    op.name = next_unique_name(node_name, "SparseSoftmaxCrossEntropyWithLogits")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "backprop"
    backprop = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss, backprop


# This api is auto-generated from IR SoftmaxCrossEntropyWithLogits
@auto_convert_to_tensor([False, False], [False, False])
def SoftmaxCrossEntropyWithLogits(features: Tensor, labels: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SoftmaxCrossEntropyWithLogits)\n
.INPUT(features, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT}))\n
.INPUT(labels, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(loss, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(backprop, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxCrossEntropyWithLogits"
    op.name = next_unique_name(node_name, "SoftmaxCrossEntropyWithLogits")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "backprop"
    backprop = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss, backprop


# This api is auto-generated from IR SoftmaxGrad
@auto_convert_to_tensor([False, False], [False, False])
def SoftmaxGrad(softmax: Tensor, grad_softmax: Tensor, *, axes: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(SoftmaxGrad)\n
.INPUT(softmax, TensorType({ DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8 }))\n
.INPUT(grad_softmax, TensorType({ DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8 }))\n
.OUTPUT(grad_x, TensorType({ DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8 }))\n
.ATTR(axes, ListInt, {-1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxGrad"
    op.name = next_unique_name(node_name, "SoftmaxGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(softmax.tensor)
    op.input_desc.add().CopyFrom(softmax.desc)
    op.input_desc[-1].name = "softmax"
    op.input.append(grad_softmax.tensor)
    op.input_desc.add().CopyFrom(grad_softmax.desc)
    op.input_desc[-1].name = "grad_softmax"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_x"
    grad_x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_x


# This api is auto-generated from IR SigmoidCrossEntropyWithLogitsGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SigmoidCrossEntropyWithLogitsGrad(predict: Tensor, target: Tensor, dout: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SigmoidCrossEntropyWithLogitsGrad)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dout, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(gradient, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SigmoidCrossEntropyWithLogitsGrad"
    op.name = next_unique_name(node_name, "SigmoidCrossEntropyWithLogitsGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "gradient"
    gradient = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return gradient


# This api is auto-generated from IR SigmoidCrossEntropyWithLogits
@auto_convert_to_tensor([False, False], [False, False])
def SigmoidCrossEntropyWithLogits(predict: Tensor, target: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SigmoidCrossEntropyWithLogits)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(loss, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SigmoidCrossEntropyWithLogits"
    op.name = next_unique_name(node_name, "SigmoidCrossEntropyWithLogits")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss


# This api is auto-generated from IR SigmoidCrossEntropyWithLogitsV2
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def SigmoidCrossEntropyWithLogitsV2(predict: Tensor, target: Tensor, weight: Optional[Tensor], pos_weight: Optional[Tensor], *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SigmoidCrossEntropyWithLogitsV2)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(pos_weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(loss, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SigmoidCrossEntropyWithLogitsV2"
    op.name = next_unique_name(node_name, "SigmoidCrossEntropyWithLogitsV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"
    if pos_weight is not None:
        op.input.append(pos_weight.tensor)
        op.input_desc.add().CopyFrom(pos_weight.desc)
        op.input_desc[-1].name = "pos_weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pos_weight"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss


# This api is auto-generated from IR SigmoidFocalLoss
@auto_convert_to_tensor([False, False, False], [False, False, True])
def SigmoidFocalLoss(pred: Tensor, target: Tensor, weight: Optional[Tensor], *, gamma: float=2.000000, alpha: float=0.250000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SigmoidFocalLoss)\n
.INPUT(pred, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(target, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(gamma, Float, 2.0)\n
.ATTR(alpha, Float, 0.25)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SigmoidFocalLoss"
    op.name = next_unique_name(node_name, "SigmoidFocalLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(pred.tensor)
    op.input_desc.add().CopyFrom(pred.desc)
    op.input_desc[-1].name = "pred"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["gamma"].f = gamma
    op.attr["alpha"].f = alpha
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DynamicQuant
@auto_convert_to_tensor([False, False], [False, True])
def DynamicQuant(x: Tensor, smooth_scales: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(DynamicQuant)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(smooth_scales, TensorType({DT_FLOAT16, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_INT8}))\n
.OUTPUT(scale, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicQuant"
    op.name = next_unique_name(node_name, "DynamicQuant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    if smooth_scales is not None:
        op.input.append(smooth_scales.tensor)
        op.input_desc.add().CopyFrom(smooth_scales.desc)
        op.input_desc[-1].name = "smooth_scales"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "smooth_scales"

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "scale"
    scale = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, scale


# This api is auto-generated from IR SoftmaxFocalLoss
@auto_convert_to_tensor([False, False, False], [False, False, True])
def SoftmaxFocalLoss(pred: Tensor, target: Tensor, weight: Optional[Tensor], *, gamma: float=2.000000, alpha: float=0.250000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SoftmaxFocalLoss)\n
.INPUT(pred, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(target, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(gamma, Float, 2.0)\n
.ATTR(alpha, Float, 0.25)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxFocalLoss"
    op.name = next_unique_name(node_name, "SoftmaxFocalLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(pred.tensor)
    op.input_desc.add().CopyFrom(pred.desc)
    op.input_desc[-1].name = "pred"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["gamma"].f = gamma
    op.attr["alpha"].f = alpha
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SmoothL1Loss
@auto_convert_to_tensor([False, False], [False, False])
def SmoothL1Loss(predict: Tensor, label: Tensor, *, sigma: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(SmoothL1Loss)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(label, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(loss, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(sigma, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SmoothL1Loss"
    op.name = next_unique_name(node_name, "SmoothL1Loss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"

    # process attrs
    op.attr["sigma"].f = sigma

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss


# This api is auto-generated from IR SmoothL1LossGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SmoothL1LossGrad(predict: Tensor, label: Tensor, dout: Tensor, *, sigma: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(SmoothL1LossGrad)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(label, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dout, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(gradient, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(sigma, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SmoothL1LossGrad"
    op.name = next_unique_name(node_name, "SmoothL1LossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"

    # process attrs
    op.attr["sigma"].f = sigma

    # process outputs
    output_index = 0
    op.output_desc.add().name = "gradient"
    gradient = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return gradient


# This api is auto-generated from IR BinaryCrossEntropy
@auto_convert_to_tensor([False, False, False], [False, False, True])
def BinaryCrossEntropy(x: Tensor, y: Tensor, weight: Optional[Tensor], *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(BinaryCrossEntropy)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BinaryCrossEntropy"
    op.name = next_unique_name(node_name, "BinaryCrossEntropy")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR BinaryCrossEntropyGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True])
def BinaryCrossEntropyGrad(x: Tensor, y: Tensor, grad_output: Tensor, weight: Optional[Tensor], *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(BinaryCrossEntropyGrad)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(grad_output, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BinaryCrossEntropyGrad"
    op.name = next_unique_name(node_name, "BinaryCrossEntropyGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(grad_output.tensor)
    op.input_desc.add().CopyFrom(grad_output.desc)
    op.input_desc[-1].name = "grad_output"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR SoftmaxV2
@auto_convert_to_tensor([False], [False])
def SoftmaxV2(x: Tensor, *, axes: List[int]=[-1], half_to_float: bool=False, dependencies=[], node_name=None):
    """REG_OP(SoftmaxV2)\n
.INPUT(x, TensorType({ DT_DOUBLE, DT_FLOAT16, DT_BF16, DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_DOUBLE, DT_FLOAT16, DT_BF16, DT_FLOAT }))\n
.ATTR(axes, ListInt, {-1})\n
.ATTR(half_to_float, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxV2"
    op.name = next_unique_name(node_name, "SoftmaxV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["half_to_float"].b = half_to_float

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SoftmaxV2WithDropOutDoMaskV3D
@auto_convert_to_tensor([False, False], [False, False])
def SoftmaxV2WithDropOutDoMaskV3D(x: Tensor, mask: Tensor, *, keep_prob: float, axes: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(SoftmaxV2WithDropOutDoMaskV3D)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(y1, TensorType({DT_FLOAT16}))\n
.OUTPUT(y2, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(keep_prob, Float)\n
.ATTR(axes, ListInt, {-1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxV2WithDropOutDoMaskV3D"
    op.name = next_unique_name(node_name, "SoftmaxV2WithDropOutDoMaskV3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR LogSoftmaxV2
@auto_convert_to_tensor([False], [False])
def LogSoftmaxV2(logits: Tensor, *, axes: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(LogSoftmaxV2)\n
.INPUT(logits, TensorType({DT_DOUBLE, DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.OUTPUT(logsoftmax, TensorType({DT_DOUBLE, DT_FLOAT16, DT_BF16, DT_FLOAT}))\n
.ATTR(axes, ListInt, {-1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogSoftmaxV2"
    op.name = next_unique_name(node_name, "LogSoftmaxV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(logits.tensor)
    op.input_desc.add().CopyFrom(logits.desc)
    op.input_desc[-1].name = "logits"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "logsoftmax"
    logsoftmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return logsoftmax


# This api is auto-generated from IR ConfusionSoftmaxGrad
@auto_convert_to_tensor([False, False], [False, False])
def ConfusionSoftmaxGrad(grad: Tensor, x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ConfusionSoftmaxGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConfusionSoftmaxGrad"
    op.name = next_unique_name(node_name, "ConfusionSoftmaxGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SoftmaxGradExt
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SoftmaxGradExt(grad: Tensor, x1: Tensor, x2: Tensor, *, axes: int=1, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(SoftmaxGradExt)\n
.INPUT(grad, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x1, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x2, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(axes, Int, 1)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxGradExt"
    op.name = next_unique_name(node_name, "SoftmaxGradExt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["axes"].i = axes
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MVN
@auto_convert_to_tensor([False], [False])
def MVN(x: Tensor, *, normalize_variance: bool=True, across_channels: bool=False, eps: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(MVN)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(normalize_variance, Bool, true)\n
.ATTR(across_channels, Bool, false)\n
.ATTR(eps, Float, 1e-9)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MVN"
    op.name = next_unique_name(node_name, "MVN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["normalize_variance"].b = normalize_variance
    op.attr["across_channels"].b = across_channels
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MVNV2
@auto_convert_to_tensor([False], [False])
def MVNV2(x: Tensor, *, eps: float=0.000000, axes: List[int]=[0, 2, 3], dependencies=[], node_name=None):
    """REG_OP(MVNV2)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(eps, Float, 1e-9)\n
.ATTR(axes, ListInt, {0, 2, 3})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MVNV2"
    op.name = next_unique_name(node_name, "MVNV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["eps"].f = eps
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Normalize
@auto_convert_to_tensor([False, False], [False, False])
def Normalize(x1: Tensor, x2: Tensor, *, across_spatial: bool=True, channel_shared: bool=True, eps: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Normalize)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8}))\n
.ATTR(across_spatial, Bool, true)\n
.ATTR(channel_shared, Bool, true)\n
.ATTR(eps, Float, 1e-10)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Normalize"
    op.name = next_unique_name(node_name, "Normalize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["across_spatial"].b = across_spatial
    op.attr["channel_shared"].b = channel_shared
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LayerNorm
@auto_convert_to_tensor([False, False, False], [False, False, False])
def LayerNorm(x: Tensor, gamma: Tensor, beta: Tensor, *, begin_norm_axis: int=0, begin_params_axis: int=0, epsilon: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LayerNorm)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(beta, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.ATTR(begin_norm_axis, Int, 0)\n
.ATTR(begin_params_axis, Int, 0)\n
.ATTR(epsilon, Float, 0.0000001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNorm"
    op.name = next_unique_name(node_name, "LayerNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["begin_norm_axis"].i = begin_norm_axis
    op.attr["begin_params_axis"].i = begin_params_axis
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, variance


# This api is auto-generated from IR LayerNormV3
@auto_convert_to_tensor([False, False, False], [False, False, False])
def LayerNormV3(x: Tensor, gamma: Tensor, beta: Tensor, *, begin_norm_axis: int=0, begin_params_axis: int=0, epsilon: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(LayerNormV3)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(beta, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(rstd, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.ATTR(begin_norm_axis, Int, 0)\n
.ATTR(begin_params_axis, Int, 0)\n
.ATTR(epsilon, Float, 0.00001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormV3"
    op.name = next_unique_name(node_name, "LayerNormV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["begin_norm_axis"].i = begin_norm_axis
    op.attr["begin_params_axis"].i = begin_params_axis
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rstd"
    rstd = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, rstd


# This api is auto-generated from IR RmsNorm
@auto_convert_to_tensor([False, False], [False, False])
def RmsNorm(x: Tensor, gamma: Tensor, *, epsilon: float=0.000001, dependencies=[], node_name=None):
    """REG_OP(RmsNorm)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(rstd, TensorType({DT_FLOAT, DT_FLOAT, DT_FLOAT}))\n
.ATTR(epsilon, Float, 1e-6)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RmsNorm"
    op.name = next_unique_name(node_name, "RmsNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rstd"
    rstd = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, rstd


# This api is auto-generated from IR RmsNorm
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def RmsNormGrad(
    dy: Tensor, 
    x: Tensor, 
    rstd: Tensor, 
    gamma: Tensor, 
    *, 
    dependencies=[], 
    node_name=None):
    """REG_OP(RmsNormGrad)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(rstd, TensorType({DT_FLOAT, DT_FLOAT, DT_FLOAT}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(dx, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(dgamma, TensorType({DT_FLOAT, DT_FLOAT, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RmsNormGrad"
    op.name = next_unique_name(node_name, "RmsNormGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rstd.tensor)
    op.input_desc.add().CopyFrom(rstd.desc)
    op.input_desc[-1].name = "rstd"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgamma"
    dgamma = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx, dgamma


# This api is auto-generated from IR Renorm
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def Renorm(x: Tensor, *, p: float, dim: int, maxnorm: float, dependencies=[], node_name=None):
    """REG_OP(Renorm)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(p, Float)\n
.REQUIRED_ATTR(dim, Int)\n
.REQUIRED_ATTR(maxnorm, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Renorm"
    op.name = next_unique_name(node_name, "Renorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].f = p
    op.attr["dim"].i = dim
    op.attr["maxnorm"].f = maxnorm

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LayerNormGrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def LayerNormGrad(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, gamma: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LayerNormGrad)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_beta, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormGrad"
    op.name = next_unique_name(node_name, "LayerNormGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_gamma"
    pd_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_beta"
    pd_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x, pd_gamma, pd_beta


# This api is auto-generated from IR LayerNormGradV3
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def LayerNormGradV3(dy: Tensor, x: Tensor, rstd: Tensor, mean: Tensor, gamma: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LayerNormGradV3)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(rstd, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_beta, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormGradV3"
    op.name = next_unique_name(node_name, "LayerNormGradV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rstd.tensor)
    op.input_desc.add().CopyFrom(rstd.desc)
    op.input_desc[-1].name = "rstd"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_gamma"
    pd_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_beta"
    pd_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x, pd_gamma, pd_beta


# This api is auto-generated from IR LayerNormXBackprop
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def LayerNormXBackprop(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, gamma: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LayerNormXBackprop)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormXBackprop"
    op.name = next_unique_name(node_name, "LayerNormXBackprop")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x


# This api is auto-generated from IR LayerNormXBackpropV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def LayerNormXBackpropV2(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, gamma: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LayerNormXBackpropV2)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(res_for_gamma, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormXBackpropV2"
    op.name = next_unique_name(node_name, "LayerNormXBackpropV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "res_for_gamma"
    res_for_gamma = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x, res_for_gamma


# This api is auto-generated from IR LayerNormXBackpropV3
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def LayerNormXBackpropV3(dy: Tensor, x: Tensor, rstd: Tensor, mean: Tensor, gamma: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LayerNormXBackpropV3)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(rstd, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(res_for_gamma, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormXBackpropV3"
    op.name = next_unique_name(node_name, "LayerNormXBackpropV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(rstd.tensor)
    op.input_desc.add().CopyFrom(rstd.desc)
    op.input_desc[-1].name = "rstd"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "res_for_gamma"
    res_for_gamma = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x, res_for_gamma


# This api is auto-generated from IR LayerNormBetaGammaBackprop
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def LayerNormBetaGammaBackprop(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, *, shape_gamma: List[int], dependencies=[], node_name=None):
    """REG_OP(LayerNormBetaGammaBackprop)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_gamma, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_beta, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(shape_gamma, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormBetaGammaBackprop"
    op.name = next_unique_name(node_name, "LayerNormBetaGammaBackprop")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"

    # process attrs
    op.attr["shape_gamma"].list.val_type = 2
    op.attr["shape_gamma"].list.i.extend(shape_gamma)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_gamma"
    pd_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_beta"
    pd_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_gamma, pd_beta


# This api is auto-generated from IR LayerNormBetaGammaBackpropV2
@auto_convert_to_tensor([False, False], [False, False])
def LayerNormBetaGammaBackpropV2(dy: Tensor, res_for_gamma: Tensor, *, shape_gamma: List[int], dependencies=[], node_name=None):
    """REG_OP(LayerNormBetaGammaBackpropV2)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(res_for_gamma, TensorType({DT_FLOAT}))\n
.OUTPUT(pd_gamma, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(pd_beta, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.REQUIRED_ATTR(shape_gamma, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormBetaGammaBackpropV2"
    op.name = next_unique_name(node_name, "LayerNormBetaGammaBackpropV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(res_for_gamma.tensor)
    op.input_desc.add().CopyFrom(res_for_gamma.desc)
    op.input_desc[-1].name = "res_for_gamma"

    # process attrs
    op.attr["shape_gamma"].list.val_type = 2
    op.attr["shape_gamma"].list.i.extend(shape_gamma)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_gamma"
    pd_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_beta"
    pd_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_gamma, pd_beta


# This api is auto-generated from IR LNDropoutGrad
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def LNDropoutGrad(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, gamma: Tensor, mask: Tensor, *, keep_prob: float, dependencies=[], node_name=None):
    """REG_OP(LNDropoutGrad)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mask, TensorType({DT_UINT8, DT_BOOL}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_x_dropout, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_gamma, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_beta, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(keep_prob, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LNDropoutGrad"
    op.name = next_unique_name(node_name, "LNDropoutGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_x_dropout"
    pd_x_dropout = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_gamma"
    pd_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_beta"
    pd_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x, pd_x_dropout, pd_gamma, pd_beta


# This api is auto-generated from IR DropOutDoMask
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DropOutDoMask(x: Tensor, mask: Tensor, keep_prob: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DropOutDoMask)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(mask, TensorType({DT_UINT8, DT_UINT1}))\n
.INPUT(keep_prob, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropOutDoMask"
    op.name = next_unique_name(node_name, "DropOutDoMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"
    op.input.append(keep_prob.tensor)
    op.input_desc.add().CopyFrom(keep_prob.desc)
    op.input_desc[-1].name = "keep_prob"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DropOutDoMaskV3
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DropOutDoMaskV3(x: Tensor, mask: Tensor, keep_prob: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DropOutDoMaskV3)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mask, TensorType({DT_UINT8, DT_BOOL}))\n
.INPUT(keep_prob, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropOutDoMaskV3"
    op.name = next_unique_name(node_name, "DropOutDoMaskV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"
    op.input.append(keep_prob.tensor)
    op.input_desc.add().CopyFrom(keep_prob.desc)
    op.input_desc[-1].name = "keep_prob"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DropOutDoMaskV3D
@auto_convert_to_tensor([False, False], [False, False])
def DropOutDoMaskV3D(x: Tensor, mask: Tensor, *, keep_prob: float, dependencies=[], node_name=None):
    """REG_OP(DropOutDoMaskV3D)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mask, TensorType({DT_UINT8, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(keep_prob, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropOutDoMaskV3D"
    op.name = next_unique_name(node_name, "DropOutDoMaskV3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Scale
@auto_convert_to_tensor([False, False, False], [False, False, True])
def Scale(x: Tensor, scale: Tensor, bias: Optional[Tensor], *, axis: int=1, num_axes: int=1, scale_from_blob: bool=True, dependencies=[], node_name=None):
    """REG_OP(Scale)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(scale, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(axis, Int, 1)\n
.ATTR(num_axes, Int, 1)\n
.ATTR(scale_from_blob, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Scale"
    op.name = next_unique_name(node_name, "Scale")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["num_axes"].i = num_axes
    op.attr["scale_from_blob"].b = scale_from_blob

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LRN
@auto_convert_to_tensor([False], [False])
def LRN(x: Tensor, *, depth_radius: int=5, bias: float=1.000000, alpha: float=1.000000, beta: float=0.500000, norm_region: str="ACROSS_CHANNELS", dependencies=[], node_name=None):
    """REG_OP(LRN)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(depth_radius, Int, 5)\n
.ATTR(bias, Float, 1.0)\n
.ATTR(alpha, Float, 1.0)\n
.ATTR(beta, Float, 0.5)\n
.ATTR(norm_region, String, "ACROSS_CHANNELS")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LRN"
    op.name = next_unique_name(node_name, "LRN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["depth_radius"].i = depth_radius
    op.attr["bias"].f = bias
    op.attr["alpha"].f = alpha
    op.attr["beta"].f = beta
    op.attr["norm_region"].s = compat_as_bytes(norm_region)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LRNGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def LRNGrad(grads: Tensor, x: Tensor, y: Tensor, *, depth_radius: int=5, bias: float=1.000000, alpha: float=1.000000, beta: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(LRNGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(depth_radius, Int, 5)\n
.ATTR(bias, Float, 1.0)\n
.ATTR(alpha, Float, 1.0)\n
.ATTR(beta, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LRNGrad"
    op.name = next_unique_name(node_name, "LRNGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"

    # process attrs
    op.attr["depth_radius"].i = depth_radius
    op.attr["bias"].f = bias
    op.attr["alpha"].f = alpha
    op.attr["beta"].f = beta

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR RNNTLoss
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def RNNTLoss(acts: Tensor, labels: Tensor, input_lengths: Tensor, label_lengths: Tensor, *, blank_label: int=0, dependencies=[], node_name=None):
    """REG_OP(RNNTLoss)\n
.INPUT(acts, TensorType({DT_FLOAT}))\n
.INPUT(labels, TensorType({DT_INT32}))\n
.INPUT(input_lengths, TensorType({DT_INT32}))\n
.INPUT(label_lengths, TensorType({DT_INT32}))\n
.ATTR(blank_label, Int, 0)\n
.OUTPUT(costs, TensorType({DT_FLOAT}))\n
.OUTPUT(grads, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RNNTLoss"
    op.name = next_unique_name(node_name, "RNNTLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(acts.tensor)
    op.input_desc.add().CopyFrom(acts.desc)
    op.input_desc[-1].name = "acts"
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"
    op.input.append(input_lengths.tensor)
    op.input_desc.add().CopyFrom(input_lengths.desc)
    op.input_desc[-1].name = "input_lengths"
    op.input.append(label_lengths.tensor)
    op.input_desc.add().CopyFrom(label_lengths.desc)
    op.input_desc[-1].name = "label_lengths"

    # process attrs
    op.attr["blank_label"].i = blank_label

    # process outputs
    output_index = 0
    op.output_desc.add().name = "costs"
    costs = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "grads"
    grads = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return costs, grads


# This api is auto-generated from IR GroupNorm
@auto_convert_to_tensor([False, False, False], [False, False, False])
def GroupNorm(x: Tensor, gamma: Tensor, beta: Tensor, *, num_groups: int, data_format: str="NHWC", eps: float=0.000100, is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(GroupNorm)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gamma, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(beta, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(variance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(num_groups, Int)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(eps, Float, 0.0001)\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GroupNorm"
    op.name = next_unique_name(node_name, "GroupNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["num_groups"].i = num_groups
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["eps"].f = eps
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, variance


# This api is auto-generated from IR GroupNormSilu
@auto_convert_to_tensor([False, False, False], [False, True, True])
def GroupNormSilu(x: Tensor, gamma: Optional[Tensor], beta: Optional[Tensor], *, num_groups: int, eps: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(GroupNormSilu)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(gamma, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(beta, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(mean, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(variance, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(num_groups, Int)\n
.ATTR(eps, Float, 0.0001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GroupNormSilu"
    op.name = next_unique_name(node_name, "GroupNormSilu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if gamma is not None:
        op.input.append(gamma.tensor)
        op.input_desc.add().CopyFrom(gamma.desc)
        op.input_desc[-1].name = "gamma"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "gamma"
    if beta is not None:
        op.input.append(beta.tensor)
        op.input_desc.add().CopyFrom(beta.desc)
        op.input_desc[-1].name = "beta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["num_groups"].i = num_groups
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, variance


# This api is auto-generated from IR InstanceNormV2
@auto_convert_to_tensor([False, False, False, False, False], [False, True, True, True, True])
def InstanceNormV2(x: Tensor, gamma: Optional[Tensor], beta: Optional[Tensor], mean: Optional[Tensor], variance: Optional[Tensor], *, is_training: bool=True, momentum: float=0.100000, epsilon: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(InstanceNormV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(gamma, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(beta, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
.ATTR(is_training, Bool, true)\n
.ATTR(momentum, Float, 0.1)\n
.ATTR(epsilon, Float, 0.00001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InstanceNormV2"
    op.name = next_unique_name(node_name, "InstanceNormV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if gamma is not None:
        op.input.append(gamma.tensor)
        op.input_desc.add().CopyFrom(gamma.desc)
        op.input_desc[-1].name = "gamma"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "gamma"
    if beta is not None:
        op.input.append(beta.tensor)
        op.input_desc.add().CopyFrom(beta.desc)
        op.input_desc[-1].name = "beta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "beta"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["is_training"].b = is_training
    op.attr["momentum"].f = momentum
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance


# This api is auto-generated from IR INInferV2D
@auto_convert_to_tensor([False, False, False, False, False, False], [False, True, True, True, True, True])
def INInferV2D(x: Tensor, gamma: Optional[Tensor], beta: Optional[Tensor], mean: Optional[Tensor], variance: Optional[Tensor], variance_sqrt: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(INInferV2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(gamma, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(beta, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance_sqrt, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "INInferV2D"
    op.name = next_unique_name(node_name, "INInferV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if gamma is not None:
        op.input.append(gamma.tensor)
        op.input_desc.add().CopyFrom(gamma.desc)
        op.input_desc[-1].name = "gamma"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "gamma"
    if beta is not None:
        op.input.append(beta.tensor)
        op.input_desc.add().CopyFrom(beta.desc)
        op.input_desc[-1].name = "beta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "beta"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"
    if variance_sqrt is not None:
        op.input.append(variance_sqrt.tensor)
        op.input_desc.add().CopyFrom(variance_sqrt.desc)
        op.input_desc[-1].name = "variance_sqrt"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance_sqrt"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance


# This api is auto-generated from IR InstanceNorm
@auto_convert_to_tensor([False, False, False], [False, False, False])
def InstanceNorm(x: Tensor, gamma: Tensor, beta: Tensor, *, data_format: str="NDHWC", epsilon: float=0.000001, dependencies=[], node_name=None):
    """REG_OP(InstanceNorm)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(gamma, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(beta, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(variance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(data_format, String, "NDHWC")\n
.ATTR(epsilon, Float, 1e-6)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InstanceNorm"
    op.name = next_unique_name(node_name, "InstanceNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, variance


# This api is auto-generated from IR InstanceNormGrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def InstanceNormGrad(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, gamma: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InstanceNormGrad)\n
.INPUT(dy, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(variance, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_gamma, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(pd_beta, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InstanceNormGrad"
    op.name = next_unique_name(node_name, "InstanceNormGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_gamma"
    pd_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_beta"
    pd_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x, pd_gamma, pd_beta


# This api is auto-generated from IR KlDivLossGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def KlDivLossGrad(grad: Tensor, input: Tensor, target: Tensor, *, reduction: str="mean", log_target: bool=False, dependencies=[], node_name=None):
    """REG_OP(KlDivLossGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
.ATTR(log_target, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "KlDivLossGrad"
    op.name = next_unique_name(node_name, "KlDivLossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)
    op.attr["log_target"].b = log_target

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR L1LossGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def L1LossGrad(grads: Tensor, predict: Tensor, label: Tensor, *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(L1LossGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(label, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "L1LossGrad"
    op.name = next_unique_name(node_name, "L1LossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LpLoss
@auto_convert_to_tensor([False, False], [False, False])
def LpLoss(predict: Tensor, label: Tensor, *, p: int, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(LpLoss)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(label, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(p, Int)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LpLoss"
    op.name = next_unique_name(node_name, "LpLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"

    # process attrs
    op.attr["p"].i = p
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MseLossGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MseLossGrad(predict: Tensor, label: Tensor, dout: Tensor, *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(MseLossGrad)\n
.INPUT(predict, TensorType({DT_FLOAT32, DT_FLOAT16}))\n
.INPUT(label, TensorType({DT_FLOAT32, DT_FLOAT16}))\n
.INPUT(dout, TensorType({DT_FLOAT32, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT32, DT_FLOAT16}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MseLossGrad"
    op.name = next_unique_name(node_name, "MseLossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MseLoss
@auto_convert_to_tensor([False, False], [False, False])
def MseLoss(predict: Tensor, label: Tensor, *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(MseLoss)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(label, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MseLoss"
    op.name = next_unique_name(node_name, "MseLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SmoothL1LossGradV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SmoothL1LossGradV2(predict: Tensor, label: Tensor, dout: Tensor, *, sigma: float=1.000000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SmoothL1LossGradV2)\n
.INPUT(predict, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(label, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(dout, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(gradient, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(sigma, Float, 1.0)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SmoothL1LossGradV2"
    op.name = next_unique_name(node_name, "SmoothL1LossGradV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"

    # process attrs
    op.attr["sigma"].f = sigma
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "gradient"
    gradient = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return gradient


# This api is auto-generated from IR SmoothL1LossV2
@auto_convert_to_tensor([False, False], [False, False])
def SmoothL1LossV2(predict: Tensor, label: Tensor, *, sigma: float=1.000000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SmoothL1LossV2)\n
.INPUT(predict, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(label, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(loss, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.ATTR(sigma, Float, 1.0)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SmoothL1LossV2"
    op.name = next_unique_name(node_name, "SmoothL1LossV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(label.tensor)
    op.input_desc.add().CopyFrom(label.desc)
    op.input_desc[-1].name = "label"

    # process attrs
    op.attr["sigma"].f = sigma
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss


# This api is auto-generated from IR Centralization
@auto_convert_to_tensor([False], [False])
def Centralization(x: Tensor, *, axes: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(Centralization)\n
.INPUT(x, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.ATTR(axes, ListInt, {-1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Centralization"
    op.name = next_unique_name(node_name, "Centralization")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Roll
@auto_convert_to_tensor([False], [False])
def Roll(x: Tensor, *, shifts: List[int], dims: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(Roll)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT32, DT_INT8, DT_UINT8, DT_BF16, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT32, DT_INT8, DT_UINT8, DT_BF16, DT_INT64}))\n
.REQUIRED_ATTR(shifts, ListInt)\n
.ATTR(dims, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Roll"
    op.name = next_unique_name(node_name, "Roll")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["shifts"].list.val_type = 2
    op.attr["shifts"].list.i.extend(shifts)
    op.attr["dims"].list.val_type = 2
    op.attr["dims"].list.i.extend(dims)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RollV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def RollV2(input: Tensor, shift: Tensor, axes: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RollV2)\n
.INPUT(input, TensorType({DT_INT8,DT_UINT8,DT_INT16,DT_UINT16,DT_INT32,DT_INT64,DT_FLOAT16, DT_FLOAT,DT_DOUBLE}))\n
.INPUT(shift, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(axes, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(output, TensorType({DT_INT8,DT_UINT8,DT_INT16,DT_UINT16,DT_INT32,DT_INT64,DT_FLOAT16, DT_FLOAT,DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RollV2"
    op.name = next_unique_name(node_name, "RollV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(shift.tensor)
    op.input_desc.add().CopyFrom(shift.desc)
    op.input_desc[-1].name = "shift"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR SoftMarginLoss
@auto_convert_to_tensor([False, False], [False, False])
def SoftMarginLoss(input_x: Tensor, input_y: Tensor, *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SoftMarginLoss)\n
.INPUT(input_x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(input_y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(reduction, String, "mean")\n
.OUTPUT(output_z, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftMarginLoss"
    op.name = next_unique_name(node_name, "SoftMarginLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"
    op.input.append(input_y.tensor)
    op.input_desc.add().CopyFrom(input_y.desc)
    op.input_desc[-1].name = "input_y"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_z"
    output_z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_z


# This api is auto-generated from IR SigmoidCrossEntropyWithLogitsGradV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def SigmoidCrossEntropyWithLogitsGradV2(predict: Tensor, target: Tensor, dout: Tensor, weight: Optional[Tensor], pos_weight: Optional[Tensor], *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SigmoidCrossEntropyWithLogitsGradV2)\n
.INPUT(predict, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dout, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(pos_weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(gradient, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SigmoidCrossEntropyWithLogitsGradV2"
    op.name = next_unique_name(node_name, "SigmoidCrossEntropyWithLogitsGradV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predict.tensor)
    op.input_desc.add().CopyFrom(predict.desc)
    op.input_desc[-1].name = "predict"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"
    if pos_weight is not None:
        op.input.append(pos_weight.tensor)
        op.input_desc.add().CopyFrom(pos_weight.desc)
        op.input_desc[-1].name = "pos_weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pos_weight"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "gradient"
    gradient = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return gradient


# This api is auto-generated from IR PoissonNllLoss
@auto_convert_to_tensor([False, False], [False, False])
def PoissonNllLoss(input_x: Tensor, target: Tensor, *, log_input: bool=True, full: bool=False, eps: float=0.000000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(PoissonNllLoss)\n
.INPUT(input_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(loss, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(log_input, Bool, true)\n
.ATTR(full, Bool, false)\n
.ATTR(eps, Float, 1e-8)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PoissonNllLoss"
    op.name = next_unique_name(node_name, "PoissonNllLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"

    # process attrs
    op.attr["log_input"].b = log_input
    op.attr["full"].b = full
    op.attr["eps"].f = eps
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss


# This api is auto-generated from IR RnnGenMask
@auto_convert_to_tensor([False], [False])
def RnnGenMask(seq_length: Tensor, *, num_step: int, hidden_size: int, dependencies=[], node_name=None):
    """REG_OP(RnnGenMask)\n
.INPUT(seq_length, TensorType({DT_INT32}))\n
.OUTPUT(seq_mask, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(num_step, Int)\n
.REQUIRED_ATTR(hidden_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RnnGenMask"
    op.name = next_unique_name(node_name, "RnnGenMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(seq_length.tensor)
    op.input_desc.add().CopyFrom(seq_length.desc)
    op.input_desc[-1].name = "seq_length"

    # process attrs
    op.attr["num_step"].i = num_step
    op.attr["hidden_size"].i = hidden_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "seq_mask"
    seq_mask = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return seq_mask


# This api is auto-generated from IR MultilabelMarginLoss
@auto_convert_to_tensor([False, False], [False, False])
def MultilabelMarginLoss(x: Tensor, target: Tensor, *, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(MultilabelMarginLoss)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(target, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(is_target, TensorType({DT_INT32}))\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultilabelMarginLoss"
    op.name = next_unique_name(node_name, "MultilabelMarginLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"

    # process attrs
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "is_target"
    is_target = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, is_target


# This api is auto-generated from IR NormalizeBatch
@auto_convert_to_tensor([False, False], [False, False])
def NormalizeBatch(input_x: Tensor, seq_len: Tensor, *, normalize_type: str, epsilon: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(NormalizeBatch)\n
.INPUT(input_x, TensorType({ DT_FLOAT }))\n
.INPUT(seq_len, TensorType({ DT_INT32 }))\n
.OUTPUT(output_y, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(normalize_type, String)\n
.ATTR(epsilon, Float, 0.00001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NormalizeBatch"
    op.name = next_unique_name(node_name, "NormalizeBatch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"
    op.input.append(seq_len.tensor)
    op.input_desc.add().CopyFrom(seq_len.desc)
    op.input_desc[-1].name = "seq_len"

    # process attrs
    op.attr["normalize_type"].s = compat_as_bytes(normalize_type)
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR GroupNormRelu
@auto_convert_to_tensor([False, False, False], [False, False, False])
def GroupNormRelu(x: Tensor, gamma: Tensor, beta: Tensor, *, num_groups: int, eps: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(GroupNormRelu)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(gamma, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(beta, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(num_groups, Int)\n
.ATTR(eps, Float, 0.00001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GroupNormRelu"
    op.name = next_unique_name(node_name, "GroupNormRelu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["num_groups"].i = num_groups
    op.attr["eps"].f = eps

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DropoutWithMulsAndSoftmaxGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DropoutWithMulsAndSoftmaxGrad(y_grad: Tensor, mask: Tensor, softmax_output: Tensor, *, input_keep_prob: float, alpha: float, axes: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(DropoutWithMulsAndSoftmaxGrad)\n
.INPUT(y_grad, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.INPUT(mask, TensorType({ DT_UINT8 }))\n
.INPUT(softmax_output, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.OUTPUT(x_grad, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.REQUIRED_ATTR(input_keep_prob, Float)\n
.REQUIRED_ATTR(alpha, Float)\n
.ATTR(axes, ListInt, { -1 })\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropoutWithMulsAndSoftmaxGrad"
    op.name = next_unique_name(node_name, "DropoutWithMulsAndSoftmaxGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"
    op.input.append(softmax_output.tensor)
    op.input_desc.add().CopyFrom(softmax_output.desc)
    op.input_desc[-1].name = "softmax_output"

    # process attrs
    op.attr["input_keep_prob"].f = input_keep_prob
    op.attr["alpha"].f = alpha
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR SoftmaxCrossEntropyLoss
@auto_convert_to_tensor([False, False, False], [False, False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def SoftmaxCrossEntropyLoss(scores: Tensor, labels: Tensor, weights: Optional[Tensor], *, ignore_index: int=0, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SoftmaxCrossEntropyLoss)\n
.INPUT(scores, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT,DT_BFLOAT16}))\n
.INPUT(labels, TensorType({DT_INT32, DT_INT64}))\n
.OPTIONAL_INPUT(weights, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT,DT_BFLOAT16}))\n
.ATTR(ignore_index, Int, 0)\n
.ATTR(reduction, String, "mean")\n
.OUTPUT(loss, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT,DT_BFLOAT16}))\n
.OUTPUT(log_prop, TensorType({DT_DOUBLE,DT_FLOAT16,DT_FLOAT,DT_BFLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxCrossEntropyLoss"
    op.name = next_unique_name(node_name, "SoftmaxCrossEntropyLoss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(scores.tensor)
    op.input_desc.add().CopyFrom(scores.desc)
    op.input_desc[-1].name = "scores"
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"
    if weights is not None:
        op.input.append(weights.tensor)
        op.input_desc.add().CopyFrom(weights.desc)
        op.input_desc[-1].name = "weights"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weights"

    # process attrs
    op.attr["ignore_index"].i = ignore_index
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "loss"
    loss = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "log_prop"
    log_prop = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return loss, log_prop


# This api is auto-generated from IR AxpyWithSoftmaxAndDropOutDoMask
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AxpyWithSoftmaxAndDropOutDoMask(x1: Tensor, x2: Tensor, mask: Tensor, *, alpha: float, input_keep_prob: float, axis: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(AxpyWithSoftmaxAndDropOutDoMask)\n
.INPUT(x1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x2, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(y1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y2, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(alpha, Float)\n
.REQUIRED_ATTR(input_keep_prob, Float)\n
.ATTR(axis, ListInt, {-1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AxpyWithSoftmaxAndDropOutDoMask"
    op.name = next_unique_name(node_name, "AxpyWithSoftmaxAndDropOutDoMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["alpha"].f = alpha
    op.attr["input_keep_prob"].f = input_keep_prob
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR SigmoidFocalLossGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True])
def SigmoidFocalLossGrad(pred: Tensor, target: Tensor, dout: Tensor, weight: Optional[Tensor], *, alpha: float=0.250000, gamma: float=2.000000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SigmoidFocalLossGrad)\n
.INPUT(pred, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_INT32}))\n
.INPUT(dout, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(alpha, Float, 0.25)\n
.ATTR(gamma, Float, 2.0)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SigmoidFocalLossGrad"
    op.name = next_unique_name(node_name, "SigmoidFocalLossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(pred.tensor)
    op.input_desc.add().CopyFrom(pred.desc)
    op.input_desc[-1].name = "pred"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["alpha"].f = alpha
    op.attr["gamma"].f = gamma
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad"
    grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad


# This api is auto-generated from IR SoftmaxFocalLossGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True])
def SoftmaxFocalLossGrad(pred: Tensor, target: Tensor, dout: Tensor, weight: Optional[Tensor], *, alpha: float=0.250000, gamma: float=2.000000, reduction: str="mean", dependencies=[], node_name=None):
    """REG_OP(SoftmaxFocalLossGrad)\n
.INPUT(pred, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(target, TensorType({DT_INT32}))\n
.INPUT(dout, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(alpha, Float, 0.25)\n
.ATTR(gamma, Float, 2.0)\n
.ATTR(reduction, String, "mean")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftmaxFocalLossGrad"
    op.name = next_unique_name(node_name, "SoftmaxFocalLossGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(pred.tensor)
    op.input_desc.add().CopyFrom(pred.desc)
    op.input_desc[-1].name = "pred"
    op.input.append(target.tensor)
    op.input_desc.add().CopyFrom(target.desc)
    op.input_desc[-1].name = "target"
    op.input.append(dout.tensor)
    op.input_desc.add().CopyFrom(dout.desc)
    op.input_desc[-1].name = "dout"
    if weight is not None:
        op.input.append(weight.tensor)
        op.input_desc.add().CopyFrom(weight.desc)
        op.input_desc[-1].name = "weight"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["alpha"].f = alpha
    op.attr["gamma"].f = gamma
    op.attr["reduction"].s = compat_as_bytes(reduction)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad"
    grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad


# This api is auto-generated from IR Pooling
@auto_convert_to_tensor([False], [False])
def Pooling(x: Tensor, *, mode: int=0, global_pooling: bool=False, window: List[int]=[1, 1], stride: List[int]=[1, 1], pad: List[int]=[0, 0, 0, 0], dilation: List[int]=[1, 1, 1, 1], ceil_mode: int=0, data_format: str="NCHW", dependencies=[], node_name=None):
    """REG_OP(Pooling)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT32}))\n
.ATTR(mode, Int, 0)\n
.ATTR(global_pooling, Bool, false)\n
.ATTR(window, ListInt, {1,1})\n
.ATTR(stride, ListInt, {1,1})\n
.ATTR(pad, ListInt, {0,0,0,0})\n
.ATTR(dilation, ListInt, {1,1,1,1})\n
.ATTR(ceil_mode, Int, 0)\n
.ATTR(data_format, String, "NCHW")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Pooling"
    op.name = next_unique_name(node_name, "Pooling")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["mode"].i = mode
    op.attr["global_pooling"].b = global_pooling
    op.attr["window"].list.val_type = 2
    op.attr["window"].list.i.extend(window)
    op.attr["stride"].list.val_type = 2
    op.attr["stride"].list.i.extend(stride)
    op.attr["pad"].list.val_type = 2
    op.attr["pad"].list.i.extend(pad)
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["ceil_mode"].i = ceil_mode
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPool
@auto_convert_to_tensor([False], [False])
def AvgPool(x: Tensor, *, ksize: List[int], strides: List[int], padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(AvgPool)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool"
    op.name = next_unique_name(node_name, "AvgPool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPoolV2
@auto_convert_to_tensor([False], [False])
def AvgPoolV2(x: Tensor, *, ksize: List[int], strides: List[int], padding_mode: str="CALCULATED", pads: List[int]=[0, 0, 0, 0], data_format: str="NCHW", global_pooling: bool=False, ceil_mode: bool=False, exclusive: bool=True, divisor_override: int=0, dependencies=[], node_name=None):
    """REG_OP(AvgPoolV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(padding_mode, String, "CALCULATED")\n
.ATTR(pads, ListInt, {0, 0, 0, 0})\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(global_pooling, Bool, false)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(exclusive, Bool, true)\n
.ATTR(divisor_override, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPoolV2"
    op.name = next_unique_name(node_name, "AvgPoolV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["global_pooling"].b = global_pooling
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["exclusive"].b = exclusive
    op.attr["divisor_override"].i = divisor_override

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPool3D
@auto_convert_to_tensor([False], [False])
def AvgPool3D(x: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], ceil_mode: bool=False, count_include_pad: bool=True, divisor_override: int=0, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(AvgPool3D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(count_include_pad, Bool, true)\n
.ATTR(divisor_override, Int, 0)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool3D"
    op.name = next_unique_name(node_name, "AvgPool3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["count_include_pad"].b = count_include_pad
    op.attr["divisor_override"].i = divisor_override
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPool3DD
@auto_convert_to_tensor([False, False, False], [False, True, True])
def AvgPool3DD(x: Tensor, filter: Optional[Tensor], multiplier: Optional[Tensor], *, ksize: List[int], strides: List[int], pads: List[int], ceil_mode: bool=False, count_include_pad: bool=True, divisor_override: int=0, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(AvgPool3DD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OPTIONAL_INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OPTIONAL_INPUT(multiplier, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(count_include_pad, Bool, true)\n
.ATTR(divisor_override, Int, 0)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool3DD"
    op.name = next_unique_name(node_name, "AvgPool3DD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if filter is not None:
        op.input.append(filter.tensor)
        op.input_desc.add().CopyFrom(filter.desc)
        op.input_desc[-1].name = "filter"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "filter"
    if multiplier is not None:
        op.input.append(multiplier.tensor)
        op.input_desc.add().CopyFrom(multiplier.desc)
        op.input_desc[-1].name = "multiplier"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "multiplier"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["count_include_pad"].b = count_include_pad
    op.attr["divisor_override"].i = divisor_override
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPool3DGrad
@auto_convert_to_tensor([False, False], [False, False])
def AvgPool3DGrad(orig_input_shape: Tensor, grads: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], ceil_mode: bool=False, count_include_pad: bool=True, divisor_override: int=0, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(AvgPool3DGrad)\n
.INPUT(orig_input_shape, TensorType({DT_INT32}))\n
.INPUT(grads, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(output, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(count_include_pad, Bool, true)\n
.ATTR(divisor_override, Int, 0)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool3DGrad"
    op.name = next_unique_name(node_name, "AvgPool3DGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_input_shape.tensor)
    op.input_desc.add().CopyFrom(orig_input_shape.desc)
    op.input_desc[-1].name = "orig_input_shape"
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["count_include_pad"].b = count_include_pad
    op.attr["divisor_override"].i = divisor_override
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR AvgPool3DGradD
@auto_convert_to_tensor([False, False, False], [False, True, True])
def AvgPool3DGradD(grads: Tensor, filter: Optional[Tensor], multiplier: Optional[Tensor], *, orig_input_shape: List[int], ksize: List[int], strides: List[int], pads: List[int], ceil_mode: bool=False, count_include_pad: bool=True, divisor_override: int=0, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(AvgPool3DGradD)\n
.INPUT(grads, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(filter, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(multiplier, TensorType({DT_FLOAT16}))\n
.OUTPUT(output, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(orig_input_shape, ListInt)\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(count_include_pad, Bool, true)\n
.ATTR(divisor_override, Int, 0)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool3DGradD"
    op.name = next_unique_name(node_name, "AvgPool3DGradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    if filter is not None:
        op.input.append(filter.tensor)
        op.input_desc.add().CopyFrom(filter.desc)
        op.input_desc[-1].name = "filter"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "filter"
    if multiplier is not None:
        op.input.append(multiplier.tensor)
        op.input_desc.add().CopyFrom(multiplier.desc)
        op.input_desc[-1].name = "multiplier"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "multiplier"

    # process attrs
    op.attr["orig_input_shape"].list.val_type = 2
    op.attr["orig_input_shape"].list.i.extend(orig_input_shape)
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["count_include_pad"].b = count_include_pad
    op.attr["divisor_override"].i = divisor_override
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR MaxPoolExt2
@auto_convert_to_tensor([False], [False])
def MaxPoolExt2(x: Tensor, *, ksize: List[int], strides: List[int], padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPoolExt2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolExt2"
    op.name = next_unique_name(node_name, "MaxPoolExt2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPool
@auto_convert_to_tensor([False], [False])
def MaxPool(x: Tensor, *, ksize: List[int], strides: List[int], padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPool)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPool"
    op.name = next_unique_name(node_name, "MaxPool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPool3D
@auto_convert_to_tensor([False], [False])
def MaxPool3D(x: Tensor, *, ksize: List[int], strides: List[int], padding: str, pads: List[int]=[0, 0, 0, 0, 0, 0], dilation: List[int]=[1, 1, 1, 1, 1], ceil_mode: int=0, data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPool3D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(pads, ListInt, {0,0,0,0,0,0})\n
.ATTR(dilation, ListInt, {1,1,1,1,1})\n
.ATTR(ceil_mode, Int, 0)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPool3D"
    op.name = next_unique_name(node_name, "MaxPool3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["ceil_mode"].i = ceil_mode
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPool3DWithArgmax
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def MaxPool3DWithArgmax(x: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], dilation: List[int]=[1, 1, 1, 1, 1], ceil_mode: bool=False, data_format: str="NCDHW", argmax_type: str="bitmask", dependencies=[], node_name=None):
    """REG_OP(MaxPool3DWithArgmax)\n
.INPUT(x, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.OUTPUT(argmax, TensorType::IndexNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dilation, ListInt, {1, 1, 1, 1, 1})\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(data_format, String, "NCDHW")\n
.ATTR(argmax_type, String, "bitmask")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPool3DWithArgmax"
    op.name = next_unique_name(node_name, "MaxPool3DWithArgmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["argmax_type"].s = compat_as_bytes(argmax_type)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, argmax


# This api is auto-generated from IR AdaptiveMaxPool2d
@auto_convert_to_tensor([False], [False])
def AdaptiveMaxPool2d(x: Tensor, *, output_size: List[int], dependencies=[], node_name=None):
    """REG_OP(AdaptiveMaxPool2d)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.OUTPUT(argmax, TensorType::IndexNumberType())\n
.REQUIRED_ATTR(output_size, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdaptiveMaxPool2d"
    op.name = next_unique_name(node_name, "AdaptiveMaxPool2d")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, argmax


# This api is auto-generated from IR MaxPool3DGradGrad
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def MaxPool3DGradGrad(orig_x: Tensor, orig_y: Tensor, grads: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPool3DGradGrad)\n
.INPUT(orig_x, TensorType::RealNumberType())\n
.INPUT(orig_y, TensorType::RealNumberType())\n
.INPUT(grads, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPool3DGradGrad"
    op.name = next_unique_name(node_name, "MaxPool3DGradGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_x.tensor)
    op.input_desc.add().CopyFrom(orig_x.desc)
    op.input_desc[-1].name = "orig_x"
    op.input.append(orig_y.tensor)
    op.input_desc.add().CopyFrom(orig_y.desc)
    op.input_desc[-1].name = "orig_y"
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolGrad
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def MaxPoolGrad(x1: Tensor, x2: Tensor, grad: Tensor, *, ksize: List[int], strides: List[int], padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPoolGrad)\n
.INPUT(x1, TensorType::RealNumberType())\n
.INPUT(x2, TensorType::RealNumberType())\n
.INPUT(grad, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolGrad"
    op.name = next_unique_name(node_name, "MaxPoolGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolGradGrad
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def MaxPoolGradGrad(x1: Tensor, x2: Tensor, grad: Tensor, *, ksize: List[int], strides: List[int], padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPoolGradGrad)\n
.INPUT(x1, TensorType::RealNumberType())\n
.INPUT(x2, TensorType::RealNumberType())\n
.INPUT(grad, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolGradGrad"
    op.name = next_unique_name(node_name, "MaxPoolGradGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MaxPoolV2(x: Tensor, ksize: Tensor, strides: Tensor, *, padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPoolV2)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(ksize, TensorType({DT_INT32}))\n
.INPUT(strides, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolV2"
    op.name = next_unique_name(node_name, "MaxPoolV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(ksize.tensor)
    op.input_desc.add().CopyFrom(ksize.desc)
    op.input_desc[-1].name = "ksize"
    op.input.append(strides.tensor)
    op.input_desc.add().CopyFrom(strides.desc)
    op.input_desc[-1].name = "strides"

    # process attrs
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolWithArgmax
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def MaxPoolWithArgmax(x: Tensor, *, ksize: List[int], strides: List[int], padding: str, Targmax: int=7, dependencies=[], node_name=None):
    """REG_OP(MaxPoolWithArgmax)\n
.INPUT(x, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.OUTPUT(argmax, TensorType::IndexNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(Targmax, Int, 7)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolWithArgmax"
    op.name = next_unique_name(node_name, "MaxPoolWithArgmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["Targmax"].i = Targmax

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, argmax


# This api is auto-generated from IR MaxPoolGradWithArgmax
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER, TensorType.TT_INDEX_NUMBER])
def MaxPoolGradWithArgmax(x: Tensor, grad: Tensor, argmax: Tensor, *, ksize: List[int], strides: List[int], padding: str, dependencies=[], node_name=None):
    """REG_OP(MaxPoolGradWithArgmax)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(grad, TensorType::RealNumberType())\n
.INPUT(argmax, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolGradWithArgmax"
    op.name = next_unique_name(node_name, "MaxPoolGradWithArgmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(argmax.tensor)
    op.input_desc.add().CopyFrom(argmax.desc)
    op.input_desc[-1].name = "argmax"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Mask2Argmax
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_INDEX_NUMBER])
def Mask2Argmax(x: Tensor, mask: Tensor, *, ksize: List[int], strides: List[int], padding: str, originshape: List[int], dependencies=[], node_name=None):
    """REG_OP(Mask2Argmax)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(mask, TensorType::IndexNumberType())\n
.OUTPUT(argmax, TensorType::IndexNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.REQUIRED_ATTR(originshape, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Mask2Argmax"
    op.name = next_unique_name(node_name, "Mask2Argmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["originshape"].list.val_type = 2
    op.attr["originshape"].list.i.extend(originshape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return argmax


# This api is auto-generated from IR MaxPoolGradGradWithArgmax
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER, TensorType.TT_INDEX_NUMBER])
def MaxPoolGradGradWithArgmax(x: Tensor, grad: Tensor, argmax: Tensor, *, ksize: List[int], strides: List[int], padding: str, dependencies=[], node_name=None):
    """REG_OP(MaxPoolGradGradWithArgmax)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(grad, TensorType::RealNumberType())\n
.INPUT(argmax, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolGradGradWithArgmax"
    op.name = next_unique_name(node_name, "MaxPoolGradGradWithArgmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(argmax.tensor)
    op.input_desc.add().CopyFrom(argmax.desc)
    op.input_desc[-1].name = "argmax"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPoolGrad
@auto_convert_to_tensor([False, False], [False, False])
def AvgPoolGrad(orig_input_shape: Tensor, input_grad: Tensor, *, ksize: List[int], strides: List[int], padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(AvgPoolGrad)\n
.INPUT(orig_input_shape, TensorType({DT_INT32}))\n
.INPUT(input_grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(out_grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPoolGrad"
    op.name = next_unique_name(node_name, "AvgPoolGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_input_shape.tensor)
    op.input_desc.add().CopyFrom(orig_input_shape.desc)
    op.input_desc[-1].name = "orig_input_shape"
    op.input.append(input_grad.tensor)
    op.input_desc.add().CopyFrom(input_grad.desc)
    op.input_desc[-1].name = "input_grad"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_grad"
    out_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_grad


# This api is auto-generated from IR AvgPoolGradD
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AvgPoolGradD(input_grad: Tensor, mean_matrix: Tensor, kernel_matrix: Tensor, *, orig_input_shape: List[int], ksize: List[int], strides: List[int], padding: str, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(AvgPoolGradD)\n
.INPUT(input_grad, TensorType({DT_FLOAT16}))\n
.INPUT(mean_matrix, TensorType({DT_FLOAT16}))\n
.INPUT(kernel_matrix, TensorType({DT_FLOAT16}))\n
.OUTPUT(out_grad, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(orig_input_shape, ListInt)\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPoolGradD"
    op.name = next_unique_name(node_name, "AvgPoolGradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_grad.tensor)
    op.input_desc.add().CopyFrom(input_grad.desc)
    op.input_desc[-1].name = "input_grad"
    op.input.append(mean_matrix.tensor)
    op.input_desc.add().CopyFrom(mean_matrix.desc)
    op.input_desc[-1].name = "mean_matrix"
    op.input.append(kernel_matrix.tensor)
    op.input_desc.add().CopyFrom(kernel_matrix.desc)
    op.input_desc[-1].name = "kernel_matrix"

    # process attrs
    op.attr["orig_input_shape"].list.val_type = 2
    op.attr["orig_input_shape"].list.i.extend(orig_input_shape)
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_grad"
    out_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_grad


# This api is auto-generated from IR AvgPoolV2Grad
@auto_convert_to_tensor([False, False], [False, False])
def AvgPoolV2Grad(orig_input_shape: Tensor, input_grad: Tensor, *, ksize: List[int], strides: List[int], padding_mode: str="CALCULATED", pads: List[int]=[0, 0, 0, 0], data_format: str="NCHW", global_pooling: bool=False, ceil_mode: bool=False, exclusive: bool=True, dependencies=[], node_name=None):
    """REG_OP(AvgPoolV2Grad)\n
.INPUT(orig_input_shape, TensorType({DT_INT32}))\n
.INPUT(input_grad, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.OUTPUT(out_grad, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(padding_mode, String, "CALCULATED")\n
.ATTR(pads, ListInt, {0,0,0,0})\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(global_pooling, Bool, false)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(exclusive, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPoolV2Grad"
    op.name = next_unique_name(node_name, "AvgPoolV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_input_shape.tensor)
    op.input_desc.add().CopyFrom(orig_input_shape.desc)
    op.input_desc[-1].name = "orig_input_shape"
    op.input.append(input_grad.tensor)
    op.input_desc.add().CopyFrom(input_grad.desc)
    op.input_desc[-1].name = "input_grad"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["global_pooling"].b = global_pooling
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["exclusive"].b = exclusive

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_grad"
    out_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_grad


# This api is auto-generated from IR AvgPoolV2GradD
@auto_convert_to_tensor([False, False, False], [False, True, True])
def AvgPoolV2GradD(input_grad: Tensor, mean_matrix: Optional[Tensor], kernel_matrix: Optional[Tensor], *, orig_input_shape: List[int], ksize: List[int], strides: List[int], padding_mode: str="CALCULATED", pads: List[int]=[0, 0, 0, 0], data_format: str="NCHW", global_pooling: bool=False, ceil_mode: bool=False, exclusive: bool=True, dependencies=[], node_name=None):
    """REG_OP(AvgPoolV2GradD)\n
.INPUT(input_grad, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(mean_matrix, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(kernel_matrix, TensorType({DT_FLOAT16}))\n
.OUTPUT(out_grad, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(orig_input_shape, ListInt)\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(padding_mode, String, "CALCULATED")\n
.ATTR(pads, ListInt, {0,0,0,0})\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(global_pooling, Bool, false)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(exclusive, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPoolV2GradD"
    op.name = next_unique_name(node_name, "AvgPoolV2GradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_grad.tensor)
    op.input_desc.add().CopyFrom(input_grad.desc)
    op.input_desc[-1].name = "input_grad"
    if mean_matrix is not None:
        op.input.append(mean_matrix.tensor)
        op.input_desc.add().CopyFrom(mean_matrix.desc)
        op.input_desc[-1].name = "mean_matrix"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean_matrix"
    if kernel_matrix is not None:
        op.input.append(kernel_matrix.tensor)
        op.input_desc.add().CopyFrom(kernel_matrix.desc)
        op.input_desc[-1].name = "kernel_matrix"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "kernel_matrix"

    # process attrs
    op.attr["orig_input_shape"].list.val_type = 2
    op.attr["orig_input_shape"].list.i.extend(orig_input_shape)
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["global_pooling"].b = global_pooling
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["exclusive"].b = exclusive

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_grad"
    out_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_grad


# This api is auto-generated from IR Upsample
@auto_convert_to_tensor([False], [False])
def Upsample(x: Tensor, *, scale: float=1.000000, stride_h: int=2, stride_w: int=2, dependencies=[], node_name=None):
    """REG_OP(Upsample)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(scale, Float, 1)\n
.ATTR(stride_h, Int, 2)\n
.ATTR(stride_w, Int, 2)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Upsample"
    op.name = next_unique_name(node_name, "Upsample")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["scale"].f = scale
    op.attr["stride_h"].i = stride_h
    op.attr["stride_w"].i = stride_w

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FractionalMaxPoolGrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def FractionalMaxPoolGrad(orig_input: Tensor, orig_output: Tensor, out_backprop: Tensor, row_pooling_sequence: Tensor, col_pooling_sequence: Tensor, *, overlapping: bool=False, dependencies=[], node_name=None):
    """REG_OP(FractionalMaxPoolGrad)\n
.INPUT(orig_input, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(orig_output, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(row_pooling_sequence, TensorType({ DT_INT64 }))\n
.INPUT(col_pooling_sequence, TensorType({ DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64 }))\n
.ATTR(overlapping, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FractionalMaxPoolGrad"
    op.name = next_unique_name(node_name, "FractionalMaxPoolGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_input.tensor)
    op.input_desc.add().CopyFrom(orig_input.desc)
    op.input_desc[-1].name = "orig_input"
    op.input.append(orig_output.tensor)
    op.input_desc.add().CopyFrom(orig_output.desc)
    op.input_desc[-1].name = "orig_output"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"
    op.input.append(row_pooling_sequence.tensor)
    op.input_desc.add().CopyFrom(row_pooling_sequence.desc)
    op.input_desc[-1].name = "row_pooling_sequence"
    op.input.append(col_pooling_sequence.tensor)
    op.input_desc.add().CopyFrom(col_pooling_sequence.desc)
    op.input_desc[-1].name = "col_pooling_sequence"

    # process attrs
    op.attr["overlapping"].b = overlapping

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FractionalAvgPool
@auto_convert_to_tensor([False], [False])
def FractionalAvgPool(x: Tensor, *, pooling_ratio: List[float]=[], pseudo_random: bool=False, overlapping: bool=False, deterministic: bool=False, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(FractionalAvgPool)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(row_pooling_sequence, TensorType({DT_INT64}))\n
.OUTPUT(col_pooling_sequence, TensorType({DT_INT64}))\n
.ATTR(pooling_ratio, ListFloat, {})\n
.ATTR(pseudo_random, Bool, false)\n
.ATTR(overlapping, Bool, false)\n
.ATTR(deterministic, Bool, false)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FractionalAvgPool"
    op.name = next_unique_name(node_name, "FractionalAvgPool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["pooling_ratio"].list.val_type = 3
    op.attr["pooling_ratio"].list.f.extend(pooling_ratio)
    op.attr["pseudo_random"].b = pseudo_random
    op.attr["overlapping"].b = overlapping
    op.attr["deterministic"].b = deterministic
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "row_pooling_sequence"
    row_pooling_sequence = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "col_pooling_sequence"
    col_pooling_sequence = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, row_pooling_sequence, col_pooling_sequence


# This api is auto-generated from IR FractionalMaxPool
@auto_convert_to_tensor([False], [False])
def FractionalMaxPool(x: Tensor, *, pooling_ratio: List[float]=[], pseudo_random: bool=False, overlapping: bool=False, deterministic: bool=False, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(FractionalMaxPool)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(row_pooling_sequence, TensorType({DT_INT64}))\n
.OUTPUT(col_pooling_sequence, TensorType({DT_INT64}))\n
.ATTR(pooling_ratio, ListFloat, {})\n
.ATTR(pseudo_random, Bool, false)\n
.ATTR(overlapping, Bool, false)\n
.ATTR(deterministic, Bool, false)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FractionalMaxPool"
    op.name = next_unique_name(node_name, "FractionalMaxPool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["pooling_ratio"].list.val_type = 3
    op.attr["pooling_ratio"].list.f.extend(pooling_ratio)
    op.attr["pseudo_random"].b = pseudo_random
    op.attr["overlapping"].b = overlapping
    op.attr["deterministic"].b = deterministic
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "row_pooling_sequence"
    row_pooling_sequence = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "col_pooling_sequence"
    col_pooling_sequence = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, row_pooling_sequence, col_pooling_sequence


# This api is auto-generated from IR NthElement
@auto_convert_to_tensor([False, False], [False, False])
def NthElement(x: Tensor, n: Tensor, *, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(NthElement)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(n, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NthElement"
    op.name = next_unique_name(node_name, "NthElement")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(n.tensor)
    op.input_desc.add().CopyFrom(n.desc)
    op.input_desc[-1].name = "n"

    # process attrs
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FractionalAvgPoolGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def FractionalAvgPoolGrad(orig_input_tensor_shape: Tensor, out_backprop: Tensor, row_pooling_sequence: Tensor, col_pooling_sequence: Tensor, *, overlapping: bool=False, dependencies=[], node_name=None):
    """REG_OP(FractionalAvgPoolGrad)\n
.INPUT(orig_input_tensor_shape, TensorType({DT_INT64}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(row_pooling_sequence, TensorType({DT_INT64}))\n
.INPUT(col_pooling_sequence, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.ATTR(overlapping, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FractionalAvgPoolGrad"
    op.name = next_unique_name(node_name, "FractionalAvgPoolGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_input_tensor_shape.tensor)
    op.input_desc.add().CopyFrom(orig_input_tensor_shape.desc)
    op.input_desc[-1].name = "orig_input_tensor_shape"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"
    op.input.append(row_pooling_sequence.tensor)
    op.input_desc.add().CopyFrom(row_pooling_sequence.desc)
    op.input_desc[-1].name = "row_pooling_sequence"
    op.input.append(col_pooling_sequence.tensor)
    op.input_desc.add().CopyFrom(col_pooling_sequence.desc)
    op.input_desc[-1].name = "col_pooling_sequence"

    # process attrs
    op.attr["overlapping"].b = overlapping

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DataFormatVecPermute
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def DataFormatVecPermute(x: Tensor, *, src_format: str="NHWC", dst_format: str="NCHW", dependencies=[], node_name=None):
    """REG_OP(DataFormatVecPermute)\n
.INPUT(x, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_INT32, DT_INT64 }))\n
.ATTR(src_format, String, "NHWC")\n
.ATTR(dst_format, String, "NCHW")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DataFormatVecPermute"
    op.name = next_unique_name(node_name, "DataFormatVecPermute")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["src_format"].s = compat_as_bytes(src_format)
    op.attr["dst_format"].s = compat_as_bytes(dst_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPool3DGrad
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def MaxPool3DGrad(orig_x: Tensor, orig_y: Tensor, grads: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], padding: str="SAME", data_format: str="NDHWC", dependencies=[], node_name=None):
    """REG_OP(MaxPool3DGrad)\n
.INPUT(orig_x, TensorType::RealNumberType())\n
.INPUT(orig_y, TensorType::RealNumberType())\n
.INPUT(grads, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(padding, String, "SAME")\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(data_format, String, "NDHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPool3DGrad"
    op.name = next_unique_name(node_name, "MaxPool3DGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_x.tensor)
    op.input_desc.add().CopyFrom(orig_x.desc)
    op.input_desc[-1].name = "orig_x"
    op.input.append(orig_y.tensor)
    op.input_desc.add().CopyFrom(orig_y.desc)
    op.input_desc[-1].name = "orig_y"
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["padding"].s = compat_as_bytes(padding)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPool1D
@auto_convert_to_tensor([False], [False])
def AvgPool1D(x: Tensor, *, ksize: int, strides: int, pads: List[int], ceil_mode: bool=False, count_include_pad: bool=False, dependencies=[], node_name=None):
    """REG_OP(AvgPool1D)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, Int)\n
.REQUIRED_ATTR(strides, Int)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(count_include_pad, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool1D"
    op.name = next_unique_name(node_name, "AvgPool1D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].i = ksize
    op.attr["strides"].i = strides
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["count_include_pad"].b = count_include_pad

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AvgPool1DD
@auto_convert_to_tensor([False, False], [False, False])
def AvgPool1DD(x: Tensor, assist_matrix: Tensor, *, ksize: int, strides: int, pads: List[int], ceil_mode: bool=False, count_include_pad: bool=False, dependencies=[], node_name=None):
    """REG_OP(AvgPool1DD)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(assist_matrix, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(ksize, Int)\n
.REQUIRED_ATTR(strides, Int)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(count_include_pad, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AvgPool1DD"
    op.name = next_unique_name(node_name, "AvgPool1DD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(assist_matrix.tensor)
    op.input_desc.add().CopyFrom(assist_matrix.desc)
    op.input_desc[-1].name = "assist_matrix"

    # process attrs
    op.attr["ksize"].i = ksize
    op.attr["strides"].i = strides
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["count_include_pad"].b = count_include_pad

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolWithArgmaxV2
@auto_convert_to_tensor([False], [False])
def MaxPoolWithArgmaxV2(x: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], dtype: int=3, dilation: List[int]=[1, 1, 1, 1], ceil_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(MaxPoolWithArgmaxV2)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.OUTPUT(argmax, TensorType({DT_UINT16}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dtype, Int, 3)\n
.ATTR(dilation, ListInt, {1, 1, 1, 1})\n
.ATTR(ceil_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolWithArgmaxV2"
    op.name = next_unique_name(node_name, "MaxPoolWithArgmaxV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dtype"].i = dtype
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["ceil_mode"].b = ceil_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, argmax


# This api is auto-generated from IR MaxPoolGradWithArgmaxV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MaxPoolGradWithArgmaxV2(x: Tensor, grad: Tensor, argmax: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], dtype: int=3, dilation: List[int]=[1, 1, 1, 1], ceil_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(MaxPoolGradWithArgmaxV2)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(grad, TensorType({DT_FLOAT16}))\n
.INPUT(argmax, TensorType({DT_UINT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dtype, Int, 3)\n
.ATTR(dilation, ListInt, {1,1,1,1})\n
.ATTR(ceil_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolGradWithArgmaxV2"
    op.name = next_unique_name(node_name, "MaxPoolGradWithArgmaxV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(argmax.tensor)
    op.input_desc.add().CopyFrom(argmax.desc)
    op.input_desc[-1].name = "argmax"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dtype"].i = dtype
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["ceil_mode"].b = ceil_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolV3
@auto_convert_to_tensor([False], [False])
def MaxPoolV3(x: Tensor, *, ksize: List[int], strides: List[int], padding_mode: str="CALCULATED", pads: List[int]=[0, 0, 0, 0], data_format: str="NCHW", global_pooling: bool=False, ceil_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(MaxPoolV3)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16, DT_QINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16, DT_QINT8}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(padding_mode, String, "CALCULATED")\n
.ATTR(pads, ListInt, {0,0,0,0})\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(global_pooling, Bool, false)\n
.ATTR(ceil_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolV3"
    op.name = next_unique_name(node_name, "MaxPoolV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["global_pooling"].b = global_pooling
    op.attr["ceil_mode"].b = ceil_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolV3Grad
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def MaxPoolV3Grad(orig_input: Tensor, orig_output: Tensor, grad: Tensor, *, ksize: List[int], strides: List[int], padding_mode: str="CALCULATED", pads: List[int]=[0, 0, 0, 0], data_format: str="NCHW", global_pooling: bool=False, ceil_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(MaxPoolV3Grad)\n
.INPUT(orig_input, TensorType::RealNumberType())\n
.INPUT(orig_output, TensorType::RealNumberType())\n
.INPUT(grad, TensorType::RealNumberType())\n
.OUTPUT(out_grad, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(padding_mode, String, "CALCULATED")\n
.ATTR(pads, ListInt, {0, 0, 0, 0})\n
.ATTR(data_format, String, "NCHW")\n
.ATTR(global_pooling, Bool, false)\n
.ATTR(ceil_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolV3Grad"
    op.name = next_unique_name(node_name, "MaxPoolV3Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(orig_input.tensor)
    op.input_desc.add().CopyFrom(orig_input.desc)
    op.input_desc[-1].name = "orig_input"
    op.input.append(orig_output.tensor)
    op.input_desc.add().CopyFrom(orig_output.desc)
    op.input_desc[-1].name = "orig_output"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["global_pooling"].b = global_pooling
    op.attr["ceil_mode"].b = ceil_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_grad"
    out_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out_grad


# This api is auto-generated from IR Dilation2D
@auto_convert_to_tensor([False, False], [False, False])
def Dilation2D(x: Tensor, filter: Tensor, *, strides: List[int], rates: List[int], padding_mode: str="SAME", pads: List[int]=[0, 0, 0, 0], ceil_mode: bool=False, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(Dilation2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(rates, ListInt)\n
.ATTR(padding_mode, String, "SAME")\n
.ATTR(pads, ListInt, {0,0,0,0})\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dilation2D"
    op.name = next_unique_name(node_name, "Dilation2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["rates"].list.val_type = 2
    op.attr["rates"].list.i.extend(rates)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Dilation2DBackpropFilter
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Dilation2DBackpropFilter(x: Tensor, filter: Tensor, out_backprop: Tensor, *, strides: List[int], rates: List[int], padding_mode: str="SAME", pads: List[int]=[0, 0, 0, 0], ceil_mode: bool=False, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(Dilation2DBackpropFilter)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(rates, ListInt)\n
.ATTR(padding_mode, String, "SAME")\n
.ATTR(pads, ListInt, {0, 0, 0, 0})\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dilation2DBackpropFilter"
    op.name = next_unique_name(node_name, "Dilation2DBackpropFilter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["rates"].list.val_type = 2
    op.attr["rates"].list.i.extend(rates)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Dilation2DBackpropInput
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Dilation2DBackpropInput(x: Tensor, filter: Tensor, out_backprop: Tensor, *, strides: List[int], rates: List[int], padding_mode: str="SAME", pads: List[int]=[0, 0, 0, 0], ceil_mode: bool=False, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(Dilation2DBackpropInput)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.INPUT(filter, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.INPUT(out_backprop, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_UINT8, DT_INT16, DT_INT8, DT_UINT16}))\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(rates, ListInt)\n
.ATTR(padding_mode, String, "SAME")\n
.ATTR(pads, ListInt, {0, 0, 0, 0})\n
.ATTR(ceil_mode, Bool, false)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dilation2DBackpropInput"
    op.name = next_unique_name(node_name, "Dilation2DBackpropInput")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(filter.tensor)
    op.input_desc.add().CopyFrom(filter.desc)
    op.input_desc[-1].name = "filter"
    op.input.append(out_backprop.tensor)
    op.input_desc.add().CopyFrom(out_backprop.desc)
    op.input_desc[-1].name = "out_backprop"

    # process attrs
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["rates"].list.val_type = 2
    op.attr["rates"].list.i.extend(rates)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["ceil_mode"].b = ceil_mode
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdaptiveAvgPool2d
@auto_convert_to_tensor([False], [False])
def AdaptiveAvgPool2d(x: Tensor, *, output_size: List[int], dependencies=[], node_name=None):
    """REG_OP(AdaptiveAvgPool2d)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(output_size, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdaptiveAvgPool2d"
    op.name = next_unique_name(node_name, "AdaptiveAvgPool2d")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_size"].list.val_type = 2
    op.attr["output_size"].list.i.extend(output_size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AdaptiveAvgPool2dGrad
@auto_convert_to_tensor([False], [False])
def AdaptiveAvgPool2dGrad(input_grad: Tensor, *, orig_input_shape: List[int], dependencies=[], node_name=None):
    """REG_OP(AdaptiveAvgPool2dGrad)\n
.INPUT(input_grad, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(output_grad, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(orig_input_shape, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AdaptiveAvgPool2dGrad"
    op.name = next_unique_name(node_name, "AdaptiveAvgPool2dGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_grad.tensor)
    op.input_desc.add().CopyFrom(input_grad.desc)
    op.input_desc[-1].name = "input_grad"

    # process attrs
    op.attr["orig_input_shape"].list.val_type = 2
    op.attr["orig_input_shape"].list.i.extend(orig_input_shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_grad"
    output_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_grad


# This api is auto-generated from IR MaxPoolGradWithArgmaxV1
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MaxPoolGradWithArgmaxV1(x: Tensor, grad: Tensor, argmax: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], dtype: int=3, dilation: List[int]=[1, 1, 1, 1], ceil_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(MaxPoolGradWithArgmaxV1)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(argmax, TensorType({DT_UINT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dtype, Int, 3)\n
.ATTR(dilation, ListInt, {1, 1, 1, 1})\n
.ATTR(ceil_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolGradWithArgmaxV1"
    op.name = next_unique_name(node_name, "MaxPoolGradWithArgmaxV1")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(argmax.tensor)
    op.input_desc.add().CopyFrom(argmax.desc)
    op.input_desc[-1].name = "argmax"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dtype"].i = dtype
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["ceil_mode"].b = ceil_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaxPoolWithArgmaxV1
@auto_convert_to_tensor([False], [False])
def MaxPoolWithArgmaxV1(x: Tensor, *, ksize: List[int], strides: List[int], pads: List[int], dtype: int=3, dilation: List[int]=[1, 1, 1, 1], ceil_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(MaxPoolWithArgmaxV1)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OUTPUT(argmax, TensorType({DT_UINT16, DT_INT32}))\n
.REQUIRED_ATTR(ksize, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(pads, ListInt)\n
.ATTR(dtype, Int, 3)\n
.ATTR(dilation, ListInt, {1, 1, 1, 1})\n
.ATTR(ceil_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaxPoolWithArgmaxV1"
    op.name = next_unique_name(node_name, "MaxPoolWithArgmaxV1")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksize"].list.val_type = 2
    op.attr["ksize"].list.i.extend(ksize)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)
    op.attr["dtype"].i = dtype
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["ceil_mode"].b = ceil_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "argmax"
    argmax = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, argmax


# This api is auto-generated from IR SubSample
@auto_convert_to_tensor([False], [False])
def SubSample(labels: Tensor, *, batch_size_per_images: int, positive_fraction: float, dependencies=[], node_name=None):
    """REG_OP(SubSample)\n
.INPUT(labels, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(batch_size_per_images, Int)\n
.REQUIRED_ATTR(positive_fraction, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SubSample"
    op.name = next_unique_name(node_name, "SubSample")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"

    # process attrs
    op.attr["batch_size_per_images"].i = batch_size_per_images
    op.attr["positive_fraction"].f = positive_fraction

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SubSampleLabels
@auto_convert_to_tensor([False, False], [False, False])
def SubSampleLabels(labels: Tensor, shuffle_matrix: Tensor, *, batch_size_per_images: int, positive_fraction: float, dependencies=[], node_name=None):
    """REG_OP(SubSampleLabels)\n
.INPUT(labels, TensorType({DT_INT32}))\n
.INPUT(shuffle_matrix, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(batch_size_per_images, Int)\n
.REQUIRED_ATTR(positive_fraction, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SubSampleLabels"
    op.name = next_unique_name(node_name, "SubSampleLabels")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(labels.tensor)
    op.input_desc.add().CopyFrom(labels.desc)
    op.input_desc[-1].name = "labels"
    op.input.append(shuffle_matrix.tensor)
    op.input_desc.add().CopyFrom(shuffle_matrix.desc)
    op.input_desc[-1].name = "shuffle_matrix"

    # process attrs
    op.attr["batch_size_per_images"].i = batch_size_per_images
    op.attr["positive_fraction"].f = positive_fraction

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GlobalLpPool
@auto_convert_to_tensor([False], [False])
def GlobalLpPool(x: Tensor, *, p: float=2.000000, dependencies=[], node_name=None):
    """REG_OP(GlobalLpPool)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(p, Float, 2.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GlobalLpPool"
    op.name = next_unique_name(node_name, "GlobalLpPool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].f = p

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GlobalAveragePool
@auto_convert_to_tensor([False], [False])
def GlobalAveragePool(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(GlobalAveragePool)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GlobalAveragePool"
    op.name = next_unique_name(node_name, "GlobalAveragePool")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InTopKV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def InTopKV2(predictions: Tensor, targets: Tensor, k: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InTopKV2)\n
.INPUT(predictions, TensorType({DT_FLOAT}))\n
.INPUT(targets, TensorType(IndexNumberType))\n
.INPUT(k, TensorType({IndexNumberType}))\n
.OUTPUT(precision, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InTopKV2"
    op.name = next_unique_name(node_name, "InTopKV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(predictions.tensor)
    op.input_desc.add().CopyFrom(predictions.desc)
    op.input_desc[-1].name = "predictions"
    op.input.append(targets.tensor)
    op.input_desc.add().CopyFrom(targets.desc)
    op.input_desc[-1].name = "targets"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "precision"
    precision = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return precision


# This api is auto-generated from IR FusedBatchNormV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True])
def FusedBatchNormV2(x: Tensor, scale: Tensor, offset: Tensor, mean: Optional[Tensor], variance: Optional[Tensor], *, epsilon: float=0.000100, data_format: str="NHWC", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(FusedBatchNormV2)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_1, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_space_2, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedBatchNormV2"
    op.name = next_unique_name(node_name, "FusedBatchNormV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_1"
    reserve_space_1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_space_2"
    reserve_space_2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance, reserve_space_1, reserve_space_2


# This api is auto-generated from IR SegmentSort
@auto_convert_to_tensor([False, False], [False, False])
def SegmentSort(input_data: Tensor, input_index: Tensor, *, k_num: int, largest: bool=True, dependencies=[], node_name=None):
    """REG_OP(SegmentSort)\n
.INPUT(input_data, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(input_index, TensorType({DT_FLOAT16,DT_INT32}))\n
.OUTPUT(output_proposal, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.REQUIRED_ATTR(k_num, Int)\n
.ATTR(largest, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SegmentSort"
    op.name = next_unique_name(node_name, "SegmentSort")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_data.tensor)
    op.input_desc.add().CopyFrom(input_data.desc)
    op.input_desc[-1].name = "input_data"
    op.input.append(input_index.tensor)
    op.input_desc.add().CopyFrom(input_index.desc)
    op.input_desc[-1].name = "input_index"

    # process attrs
    op.attr["k_num"].i = k_num
    op.attr["largest"].b = largest

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_proposal"
    output_proposal = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_proposal


# This api is auto-generated from IR MultiMerge
@auto_convert_to_tensor([False], [False])
def MultiMerge(input_proposal: Tensor, *, k_num: int, include_index: bool=False, largest: bool=True, dependencies=[], node_name=None):
    """REG_OP(MultiMerge)\n
.INPUT(input_proposal, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output_proposal, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(output_index, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(k_num, Int)\n
.ATTR(include_index, Bool, false)\n
.ATTR(largest, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultiMerge"
    op.name = next_unique_name(node_name, "MultiMerge")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_proposal.tensor)
    op.input_desc.add().CopyFrom(input_proposal.desc)
    op.input_desc[-1].name = "input_proposal"

    # process attrs
    op.attr["k_num"].i = k_num
    op.attr["include_index"].b = include_index
    op.attr["largest"].b = largest

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_proposal"
    output_proposal = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_index"
    output_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_proposal, output_index


# This api is auto-generated from IR SingleMerge
@auto_convert_to_tensor([False], [False])
def SingleMerge(input_proposal: Tensor, *, k_num: int, largest: bool=True, dependencies=[], node_name=None):
    """REG_OP(SingleMerge)\n
.INPUT(input_proposal, TensorType({ DT_FLOAT16 }))\n
.OUTPUT(output_data, TensorType({ DT_FLOAT16 }))\n
.OUTPUT(output_index, TensorType({ DT_INT32 }))\n
.REQUIRED_ATTR(k_num, Int)\n
.ATTR(largest, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SingleMerge"
    op.name = next_unique_name(node_name, "SingleMerge")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_proposal.tensor)
    op.input_desc.add().CopyFrom(input_proposal.desc)
    op.input_desc[-1].name = "input_proposal"

    # process attrs
    op.attr["k_num"].i = k_num
    op.attr["largest"].b = largest

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_data"
    output_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_index"
    output_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_data, output_index


# This api is auto-generated from IR MultiHeadAttention
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, True, True, True, True, True])
def MultiHeadAttention(query: Tensor, key: Tensor, value: Tensor, query_weight: Tensor, key_weight: Tensor, value_weight: Tensor, attn_mask: Tensor, out_proj_weight: Tensor, query_bias: Optional[Tensor], key_bias: Optional[Tensor], value_bias: Optional[Tensor], out_proj_bias: Optional[Tensor], dropout_mask_input: Optional[Tensor], *, attn_head_num: int, attn_dim_per_head: int, src_len: int, tgt_len: int, keep_prob: float, softmax_use_float: bool, dependencies=[], node_name=None):
    """REG_OP(MultiHeadAttention)\n
.INPUT(query, TensorType({DT_FLOAT16}))\n
.INPUT(key, TensorType({DT_FLOAT16}))\n
.INPUT(value, TensorType({DT_FLOAT16}))\n
.INPUT(query_weight, TensorType({DT_FLOAT16}))\n
.INPUT(key_weight, TensorType({DT_FLOAT16}))\n
.INPUT(value_weight, TensorType({DT_FLOAT16}))\n
.INPUT(attn_mask, TensorType({DT_FLOAT16}))\n
.INPUT(out_proj_weight, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(query_bias, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(key_bias, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(value_bias, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(out_proj_bias, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(dropout_mask_input, TensorType({DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16}))\n
.OUTPUT(dropout_mask, TensorType({DT_UINT8}))\n
.OUTPUT(query_res, TensorType({DT_FLOAT16}))\n
.OUTPUT(key_res, TensorType({DT_FLOAT16}))\n
.OUTPUT(value_res, TensorType({DT_FLOAT16}))\n
.OUTPUT(attn_scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(attn_res, TensorType({DT_FLOAT16}))\n
.OUTPUT(context, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(attn_head_num, Int)\n
.REQUIRED_ATTR(attn_dim_per_head, Int)\n
.REQUIRED_ATTR(src_len, Int)\n
.REQUIRED_ATTR(tgt_len, Int)\n
.REQUIRED_ATTR(keep_prob, Float)\n
.REQUIRED_ATTR(softmax_use_float, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultiHeadAttention"
    op.name = next_unique_name(node_name, "MultiHeadAttention")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(query_weight.tensor)
    op.input_desc.add().CopyFrom(query_weight.desc)
    op.input_desc[-1].name = "query_weight"
    op.input.append(key_weight.tensor)
    op.input_desc.add().CopyFrom(key_weight.desc)
    op.input_desc[-1].name = "key_weight"
    op.input.append(value_weight.tensor)
    op.input_desc.add().CopyFrom(value_weight.desc)
    op.input_desc[-1].name = "value_weight"
    op.input.append(attn_mask.tensor)
    op.input_desc.add().CopyFrom(attn_mask.desc)
    op.input_desc[-1].name = "attn_mask"
    op.input.append(out_proj_weight.tensor)
    op.input_desc.add().CopyFrom(out_proj_weight.desc)
    op.input_desc[-1].name = "out_proj_weight"
    if query_bias is not None:
        op.input.append(query_bias.tensor)
        op.input_desc.add().CopyFrom(query_bias.desc)
        op.input_desc[-1].name = "query_bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "query_bias"
    if key_bias is not None:
        op.input.append(key_bias.tensor)
        op.input_desc.add().CopyFrom(key_bias.desc)
        op.input_desc[-1].name = "key_bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "key_bias"
    if value_bias is not None:
        op.input.append(value_bias.tensor)
        op.input_desc.add().CopyFrom(value_bias.desc)
        op.input_desc[-1].name = "value_bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "value_bias"
    if out_proj_bias is not None:
        op.input.append(out_proj_bias.tensor)
        op.input_desc.add().CopyFrom(out_proj_bias.desc)
        op.input_desc[-1].name = "out_proj_bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "out_proj_bias"
    if dropout_mask_input is not None:
        op.input.append(dropout_mask_input.tensor)
        op.input_desc.add().CopyFrom(dropout_mask_input.desc)
        op.input_desc[-1].name = "dropout_mask_input"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dropout_mask_input"

    # process attrs
    op.attr["attn_head_num"].i = attn_head_num
    op.attr["attn_dim_per_head"].i = attn_dim_per_head
    op.attr["src_len"].i = src_len
    op.attr["tgt_len"].i = tgt_len
    op.attr["keep_prob"].f = keep_prob
    op.attr["softmax_use_float"].b = softmax_use_float

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dropout_mask"
    dropout_mask = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "query_res"
    query_res = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "key_res"
    key_res = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_res"
    value_res = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "attn_scores"
    attn_scores = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "attn_res"
    attn_res = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "context"
    context = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, dropout_mask, query_res, key_res, value_res, attn_scores, attn_res, context


# This api is auto-generated from IR MultiHeadAttentionGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, True])
def MultiHeadAttentionGrad(query: Tensor, key: Tensor, value: Tensor, query_weight: Tensor, key_weight: Tensor, value_weight: Tensor, out_proj_weight: Tensor, query_res: Tensor, key_res: Tensor, value_res: Tensor, attn_scores: Tensor, attn_res: Tensor, context: Tensor, y_grad: Tensor, dropout_mask: Optional[Tensor], *, attn_head_num: int, attn_dim_per_head: int, src_len: int, tgt_len: int, keep_prob: float, softmax_use_float: bool, bias_grad_mask: List[bool], dependencies=[], node_name=None):
    """REG_OP(MultiHeadAttentionGrad)\n
.INPUT(query, TensorType({DT_FLOAT16}))\n
.INPUT(key, TensorType({DT_FLOAT16}))\n
.INPUT(value, TensorType({DT_FLOAT16}))\n
.INPUT(query_weight, TensorType({DT_FLOAT16}))\n
.INPUT(key_weight, TensorType({DT_FLOAT16}))\n
.INPUT(value_weight, TensorType({DT_FLOAT16}))\n
.INPUT(out_proj_weight, TensorType({DT_FLOAT16}))\n
.INPUT(query_res, TensorType({DT_FLOAT16}))\n
.INPUT(key_res, TensorType({DT_FLOAT16}))\n
.INPUT(value_res, TensorType({DT_FLOAT16}))\n
.INPUT(attn_scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(attn_res, TensorType({DT_FLOAT16}))\n
.INPUT(context, TensorType({DT_FLOAT16}))\n
.INPUT(y_grad, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(dropout_mask, TensorType({DT_UINT8}))\n
.OUTPUT(query_weight_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(key_weight_grad, TensorType({DT_UINT8}))\n
.OUTPUT(value_weight_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(out_proj_weight_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(query_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(key_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(value_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(query_bias_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(key_bias_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(value_bias_grad, TensorType({DT_FLOAT16}))\n
.OUTPUT(out_proj_bias_grad, TensorType({DT_FLOAT16}))\n
.REQUIRED_ATTR(attn_head_num, Int)\n
.REQUIRED_ATTR(attn_dim_per_head, Int)\n
.REQUIRED_ATTR(src_len, Int)\n
.REQUIRED_ATTR(tgt_len, Int)\n
.REQUIRED_ATTR(keep_prob, Float)\n
.REQUIRED_ATTR(softmax_use_float, Bool)\n
.REQUIRED_ATTR(bias_grad_mask, ListBool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultiHeadAttentionGrad"
    op.name = next_unique_name(node_name, "MultiHeadAttentionGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(query_weight.tensor)
    op.input_desc.add().CopyFrom(query_weight.desc)
    op.input_desc[-1].name = "query_weight"
    op.input.append(key_weight.tensor)
    op.input_desc.add().CopyFrom(key_weight.desc)
    op.input_desc[-1].name = "key_weight"
    op.input.append(value_weight.tensor)
    op.input_desc.add().CopyFrom(value_weight.desc)
    op.input_desc[-1].name = "value_weight"
    op.input.append(out_proj_weight.tensor)
    op.input_desc.add().CopyFrom(out_proj_weight.desc)
    op.input_desc[-1].name = "out_proj_weight"
    op.input.append(query_res.tensor)
    op.input_desc.add().CopyFrom(query_res.desc)
    op.input_desc[-1].name = "query_res"
    op.input.append(key_res.tensor)
    op.input_desc.add().CopyFrom(key_res.desc)
    op.input_desc[-1].name = "key_res"
    op.input.append(value_res.tensor)
    op.input_desc.add().CopyFrom(value_res.desc)
    op.input_desc[-1].name = "value_res"
    op.input.append(attn_scores.tensor)
    op.input_desc.add().CopyFrom(attn_scores.desc)
    op.input_desc[-1].name = "attn_scores"
    op.input.append(attn_res.tensor)
    op.input_desc.add().CopyFrom(attn_res.desc)
    op.input_desc[-1].name = "attn_res"
    op.input.append(context.tensor)
    op.input_desc.add().CopyFrom(context.desc)
    op.input_desc[-1].name = "context"
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    if dropout_mask is not None:
        op.input.append(dropout_mask.tensor)
        op.input_desc.add().CopyFrom(dropout_mask.desc)
        op.input_desc[-1].name = "dropout_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dropout_mask"

    # process attrs
    op.attr["attn_head_num"].i = attn_head_num
    op.attr["attn_dim_per_head"].i = attn_dim_per_head
    op.attr["src_len"].i = src_len
    op.attr["tgt_len"].i = tgt_len
    op.attr["keep_prob"].f = keep_prob
    op.attr["softmax_use_float"].b = softmax_use_float
    op.attr["bias_grad_mask"].list.val_type = 4
    op.attr["bias_grad_mask"].list.b.extend(bias_grad_mask)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "query_weight_grad"
    query_weight_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "key_weight_grad"
    key_weight_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_weight_grad"
    value_weight_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "out_proj_weight_grad"
    out_proj_weight_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "query_grad"
    query_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "key_grad"
    key_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_grad"
    value_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "query_bias_grad"
    query_bias_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "key_bias_grad"
    key_bias_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "value_bias_grad"
    value_bias_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "out_proj_bias_grad"
    out_proj_bias_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return query_weight_grad, key_weight_grad, value_weight_grad, out_proj_weight_grad, query_grad, key_grad, value_grad, query_bias_grad, key_bias_grad, value_bias_grad, out_proj_bias_grad


# This api is auto-generated from IR ApplyAdaMax
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdaMax(var: Tensor, m: Tensor, v: Tensor, beta1_power: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdaMax)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(v, TensorType::NumberType())\n
.INPUT(beta1_power, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(beta1, TensorType::NumberType())\n
.INPUT(beta2, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdaMax"
    op.name = next_unique_name(node_name, "ApplyAdaMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdaMaxD
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdaMaxD(var: Tensor, m: Tensor, v: Tensor, beta1_power: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdaMaxD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(v, TensorType::NumberType())\n
.INPUT(beta1_power, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(beta1, TensorType::NumberType())\n
.INPUT(beta2, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(m, TensorType::NumberType())\n
.OUTPUT(v, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdaMaxD"
    op.name = next_unique_name(node_name, "ApplyAdaMaxD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "v"
    v = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m, v


# This api is auto-generated from IR SparseApplyAdagrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def SparseApplyAdagrad(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, indices: Tensor, *, use_locking: bool=False, update_slots: bool=True, dependencies=[], node_name=None):
    """REG_OP(SparseApplyAdagrad)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(lr, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.ATTR(use_locking, Bool, false)\n
.ATTR(update_slots, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyAdagrad"
    op.name = next_unique_name(node_name, "SparseApplyAdagrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["use_locking"].b = use_locking
    op.attr["update_slots"].b = update_slots

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR SparseApplyAdagradD
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseApplyAdagradD(var: Tensor, accum: Tensor, grad: Tensor, indices: Tensor, *, lr: float, use_locking: bool=False, update_slots: bool=True, dependencies=[], node_name=None):
    """REG_OP(SparseApplyAdagradD)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(lr, Float)\n
.ATTR(use_locking, Bool, false)\n
.ATTR(update_slots, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyAdagradD"
    op.name = next_unique_name(node_name, "SparseApplyAdagradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["lr"].f = lr
    op.attr["use_locking"].b = use_locking
    op.attr["update_slots"].b = update_slots

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR SparseApplyAdagradV2
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def SparseApplyAdagradV2(var: Tensor, accum: Tensor, lr: Tensor, epsilon: Tensor, grad: Tensor, indices: Tensor, *, use_locking: bool=False, update_slots: bool=True, dependencies=[], node_name=None):
    """REG_OP(SparseApplyAdagradV2)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(lr, TensorType({DT_FLOAT}))\n
.INPUT(epsilon, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.ATTR(use_locking, Bool, false)\n
.ATTR(update_slots, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyAdagradV2"
    op.name = next_unique_name(node_name, "SparseApplyAdagradV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["use_locking"].b = use_locking
    op.attr["update_slots"].b = update_slots

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR SparseApplyAdagradV2D
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseApplyAdagradV2D(var: Tensor, accum: Tensor, grad: Tensor, indices: Tensor, *, lr: float, epsilon: float, use_locking: bool=False, update_slots: bool=True, dependencies=[], node_name=None):
    """REG_OP(SparseApplyAdagradV2D)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(lr, Float)\n
.REQUIRED_ATTR(epsilon, Float)\n
.ATTR(use_locking, Bool, false)\n
.ATTR(update_slots, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyAdagradV2D"
    op.name = next_unique_name(node_name, "SparseApplyAdagradV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["lr"].f = lr
    op.attr["epsilon"].f = epsilon
    op.attr["use_locking"].b = use_locking
    op.attr["update_slots"].b = update_slots

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR ApplyMomentum
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyMomentum(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, momentum: Tensor, *, use_nesterov: bool=False, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyMomentum)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_nesterov, Bool, false)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyMomentum"
    op.name = next_unique_name(node_name, "ApplyMomentum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"

    # process attrs
    op.attr["use_nesterov"].b = use_nesterov
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyMomentumD
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyMomentumD(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, momentum: Tensor, *, use_nesterov: bool=False, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyMomentumD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_nesterov, Bool, false)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyMomentumD"
    op.name = next_unique_name(node_name, "ApplyMomentumD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"

    # process attrs
    op.attr["use_nesterov"].b = use_nesterov
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR ApplyKerasMomentum
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyKerasMomentum(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, momentum: Tensor, *, use_locking: bool=False, use_nesterov: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyKerasMomentum)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
.ATTR(use_nesterov, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyKerasMomentum"
    op.name = next_unique_name(node_name, "ApplyKerasMomentum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"

    # process attrs
    op.attr["use_locking"].b = use_locking
    op.attr["use_nesterov"].b = use_nesterov

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyKerasMomentumD
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyKerasMomentumD(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, momentum: Tensor, *, use_locking: bool=False, use_nesterov: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyKerasMomentumD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
.ATTR(use_nesterov, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyKerasMomentumD"
    op.name = next_unique_name(node_name, "ApplyKerasMomentumD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"

    # process attrs
    op.attr["use_locking"].b = use_locking
    op.attr["use_nesterov"].b = use_nesterov

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR ApplyAdamWithAmsgradD
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdamWithAmsgradD(var: Tensor, m: Tensor, v: Tensor, vhat: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, grad: Tensor, *, beta1: float, beta2: float, epsilon: float, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdamWithAmsgradD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(v, TensorType::NumberType())\n
.INPUT(vhat, TensorType::NumberType())\n
.INPUT(beta1_power, TensorType::NumberType())\n
.INPUT(beta2_power, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(m, TensorType::NumberType())\n
.OUTPUT(v, TensorType::NumberType())\n
.OUTPUT(vhat, TensorType::NumberType())\n
.REQUIRED_ATTR(beta1, Float)\n
.REQUIRED_ATTR(beta2, Float)\n
.REQUIRED_ATTR(epsilon, Float)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdamWithAmsgradD"
    op.name = next_unique_name(node_name, "ApplyAdamWithAmsgradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(vhat.tensor)
    op.input_desc.add().CopyFrom(vhat.desc)
    op.input_desc[-1].name = "vhat"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["beta1"].f = beta1
    op.attr["beta2"].f = beta2
    op.attr["epsilon"].f = epsilon
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "v"
    v = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "vhat"
    vhat = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m, v, vhat


# This api is auto-generated from IR ApplyAdamWithAmsgrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdamWithAmsgrad(var: Tensor, m: Tensor, v: Tensor, vhat: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdamWithAmsgrad)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(v, TensorType::NumberType())\n
.INPUT(vhat, TensorType::NumberType())\n
.INPUT(beta1_power, TensorType::NumberType())\n
.INPUT(beta2_power, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(beta1, TensorType::NumberType())\n
.INPUT(beta2, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdamWithAmsgrad"
    op.name = next_unique_name(node_name, "ApplyAdamWithAmsgrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(vhat.tensor)
    op.input_desc.add().CopyFrom(vhat.desc)
    op.input_desc[-1].name = "vhat"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdamWithAmsgradV2
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False])
def ApplyAdamWithAmsgradV2(var: Tensor, m: Tensor, v: Tensor, vhat: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdamWithAmsgradV2)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(m, TensorType({DT_FLOAT}))\n
.INPUT(v, TensorType({DT_FLOAT}))\n
.INPUT(vhat, TensorType({DT_FLOAT}))\n
.INPUT(beta1_power, TensorType({DT_FLOAT}))\n
.INPUT(beta2_power, TensorType({DT_FLOAT}))\n
.INPUT(lr, TensorType({DT_FLOAT}))\n
.INPUT(beta1, TensorType({DT_FLOAT}))\n
.INPUT(beta2, TensorType({DT_FLOAT}))\n
.INPUT(epsilon, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(m, TensorType({DT_FLOAT}))\n
.OUTPUT(v, TensorType({DT_FLOAT}))\n
.OUTPUT(vhat, TensorType({DT_FLOAT}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdamWithAmsgradV2"
    op.name = next_unique_name(node_name, "ApplyAdamWithAmsgradV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(vhat.tensor)
    op.input_desc.add().CopyFrom(vhat.desc)
    op.input_desc[-1].name = "vhat"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "v"
    v = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "vhat"
    vhat = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m, v, vhat


# This api is auto-generated from IR ApplyPowerSign
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyPowerSign(var: Tensor, m: Tensor, lr: Tensor, logbase: Tensor, sign_decay: Tensor, beta: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyPowerSign)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(logbase, TensorType::NumberType())\n
.INPUT(sign_decay, TensorType::NumberType())\n
.INPUT(beta, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyPowerSign"
    op.name = next_unique_name(node_name, "ApplyPowerSign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(logbase.tensor)
    op.input_desc.add().CopyFrom(logbase.desc)
    op.input_desc[-1].name = "logbase"
    op.input.append(sign_decay.tensor)
    op.input_desc.add().CopyFrom(sign_decay.desc)
    op.input_desc[-1].name = "sign_decay"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyPowerSignD
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyPowerSignD(var: Tensor, m: Tensor, lr: Tensor, logbase: Tensor, sign_decay: Tensor, beta: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyPowerSignD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(logbase, TensorType::NumberType())\n
.INPUT(sign_decay, TensorType::NumberType())\n
.INPUT(beta, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(m, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyPowerSignD"
    op.name = next_unique_name(node_name, "ApplyPowerSignD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(logbase.tensor)
    op.input_desc.add().CopyFrom(logbase.desc)
    op.input_desc[-1].name = "logbase"
    op.input.append(sign_decay.tensor)
    op.input_desc.add().CopyFrom(sign_decay.desc)
    op.input_desc[-1].name = "sign_decay"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m


# This api is auto-generated from IR ApplyProximalGradientDescent
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyProximalGradientDescent(var: Tensor, alpha: Tensor, l1: Tensor, l2: Tensor, delta: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyProximalGradientDescent)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(alpha, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(delta, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyProximalGradientDescent"
    op.name = next_unique_name(node_name, "ApplyProximalGradientDescent")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(delta.tensor)
    op.input_desc.add().CopyFrom(delta.desc)
    op.input_desc[-1].name = "delta"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAddSign
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAddSign(var: Tensor, m: Tensor, lr: Tensor, alpha: Tensor, sign_decay: Tensor, beta: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAddSign)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(alpha, TensorType::NumberType())\n
.INPUT(sign_decay, TensorType::NumberType())\n
.INPUT(beta, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAddSign"
    op.name = next_unique_name(node_name, "ApplyAddSign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"
    op.input.append(sign_decay.tensor)
    op.input_desc.add().CopyFrom(sign_decay.desc)
    op.input_desc[-1].name = "sign_decay"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAddSignD
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAddSignD(var: Tensor, m: Tensor, lr: Tensor, alpha: Tensor, sign_decay: Tensor, beta: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAddSignD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(alpha, TensorType::NumberType())\n
.INPUT(sign_decay, TensorType::NumberType())\n
.INPUT(beta, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(m, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAddSignD"
    op.name = next_unique_name(node_name, "ApplyAddSignD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"
    op.input.append(sign_decay.tensor)
    op.input_desc.add().CopyFrom(sign_decay.desc)
    op.input_desc[-1].name = "sign_decay"
    op.input.append(beta.tensor)
    op.input_desc.add().CopyFrom(beta.desc)
    op.input_desc[-1].name = "beta"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m


# This api is auto-generated from IR ApplyCenteredRMSProp
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyCenteredRMSProp(var: Tensor, mg: Tensor, ms: Tensor, mom: Tensor, lr: Tensor, rho: Tensor, momentum: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyCenteredRMSProp)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(mg, TensorType::NumberType())\n
.INPUT(ms, TensorType::NumberType())\n
.INPUT(mom, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyCenteredRMSProp"
    op.name = next_unique_name(node_name, "ApplyCenteredRMSProp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(mg.tensor)
    op.input_desc.add().CopyFrom(mg.desc)
    op.input_desc[-1].name = "mg"
    op.input.append(ms.tensor)
    op.input_desc.add().CopyFrom(ms.desc)
    op.input_desc[-1].name = "ms"
    op.input.append(mom.tensor)
    op.input_desc.add().CopyFrom(mom.desc)
    op.input_desc[-1].name = "mom"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyCenteredRMSPropD
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyCenteredRMSPropD(var: Tensor, mg: Tensor, ms: Tensor, mom: Tensor, lr: Tensor, rho: Tensor, momentum: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyCenteredRMSPropD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(mg, TensorType::NumberType())\n
.INPUT(ms, TensorType::NumberType())\n
.INPUT(mom, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(mg, TensorType::NumberType())\n
.OUTPUT(ms, TensorType::NumberType())\n
.OUTPUT(mom, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyCenteredRMSPropD"
    op.name = next_unique_name(node_name, "ApplyCenteredRMSPropD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(mg.tensor)
    op.input_desc.add().CopyFrom(mg.desc)
    op.input_desc[-1].name = "mg"
    op.input.append(ms.tensor)
    op.input_desc.add().CopyFrom(ms.desc)
    op.input_desc[-1].name = "ms"
    op.input.append(mom.tensor)
    op.input_desc.add().CopyFrom(mom.desc)
    op.input_desc[-1].name = "mom"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mg"
    mg = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ms"
    ms = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mom"
    mom = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, mg, ms, mom


# This api is auto-generated from IR ApplyGradientDescent
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyGradientDescent(var: Tensor, alpha: Tensor, delta: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyGradientDescent)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(alpha, TensorType::NumberType())\n
.INPUT(delta, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyGradientDescent"
    op.name = next_unique_name(node_name, "ApplyGradientDescent")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"
    op.input.append(delta.tensor)
    op.input_desc.add().CopyFrom(delta.desc)
    op.input_desc[-1].name = "delta"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdagrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdagrad(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, *, update_slots: bool=True, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdagrad)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(update_slots, Bool, true)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdagrad"
    op.name = next_unique_name(node_name, "ApplyAdagrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["update_slots"].b = update_slots
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdagradD
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdagradD(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, *, update_slots: bool=True, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdagradD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(update_slots, Bool, true)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdagradD"
    op.name = next_unique_name(node_name, "ApplyAdagradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["update_slots"].b = update_slots
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR ApplyAdagradV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdagradV2(var: Tensor, accum: Tensor, lr: Tensor, epsilon: Tensor, grad: Tensor, *, update_slots: bool=True, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdagradV2)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(update_slots, Bool, true)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdagradV2"
    op.name = next_unique_name(node_name, "ApplyAdagradV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["update_slots"].b = update_slots
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdagradV2D
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdagradV2D(var: Tensor, accum: Tensor, lr: Tensor, grad: Tensor, *, epsilon: float, update_slots: bool=True, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdagradV2D)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.REQUIRED_ATTR(epsilon, Float)\n
.ATTR(update_slots, Bool, true)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdagradV2D"
    op.name = next_unique_name(node_name, "ApplyAdagradV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["update_slots"].b = update_slots
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR ApplyAdagradDA
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ApplyAdagradDA(var: Tensor, gradient_accumulator: Tensor, gradient_squared_accumulator: Tensor, grad: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, global_step: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdagradDA)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(gradient_accumulator, TensorType::NumberType())\n
.INPUT(gradient_squared_accumulator, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(global_step, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdagradDA"
    op.name = next_unique_name(node_name, "ApplyAdagradDA")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(gradient_accumulator.tensor)
    op.input_desc.add().CopyFrom(gradient_accumulator.desc)
    op.input_desc[-1].name = "gradient_accumulator"
    op.input.append(gradient_squared_accumulator.tensor)
    op.input_desc.add().CopyFrom(gradient_squared_accumulator.desc)
    op.input_desc[-1].name = "gradient_squared_accumulator"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(global_step.tensor)
    op.input_desc.add().CopyFrom(global_step.desc)
    op.input_desc[-1].name = "global_step"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdagradDAD
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ApplyAdagradDAD(var: Tensor, gradient_accumulator: Tensor, gradient_squared_accumulator: Tensor, grad: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, global_step: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdagradDAD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(gradient_accumulator, TensorType::NumberType())\n
.INPUT(gradient_squared_accumulator, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(global_step, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(gradient_accumulator, TensorType::NumberType())\n
.OUTPUT(gradient_squared_accumulator, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdagradDAD"
    op.name = next_unique_name(node_name, "ApplyAdagradDAD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(gradient_accumulator.tensor)
    op.input_desc.add().CopyFrom(gradient_accumulator.desc)
    op.input_desc[-1].name = "gradient_accumulator"
    op.input.append(gradient_squared_accumulator.tensor)
    op.input_desc.add().CopyFrom(gradient_squared_accumulator.desc)
    op.input_desc[-1].name = "gradient_squared_accumulator"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(global_step.tensor)
    op.input_desc.add().CopyFrom(global_step.desc)
    op.input_desc[-1].name = "global_step"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "gradient_accumulator"
    gradient_accumulator = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "gradient_squared_accumulator"
    gradient_squared_accumulator = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, gradient_accumulator, gradient_squared_accumulator


# This api is auto-generated from IR DataFormatDimMap
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def DataFormatDimMap(x: Tensor, *, src_format: str="NHWC", dst_format: str="NCHW", dependencies=[], node_name=None):
    """REG_OP(DataFormatDimMap)\n
.INPUT(x, TensorType::IndexNumberType())\n
.ATTR(src_format, String, "NHWC")\n
.ATTR(dst_format, String, "NCHW")\n
.OUTPUT(y, TensorType::IndexNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DataFormatDimMap"
    op.name = next_unique_name(node_name, "DataFormatDimMap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["src_format"].s = compat_as_bytes(src_format)
    op.attr["dst_format"].s = compat_as_bytes(dst_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SGD
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def SGD(parameters: Tensor, gradient: Tensor, learning_rate: Tensor, accum: Tensor, momentum: Tensor, stat: Tensor, *, dampening: float=0.000000, weight_decay: float=0.000000, nesterov: bool=False, dependencies=[], node_name=None):
    """REG_OP(SGD)\n
.INPUT(parameters, TensorType(DT_FLOAT, DT_FLOAT16))\n
.INPUT(gradient, TensorType(DT_FLOAT, DT_FLOAT16))\n
.INPUT(learning_rate, TensorType(DT_FLOAT, DT_FLOAT16))\n
.INPUT(accum, TensorType(DT_FLOAT, DT_FLOAT16))\n
.INPUT(momentum, TensorType(DT_FLOAT, DT_FLOAT16))\n
.INPUT(stat, TensorType(DT_FLOAT, DT_FLOAT16))\n
.OUTPUT(parameters, TensorType(DT_FLOAT, DT_FLOAT16))\n
.ATTR(dampening, Float, 0.0)\n
.ATTR(weight_decay, Float, 0.0)\n
.ATTR(nesterov, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SGD"
    op.name = next_unique_name(node_name, "SGD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(parameters.tensor)
    op.input_desc.add().CopyFrom(parameters.desc)
    op.input_desc[-1].name = "parameters"
    op.input.append(gradient.tensor)
    op.input_desc.add().CopyFrom(gradient.desc)
    op.input_desc[-1].name = "gradient"
    op.input.append(learning_rate.tensor)
    op.input_desc.add().CopyFrom(learning_rate.desc)
    op.input_desc[-1].name = "learning_rate"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(stat.tensor)
    op.input_desc.add().CopyFrom(stat.desc)
    op.input_desc[-1].name = "stat"

    # process attrs
    op.attr["dampening"].f = dampening
    op.attr["weight_decay"].f = weight_decay
    op.attr["nesterov"].b = nesterov

    # process outputs
    output_index = 0
    op.output_desc.add().name = "parameters"
    parameters = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return parameters


# This api is auto-generated from IR ApplyRMSProp
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyRMSProp(var: Tensor, ms: Tensor, mom: Tensor, lr: Tensor, rho: Tensor, momentum: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyRMSProp)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(ms, TensorType::NumberType())\n
.INPUT(mom, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyRMSProp"
    op.name = next_unique_name(node_name, "ApplyRMSProp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(ms.tensor)
    op.input_desc.add().CopyFrom(ms.desc)
    op.input_desc[-1].name = "ms"
    op.input.append(mom.tensor)
    op.input_desc.add().CopyFrom(mom.desc)
    op.input_desc[-1].name = "mom"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyRMSPropD
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyRMSPropD(var: Tensor, ms: Tensor, mom: Tensor, lr: Tensor, grad: Tensor, *, rho: float, momentum: float, epsilon: float, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyRMSPropD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(ms, TensorType::NumberType())\n
.INPUT(mom, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(ms, TensorType::NumberType())\n
.OUTPUT(mom, TensorType::NumberType())\n
.REQUIRED_ATTR(rho, Float)\n
.REQUIRED_ATTR(momentum, Float)\n
.REQUIRED_ATTR(epsilon, Float)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyRMSPropD"
    op.name = next_unique_name(node_name, "ApplyRMSPropD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(ms.tensor)
    op.input_desc.add().CopyFrom(ms.desc)
    op.input_desc[-1].name = "ms"
    op.input.append(mom.tensor)
    op.input_desc.add().CopyFrom(mom.desc)
    op.input_desc[-1].name = "mom"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["rho"].f = rho
    op.attr["momentum"].f = momentum
    op.attr["epsilon"].f = epsilon
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ms"
    ms = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mom"
    mom = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, ms, mom


# This api is auto-generated from IR ApplyProximalAdagrad
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyProximalAdagrad(var: Tensor, accum: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyProximalAdagrad)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyProximalAdagrad"
    op.name = next_unique_name(node_name, "ApplyProximalAdagrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyProximalAdagradD
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyProximalAdagradD(var: Tensor, accum: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyProximalAdagradD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyProximalAdagradD"
    op.name = next_unique_name(node_name, "ApplyProximalAdagradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR SparseApplyProximalAdagrad
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseApplyProximalAdagrad(var: Tensor, accum: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, grad: Tensor, indices: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyProximalAdagrad)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyProximalAdagrad"
    op.name = next_unique_name(node_name, "SparseApplyProximalAdagrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR SparseApplyProximalAdagradD
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseApplyProximalAdagradD(var: Tensor, accum: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, grad: Tensor, indices: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyProximalAdagradD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyProximalAdagradD"
    op.name = next_unique_name(node_name, "SparseApplyProximalAdagradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR ApplyFtrl
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyFtrl(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, lr_power: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyFtrl)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(linear, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(lr_power, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyFtrl"
    op.name = next_unique_name(node_name, "ApplyFtrl")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(lr_power.tensor)
    op.input_desc.add().CopyFrom(lr_power.desc)
    op.input_desc[-1].name = "lr_power"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyFtrlD
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyFtrlD(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, lr_power: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyFtrlD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(linear, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(lr_power, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.OUTPUT(linear, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyFtrlD"
    op.name = next_unique_name(node_name, "ApplyFtrlD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(lr_power.tensor)
    op.input_desc.add().CopyFrom(lr_power.desc)
    op.input_desc[-1].name = "lr_power"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "linear"
    linear = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, linear


# This api is auto-generated from IR ApplyFtrlV2
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyFtrlV2(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, l2_shrinkage: Tensor, lr_power: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyFtrlV2)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(linear, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(l2_shrinkage, TensorType::NumberType())\n
.INPUT(lr_power, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyFtrlV2"
    op.name = next_unique_name(node_name, "ApplyFtrlV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(l2_shrinkage.tensor)
    op.input_desc.add().CopyFrom(l2_shrinkage.desc)
    op.input_desc[-1].name = "l2_shrinkage"
    op.input.append(lr_power.tensor)
    op.input_desc.add().CopyFrom(lr_power.desc)
    op.input_desc[-1].name = "lr_power"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyFtrlV2D
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyFtrlV2D(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, l2_shrinkage: Tensor, lr_power: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyFtrlV2D)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(linear, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(l1, TensorType::NumberType())\n
.INPUT(l2, TensorType::NumberType())\n
.INPUT(l2_shrinkage, TensorType::NumberType())\n
.INPUT(lr_power, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.OUTPUT(linear, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyFtrlV2D"
    op.name = next_unique_name(node_name, "ApplyFtrlV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(l2_shrinkage.tensor)
    op.input_desc.add().CopyFrom(l2_shrinkage.desc)
    op.input_desc[-1].name = "l2_shrinkage"
    op.input.append(lr_power.tensor)
    op.input_desc.add().CopyFrom(lr_power.desc)
    op.input_desc[-1].name = "lr_power"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "linear"
    linear = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, linear


# This api is auto-generated from IR ApplyAdam
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdam(var: Tensor, m: Tensor, v: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, use_nesterov: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdam)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(v, TensorType::NumberType())\n
.INPUT(beta1_power, TensorType::NumberType())\n
.INPUT(beta2_power, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(beta1, TensorType::NumberType())\n
.INPUT(beta2, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
.ATTR(use_nesterov, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdam"
    op.name = next_unique_name(node_name, "ApplyAdam")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking
    op.attr["use_nesterov"].b = use_nesterov

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdamD
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdamD(var: Tensor, m: Tensor, v: Tensor, beta1_power: Tensor, beta2_power: Tensor, lr: Tensor, beta1: Tensor, beta2: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, use_nesterov: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdamD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(m, TensorType::NumberType())\n
.INPUT(v, TensorType::NumberType())\n
.INPUT(beta1_power, TensorType::NumberType())\n
.INPUT(beta2_power, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(beta1, TensorType::NumberType())\n
.INPUT(beta2, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(m, TensorType::NumberType())\n
.OUTPUT(v, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
.ATTR(use_nesterov, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdamD"
    op.name = next_unique_name(node_name, "ApplyAdamD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(m.tensor)
    op.input_desc.add().CopyFrom(m.desc)
    op.input_desc[-1].name = "m"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"
    op.input.append(beta1_power.tensor)
    op.input_desc.add().CopyFrom(beta1_power.desc)
    op.input_desc[-1].name = "beta1_power"
    op.input.append(beta2_power.tensor)
    op.input_desc.add().CopyFrom(beta2_power.desc)
    op.input_desc[-1].name = "beta2_power"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(beta1.tensor)
    op.input_desc.add().CopyFrom(beta1.desc)
    op.input_desc[-1].name = "beta1"
    op.input.append(beta2.tensor)
    op.input_desc.add().CopyFrom(beta2.desc)
    op.input_desc[-1].name = "beta2"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking
    op.attr["use_nesterov"].b = use_nesterov

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "m"
    m = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "v"
    v = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, m, v


# This api is auto-generated from IR ApplyAdadelta
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdadelta(var: Tensor, accum: Tensor, accum_update: Tensor, lr: Tensor, rho: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdadelta)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(accum_update, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdadelta"
    op.name = next_unique_name(node_name, "ApplyAdadelta")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(accum_update.tensor)
    op.input_desc.add().CopyFrom(accum_update.desc)
    op.input_desc[-1].name = "accum_update"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR ApplyAdadeltaD
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def ApplyAdadeltaD(var: Tensor, accum: Tensor, accum_update: Tensor, lr: Tensor, rho: Tensor, epsilon: Tensor, grad: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(ApplyAdadeltaD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(accum_update, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.OUTPUT(accum_update, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ApplyAdadeltaD"
    op.name = next_unique_name(node_name, "ApplyAdadeltaD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(accum_update.tensor)
    op.input_desc.add().CopyFrom(accum_update.desc)
    op.input_desc[-1].name = "accum_update"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum_update"
    accum_update = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, accum_update


# This api is auto-generated from IR FusedMulApplyMomentum
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def FusedMulApplyMomentum(var: Tensor, accum: Tensor, lr: Tensor, x1: Tensor, momentum: Tensor, x2: Tensor, *, use_nesterov: bool=False, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(FusedMulApplyMomentum)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(x1, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.INPUT(x2, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_nesterov, Bool, false)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedMulApplyMomentum"
    op.name = next_unique_name(node_name, "FusedMulApplyMomentum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["use_nesterov"].b = use_nesterov
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR FusedMulApplyMomentumExtern
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_UNKNOWN])
def FusedMulApplyMomentumExtern(var: Tensor, accum: Tensor, lr: Tensor, x1: Tensor, momentum: Tensor, x2: Tensor, var_copy: Tensor, *, use_nesterov: bool=False, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(FusedMulApplyMomentumExtern)\n
.INPUT(var, TensorType(DT_FLOAT))\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(x1, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.INPUT(x2, TensorType::NumberType())\n
.INPUT(var_copy, TensorType(DT_FLOAT16))\n
.OUTPUT(var, TensorType(DT_FLOAT))\n
.OUTPUT(var_copy, TensorType(DT_FLOAT16))\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_nesterov, Bool, false)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedMulApplyMomentumExtern"
    op.name = next_unique_name(node_name, "FusedMulApplyMomentumExtern")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(var_copy.tensor)
    op.input_desc.add().CopyFrom(var_copy.desc)
    op.input_desc[-1].name = "var_copy"

    # process attrs
    op.attr["use_nesterov"].b = use_nesterov
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "var_copy"
    var_copy = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, var_copy, accum


# This api is auto-generated from IR FusedMulApplyKerasMomentum
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER])
def FusedMulApplyKerasMomentum(var: Tensor, accum: Tensor, lr: Tensor, x1: Tensor, momentum: Tensor, x2: Tensor, *, use_locking: bool=False, use_nesterov: bool=False, dependencies=[], node_name=None):
    """REG_OP(FusedMulApplyKerasMomentum)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(x1, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.INPUT(x2, TensorType::NumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
.ATTR(use_nesterov, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedMulApplyKerasMomentum"
    op.name = next_unique_name(node_name, "FusedMulApplyKerasMomentum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["use_locking"].b = use_locking
    op.attr["use_nesterov"].b = use_nesterov

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum


# This api is auto-generated from IR LarsV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def LarsV2(w: Tensor, g: Tensor, weight_decay: Tensor, learning_rate: Tensor, *, hyperpara: float=0.001000, epsilon: float=0.000010, use_clip: bool=False, dependencies=[], node_name=None):
    """REG_OP(LarsV2)\n
.INPUT(w, TensorType(DT_FLOAT))\n
.INPUT(g, TensorType(DT_FLOAT))\n
.INPUT(weight_decay, TensorType(DT_FLOAT))\n
.INPUT(learning_rate, TensorType(DT_FLOAT))\n
.OUTPUT(g_new, TensorType(DT_FLOAT))\n
.ATTR(hyperpara, Float, 0.001)\n
.ATTR(epsilon, Float, 0.00001)\n
.ATTR(use_clip, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LarsV2"
    op.name = next_unique_name(node_name, "LarsV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(g.tensor)
    op.input_desc.add().CopyFrom(g.desc)
    op.input_desc[-1].name = "g"
    op.input.append(weight_decay.tensor)
    op.input_desc.add().CopyFrom(weight_decay.desc)
    op.input_desc[-1].name = "weight_decay"
    op.input.append(learning_rate.tensor)
    op.input_desc.add().CopyFrom(learning_rate.desc)
    op.input_desc[-1].name = "learning_rate"

    # process attrs
    op.attr["hyperpara"].f = hyperpara
    op.attr["epsilon"].f = epsilon
    op.attr["use_clip"].b = use_clip

    # process outputs
    output_index = 0
    op.output_desc.add().name = "g_new"
    g_new = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return g_new


# This api is auto-generated from IR LarsV2Update
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def LarsV2Update(w: Tensor, g: Tensor, w_square_sum: Tensor, g_square_sum: Tensor, weight_decay: Tensor, learning_rate: Tensor, *, hyperpara: float=0.001000, epsilon: float=0.000010, use_clip: bool=False, dependencies=[], node_name=None):
    """REG_OP(LarsV2Update)\n
.INPUT(w, TensorType(DT_FLOAT))\n
.INPUT(g, TensorType(DT_FLOAT))\n
.INPUT(w_square_sum, TensorType(DT_FLOAT))\n
.INPUT(g_square_sum, TensorType(DT_FLOAT))\n
.INPUT(weight_decay, TensorType(DT_FLOAT))\n
.INPUT(learning_rate, TensorType(DT_FLOAT))\n
.OUTPUT(g_new, TensorType(DT_FLOAT))\n
.ATTR(hyperpara, Float, 0.001)\n
.ATTR(epsilon, Float, 0.00001)\n
.ATTR(use_clip, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LarsV2Update"
    op.name = next_unique_name(node_name, "LarsV2Update")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(g.tensor)
    op.input_desc.add().CopyFrom(g.desc)
    op.input_desc[-1].name = "g"
    op.input.append(w_square_sum.tensor)
    op.input_desc.add().CopyFrom(w_square_sum.desc)
    op.input_desc[-1].name = "w_square_sum"
    op.input.append(g_square_sum.tensor)
    op.input_desc.add().CopyFrom(g_square_sum.desc)
    op.input_desc[-1].name = "g_square_sum"
    op.input.append(weight_decay.tensor)
    op.input_desc.add().CopyFrom(weight_decay.desc)
    op.input_desc[-1].name = "weight_decay"
    op.input.append(learning_rate.tensor)
    op.input_desc.add().CopyFrom(learning_rate.desc)
    op.input_desc[-1].name = "learning_rate"

    # process attrs
    op.attr["hyperpara"].f = hyperpara
    op.attr["epsilon"].f = epsilon
    op.attr["use_clip"].b = use_clip

    # process outputs
    output_index = 0
    op.output_desc.add().name = "g_new"
    g_new = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return g_new


# This api is auto-generated from IR SparseApplyFtrl
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False])
def SparseApplyFtrl(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, indices: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, lr_power: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyFtrl)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(linear, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(lr, TensorType({DT_FLOAT}))\n
.INPUT(l1, TensorType({DT_FLOAT}))\n
.INPUT(l2, TensorType({DT_FLOAT}))\n
.INPUT(lr_power, TensorType({DT_FLOAT}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.OUTPUT(linear, TensorType({DT_FLOAT}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyFtrl"
    op.name = next_unique_name(node_name, "SparseApplyFtrl")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(lr_power.tensor)
    op.input_desc.add().CopyFrom(lr_power.desc)
    op.input_desc[-1].name = "lr_power"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "linear"
    linear = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, linear


# This api is auto-generated from IR SparseApplyFtrlD
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def SparseApplyFtrlD(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, indices: Tensor, *, lr: float, l1: float, l2: float, lr_power: float, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyFtrlD)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(linear, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.OUTPUT(linear, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(lr, Float)\n
.REQUIRED_ATTR(l1, Float)\n
.REQUIRED_ATTR(l2, Float)\n
.REQUIRED_ATTR(lr_power, Float)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyFtrlD"
    op.name = next_unique_name(node_name, "SparseApplyFtrlD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["lr"].f = lr
    op.attr["l1"].f = l1
    op.attr["l2"].f = l2
    op.attr["lr_power"].f = lr_power
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "linear"
    linear = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, linear


# This api is auto-generated from IR SparseApplyFtrlV2
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False])
def SparseApplyFtrlV2(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, indices: Tensor, lr: Tensor, l1: Tensor, l2: Tensor, l2_shrinkage: Tensor, lr_power: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyFtrlV2)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(linear, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(lr, TensorType({DT_FLOAT}))\n
.INPUT(l1, TensorType({DT_FLOAT}))\n
.INPUT(l2, TensorType({DT_FLOAT}))\n
.INPUT(l2_shrinkage, TensorType({DT_FLOAT}))\n
.INPUT(lr_power, TensorType({DT_FLOAT}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.OUTPUT(linear, TensorType({DT_FLOAT}))\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyFtrlV2"
    op.name = next_unique_name(node_name, "SparseApplyFtrlV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(l1.tensor)
    op.input_desc.add().CopyFrom(l1.desc)
    op.input_desc[-1].name = "l1"
    op.input.append(l2.tensor)
    op.input_desc.add().CopyFrom(l2.desc)
    op.input_desc[-1].name = "l2"
    op.input.append(l2_shrinkage.tensor)
    op.input_desc.add().CopyFrom(l2_shrinkage.desc)
    op.input_desc[-1].name = "l2_shrinkage"
    op.input.append(lr_power.tensor)
    op.input_desc.add().CopyFrom(lr_power.desc)
    op.input_desc[-1].name = "lr_power"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "linear"
    linear = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, linear


# This api is auto-generated from IR SparseApplyFtrlV2D
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def SparseApplyFtrlV2D(var: Tensor, accum: Tensor, linear: Tensor, grad: Tensor, indices: Tensor, *, lr: float, l1: float, l2: float, l2_shrinkage: float, lr_power: float, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyFtrlV2D)\n
.INPUT(var, TensorType({DT_FLOAT}))\n
.INPUT(accum, TensorType({DT_FLOAT}))\n
.INPUT(linear, TensorType({DT_FLOAT}))\n
.INPUT(grad, TensorType({DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(var, TensorType({DT_FLOAT}))\n
.OUTPUT(accum, TensorType({DT_FLOAT}))\n
.OUTPUT(linear, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(lr, Float)\n
.REQUIRED_ATTR(l1, Float)\n
.REQUIRED_ATTR(l2, Float)\n
.REQUIRED_ATTR(l2_shrinkage, Float)\n
.REQUIRED_ATTR(lr_power, Float)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyFtrlV2D"
    op.name = next_unique_name(node_name, "SparseApplyFtrlV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(linear.tensor)
    op.input_desc.add().CopyFrom(linear.desc)
    op.input_desc[-1].name = "linear"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["lr"].f = lr
    op.attr["l1"].f = l1
    op.attr["l2"].f = l2
    op.attr["l2_shrinkage"].f = l2_shrinkage
    op.attr["lr_power"].f = lr_power
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "linear"
    linear = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, linear


# This api is auto-generated from IR SparseApplyRMSProp
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseApplyRMSProp(var: Tensor, ms: Tensor, mom: Tensor, lr: Tensor, rho: Tensor, momentum: Tensor, epsilon: Tensor, grad: Tensor, indices: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyRMSProp)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(ms, TensorType::NumberType())\n
.INPUT(mom, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(momentum, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(ms, TensorType::NumberType())\n
.OUTPUT(mom, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyRMSProp"
    op.name = next_unique_name(node_name, "SparseApplyRMSProp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(ms.tensor)
    op.input_desc.add().CopyFrom(ms.desc)
    op.input_desc[-1].name = "ms"
    op.input.append(mom.tensor)
    op.input_desc.add().CopyFrom(mom.desc)
    op.input_desc[-1].name = "mom"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(momentum.tensor)
    op.input_desc.add().CopyFrom(momentum.desc)
    op.input_desc[-1].name = "momentum"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ms"
    ms = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mom"
    mom = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, ms, mom


# This api is auto-generated from IR SparseApplyRMSPropD
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseApplyRMSPropD(var: Tensor, ms: Tensor, mom: Tensor, lr: Tensor, grad: Tensor, indices: Tensor, *, rho: float, momentum: float, epsilon: float, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyRMSPropD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(ms, TensorType::NumberType())\n
.INPUT(mom, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(ms, TensorType::NumberType())\n
.OUTPUT(mom, TensorType::NumberType())\n
.REQUIRED_ATTR(rho, Float)\n
.REQUIRED_ATTR(momentum, Float)\n
.REQUIRED_ATTR(epsilon, Float)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyRMSPropD"
    op.name = next_unique_name(node_name, "SparseApplyRMSPropD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(ms.tensor)
    op.input_desc.add().CopyFrom(ms.desc)
    op.input_desc[-1].name = "ms"
    op.input.append(mom.tensor)
    op.input_desc.add().CopyFrom(mom.desc)
    op.input_desc[-1].name = "mom"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["rho"].f = rho
    op.attr["momentum"].f = momentum
    op.attr["epsilon"].f = epsilon
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ms"
    ms = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mom"
    mom = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, ms, mom


# This api is auto-generated from IR SparseApplyAdadelta
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseApplyAdadelta(var: Tensor, accum: Tensor, accum_update: Tensor, lr: Tensor, rho: Tensor, epsilon: Tensor, grad: Tensor, indices: Tensor, *, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyAdadelta)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(accum_update, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(epsilon, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.OUTPUT(accum_update, TensorType::NumberType())\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyAdadelta"
    op.name = next_unique_name(node_name, "SparseApplyAdadelta")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(accum_update.tensor)
    op.input_desc.add().CopyFrom(accum_update.desc)
    op.input_desc[-1].name = "accum_update"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(epsilon.tensor)
    op.input_desc.add().CopyFrom(epsilon.desc)
    op.input_desc[-1].name = "epsilon"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum_update"
    accum_update = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, accum_update


# This api is auto-generated from IR SparseApplyAdadeltaD
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseApplyAdadeltaD(var: Tensor, accum: Tensor, accum_update: Tensor, lr: Tensor, rho: Tensor, grad: Tensor, indices: Tensor, *, epsilon: float, use_locking: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseApplyAdadeltaD)\n
.INPUT(var, TensorType::NumberType())\n
.INPUT(accum, TensorType::NumberType())\n
.INPUT(accum_update, TensorType::NumberType())\n
.INPUT(lr, TensorType::NumberType())\n
.INPUT(rho, TensorType::NumberType())\n
.INPUT(grad, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(var, TensorType::NumberType())\n
.OUTPUT(accum, TensorType::NumberType())\n
.OUTPUT(accum_update, TensorType::NumberType())\n
.REQUIRED_ATTR(epsilon, Float)\n
.ATTR(use_locking, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseApplyAdadeltaD"
    op.name = next_unique_name(node_name, "SparseApplyAdadeltaD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(accum.tensor)
    op.input_desc.add().CopyFrom(accum.desc)
    op.input_desc[-1].name = "accum"
    op.input.append(accum_update.tensor)
    op.input_desc.add().CopyFrom(accum_update.desc)
    op.input_desc[-1].name = "accum_update"
    op.input.append(lr.tensor)
    op.input_desc.add().CopyFrom(lr.desc)
    op.input_desc[-1].name = "lr"
    op.input.append(rho.tensor)
    op.input_desc.add().CopyFrom(rho.desc)
    op.input_desc[-1].name = "rho"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["epsilon"].f = epsilon
    op.attr["use_locking"].b = use_locking

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum"
    accum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "accum_update"
    accum_update = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var, accum, accum_update


# This api is auto-generated from IR AtomicAddrClean
@auto_convert_to_tensor([], [])
def AtomicAddrClean(*, automic_add_mem_size: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(AtomicAddrClean)\n
.ATTR(automic_add_mem_size, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AtomicAddrClean"
    op.name = next_unique_name(node_name, "AtomicAddrClean")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["automic_add_mem_size"].list.val_type = 2
    op.attr["automic_add_mem_size"].list.i.extend(automic_add_mem_size)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR DynamicAtomicAddrClean
@auto_convert_to_tensor([], [])
def DynamicAtomicAddrClean(*, automic_add_mem_size: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(DynamicAtomicAddrClean)\n
.ATTR(automic_add_mem_size, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicAtomicAddrClean"
    op.name = next_unique_name(node_name, "DynamicAtomicAddrClean")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["automic_add_mem_size"].list.val_type = 2
    op.attr["automic_add_mem_size"].list.i.extend(automic_add_mem_size)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR Gelu
@auto_convert_to_tensor([False], [False])
def Gelu(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Gelu)\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Gelu"
    op.name = next_unique_name(node_name, "Gelu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HardSwish
@auto_convert_to_tensor([False], [False])
def HardSwish(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(HardSwish)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardSwish"
    op.name = next_unique_name(node_name, "HardSwish")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HardSwishGrad
@auto_convert_to_tensor([False, False], [False, False])
def HardSwishGrad(grad: Tensor, x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(HardSwishGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardSwishGrad"
    op.name = next_unique_name(node_name, "HardSwishGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Swish
@auto_convert_to_tensor([False], [False])
def Swish(x: Tensor, *, scale: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(Swish)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(scale, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Swish"
    op.name = next_unique_name(node_name, "Swish")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["scale"].f = scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SwishGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SwishGrad(grad: Tensor, x: Tensor, y: Tensor, *, scale: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(SwishGrad)\n
.INPUT(grad, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(grad_x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(scale, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SwishGrad"
    op.name = next_unique_name(node_name, "SwishGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"

    # process attrs
    op.attr["scale"].f = scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "grad_x"
    grad_x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return grad_x


# This api is auto-generated from IR GeluGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def GeluGrad(dy: Tensor, x: Tensor, y: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(GeluGrad)\n
.INPUT(dy, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.INPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(z, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GeluGrad"
    op.name = next_unique_name(node_name, "GeluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR FastGelu
@auto_convert_to_tensor([False], [False])
def FastGelu(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FastGelu)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FastGelu"
    op.name = next_unique_name(node_name, "FastGelu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FastGeluV2
@auto_convert_to_tensor([False], [False])
def FastGeluV2(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FastGeluV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FastGeluV2"
    op.name = next_unique_name(node_name, "FastGeluV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FastGeluGrad
@auto_convert_to_tensor([False, False], [False, False])
def FastGeluGrad(dy: Tensor, x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FastGeluGrad)\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(z, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FastGeluGrad"
    op.name = next_unique_name(node_name, "FastGeluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR TanhGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNARY, TensorType.TT_UNARY])
def TanhGrad(y: Tensor, dy: Tensor, *, complex_conj: bool=False, dependencies=[], node_name=None):
    """REG_OP(TanhGrad)\n
.INPUT(y, TensorType::UnaryDataType())\n
.INPUT(dy, TensorType::UnaryDataType())\n
.ATTR(complex_conj, Bool, false)\n
.OUTPUT(z, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TanhGrad"
    op.name = next_unique_name(node_name, "TanhGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs
    op.attr["complex_conj"].b = complex_conj

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR Tanh
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Tanh(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Tanh)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Tanh"
    op.name = next_unique_name(node_name, "Tanh")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Relu
@auto_convert_to_tensor([False], [False])
def Relu(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Relu)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT32, DT_INT16, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT32, DT_INT16, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Relu"
    op.name = next_unique_name(node_name, "Relu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Relu6
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def Relu6(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Relu6)\n
.INPUT(x, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Relu6"
    op.name = next_unique_name(node_name, "Relu6")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Relu6D
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def Relu6D(x: Tensor, *, scale: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(Relu6D)\n
.INPUT(x, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.ATTR(scale, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Relu6D"
    op.name = next_unique_name(node_name, "Relu6D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["scale"].f = scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Relu6Grad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def Relu6Grad(gradients: Tensor, features: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Relu6Grad)\n
.INPUT(gradients, TensorType::RealNumberType())\n
.INPUT(features, TensorType::RealNumberType())\n
.OUTPUT(backprops, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Relu6Grad"
    op.name = next_unique_name(node_name, "Relu6Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR EluGradV2
@auto_convert_to_tensor([False, False], [False, False])
def EluGradV2(grads: Tensor, activations: Tensor, *, alpha: float=1.000000, scale: float=1.000000, input_scale: float=1.000000, is_result: bool=False, dependencies=[], node_name=None):
    """REG_OP(EluGradV2)\n
.INPUT(grads, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(activations, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(alpha, Float, 1.0)\n
.ATTR(scale, Float, 1.0)\n
.ATTR(input_scale, Float, 1.0)\n
.ATTR(is_result, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EluGradV2"
    op.name = next_unique_name(node_name, "EluGradV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(activations.tensor)
    op.input_desc.add().CopyFrom(activations.desc)
    op.input_desc[-1].name = "activations"

    # process attrs
    op.attr["alpha"].f = alpha
    op.attr["scale"].f = scale
    op.attr["input_scale"].f = input_scale
    op.attr["is_result"].b = is_result

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Sigmoid
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_UNARY])
def Sigmoid(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Sigmoid)\n
.INPUT(x, TensorType::UnaryDataType())\n
.OUTPUT(y, TensorType::UnaryDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Sigmoid"
    op.name = next_unique_name(node_name, "Sigmoid")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SigmoidGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNARY, TensorType.TT_UNARY])
def SigmoidGrad(y: Tensor, dy: Tensor, *, complex_conj: bool=False, dependencies=[], node_name=None):
    """REG_OP(SigmoidGrad)\n
.INPUT(y, TensorType(UnaryDataType))\n
.INPUT(dy, TensorType(UnaryDataType))\n
.OUTPUT(z, TensorType(UnaryDataType))\n
.ATTR(complex_conj, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SigmoidGrad"
    op.name = next_unique_name(node_name, "SigmoidGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs
    op.attr["complex_conj"].b = complex_conj

    # process outputs
    output_index = 0
    op.output_desc.add().name = "z"
    z = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return z


# This api is auto-generated from IR BNLL
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def BNLL(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BNLL)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNLL"
    op.name = next_unique_name(node_name, "BNLL")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Softplus
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def Softplus(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Softplus)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Softplus"
    op.name = next_unique_name(node_name, "Softplus")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SoftplusGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_FLOATING, TensorType.TT_FLOATING])
def SoftplusGrad(gradients: Tensor, features: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SoftplusGrad)\n
.INPUT(gradients, TensorType::FloatingDataType())\n
.INPUT(features, TensorType::FloatingDataType())\n
.OUTPUT(backprops, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftplusGrad"
    op.name = next_unique_name(node_name, "SoftplusGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR Softsign
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def Softsign(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Softsign)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Softsign"
    op.name = next_unique_name(node_name, "Softsign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SoftsignGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_FLOATING, TensorType.TT_FLOATING])
def SoftsignGrad(gradients: Tensor, features: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SoftsignGrad)\n
.INPUT(gradients, TensorType::FloatingDataType())\n
.INPUT(features, TensorType::FloatingDataType())\n
.OUTPUT(output, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftsignGrad"
    op.name = next_unique_name(node_name, "SoftsignGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR Selu
@auto_convert_to_tensor([False], [False])
def Selu(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Selu)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE, DT_INT8,DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE, DT_INT8,DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Selu"
    op.name = next_unique_name(node_name, "Selu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SeluGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def SeluGrad(gradients: Tensor, outputs: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SeluGrad)\n
.INPUT(gradients, TensorType::RealNumberType())\n
.INPUT(outputs, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SeluGrad"
    op.name = next_unique_name(node_name, "SeluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(outputs.tensor)
    op.input_desc.add().CopyFrom(outputs.desc)
    op.input_desc[-1].name = "outputs"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReluGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_REAL_NUMBER])
def ReluGrad(gradients: Tensor, features: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ReluGrad)\n
.INPUT(gradients, TensorType::RealNumberType())\n
.INPUT(features, TensorType::RealNumberType())\n
.OUTPUT(backprops, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReluGrad"
    op.name = next_unique_name(node_name, "ReluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR ReluGradV2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_UNKNOWN])
def ReluGradV2(gradients: Tensor, mask: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ReluGradV2)\n
.INPUT(gradients, TensorType::RealNumberType())\n
.INPUT(mask, TensorType({DT_UINT8, DT_UINT1}))\n
.OUTPUT(backprops, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReluGradV2"
    op.name = next_unique_name(node_name, "ReluGradV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR ReluV2
@auto_convert_to_tensor([False], [False])
def ReluV2(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ReluV2)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT32, DT_INT16, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT32, DT_INT16, DT_INT64, DT_UINT8, DT_UINT16, DT_QINT8}))\n
.OUTPUT(mask, TensorType({DT_UINT8, DT_UINT1}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReluV2"
    op.name = next_unique_name(node_name, "ReluV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mask"
    mask = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mask


# This api is auto-generated from IR PRelu
@auto_convert_to_tensor([False, False], [False, False])
def PRelu(x: Tensor, weight: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(PRelu)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(weight, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PRelu"
    op.name = next_unique_name(node_name, "PRelu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PReluGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def PReluGrad(grads: Tensor, features: Tensor, weights: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(PReluGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(features, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weights, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(da, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PReluGrad"
    op.name = next_unique_name(node_name, "PReluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"
    op.input.append(weights.tensor)
    op.input_desc.add().CopyFrom(weights.desc)
    op.input_desc[-1].name = "weights"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "da"
    da = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx, da


# This api is auto-generated from IR Elu
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def Elu(x: Tensor, *, alpha: float=1.000000, scale: float=1.000000, input_scale: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(Elu)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
.ATTR(alpha, Float, 1.0)\n
.ATTR(scale, Float, 1.0)\n
.ATTR(input_scale, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Elu"
    op.name = next_unique_name(node_name, "Elu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["alpha"].f = alpha
    op.attr["scale"].f = scale
    op.attr["input_scale"].f = input_scale

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Celu
@auto_convert_to_tensor([False], [False])
def Celu(x: Tensor, *, alpha1: float=1.000000, alpha2: float=1.000000, alpha3: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(Celu)\n
.INPUT(x, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.ATTR(alpha1, Float, 1.0)\n
.ATTR(alpha2, Float, 1.0)\n
.ATTR(alpha3, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Celu"
    op.name = next_unique_name(node_name, "Celu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["alpha1"].f = alpha1
    op.attr["alpha2"].f = alpha2
    op.attr["alpha3"].f = alpha3

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CeluV2
@auto_convert_to_tensor([False], [False])
def CeluV2(x: Tensor, *, alpha: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(CeluV2)\n
.INPUT(x, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT,DT_FLOAT16}))\n
.ATTR(alpha, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CeluV2"
    op.name = next_unique_name(node_name, "CeluV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["alpha"].f = alpha

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EluGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_FLOATING, TensorType.TT_FLOATING])
def EluGrad(grads: Tensor, activations: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(EluGrad)\n
.INPUT(grads, TensorType::FloatingDataType())\n
.INPUT(activations, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EluGrad"
    op.name = next_unique_name(node_name, "EluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(activations.tensor)
    op.input_desc.add().CopyFrom(activations.desc)
    op.input_desc[-1].name = "activations"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LeakyRelu
@auto_convert_to_tensor([False], [False])
def LeakyRelu(x: Tensor, *, negative_slope: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LeakyRelu)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
.ATTR(negative_slope, Float, 0.0)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LeakyRelu"
    op.name = next_unique_name(node_name, "LeakyRelu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["negative_slope"].f = negative_slope

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LeakyReluGrad
@auto_convert_to_tensor([False, False], [False, False])
def LeakyReluGrad(gradients: Tensor, features: Tensor, *, negative_slope: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(LeakyReluGrad)\n
.INPUT(gradients, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(features, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(negative_slope, Float, 0.0)\n
.OUTPUT(backprops, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LeakyReluGrad"
    op.name = next_unique_name(node_name, "LeakyReluGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs
    op.attr["negative_slope"].f = negative_slope

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR ThresholdGradV2D
@auto_convert_to_tensor([False, False], [False, False])
def ThresholdGradV2D(gradients: Tensor, features: Tensor, *, threshold: float, dependencies=[], node_name=None):
    """REG_OP(ThresholdGradV2D)\n
.INPUT(gradients, TensorType({DT_INT32, DT_FLOAT16}))\n
.INPUT(features, TensorType({DT_INT32, DT_FLOAT16}))\n
.OUTPUT(backprops, TensorType({DT_INT32, DT_FLOAT16}))\n
.REQUIRED_ATTR(threshold, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ThresholdGradV2D"
    op.name = next_unique_name(node_name, "ThresholdGradV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs
    op.attr["threshold"].f = threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR ThresholdV2D
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def ThresholdV2D(x: Tensor, *, threshold: float, value: float, dependencies=[], node_name=None):
    """REG_OP(ThresholdV2D)\n
.INPUT(x, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(threshold, Float)\n
.REQUIRED_ATTR(value, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ThresholdV2D"
    op.name = next_unique_name(node_name, "ThresholdV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["threshold"].f = threshold
    op.attr["value"].f = value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Mish
@auto_convert_to_tensor([False], [False])
def Mish(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Mish)\n
.INPUT(x, TensorType({ DT_FLOAT,DT_FLOAT16 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT,DT_FLOAT16 }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Mish"
    op.name = next_unique_name(node_name, "Mish")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MishGrad
@auto_convert_to_tensor([False, False, False], [False, False, True])
def MishGrad(grad: Tensor, x: Tensor, tanhx: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(MishGrad)\n
.INPUT(grad, TensorType({ DT_FLOAT,DT_FLOAT16 }))\n
.INPUT(x, TensorType({ DT_FLOAT,DT_FLOAT16 }))\n
.OPTIONAL_INPUT(tanhx, TensorType({ DT_FLOAT,DT_FLOAT16 }))\n
.OUTPUT(x_grad, TensorType({ DT_FLOAT,DT_FLOAT16 }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MishGrad"
    op.name = next_unique_name(node_name, "MishGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if tanhx is not None:
        op.input.append(tanhx.tensor)
        op.input_desc.add().CopyFrom(tanhx.desc)
        op.input_desc[-1].name = "tanhx"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "tanhx"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR HardtanhGrad
@auto_convert_to_tensor([False, False], [False, False])
def HardtanhGrad(result: Tensor, grad: Tensor, *, min_val: float=-1.000000, max_val: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(HardtanhGrad)\n
.INPUT(result, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.INPUT(grad, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.ATTR(min_val, Float, -1.0)\n
.ATTR(max_val, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardtanhGrad"
    op.name = next_unique_name(node_name, "HardtanhGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(result.tensor)
    op.input_desc.add().CopyFrom(result.desc)
    op.input_desc[-1].name = "result"
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"

    # process attrs
    op.attr["min_val"].f = min_val
    op.attr["max_val"].f = max_val

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SoftplusV2
@auto_convert_to_tensor([False], [False])
def SoftplusV2(x: Tensor, *, beta: float=1.000000, threshold: float=20.000000, dependencies=[], node_name=None):
    """REG_OP(SoftplusV2)\n
.INPUT(x, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.ATTR(beta, Float, 1.0)\n
.ATTR(threshold, Float, 20.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftplusV2"
    op.name = next_unique_name(node_name, "SoftplusV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["beta"].f = beta
    op.attr["threshold"].f = threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SoftplusV2Grad
@auto_convert_to_tensor([False, False], [False, False])
def SoftplusV2Grad(input_gradients: Tensor, input_features: Tensor, *, beta: float=1.000000, threshold: float=20.000000, dependencies=[], node_name=None):
    """REG_OP(SoftplusV2Grad)\n
.INPUT(input_gradients, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(input_features, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(output_backprops, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.ATTR(beta, Float, 1.0)\n
.ATTR(threshold, Float, 20.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftplusV2Grad"
    op.name = next_unique_name(node_name, "SoftplusV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_gradients.tensor)
    op.input_desc.add().CopyFrom(input_gradients.desc)
    op.input_desc[-1].name = "input_gradients"
    op.input.append(input_features.tensor)
    op.input_desc.add().CopyFrom(input_features.desc)
    op.input_desc[-1].name = "input_features"

    # process attrs
    op.attr["beta"].f = beta
    op.attr["threshold"].f = threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_backprops"
    output_backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_backprops


# This api is auto-generated from IR ThresholdedRelu
@auto_convert_to_tensor([False], [False])
def ThresholdedRelu(x: Tensor, *, alpha: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(ThresholdedRelu)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(alpha, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ThresholdedRelu"
    op.name = next_unique_name(node_name, "ThresholdedRelu")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["alpha"].f = alpha

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HardShrink
@auto_convert_to_tensor([False], [False])
def HardShrink(input_x: Tensor, *, lambd: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(HardShrink)\n
.INPUT(input_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(lambd, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardShrink"
    op.name = next_unique_name(node_name, "HardShrink")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs
    op.attr["lambd"].f = lambd

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR HardShrinkGrad
@auto_convert_to_tensor([False, False], [False, False])
def HardShrinkGrad(gradients: Tensor, features: Tensor, *, lambd: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(HardShrinkGrad)\n
.INPUT(gradients, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(features, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(backprops, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(lambd, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardShrinkGrad"
    op.name = next_unique_name(node_name, "HardShrinkGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(gradients.tensor)
    op.input_desc.add().CopyFrom(gradients.desc)
    op.input_desc[-1].name = "gradients"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs
    op.attr["lambd"].f = lambd

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR HardSigmoid
@auto_convert_to_tensor([False], [False])
def HardSigmoid(input_x: Tensor, *, alpha: float=0.16666666, beta: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(HardSigmoid)\n
.INPUT(input_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(alpha, Float, 0.16666666)\n
.ATTR(beta, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardSigmoid"
    op.name = next_unique_name(node_name, "HardSigmoid")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs
    op.attr["alpha"].f = alpha
    op.attr["beta"].f = beta

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR SoftShrink
@auto_convert_to_tensor([False], [False])
def SoftShrink(input_x: Tensor, *, lambd: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(SoftShrink)\n
.INPUT(input_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(lambd, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftShrink"
    op.name = next_unique_name(node_name, "SoftShrink")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs
    op.attr["lambd"].f = lambd

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR SoftShrinkGrad
@auto_convert_to_tensor([False, False], [False, False])
def SoftShrinkGrad(input_grad: Tensor, input_x: Tensor, *, lambd: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(SoftShrinkGrad)\n
.INPUT(input_grad, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(input_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(lambd, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SoftShrinkGrad"
    op.name = next_unique_name(node_name, "SoftShrinkGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_grad.tensor)
    op.input_desc.add().CopyFrom(input_grad.desc)
    op.input_desc[-1].name = "input_grad"
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs
    op.attr["lambd"].f = lambd

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR LogSigmoidGrad
@auto_convert_to_tensor([False, False], [False, False])
def LogSigmoidGrad(grads: Tensor, features: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LogSigmoidGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(features, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(backprops, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogSigmoidGrad"
    op.name = next_unique_name(node_name, "LogSigmoidGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(features.tensor)
    op.input_desc.add().CopyFrom(features.desc)
    op.input_desc[-1].name = "features"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "backprops"
    backprops = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return backprops


# This api is auto-generated from IR LogSigmoid
@auto_convert_to_tensor([False], [False])
def LogSigmoid(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LogSigmoid)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogSigmoid"
    op.name = next_unique_name(node_name, "LogSigmoid")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR HardSigmoidGrad
@auto_convert_to_tensor([False, False], [False, False])
def HardSigmoidGrad(grads: Tensor, input_x: Tensor, *, alpha: float=0.16666666, beta: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(HardSigmoidGrad)\n
.INPUT(grads, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(input_x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(alpha, Float, 0.16666666)\n
.ATTR(beta, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "HardSigmoidGrad"
    op.name = next_unique_name(node_name, "HardSigmoidGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs
    op.attr["alpha"].f = alpha
    op.attr["beta"].f = beta

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Shrink
@auto_convert_to_tensor([False], [False])
def Shrink(input_x: Tensor, *, lambd: float=0.500000, bias: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Shrink)\n
.INPUT(input_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(lambd, Float, 0.5)\n
.ATTR(bias, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Shrink"
    op.name = next_unique_name(node_name, "Shrink")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_x.tensor)
    op.input_desc.add().CopyFrom(input_x.desc)
    op.input_desc[-1].name = "input_x"

    # process attrs
    op.attr["lambd"].f = lambd
    op.attr["bias"].f = bias

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_y"
    output_y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_y


# This api is auto-generated from IR ThresholdV2
@auto_convert_to_tensor([False, False, False], [False, False, True])
def ThresholdV2(x: Tensor, threshold: Tensor, value: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(ThresholdV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT8, DT_INT32, DT_UINT8, DT_INT64}))\n
.INPUT(threshold, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT8, DT_INT32, DT_UINT8, DT_INT64}))\n
.OPTIONAL_INPUT(value, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT8, DT_INT32, DT_UINT8, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT8, DT_INT32, DT_UINT8, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ThresholdV2"
    op.name = next_unique_name(node_name, "ThresholdV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(threshold.tensor)
    op.input_desc.add().CopyFrom(threshold.desc)
    op.input_desc[-1].name = "threshold"
    if value is not None:
        op.input.append(value.tensor)
        op.input_desc.add().CopyFrom(value.desc)
        op.input_desc[-1].name = "value"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GLU
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_FLOATING])
def GLU(x: Tensor, *, dim: int=-1, dependencies=[], node_name=None):
    """REG_OP(GLU)\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(y, TensorType::FloatingDataType())\n
.ATTR(dim, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GLU"
    op.name = next_unique_name(node_name, "GLU")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dim"].i = dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GLUGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_FLOATING, TensorType.TT_FLOATING])
def GLUGrad(y_grad: Tensor, x: Tensor, *, dim: int=-1, dependencies=[], node_name=None):
    """REG_OP(GLUGrad)\n
.INPUT(y_grad, TensorType::FloatingDataType())\n
.INPUT(x, TensorType::FloatingDataType())\n
.OUTPUT(x_grad, TensorType::FloatingDataType())\n
.ATTR(dim, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GLUGrad"
    op.name = next_unique_name(node_name, "GLUGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(y_grad.tensor)
    op.input_desc.add().CopyFrom(y_grad.desc)
    op.input_desc[-1].name = "y_grad"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dim"].i = dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_grad"
    x_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_grad


# This api is auto-generated from IR NoOp
@auto_convert_to_tensor([], [])
def NoOp(*, dependencies=[], node_name=None):
    """REG_OP(NoOp)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NoOp"
    op.name = next_unique_name(node_name, "NoOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR NPUAllocFloatStatusOperator
@auto_convert_to_tensor([], [])
def NPUAllocFloatStatusOperator(*, dependencies=[], node_name=None):
    """REG_OP(NPUAllocFloatStatusOperator)\n
.OUTPUT(data, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUAllocFloatStatusOperator"
    op.name = next_unique_name(node_name, "NPUAllocFloatStatusOperator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR NPUClearFloatStatusOperator
@auto_convert_to_tensor([False], [False])
def NPUClearFloatStatusOperator(addr: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NPUClearFloatStatusOperator)\n
.INPUT(addr, TensorType{DT_FLOAT})\n
.OUTPUT(data, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUClearFloatStatusOperator"
    op.name = next_unique_name(node_name, "NPUClearFloatStatusOperator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(addr.tensor)
    op.input_desc.add().CopyFrom(addr.desc)
    op.input_desc[-1].name = "addr"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR NPUGetFloatStatusOperator
@auto_convert_to_tensor([False], [False])
def NPUGetFloatStatusOperator(addr: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NPUGetFloatStatusOperator)\n
.INPUT(addr, TensorType{DT_FLOAT})\n
.OUTPUT(data, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUGetFloatStatusOperator"
    op.name = next_unique_name(node_name, "NPUGetFloatStatusOperator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(addr.tensor)
    op.input_desc.add().CopyFrom(addr.desc)
    op.input_desc[-1].name = "addr"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR NPUAllocFloatStatus
@auto_convert_to_tensor([], [])
def NPUAllocFloatStatus(*, dependencies=[], node_name=None):
    """REG_OP(NPUAllocFloatStatus)\n
.OUTPUT(data, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUAllocFloatStatus"
    op.name = next_unique_name(node_name, "NPUAllocFloatStatus")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR NPUClearFloatStatus
@auto_convert_to_tensor([False], [False])
def NPUClearFloatStatus(addr: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NPUClearFloatStatus)\n
.INPUT(addr, TensorType{DT_FLOAT})\n
.OUTPUT(data, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUClearFloatStatus"
    op.name = next_unique_name(node_name, "NPUClearFloatStatus")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(addr.tensor)
    op.input_desc.add().CopyFrom(addr.desc)
    op.input_desc[-1].name = "addr"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR NPUGetFloatStatus
@auto_convert_to_tensor([False], [False])
def NPUGetFloatStatus(addr: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NPUGetFloatStatus)\n
.INPUT(addr, TensorType{DT_FLOAT})\n
.OUTPUT(data, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUGetFloatStatus"
    op.name = next_unique_name(node_name, "NPUGetFloatStatus")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(addr.tensor)
    op.input_desc.add().CopyFrom(addr.desc)
    op.input_desc[-1].name = "addr"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR NPUClearFloatStatusV2
@auto_convert_to_tensor([], [])
def NPUClearFloatStatusV2(*, dependencies=[], node_name=None):
    """REG_OP(NPUClearFloatStatusV2)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUClearFloatStatusV2"
    op.name = next_unique_name(node_name, "NPUClearFloatStatusV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR NPUGetFloatStatusV2
@auto_convert_to_tensor([], [])
def NPUGetFloatStatusV2(*, dependencies=[], node_name=None):
    """REG_OP(NPUGetFloatStatusV2)\n
.OUTPUT(data, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NPUGetFloatStatusV2"
    op.name = next_unique_name(node_name, "NPUGetFloatStatusV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR BatchEnqueue
@auto_convert_to_tensor([False, False], [False, True])
def BatchEnqueue(x: Tensor, queue_id: Optional[Tensor], *, batch_size: int=8, queue_name: str="", queue_depth: int=100, pad_mode: str="REPLICATE", dependencies=[], node_name=None):
    """REG_OP(BatchEnqueue)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT8, DT_INT32, DT_INT64, DT_UINT8, DT_UINT32, DT_UINT64}))\n
.OPTIONAL_INPUT(queue_id, TensorType({DT_UINT32}))\n
.OUTPUT(enqueue_count, TensorType({DT_INT32}))\n
.ATTR(batch_size, Int, 8)\n
.ATTR(queue_name, String, "")\n
.ATTR(queue_depth, Int, 100)\n
.ATTR(pad_mode, String, "REPLICATE")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchEnqueue"
    op.name = next_unique_name(node_name, "BatchEnqueue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if queue_id is not None:
        op.input.append(queue_id.tensor)
        op.input_desc.add().CopyFrom(queue_id.desc)
        op.input_desc[-1].name = "queue_id"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "queue_id"

    # process attrs
    op.attr["batch_size"].i = batch_size
    op.attr["queue_name"].s = compat_as_bytes(queue_name)
    op.attr["queue_depth"].i = queue_depth
    op.attr["pad_mode"].s = compat_as_bytes(pad_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "enqueue_count"
    enqueue_count = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return enqueue_count


# This api is auto-generated from IR OCRRecognitionPreHandle
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def OCRRecognitionPreHandle(imgs_data: Tensor, imgs_offset: Tensor, imgs_size: Tensor, langs: Tensor, langs_score: Tensor, *, batch_size: int=8, data_format: str="NHWC", pad_mode: str="REPLICATE", dependencies=[], node_name=None):
    """REG_OP(OCRRecognitionPreHandle)\n
.INPUT(imgs_data, TensorType({DT_UINT8}))\n
.INPUT(imgs_offset, TensorType({DT_INT32}))\n
.INPUT(imgs_size, TensorType({DT_INT32}))\n
.INPUT(langs, TensorType({DT_INT32}))\n
.INPUT(langs_score, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(imgs, TensorType({DT_UINT8}))\n
.OUTPUT(imgs_relation, TensorType({DT_INT32}))\n
.OUTPUT(imgs_lang, TensorType({DT_INT32}))\n
.OUTPUT(imgs_piece_fillers, TensorType({DT_INT32}))\n
.ATTR(batch_size, Int, 8)\n
.ATTR(data_format, String, "NHWC")\n
.ATTR(pad_mode, String, "REPLICATE")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OCRRecognitionPreHandle"
    op.name = next_unique_name(node_name, "OCRRecognitionPreHandle")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(imgs_data.tensor)
    op.input_desc.add().CopyFrom(imgs_data.desc)
    op.input_desc[-1].name = "imgs_data"
    op.input.append(imgs_offset.tensor)
    op.input_desc.add().CopyFrom(imgs_offset.desc)
    op.input_desc[-1].name = "imgs_offset"
    op.input.append(imgs_size.tensor)
    op.input_desc.add().CopyFrom(imgs_size.desc)
    op.input_desc[-1].name = "imgs_size"
    op.input.append(langs.tensor)
    op.input_desc.add().CopyFrom(langs.desc)
    op.input_desc[-1].name = "langs"
    op.input.append(langs_score.tensor)
    op.input_desc.add().CopyFrom(langs_score.desc)
    op.input_desc[-1].name = "langs_score"

    # process attrs
    op.attr["batch_size"].i = batch_size
    op.attr["data_format"].s = compat_as_bytes(data_format)
    op.attr["pad_mode"].s = compat_as_bytes(pad_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "imgs"
    imgs = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "imgs_relation"
    imgs_relation = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "imgs_lang"
    imgs_lang = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "imgs_piece_fillers"
    imgs_piece_fillers = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return imgs, imgs_relation, imgs_lang, imgs_piece_fillers


# This api is auto-generated from IR OCRDetectionPreHandle
@auto_convert_to_tensor([False], [False])
def OCRDetectionPreHandle(img: Tensor, *, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(OCRDetectionPreHandle)\n
.INPUT(img, TensorType({DT_UINT8}))\n
.OUTPUT(resized_img, TensorType({DT_UINT8}))\n
.OUTPUT(h_scale, TensorType({DT_FLOAT}))\n
.OUTPUT(w_scale, TensorType({DT_FLOAT}))\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OCRDetectionPreHandle"
    op.name = next_unique_name(node_name, "OCRDetectionPreHandle")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "resized_img"
    resized_img = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "h_scale"
    h_scale = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "w_scale"
    w_scale = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return resized_img, h_scale, w_scale


# This api is auto-generated from IR OCRIdentifyPreHandle
@auto_convert_to_tensor([False, False, False], [False, False, False])
def OCRIdentifyPreHandle(imgs_data: Tensor, imgs_offset: Tensor, imgs_size: Tensor, *, size: List[int], data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(OCRIdentifyPreHandle)\n
.INPUT(imgs_data, TensorType({DT_UINT8}))\n
.INPUT(imgs_offset, TensorType({DT_INT32}))\n
.INPUT(imgs_size, TensorType({DT_INT32}))\n
.OUTPUT(resized_imgs, TensorType({DT_UINT8}))\n
.REQUIRED_ATTR(size, ListInt)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OCRIdentifyPreHandle"
    op.name = next_unique_name(node_name, "OCRIdentifyPreHandle")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(imgs_data.tensor)
    op.input_desc.add().CopyFrom(imgs_data.desc)
    op.input_desc[-1].name = "imgs_data"
    op.input.append(imgs_offset.tensor)
    op.input_desc.add().CopyFrom(imgs_offset.desc)
    op.input_desc[-1].name = "imgs_offset"
    op.input.append(imgs_size.tensor)
    op.input_desc.add().CopyFrom(imgs_size.desc)
    op.input_desc[-1].name = "imgs_size"

    # process attrs
    op.attr["size"].list.val_type = 2
    op.attr["size"].list.i.extend(size)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "resized_imgs"
    resized_imgs = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return resized_imgs


# This api is auto-generated from IR BatchDilatePolys
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False])
def BatchDilatePolys(polys_data: Tensor, polys_offset: Tensor, polys_size: Tensor, score: Tensor, min_border: Tensor, min_area_thr: Tensor, score_thr: Tensor, expands_cale: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BatchDilatePolys)\n
.INPUT(polys_data, TensorType({DT_INT32}))\n
.INPUT(polys_offset, TensorType({DT_INT32}))\n
.INPUT(polys_size, TensorType({DT_INT32}))\n
.INPUT(score, TensorType({DT_FLOAT}))\n
.INPUT(min_border, TensorType({DT_INT32}))\n
.INPUT(min_area_thr, TensorType({DT_INT32}))\n
.INPUT(score_thr, TensorType({DT_FLOAT}))\n
.INPUT(expands_cale, TensorType({DT_FLOAT}))\n
.OUTPUT(dilated_polys_data, TensorType({DT_INT32}))\n
.OUTPUT(dilated_polys_offset, TensorType({DT_INT32}))\n
.OUTPUT(dilated_polys_size, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchDilatePolys"
    op.name = next_unique_name(node_name, "BatchDilatePolys")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(polys_data.tensor)
    op.input_desc.add().CopyFrom(polys_data.desc)
    op.input_desc[-1].name = "polys_data"
    op.input.append(polys_offset.tensor)
    op.input_desc.add().CopyFrom(polys_offset.desc)
    op.input_desc[-1].name = "polys_offset"
    op.input.append(polys_size.tensor)
    op.input_desc.add().CopyFrom(polys_size.desc)
    op.input_desc[-1].name = "polys_size"
    op.input.append(score.tensor)
    op.input_desc.add().CopyFrom(score.desc)
    op.input_desc[-1].name = "score"
    op.input.append(min_border.tensor)
    op.input_desc.add().CopyFrom(min_border.desc)
    op.input_desc[-1].name = "min_border"
    op.input.append(min_area_thr.tensor)
    op.input_desc.add().CopyFrom(min_area_thr.desc)
    op.input_desc[-1].name = "min_area_thr"
    op.input.append(score_thr.tensor)
    op.input_desc.add().CopyFrom(score_thr.desc)
    op.input_desc[-1].name = "score_thr"
    op.input.append(expands_cale.tensor)
    op.input_desc.add().CopyFrom(expands_cale.desc)
    op.input_desc[-1].name = "expands_cale"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dilated_polys_data"
    dilated_polys_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dilated_polys_offset"
    dilated_polys_offset = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dilated_polys_size"
    dilated_polys_size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dilated_polys_data, dilated_polys_offset, dilated_polys_size


# This api is auto-generated from IR OCRFindContours
@auto_convert_to_tensor([False], [False])
def OCRFindContours(img: Tensor, *, value_mode: int=0, dependencies=[], node_name=None):
    """REG_OP(OCRFindContours)\n
.INPUT(img, TensorType({DT_UINT8}))\n
.OUTPUT(polys_data, TensorType({DT_INT32}))\n
.OUTPUT(polys_offset, TensorType({DT_INT32}))\n
.OUTPUT(polys_size, TensorType({DT_INT32}))\n
.ATTR(value_mode, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OCRFindContours"
    op.name = next_unique_name(node_name, "OCRFindContours")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"

    # process attrs
    op.attr["value_mode"].i = value_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "polys_data"
    polys_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "polys_offset"
    polys_offset = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "polys_size"
    polys_size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return polys_data, polys_offset, polys_size


# This api is auto-generated from IR Dequeue
@auto_convert_to_tensor([False], [True])
def Dequeue(queue_id: Optional[Tensor], *, output_type: int, output_shape: List[int], queue_name: str="", dependencies=[], node_name=None):
    """REG_OP(Dequeue)\n
.OPTIONAL_INPUT(queue_id, TensorType({DT_UINT32}))\n
.OUTPUT(data, TensorType::RealNumberType())\n
.REQUIRED_ATTR(output_type, Type)\n
.REQUIRED_ATTR(output_shape, ListInt)\n
.ATTR(queue_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dequeue"
    op.name = next_unique_name(node_name, "Dequeue")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if queue_id is not None:
        op.input.append(queue_id.tensor)
        op.input_desc.add().CopyFrom(queue_id.desc)
        op.input_desc[-1].name = "queue_id"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "queue_id"

    # process attrs
    op.attr["output_type"].dt = output_type
    op.attr["output_shape"].list.val_type = 2
    op.attr["output_shape"].list.i.extend(output_shape)
    op.attr["queue_name"].s = compat_as_bytes(queue_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "data"
    data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return data


# This api is auto-generated from IR OCRDetectionPostHandle
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def OCRDetectionPostHandle(img: Tensor, polys_data: Tensor, polys_offset: Tensor, polys_size: Tensor, *, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(OCRDetectionPostHandle)\n
.INPUT(img, TensorType({DT_UINT8}))\n
.INPUT(polys_data, TensorType({DT_INT32}))\n
.INPUT(polys_offset, TensorType({DT_INT32}))\n
.INPUT(polys_size, TensorType({DT_INT32}))\n
.OUTPUT(imgs_data, TensorType({DT_UINT8}))\n
.OUTPUT(imgs_offset, TensorType({DT_INT32}))\n
.OUTPUT(imgs_size, TensorType({DT_INT32}))\n
.OUTPUT(rect_points, TensorType({DT_INT32}))\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OCRDetectionPostHandle"
    op.name = next_unique_name(node_name, "OCRDetectionPostHandle")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(img.tensor)
    op.input_desc.add().CopyFrom(img.desc)
    op.input_desc[-1].name = "img"
    op.input.append(polys_data.tensor)
    op.input_desc.add().CopyFrom(polys_data.desc)
    op.input_desc[-1].name = "polys_data"
    op.input.append(polys_offset.tensor)
    op.input_desc.add().CopyFrom(polys_offset.desc)
    op.input_desc[-1].name = "polys_offset"
    op.input.append(polys_size.tensor)
    op.input_desc.add().CopyFrom(polys_size.desc)
    op.input_desc[-1].name = "polys_size"

    # process attrs
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "imgs_data"
    imgs_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "imgs_offset"
    imgs_offset = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "imgs_size"
    imgs_size = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rect_points"
    rect_points = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return imgs_data, imgs_offset, imgs_size, rect_points


# This api is auto-generated from IR ResizeAndClipPolys
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def ResizeAndClipPolys(polys_data: Tensor, polys_offset: Tensor, polys_size: Tensor, h_scale: Tensor, w_scale: Tensor, img_h: Tensor, img_w: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ResizeAndClipPolys)\n
.INPUT(polys_data, TensorType({DT_INT32}))\n
.INPUT(polys_offset, TensorType({DT_INT32}))\n
.INPUT(polys_size, TensorType({DT_INT32}))\n
.INPUT(h_scale, TensorType({DT_FLOAT}))\n
.INPUT(w_scale, TensorType({DT_FLOAT}))\n
.INPUT(img_h, TensorType({DT_INT32}))\n
.INPUT(img_w, TensorType({DT_INT32}))\n
.OUTPUT(clipped_polys_data, TensorType({DT_INT32}))\n
.OUTPUT(clipped_polys_offset, TensorType({DT_INT32}))\n
.OUTPUT(clipped_polys_size, TensorType({DT_INT32}))\n
.OUTPUT(clipped_polys_num, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ResizeAndClipPolys"
    op.name = next_unique_name(node_name, "ResizeAndClipPolys")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(polys_data.tensor)
    op.input_desc.add().CopyFrom(polys_data.desc)
    op.input_desc[-1].name = "polys_data"
    op.input.append(polys_offset.tensor)
    op.input_desc.add().CopyFrom(polys_offset.desc)
    op.input_desc[-1].name = "polys_offset"
    op.input.append(polys_size.tensor)
    op.input_desc.add().CopyFrom(polys_size.desc)
    op.input_desc[-1].name = "polys_size"
    op.input.append(h_scale.tensor)
    op.input_desc.add().CopyFrom(h_scale.desc)
    op.input_desc[-1].name = "h_scale"
    op.input.append(w_scale.tensor)
    op.input_desc.add().CopyFrom(w_scale.desc)
    op.input_desc[-1].name = "w_scale"
    op.input.append(img_h.tensor)
    op.input_desc.add().CopyFrom(img_h.desc)
    op.input_desc[-1].name = "img_h"
    op.input.append(img_w.tensor)
    op.input_desc.add().CopyFrom(img_w.desc)
    op.input_desc[-1].name = "img_w"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "clipped_polys_data"
    clipped_polys_data = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "clipped_polys_offset"
    clipped_polys_offset = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "clipped_polys_size"
    clipped_polys_size = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "clipped_polys_num"
    clipped_polys_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return clipped_polys_data, clipped_polys_offset, clipped_polys_size, clipped_polys_num


# This api is auto-generated from IR Fill
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def Fill(dims: Tensor, value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Fill)\n
.INPUT(dims, TensorType::IndexNumberType())\n
.INPUT(value, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_FLOAT16, DT_BF16, DT_UINT32, DT_UINT64, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16, DT_UINT16, DT_COMPLEX128, DT_FLOAT16, DT_BF16, DT_UINT32, DT_UINT64, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Fill"
    op.name = next_unique_name(node_name, "Fill")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dims.tensor)
    op.input_desc.add().CopyFrom(dims.desc)
    op.input_desc[-1].name = "dims"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FillD
@auto_convert_to_tensor([False], [False])
def FillD(value: Tensor, *, dims: List[int], dependencies=[], node_name=None):
    """REG_OP(FillD)\n
.INPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.REQUIRED_ATTR(dims, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FillD"
    op.name = next_unique_name(node_name, "FillD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["dims"].list.val_type = 2
    op.attr["dims"].list.i.extend(dims)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BroadcastTo
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def BroadcastTo(x: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BroadcastTo)\n
.INPUT(x, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BroadcastTo"
    op.name = next_unique_name(node_name, "BroadcastTo")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BroadcastToD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def BroadcastToD(x: Tensor, *, shape: List[int], dependencies=[], node_name=None):
    """REG_OP(BroadcastToD)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(shape, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BroadcastToD"
    op.name = next_unique_name(node_name, "BroadcastToD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Pad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def Pad(x: Tensor, paddings: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Pad)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(paddings, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Pad"
    op.name = next_unique_name(node_name, "Pad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PadD
@auto_convert_to_tensor([False], [False])
def PadD(x: Tensor, *, paddings: List[List[int]], dependencies=[], node_name=None):
    """REG_OP(PadD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.REQUIRED_ATTR(paddings, ListListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PadD"
    op.name = next_unique_name(node_name, "PadD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["paddings"].list_list_int.CopyFrom(trans_to_list_list_int(paddings))

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PadV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def PadV2(x: Tensor, paddings: Tensor, constant_values: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(PadV2)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(paddings, TensorType::IndexNumberType())\n
.INPUT(constant_values, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PadV2"
    op.name = next_unique_name(node_name, "PadV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"
    op.input.append(constant_values.tensor)
    op.input_desc.add().CopyFrom(constant_values.desc)
    op.input_desc[-1].name = "constant_values"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PadV2D
@auto_convert_to_tensor([False, False], [False, False])
def PadV2D(x: Tensor, constant_values: Tensor, *, paddings: List[List[int]], dependencies=[], node_name=None):
    """REG_OP(PadV2D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.INPUT(constant_values, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.REQUIRED_ATTR(paddings, ListListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PadV2D"
    op.name = next_unique_name(node_name, "PadV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(constant_values.tensor)
    op.input_desc.add().CopyFrom(constant_values.desc)
    op.input_desc[-1].name = "constant_values"

    # process attrs
    op.attr["paddings"].list_list_int.CopyFrom(trans_to_list_list_int(paddings))

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PadV3
@auto_convert_to_tensor([False, False, False], [False, False, True], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def PadV3(x: Tensor, paddings: Tensor, constant_values: Optional[Tensor], *, mode: str="constant", paddings_contiguous: bool=True, dependencies=[], node_name=None):
    """REG_OP(PadV3)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(paddings, TensorType::IndexNumberType())\n
.OPTIONAL_INPUT(constant_values, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(mode, String, "constant")\n
.ATTR(paddings_contiguous, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PadV3"
    op.name = next_unique_name(node_name, "PadV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"
    if constant_values is not None:
        op.input.append(constant_values.tensor)
        op.input_desc.add().CopyFrom(constant_values.desc)
        op.input_desc[-1].name = "constant_values"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "constant_values"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["paddings_contiguous"].b = paddings_contiguous

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PadV3Grad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def PadV3Grad(x: Tensor, paddings: Tensor, *, mode: str="reflect", paddings_contiguous: bool=True, dependencies=[], node_name=None):
    """REG_OP(PadV3Grad)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(paddings, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(mode, String, "reflect")\n
.ATTR(paddings_contiguous, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PadV3Grad"
    op.name = next_unique_name(node_name, "PadV3Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["paddings_contiguous"].b = paddings_contiguous

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR PadV3D
@auto_convert_to_tensor([False], [False])
def PadV3D(x: Tensor, *, paddings: List[List[int]], constant_values: int=0, mode: str="constant", paddings_contiguous: bool=True, dependencies=[], node_name=None):
    """REG_OP(PadV3D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8}))\n
.REQUIRED_ATTR(paddings, ListListInt)\n
.ATTR(constant_values, Int, 0)\n
.ATTR(mode, String, "constant")\n
.ATTR(paddings_contiguous, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PadV3D"
    op.name = next_unique_name(node_name, "PadV3D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["paddings"].list_list_int.CopyFrom(trans_to_list_list_int(paddings))
    op.attr["constant_values"].i = constant_values
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["paddings_contiguous"].b = paddings_contiguous

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DiagD
@auto_convert_to_tensor([False, False], [False, False])
def DiagD(x: Tensor, assist: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DiagD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.INPUT(assist, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DiagD"
    op.name = next_unique_name(node_name, "DiagD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Diag
@auto_convert_to_tensor([False], [False])
def Diag(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Diag)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Diag"
    op.name = next_unique_name(node_name, "Diag")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendPadding
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def AscendPadding(x: Tensor, *, pad_dim_size: int=8, dependencies=[], node_name=None):
    """REG_OP(AscendPadding)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(pad_dim_size, Int, 8)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendPadding"
    op.name = next_unique_name(node_name, "AscendPadding")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["pad_dim_size"].i = pad_dim_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EmbeddingRankId
@auto_convert_to_tensor([False, False], [False, False])
def EmbeddingRankId(addr_table: Tensor, index: Tensor, *, row_memory: int=320, mode: str="mod", dependencies=[], node_name=None):
    """REG_OP(EmbeddingRankId)\n
.INPUT(addr_table, TensorType({DT_UINT64}))\n
.INPUT(index, TensorType({DT_INT64,DT_INT32,DT_UINT64}))\n
.OUTPUT(rank_id, TensorType({DT_UINT64}))\n
.ATTR(row_memory, Int, 320)\n
.ATTR(mode, String, "mod")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingRankId"
    op.name = next_unique_name(node_name, "EmbeddingRankId")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(addr_table.tensor)
    op.input_desc.add().CopyFrom(addr_table.desc)
    op.input_desc[-1].name = "addr_table"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"

    # process attrs
    op.attr["row_memory"].i = row_memory
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "rank_id"
    rank_id = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return rank_id


# This api is auto-generated from IR EmbeddingLocalIndex
@auto_convert_to_tensor([False, False], [False, False])
def EmbeddingLocalIndex(addr_table: Tensor, index: Tensor, *, row_memory: int=320, mode: str="mod", dependencies=[], node_name=None):
    """REG_OP(EmbeddingLocalIndex)\n
.INPUT(addr_table, TensorType({DT_UINT64}))\n
.INPUT(index, TensorType({DT_INT64,DT_INT32,DT_UINT32,DT_UINT64}))\n
.OUTPUT(local_idx, TensorType({DT_INT64,DT_INT32,DT_UINT32,DT_UINT64}))\n
.OUTPUT(nums, TensorType({DT_INT64,DT_INT32,DT_UINT32,DT_UINT64}))\n
.OUTPUT(recover_idx, TensorType({DT_INT64,DT_INT32,DT_UINT32,DT_UINT64}))\n
.ATTR(row_memory, Int, 320)\n
.ATTR(mode, String, "mod")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingLocalIndex"
    op.name = next_unique_name(node_name, "EmbeddingLocalIndex")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(addr_table.tensor)
    op.input_desc.add().CopyFrom(addr_table.desc)
    op.input_desc[-1].name = "addr_table"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"

    # process attrs
    op.attr["row_memory"].i = row_memory
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "local_idx"
    local_idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "nums"
    nums = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "recover_idx"
    recover_idx = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return local_idx, nums, recover_idx


# This api is auto-generated from IR FillV2
@auto_convert_to_tensor([False], [False])
def FillV2(dims: Tensor, *, value: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(FillV2)\n
.INPUT(dims, TensorType({DT_INT16, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64}))\n
.ATTR(value, Float, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FillV2"
    op.name = next_unique_name(node_name, "FillV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dims.tensor)
    op.input_desc.add().CopyFrom(dims.desc)
    op.input_desc[-1].name = "dims"

    # process attrs
    op.attr["value"].f = value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FillV2D
@auto_convert_to_tensor([], [])
def FillV2D(*, dims: List[int], value: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(FillV2D)\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_UINT8, DT_INT16, DT_INT32, DT_INT64}))\n
.ATTR(value, Float, 0)\n
.REQUIRED_ATTR(dims, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FillV2D"
    op.name = next_unique_name(node_name, "FillV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dims"].list.val_type = 2
    op.attr["dims"].list.i.extend(dims)
    op.attr["value"].f = value

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringToNumber
@auto_convert_to_tensor([False], [False])
def StringToNumber(x: Tensor, *, out_type: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(StringToNumber)\n
.INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.ATTR(out_type, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringToNumber"
    op.name = next_unique_name(node_name, "StringToNumber")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["out_type"].dt = out_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ParseSingleExample
@auto_convert_to_tensor([False, True], [False, False])
def _ParseSingleExample(serialized: Tensor, dense_defaults: List[Tensor], *, size_of_sparse_indices: int, size_of_sparse_values: int, size_of_sparse_shapes: int, size_of_dense_values: int, num_sparse: int=0, sparse_keys: List[str]=[], dense_keys: List[str]=[], sparse_types: List[int]=[], Tdense: List[int]=[], dense_shapes: List[List[int]]=[], dependencies=[], node_name=None):
    """REG_OP(ParseSingleExample)\n
.INPUT(serialized, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(dense_defaults, TensorType({DT_STRING,DT_FLOAT,DT_INT64}))\n
.DYNAMIC_OUTPUT(sparse_indices, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(sparse_values, TensorType({DT_STRING,DT_FLOAT,DT_INT64}))\n
.DYNAMIC_OUTPUT(sparse_shapes, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(dense_values, TensorType({DT_STRING,DT_FLOAT,DT_INT64}))\n
.ATTR(num_sparse, Int, 0)\n
.ATTR(sparse_keys, ListString, {})\n
.ATTR(dense_keys, ListString, {})\n
.ATTR(sparse_types, ListType, {})\n
.ATTR(Tdense, ListType, {})\n
.ATTR(dense_shapes, ListListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ParseSingleExample"
    op.name = next_unique_name(node_name, "ParseSingleExample")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(serialized.tensor)
    op.input_desc.add().CopyFrom(serialized.desc)
    op.input_desc[-1].name = "serialized"
    if not isinstance(dense_defaults, (tuple, list)):
        raise AssertionError("dense_defaults must be a tuple or a list.")
    for i, v in enumerate(dense_defaults):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "dense_defaults" + str(i)

    # process attrs
    op.attr["num_sparse"].i = num_sparse
    op.attr["sparse_keys"].list.val_type = 0
    op.attr["sparse_keys"].list.s.extend(compat_as_bytes_list(sparse_keys))
    op.attr["dense_keys"].list.val_type = 0
    op.attr["dense_keys"].list.s.extend(compat_as_bytes_list(dense_keys))
    op.attr["sparse_types"].list.val_type = 10
    op.attr["sparse_types"].list.dt.extend(sparse_types)
    op.attr["Tdense"].list.val_type = 10
    op.attr["Tdense"].list.dt.extend(Tdense)
    op.attr["dense_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(dense_shapes))

    # process outputs
    output_index = 0
    sparse_indices = []
    for i in range(output_index, output_index + size_of_sparse_indices):
        op.output_desc.add().name = "sparse_indices" + str(i - output_index)
        sparse_indices.append(Tensor(op, i))
    output_index += size_of_sparse_indices
    sparse_values = []
    for i in range(output_index, output_index + size_of_sparse_values):
        op.output_desc.add().name = "sparse_values" + str(i - output_index)
        sparse_values.append(Tensor(op, i))
    output_index += size_of_sparse_values
    sparse_shapes = []
    for i in range(output_index, output_index + size_of_sparse_shapes):
        op.output_desc.add().name = "sparse_shapes" + str(i - output_index)
        sparse_shapes.append(Tensor(op, i))
    output_index += size_of_sparse_shapes
    dense_values = []
    for i in range(output_index, output_index + size_of_dense_values):
        op.output_desc.add().name = "dense_values" + str(i - output_index)
        dense_values.append(Tensor(op, i))
    output_index += size_of_dense_values

    # return outputs
    return sparse_indices, sparse_values, sparse_shapes, dense_values


# This api is auto-generated from IR DecodeRaw
@auto_convert_to_tensor([False], [False])
def DecodeRaw(bytes: Tensor, *, out_type: int=DataType.DT_FLOAT, little_endian: bool=True, dependencies=[], node_name=None):
    """REG_OP(DecodeRaw)\n
.INPUT(bytes, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType({DT_BOOL,DT_FLOAT16,DT_DOUBLE,DT_FLOAT, DT_INT64,DT_INT32,DT_INT8,DT_UINT8,DT_INT16, DT_UINT16,DT_COMPLEX64,DT_COMPLEX128}))\n
.ATTR(out_type, Type, DT_FLOAT)\n
.ATTR(little_endian, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeRaw"
    op.name = next_unique_name(node_name, "DecodeRaw")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bytes.tensor)
    op.input_desc.add().CopyFrom(bytes.desc)
    op.input_desc[-1].name = "bytes"

    # process attrs
    op.attr["out_type"].dt = out_type
    op.attr["little_endian"].b = little_endian

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR ParseTensor
@auto_convert_to_tensor([False], [False])
def ParseTensor(serialized: Tensor, *, out_type: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(ParseTensor)\n
.INPUT(serialized, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType(DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE, DT_STRING, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(out_type, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ParseTensor"
    op.name = next_unique_name(node_name, "ParseTensor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(serialized.tensor)
    op.input_desc.add().CopyFrom(serialized.desc)
    op.input_desc[-1].name = "serialized"

    # process attrs
    op.attr["out_type"].dt = out_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR DecodeCSV
@auto_convert_to_tensor([False, True], [False, False])
def _DecodeCSV(records: Tensor, record_defaults: List[Tensor], *, size_of_output: int, OUT_TYPE: List[int]=[], field_delim: str=",", use_quote_delim: bool=True, na_value: str=",", select_cols: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(DecodeCSV)\n
.INPUT(records, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(record_defaults, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_STRING}))\n
.DYNAMIC_OUTPUT(output, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_STRING}))\n
.ATTR(OUT_TYPE, ListType, {})\n
.ATTR(field_delim, String, ",")\n
.ATTR(use_quote_delim, Bool, true)\n
.ATTR(na_value, String, ",")\n
.ATTR(select_cols, ListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeCSV"
    op.name = next_unique_name(node_name, "DecodeCSV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(records.tensor)
    op.input_desc.add().CopyFrom(records.desc)
    op.input_desc[-1].name = "records"
    if not isinstance(record_defaults, (tuple, list)):
        raise AssertionError("record_defaults must be a tuple or a list.")
    for i, v in enumerate(record_defaults):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "record_defaults" + str(i)

    # process attrs
    op.attr["OUT_TYPE"].list.val_type = 10
    op.attr["OUT_TYPE"].list.dt.extend(OUT_TYPE)
    op.attr["field_delim"].s = compat_as_bytes(field_delim)
    op.attr["use_quote_delim"].b = use_quote_delim
    op.attr["na_value"].s = compat_as_bytes(na_value)
    op.attr["select_cols"].list.val_type = 2
    op.attr["select_cols"].list.i.extend(select_cols)

    # process outputs
    output_index = 0
    output = []
    for i in range(output_index, output_index + size_of_output):
        op.output_desc.add().name = "output" + str(i - output_index)
        output.append(Tensor(op, i))
    output_index += size_of_output

    # return outputs
    return output


# This api is auto-generated from IR ParseExample
@auto_convert_to_tensor([False, False, True, True, True], [False, False, False, False, False])
def _ParseExample(serialized: Tensor, name: Tensor, sparse_keys: List[Tensor], dense_keys: List[Tensor], dense_defaults: List[Tensor], *, size_of_sparse_indices: int, size_of_sparse_values: int, size_of_sparse_shapes: int, size_of_dense_values: int, Nsparse: int=0, Ndense: int=0, sparse_types: List[int]=[], Tdense: List[int]=[], dense_shapes: List[List[int]]=[], dependencies=[], node_name=None):
    """REG_OP(ParseExample)\n
.INPUT(serialized, TensorType({DT_STRING}))\n
.INPUT(name, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(sparse_keys, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(dense_keys, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(dense_defaults, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.DYNAMIC_OUTPUT(sparse_indices, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(sparse_values, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.DYNAMIC_OUTPUT(sparse_shapes, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(dense_values, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.ATTR(Nsparse, Int, 0)\n
.ATTR(Ndense, Int, 0)\n
.ATTR(sparse_types, ListType, {})\n
.ATTR(Tdense, ListType, {})\n
.ATTR(dense_shapes, ListListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ParseExample"
    op.name = next_unique_name(node_name, "ParseExample")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(serialized.tensor)
    op.input_desc.add().CopyFrom(serialized.desc)
    op.input_desc[-1].name = "serialized"
    op.input.append(name.tensor)
    op.input_desc.add().CopyFrom(name.desc)
    op.input_desc[-1].name = "name"
    if not isinstance(sparse_keys, (tuple, list)):
        raise AssertionError("sparse_keys must be a tuple or a list.")
    for i, v in enumerate(sparse_keys):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "sparse_keys" + str(i)
    if not isinstance(dense_keys, (tuple, list)):
        raise AssertionError("dense_keys must be a tuple or a list.")
    for i, v in enumerate(dense_keys):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "dense_keys" + str(i)
    if not isinstance(dense_defaults, (tuple, list)):
        raise AssertionError("dense_defaults must be a tuple or a list.")
    for i, v in enumerate(dense_defaults):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "dense_defaults" + str(i)

    # process attrs
    op.attr["Nsparse"].i = Nsparse
    op.attr["Ndense"].i = Ndense
    op.attr["sparse_types"].list.val_type = 10
    op.attr["sparse_types"].list.dt.extend(sparse_types)
    op.attr["Tdense"].list.val_type = 10
    op.attr["Tdense"].list.dt.extend(Tdense)
    op.attr["dense_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(dense_shapes))

    # process outputs
    output_index = 0
    sparse_indices = []
    for i in range(output_index, output_index + size_of_sparse_indices):
        op.output_desc.add().name = "sparse_indices" + str(i - output_index)
        sparse_indices.append(Tensor(op, i))
    output_index += size_of_sparse_indices
    sparse_values = []
    for i in range(output_index, output_index + size_of_sparse_values):
        op.output_desc.add().name = "sparse_values" + str(i - output_index)
        sparse_values.append(Tensor(op, i))
    output_index += size_of_sparse_values
    sparse_shapes = []
    for i in range(output_index, output_index + size_of_sparse_shapes):
        op.output_desc.add().name = "sparse_shapes" + str(i - output_index)
        sparse_shapes.append(Tensor(op, i))
    output_index += size_of_sparse_shapes
    dense_values = []
    for i in range(output_index, output_index + size_of_dense_values):
        op.output_desc.add().name = "dense_values" + str(i - output_index)
        dense_values.append(Tensor(op, i))
    output_index += size_of_dense_values

    # return outputs
    return sparse_indices, sparse_values, sparse_shapes, dense_values


# This api is auto-generated from IR ParseSingleSequenceExample
@auto_convert_to_tensor([False, False, True, True, True, True, True, False], [False, False, False, False, False, False, False, False])
def _ParseSingleSequenceExample(serialized: Tensor, feature_list_dense_missing_assumed_empty: Tensor, context_sparse_keys: List[Tensor], context_dense_keys: List[Tensor], feature_list_sparse_keys: List[Tensor], feature_list_dense_keys: List[Tensor], context_dense_defaults: List[Tensor], debug_name: Tensor, *, size_of_context_sparse_indices: int, size_of_context_sparse_values: int, size_of_context_sparse_shapes: int, size_of_context_dense_values: int, size_of_feature_list_sparse_indices: int, size_of_feature_list_sparse_values: int, size_of_feature_list_sparse_shapes: int, size_of_feature_list_dense_values: int, Ncontext_sparse: int=0, Ncontext_dense: int=0, Nfeature_list_sparse: int=0, Nfeature_list_dense: int=0, context_sparse_types: List[int]=[], Tcontext_dense: List[int]=[], feature_list_dense_types: List[int]=[], context_dense_shapes: List[List[int]]=[], feature_list_sparse_types: List[int]=[], feature_list_dense_shapes: List[List[int]]=[], dependencies=[], node_name=None):
    """REG_OP(ParseSingleSequenceExample)\n
.INPUT(serialized, TensorType({DT_STRING}))\n
.INPUT(feature_list_dense_missing_assumed_empty, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(context_sparse_keys, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(context_dense_keys, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(feature_list_sparse_keys, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(feature_list_dense_keys, TensorType({DT_STRING}))\n
.DYNAMIC_INPUT(context_dense_defaults, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.INPUT(debug_name, TensorType({DT_STRING}))\n
.DYNAMIC_OUTPUT(context_sparse_indices, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(context_sparse_values, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.DYNAMIC_OUTPUT(context_sparse_shapes, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(context_dense_values, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.DYNAMIC_OUTPUT(feature_list_sparse_indices, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(feature_list_sparse_values, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.DYNAMIC_OUTPUT(feature_list_sparse_shapes, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(feature_list_dense_values, TensorType({DT_FLOAT, DT_INT64, DT_STRING}))\n
.ATTR(Ncontext_sparse, Int, 0)\n
.ATTR(Ncontext_dense, Int, 0)\n
.ATTR(Nfeature_list_sparse, Int, 0)\n
.ATTR(Nfeature_list_dense, Int, 0)\n
.ATTR(context_sparse_types, ListType, {})\n
.ATTR(Tcontext_dense, ListType, {})\n
.ATTR(feature_list_dense_types, ListType, {})\n
.ATTR(context_dense_shapes, ListListInt, {})\n
.ATTR(feature_list_sparse_types, ListType, {})\n
.ATTR(feature_list_dense_shapes, ListListInt, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ParseSingleSequenceExample"
    op.name = next_unique_name(node_name, "ParseSingleSequenceExample")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(serialized.tensor)
    op.input_desc.add().CopyFrom(serialized.desc)
    op.input_desc[-1].name = "serialized"
    op.input.append(feature_list_dense_missing_assumed_empty.tensor)
    op.input_desc.add().CopyFrom(feature_list_dense_missing_assumed_empty.desc)
    op.input_desc[-1].name = "feature_list_dense_missing_assumed_empty"
    if not isinstance(context_sparse_keys, (tuple, list)):
        raise AssertionError("context_sparse_keys must be a tuple or a list.")
    for i, v in enumerate(context_sparse_keys):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "context_sparse_keys" + str(i)
    if not isinstance(context_dense_keys, (tuple, list)):
        raise AssertionError("context_dense_keys must be a tuple or a list.")
    for i, v in enumerate(context_dense_keys):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "context_dense_keys" + str(i)
    if not isinstance(feature_list_sparse_keys, (tuple, list)):
        raise AssertionError("feature_list_sparse_keys must be a tuple or a list.")
    for i, v in enumerate(feature_list_sparse_keys):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "feature_list_sparse_keys" + str(i)
    if not isinstance(feature_list_dense_keys, (tuple, list)):
        raise AssertionError("feature_list_dense_keys must be a tuple or a list.")
    for i, v in enumerate(feature_list_dense_keys):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "feature_list_dense_keys" + str(i)
    if not isinstance(context_dense_defaults, (tuple, list)):
        raise AssertionError("context_dense_defaults must be a tuple or a list.")
    for i, v in enumerate(context_dense_defaults):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "context_dense_defaults" + str(i)
    op.input.append(debug_name.tensor)
    op.input_desc.add().CopyFrom(debug_name.desc)
    op.input_desc[-1].name = "debug_name"

    # process attrs
    op.attr["Ncontext_sparse"].i = Ncontext_sparse
    op.attr["Ncontext_dense"].i = Ncontext_dense
    op.attr["Nfeature_list_sparse"].i = Nfeature_list_sparse
    op.attr["Nfeature_list_dense"].i = Nfeature_list_dense
    op.attr["context_sparse_types"].list.val_type = 10
    op.attr["context_sparse_types"].list.dt.extend(context_sparse_types)
    op.attr["Tcontext_dense"].list.val_type = 10
    op.attr["Tcontext_dense"].list.dt.extend(Tcontext_dense)
    op.attr["feature_list_dense_types"].list.val_type = 10
    op.attr["feature_list_dense_types"].list.dt.extend(feature_list_dense_types)
    op.attr["context_dense_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(context_dense_shapes))
    op.attr["feature_list_sparse_types"].list.val_type = 10
    op.attr["feature_list_sparse_types"].list.dt.extend(feature_list_sparse_types)
    op.attr["feature_list_dense_shapes"].list_list_int.CopyFrom(trans_to_list_list_int(feature_list_dense_shapes))

    # process outputs
    output_index = 0
    context_sparse_indices = []
    for i in range(output_index, output_index + size_of_context_sparse_indices):
        op.output_desc.add().name = "context_sparse_indices" + str(i - output_index)
        context_sparse_indices.append(Tensor(op, i))
    output_index += size_of_context_sparse_indices
    context_sparse_values = []
    for i in range(output_index, output_index + size_of_context_sparse_values):
        op.output_desc.add().name = "context_sparse_values" + str(i - output_index)
        context_sparse_values.append(Tensor(op, i))
    output_index += size_of_context_sparse_values
    context_sparse_shapes = []
    for i in range(output_index, output_index + size_of_context_sparse_shapes):
        op.output_desc.add().name = "context_sparse_shapes" + str(i - output_index)
        context_sparse_shapes.append(Tensor(op, i))
    output_index += size_of_context_sparse_shapes
    context_dense_values = []
    for i in range(output_index, output_index + size_of_context_dense_values):
        op.output_desc.add().name = "context_dense_values" + str(i - output_index)
        context_dense_values.append(Tensor(op, i))
    output_index += size_of_context_dense_values
    feature_list_sparse_indices = []
    for i in range(output_index, output_index + size_of_feature_list_sparse_indices):
        op.output_desc.add().name = "feature_list_sparse_indices" + str(i - output_index)
        feature_list_sparse_indices.append(Tensor(op, i))
    output_index += size_of_feature_list_sparse_indices
    feature_list_sparse_values = []
    for i in range(output_index, output_index + size_of_feature_list_sparse_values):
        op.output_desc.add().name = "feature_list_sparse_values" + str(i - output_index)
        feature_list_sparse_values.append(Tensor(op, i))
    output_index += size_of_feature_list_sparse_values
    feature_list_sparse_shapes = []
    for i in range(output_index, output_index + size_of_feature_list_sparse_shapes):
        op.output_desc.add().name = "feature_list_sparse_shapes" + str(i - output_index)
        feature_list_sparse_shapes.append(Tensor(op, i))
    output_index += size_of_feature_list_sparse_shapes
    feature_list_dense_values = []
    for i in range(output_index, output_index + size_of_feature_list_dense_values):
        op.output_desc.add().name = "feature_list_dense_values" + str(i - output_index)
        feature_list_dense_values.append(Tensor(op, i))
    output_index += size_of_feature_list_dense_values

    # return outputs
    return context_sparse_indices, context_sparse_values, context_sparse_shapes, context_dense_values, feature_list_sparse_indices, feature_list_sparse_values, feature_list_sparse_shapes, feature_list_dense_values


# This api is auto-generated from IR Dequantize
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Dequantize(x: Tensor, min_range: Tensor, max_range: Tensor, *, mode: str="MIN_COMBINED", dependencies=[], node_name=None):
    """REG_OP(Dequantize)\n
.INPUT(x, TensorType(DT_QINT8, DT_QUINT8, DT_QINT32, DT_QINT16, DT_QUINT16))\n
.INPUT(min_range, TensorType{DT_FLOAT})\n
.INPUT(max_range, TensorType{DT_FLOAT})\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
.ATTR(mode, String, "MIN_COMBINED")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dequantize"
    op.name = next_unique_name(node_name, "Dequantize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(min_range.tensor)
    op.input_desc.add().CopyFrom(min_range.desc)
    op.input_desc[-1].name = "min_range"
    op.input.append(max_range.tensor)
    op.input_desc.add().CopyFrom(max_range.desc)
    op.input_desc[-1].name = "max_range"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Quantize
@auto_convert_to_tensor([False, False, False], [False, False, True])
def Quantize(x: Tensor, scales: Tensor, zero_points: Optional[Tensor], *, dtype: str, axis: int=1, dependencies=[], node_name=None):
    """REG_OP(Quantize)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(scales, TensorType({DT_FLOAT, DT_BF16}))\n
.OPTIONAL_INPUT(zero_points, TensorType({DT_INT8, DT_UINT8, DT_INT32, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT32}))\n
.REQUIRED_ATTR(dtype, String)\n
.ATTR(axis, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Quantize"
    op.name = next_unique_name(node_name, "Quantize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scales.tensor)
    op.input_desc.add().CopyFrom(scales.desc)
    op.input_desc[-1].name = "scales"
    if zero_points is not None:
        op.input.append(zero_points.tensor)
        op.input_desc.add().CopyFrom(zero_points.desc)
        op.input_desc[-1].name = "zero_points"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "zero_points"

    # process attrs
    op.attr["dtype"].s = compat_as_bytes(dtype)
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendQuant
@auto_convert_to_tensor([False], [False])
def AscendQuant(x: Tensor, *, scale: float, offset: float, sqrt_mode: bool=False, round_mode: str="Round", dst_type: int=2, dependencies=[], node_name=None):
    """REG_OP(AscendQuant)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_INT4}))\n
.REQUIRED_ATTR(scale, Float)\n
.REQUIRED_ATTR(offset, Float)\n
.ATTR(sqrt_mode, Bool, false)\n
.ATTR(round_mode, String, "Round")\n
.ATTR(dst_type, Int, DT_INT8)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendQuant"
    op.name = next_unique_name(node_name, "AscendQuant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["scale"].f = scale
    op.attr["offset"].f = offset
    op.attr["sqrt_mode"].b = sqrt_mode
    op.attr["round_mode"].s = compat_as_bytes(round_mode)
    op.attr["dst_type"].i = dst_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y

# This api is auto-generated from IR AscendQuantV2
@auto_convert_to_tensor([False, False, False], [False, False, True])
def AscendQuantV2(x: Tensor,
                  scale: Tensor,
                  offset: Optional[Tensor],
                  *,
                  sqrt_mode: bool = False,
                  round_mode: str = "round",
                  dst_type: int = 2,
                  axis: int = -1,
                  dependencies=[],
                  node_name=None):
    """REG_OP(AscendQuantV2)\n
    .INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
    .INPUT(scale, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
    .OPTIONAL_INPUT(offset, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
    .OUTPUT(y, TensorType({DT_INT8, DT_INT4}))\n
    .ATTR(sqrt_mode, Bool, false)\n
    .ATTR(round_mode, String, "round")\n
    .ATTR(dst_type, Int, DT_INT8)\n
    .ATTR(axis, Int, -1)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "AscendQuantV2"
    op.name = next_unique_name(node_name, "AscendQuantV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["sqrt_mode"].b = sqrt_mode
    op.attr["round_mode"].s = compat_as_bytes(round_mode)
    op.attr["dst_type"].i = dst_type
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendDequant
@auto_convert_to_tensor([False, False], [False, False])
def AscendDequant(x: Tensor, deq_scale: Tensor, *, sqrt_mode: bool=False, relu_flag: bool=False, dtype: int=0, dependencies=[], node_name=None):
    """REG_OP(AscendDequant)\n
.INPUT(x, TensorType({DT_INT32}))\n
.INPUT(deq_scale, TensorType({DT_FLOAT16, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(sqrt_mode, Bool, false)\n
.ATTR(relu_flag, Bool, false)\n
.ATTR(dtype, Int, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendDequant"
    op.name = next_unique_name(node_name, "AscendDequant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(deq_scale.tensor)
    op.input_desc.add().CopyFrom(deq_scale.desc)
    op.input_desc[-1].name = "deq_scale"

    # process attrs
    op.attr["sqrt_mode"].b = sqrt_mode
    op.attr["relu_flag"].b = relu_flag
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendAntiQuant
@auto_convert_to_tensor([False], [False])
def AscendAntiQuant(x: Tensor, *, scale: float, offset: float, dtype: int=0, sqrt_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(AscendAntiQuant)\n
.INPUT(x, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(scale, Float)\n
.REQUIRED_ATTR(offset, Float)\n
.ATTR(dtype, Int, DT_FLOAT)\n
.ATTR(sqrt_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendAntiQuant"
    op.name = next_unique_name(node_name, "AscendAntiQuant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["scale"].f = scale
    op.attr["offset"].f = offset
    op.attr["dtype"].i = dtype
    op.attr["sqrt_mode"].b = sqrt_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendAntiQuantV2
@auto_convert_to_tensor([False, False, False], [False, False, True])
def AscendAntiQuantV2(x: Tensor, scale: Tensor, offset: Optional[Tensor], *, dst_type: int=1, sqrt_mode: bool=False, dependencies=[], node_name=None):
    """REG_OP(AscendAntiQuantV2)\n
.INPUT(x, TensorType({DT_INT8, DT_INT4}))\n
.INPUT(scale, TensorType({DT_FLOAT, DT_BFLOAT16}))\n
.OPTIONAL_INPUT(offset, TensorType({DT_FLOAT, DT_BFLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_BFLOAT16}))\n
.ATTR(dst_type, Int, DT_FLOAT16)\n
.ATTR(sqrt_mode, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendAntiQuantV2"
    op.name = next_unique_name(node_name, "AscendAntiQuantV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["dst_type"].i = dst_type
    op.attr["sqrt_mode"].b = sqrt_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendDequantS16
@auto_convert_to_tensor([False, False, False], [False, False, True])
def AscendDequantS16(x0: Tensor, deq_scale: Tensor, x1: Optional[Tensor], *, relu_flag: bool=False, dependencies=[], node_name=None):
    """REG_OP(AscendDequantS16)\n
.INPUT(x0, TensorType({DT_INT32}))\n
.INPUT(deq_scale, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(x1, TensorType({DT_INT16}))\n
.OUTPUT(y, TensorType({DT_INT16}))\n
.ATTR(relu_flag, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendDequantS16"
    op.name = next_unique_name(node_name, "AscendDequantS16")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x0.tensor)
    op.input_desc.add().CopyFrom(x0.desc)
    op.input_desc[-1].name = "x0"
    op.input.append(deq_scale.tensor)
    op.input_desc.add().CopyFrom(deq_scale.desc)
    op.input_desc[-1].name = "deq_scale"
    if x1 is not None:
        op.input.append(x1.tensor)
        op.input_desc.add().CopyFrom(x1.desc)
        op.input_desc[-1].name = "x1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x1"

    # process attrs
    op.attr["relu_flag"].b = relu_flag

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendRequant
@auto_convert_to_tensor([False, False], [False, False])
def AscendRequant(x: Tensor, req_scale: Tensor, *, relu_flag: bool=False, dependencies=[], node_name=None):
    """REG_OP(AscendRequant)\n
.INPUT(x, TensorType({DT_INT32}))\n
.INPUT(req_scale, TensorType({DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_INT8}))\n
.ATTR(relu_flag, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendRequant"
    op.name = next_unique_name(node_name, "AscendRequant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(req_scale.tensor)
    op.input_desc.add().CopyFrom(req_scale.desc)
    op.input_desc[-1].name = "req_scale"

    # process attrs
    op.attr["relu_flag"].b = relu_flag

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AscendRequantS16
@auto_convert_to_tensor([False, False, False], [False, False, True])
def AscendRequantS16(x0: Tensor, req_scale: Tensor, x1: Optional[Tensor], *, dual_output: bool=False, relu_flag: bool=False, dependencies=[], node_name=None):
    """REG_OP(AscendRequantS16)\n
.INPUT(x0, TensorType({DT_INT16}))\n
.INPUT(req_scale, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(x1, TensorType({DT_INT16}))\n
.OUTPUT(y0, TensorType({DT_INT8}))\n
.OUTPUT(y1, TensorType({DT_INT16}))\n
.ATTR(dual_output, Bool, false)\n
.ATTR(relu_flag, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendRequantS16"
    op.name = next_unique_name(node_name, "AscendRequantS16")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x0.tensor)
    op.input_desc.add().CopyFrom(x0.desc)
    op.input_desc[-1].name = "x0"
    op.input.append(req_scale.tensor)
    op.input_desc.add().CopyFrom(req_scale.desc)
    op.input_desc[-1].name = "req_scale"
    if x1 is not None:
        op.input.append(x1.tensor)
        op.input_desc.add().CopyFrom(x1.desc)
        op.input_desc[-1].name = "x1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x1"

    # process attrs
    op.attr["dual_output"].b = dual_output
    op.attr["relu_flag"].b = relu_flag

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y0"
    y0 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y0, y1


# This api is auto-generated from IR AscendWeightQuant
@auto_convert_to_tensor([False, False], [False, False])
def AscendWeightQuant(x: Tensor, offset: Tensor, *, dst_type: int=2, dependencies=[], node_name=None):
    """REG_OP(AscendWeightQuant)\n
.INPUT(x, TensorType({DT_INT8}))\n
.INPUT(offset, TensorType({DT_INT8}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_INT4}))\n
.ATTR(dst_type, Int, DT_INT8)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AscendWeightQuant"
    op.name = next_unique_name(node_name, "AscendWeightQuant")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["dst_type"].i = dst_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RaggedGather
@auto_convert_to_tensor([True, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def _RaggedGather(params_nested_splits: List[Tensor], params_dense_values: Tensor, indices: Tensor, *, size_of_output_nested_splits: int, Tsplits: int, PARAMS_RAGGED_RANK: int=1, OUTPUT_RAGGED_RANK: int=0, dependencies=[], node_name=None):
    """REG_OP(RaggedGather)\n
.DYNAMIC_INPUT(params_nested_splits, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(params_dense_values, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.DYNAMIC_OUTPUT(output_nested_splits, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(output_dense_values, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(Tsplits, Type)\n
.ATTR(PARAMS_RAGGED_RANK, Int, 1)\n
.ATTR(OUTPUT_RAGGED_RANK, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedGather"
    op.name = next_unique_name(node_name, "RaggedGather")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(params_nested_splits, (tuple, list)):
        raise AssertionError("params_nested_splits must be a tuple or a list.")
    for i, v in enumerate(params_nested_splits):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "params_nested_splits" + str(i)
    op.input.append(params_dense_values.tensor)
    op.input_desc.add().CopyFrom(params_dense_values.desc)
    op.input_desc[-1].name = "params_dense_values"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["Tsplits"].dt = Tsplits
    op.attr["PARAMS_RAGGED_RANK"].i = PARAMS_RAGGED_RANK
    op.attr["OUTPUT_RAGGED_RANK"].i = OUTPUT_RAGGED_RANK

    # process outputs
    output_index = 0
    output_nested_splits = []
    for i in range(output_index, output_index + size_of_output_nested_splits):
        op.output_desc.add().name = "output_nested_splits" + str(i - output_index)
        output_nested_splits.append(Tensor(op, i))
    output_index += size_of_output_nested_splits
    op.output_desc.add().name = "output_dense_values"
    output_dense_values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_nested_splits, output_dense_values


# This api is auto-generated from IR RaggedTensorToSparse
@auto_convert_to_tensor([True, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def RaggedTensorToSparse(rt_nested_splits: List[Tensor], rt_dense_values: Tensor, *, RAGGED_RANK: int=1, Tsplits: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(RaggedTensorToSparse)\n
.DYNAMIC_INPUT(rt_nested_splits, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(rt_dense_values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(sparse_indices, TensorType({DT_INT64}))\n
.OUTPUT(sparse_values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(sparse_dense_shape, TensorType({DT_INT64}))\n
.ATTR(RAGGED_RANK, Int, 1)\n
.ATTR(Tsplits, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedTensorToSparse"
    op.name = next_unique_name(node_name, "RaggedTensorToSparse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(rt_nested_splits, (tuple, list)):
        raise AssertionError("rt_nested_splits must be a tuple or a list.")
    for i, v in enumerate(rt_nested_splits):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "rt_nested_splits" + str(i)
    op.input.append(rt_dense_values.tensor)
    op.input_desc.add().CopyFrom(rt_dense_values.desc)
    op.input_desc[-1].name = "rt_dense_values"

    # process attrs
    op.attr["RAGGED_RANK"].i = RAGGED_RANK
    op.attr["Tsplits"].dt = Tsplits

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sparse_indices"
    sparse_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sparse_values"
    sparse_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sparse_dense_shape"
    sparse_dense_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sparse_indices, sparse_values, sparse_dense_shape


# This api is auto-generated from IR RaggedTensorToTensor
@auto_convert_to_tensor([False, False, False, True], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def RaggedTensorToTensor(shape: Tensor, values: Tensor, default_value: Tensor, row_partition_tensors: List[Tensor], *, num_row_partition_tensors: int, row_partition_types: List[str], dependencies=[], node_name=None):
    """REG_OP(RaggedTensorToTensor)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.INPUT(default_value, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.DYNAMIC_INPUT(row_partition_tensors, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(result, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(num_row_partition_tensors, Int)\n
.REQUIRED_ATTR(row_partition_types, ListString)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedTensorToTensor"
    op.name = next_unique_name(node_name, "RaggedTensorToTensor")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(default_value.tensor)
    op.input_desc.add().CopyFrom(default_value.desc)
    op.input_desc[-1].name = "default_value"
    if not isinstance(row_partition_tensors, (tuple, list)):
        raise AssertionError("row_partition_tensors must be a tuple or a list.")
    for i, v in enumerate(row_partition_tensors):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "row_partition_tensors" + str(i)

    # process attrs
    op.attr["num_row_partition_tensors"].i = num_row_partition_tensors
    op.attr["row_partition_types"].list.val_type = 0
    op.attr["row_partition_types"].list.s.extend(compat_as_bytes_list(row_partition_types))

    # process outputs
    output_index = 0
    op.output_desc.add().name = "result"
    result = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return result


# This api is auto-generated from IR RaggedRange
@auto_convert_to_tensor([False, False, False], [False, False, False])
def RaggedRange(starts: Tensor, limits: Tensor, deltas: Tensor, *, Tsplits: int, dependencies=[], node_name=None):
    """REG_OP(RaggedRange)\n
.INPUT(starts, TensorType({DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64}))\n
.INPUT(limits, TensorType({DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64}))\n
.INPUT(deltas, TensorType({DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64}))\n
.OUTPUT(rt_nested_splits, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(rt_dense_values, TensorType({DT_FLOAT,DT_DOUBLE,DT_INT32,DT_INT64}))\n
.REQUIRED_ATTR(Tsplits, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RaggedRange"
    op.name = next_unique_name(node_name, "RaggedRange")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(starts.tensor)
    op.input_desc.add().CopyFrom(starts.desc)
    op.input_desc[-1].name = "starts"
    op.input.append(limits.tensor)
    op.input_desc.add().CopyFrom(limits.desc)
    op.input_desc[-1].name = "limits"
    op.input.append(deltas.tensor)
    op.input_desc.add().CopyFrom(deltas.desc)
    op.input_desc[-1].name = "deltas"

    # process attrs
    op.attr["Tsplits"].dt = Tsplits

    # process outputs
    output_index = 0
    op.output_desc.add().name = "rt_nested_splits"
    rt_nested_splits = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rt_dense_values"
    rt_dense_values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return rt_nested_splits, rt_dense_values


# This api is auto-generated from IR DSAGenBitMask
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DSAGenBitMask(count: Tensor, seed: Tensor, dropout: Tensor, *, random_algorithm: str="Philox", output_dtype: str="uint8", dependencies=[], node_name=None):
    """REG_OP(DSAGenBitMask)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(dropout, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(out, TensorType({DT_UINT1, DT_UINT8}))\n
.ATTR(random_algorithm, String, "Philox")\n
.ATTR(output_dtype, String, "uint8")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSAGenBitMask"
    op.name = next_unique_name(node_name, "DSAGenBitMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(dropout.tensor)
    op.input_desc.add().CopyFrom(dropout.desc)
    op.input_desc[-1].name = "dropout"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)
    op.attr["output_dtype"].s = compat_as_bytes(output_dtype)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR DSARandomTruncatedNormal
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def DSARandomTruncatedNormal(count: Tensor, seed: Tensor, mean: Tensor, stdev: Tensor, *, random_algorithm: str="Philox", dependencies=[], node_name=None):
    """REG_OP(DSARandomTruncatedNormal)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(stdev, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(out, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BF16}))\n
.ATTR(random_algorithm, String, "Philox")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSARandomTruncatedNormal"
    op.name = next_unique_name(node_name, "DSARandomTruncatedNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(stdev.tensor)
    op.input_desc.add().CopyFrom(stdev.desc)
    op.input_desc[-1].name = "stdev"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR DSAStatelessRandomTruncatedNormal
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def DSAStatelessRandomTruncatedNormal(count: Tensor, seed: Tensor, mean: Tensor, stdev: Tensor, counter: Tensor, *, random_algorithm: str="Philox", dependencies=[], node_name=None):
    """REG_OP(DSAStatelessRandomTruncatedNormal)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(stdev, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(counter, TensorType({DT_UINT64}))\n
.OUTPUT(out, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BF16}))\n
.ATTR(random_algorithm, String, "Philox")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSAStatelessRandomTruncatedNormal"
    op.name = next_unique_name(node_name, "DSAStatelessRandomTruncatedNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(stdev.tensor)
    op.input_desc.add().CopyFrom(stdev.desc)
    op.input_desc[-1].name = "stdev"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR DSARandomNormal
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def DSARandomNormal(count: Tensor, seed: Tensor, mean: Tensor, stdev: Tensor, *, random_algorithm: str="Philox", dependencies=[], node_name=None):
    """REG_OP(DSARandomNormal)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(stdev, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(out, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BF16}))\n
.ATTR(random_algorithm, String, "Philox")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSARandomNormal"
    op.name = next_unique_name(node_name, "DSARandomNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(stdev.tensor)
    op.input_desc.add().CopyFrom(stdev.desc)
    op.input_desc[-1].name = "stdev"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR DSAStatelessGenBitMask
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def DSAStatelessGenBitMask(count: Tensor, seed: Tensor, dropout: Tensor, offset: Tensor, *, random_algorithm: str="Philox", output_dtype: str="uint8", dependencies=[], node_name=None):
    """REG_OP(DSAStatelessGenBitMask)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(dropout, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(offset, TensorType({DT_UINT64}))\n
.OUTPUT(out, TensorType({DT_UINT1, DT_UINT8}))\n
.ATTR(random_algorithm, String, "Philox")\n
.ATTR(output_dtype, String, "uint8")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSAStatelessGenBitMask"
    op.name = next_unique_name(node_name, "DSAStatelessGenBitMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(dropout.tensor)
    op.input_desc.add().CopyFrom(dropout.desc)
    op.input_desc[-1].name = "dropout"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)
    op.attr["output_dtype"].s = compat_as_bytes(output_dtype)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR DSAStatelessRandomNormal
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def DSAStatelessRandomNormal(count: Tensor, seed: Tensor, mean: Tensor, stdev: Tensor, counter: Tensor, *, random_algorithm: str="Philox", dependencies=[], node_name=None):
    """REG_OP(DSAStatelessRandomNormal)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(stdev, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.INPUT(counter, TensorType({DT_UINT64}))\n
.OUTPUT(out, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.ATTR(random_algorithm, String, "Philox")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSAStatelessRandomNormal"
    op.name = next_unique_name(node_name, "DSAStatelessRandomNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(stdev.tensor)
    op.input_desc.add().CopyFrom(stdev.desc)
    op.input_desc[-1].name = "stdev"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR DSAStatelessRandomUniform
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def DSAStatelessRandomUniform(count: Tensor, seed: Tensor, low: Tensor, high: Tensor, counter: Tensor, *, random_algorithm: str="Philox", dependencies=[], node_name=None):
    """REG_OP(DSAStatelessRandomUniform)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(low, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.INPUT(high, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.INPUT(counter, TensorType({DT_UINT64}))\n
.OUTPUT(out, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.ATTR(random_algorithm, String, "Philox")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSAStatelessRandomUniform"
    op.name = next_unique_name(node_name, "DSAStatelessRandomUniform")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(low.tensor)
    op.input_desc.add().CopyFrom(low.desc)
    op.input_desc[-1].name = "low"
    op.input.append(high.tensor)
    op.input_desc.add().CopyFrom(high.desc)
    op.input_desc[-1].name = "high"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR DSARandomUniform
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def DSARandomUniform(count: Tensor, seed: Tensor, low: Tensor, high: Tensor, *, random_algorithm: str="Philox", dependencies=[], node_name=None):
    """REG_OP(DSARandomUniform)\n
.INPUT(count, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_UINT64}))\n
.INPUT(low, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.INPUT(high, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.OUTPUT(out, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.ATTR(random_algorithm, String, "Philox")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DSARandomUniform"
    op.name = next_unique_name(node_name, "DSARandomUniform")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(low.tensor)
    op.input_desc.add().CopyFrom(low.desc)
    op.input_desc[-1].name = "low"
    op.input.append(high.tensor)
    op.input_desc.add().CopyFrom(high.desc)
    op.input_desc[-1].name = "high"

    # process attrs
    op.attr["random_algorithm"].s = compat_as_bytes(random_algorithm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR Multinomial
@auto_convert_to_tensor([False, False], [False, False])
def Multinomial(logits: Tensor, num_samples: Tensor, *, dtype: int=DataType.DT_INT64, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(Multinomial)\n
.INPUT(logits, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(num_samples, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(dtype, Type, DT_INT64)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Multinomial"
    op.name = next_unique_name(node_name, "Multinomial")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(logits.tensor)
    op.input_desc.add().CopyFrom(logits.desc)
    op.input_desc[-1].name = "logits"
    op.input.append(num_samples.tensor)
    op.input_desc.add().CopyFrom(num_samples.desc)
    op.input_desc[-1].name = "num_samples"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MultinomialAliasDraw
@auto_convert_to_tensor([False, False], [False, False])
def MultinomialAliasDraw(q: Tensor, j: Tensor, *, num_samples: int, seed: int=0, dependencies=[], node_name=None):
    """REG_OP(MultinomialAliasDraw)\n
.INPUT(q, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(j, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(num_samples, Int)\n
.ATTR(seed, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultinomialAliasDraw"
    op.name = next_unique_name(node_name, "MultinomialAliasDraw")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(q.tensor)
    op.input_desc.add().CopyFrom(q.desc)
    op.input_desc[-1].name = "q"
    op.input.append(j.tensor)
    op.input_desc.add().CopyFrom(j.desc)
    op.input_desc[-1].name = "j"

    # process attrs
    op.attr["num_samples"].i = num_samples
    op.attr["seed"].i = seed

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MultinomialAliasSetup
@auto_convert_to_tensor([False], [False])
def MultinomialAliasSetup(probs: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MultinomialAliasSetup)\n
.INPUT(probs, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(j, TensorType({DT_INT64}))\n
.OUTPUT(q, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultinomialAliasSetup"
    op.name = next_unique_name(node_name, "MultinomialAliasSetup")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(probs.tensor)
    op.input_desc.add().CopyFrom(probs.desc)
    op.input_desc[-1].name = "probs"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "j"
    j = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "q"
    q = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return j, q


# This api is auto-generated from IR ParameterizedTruncatedNormal
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def ParameterizedTruncatedNormal(shape: Tensor, means: Tensor, stdevs: Tensor, min: Tensor, max: Tensor, *, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(ParameterizedTruncatedNormal)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(means, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(stdevs, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(min, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(max, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ParameterizedTruncatedNormal"
    op.name = next_unique_name(node_name, "ParameterizedTruncatedNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(means.tensor)
    op.input_desc.add().CopyFrom(means.desc)
    op.input_desc[-1].name = "means"
    op.input.append(stdevs.tensor)
    op.input_desc.add().CopyFrom(stdevs.desc)
    op.input_desc[-1].name = "stdevs"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomGammaGrad
@auto_convert_to_tensor([False, False], [False, False])
def RandomGammaGrad(alpha: Tensor, sample: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RandomGammaGrad)\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(sample, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomGammaGrad"
    op.name = next_unique_name(node_name, "RandomGammaGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"
    op.input.append(sample.tensor)
    op.input_desc.add().CopyFrom(sample.desc)
    op.input_desc[-1].name = "sample"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomGamma
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def RandomGamma(shape: Tensor, alpha: Tensor, *, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(RandomGamma)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(alpha, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomGamma"
    op.name = next_unique_name(node_name, "RandomGamma")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Randperm
@auto_convert_to_tensor([], [])
def Randperm(*, n: int, layout: int=0, dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(Randperm)\n
.OUTPUT(out, TensorType({DT_INT64, DT_INT32, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT32, DT_DOUBLE}))\n
.REQUIRED_ATTR(n, Int)\n
.ATTR(layout, Int, 0)\n
.ATTR(dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Randperm"
    op.name = next_unique_name(node_name, "Randperm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["n"].i = n
    op.attr["layout"].i = layout
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR Poisson
@auto_convert_to_tensor([False], [False])
def Poisson(x: Tensor, *, seed: int=0, dependencies=[], node_name=None):
    """REG_OP(Poisson)\n
.INPUT(x, TensorType({ DT_FLOAT16,DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_FLOAT16,DT_FLOAT }))\n
.ATTR(seed, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Poisson"
    op.name = next_unique_name(node_name, "Poisson")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["seed"].i = seed

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomPoisson
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def RandomPoisson(shape: Tensor, rate: Tensor, *, dtype: int=DataType.DT_INT64, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(RandomPoisson)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(rate, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.ATTR(dtype, Type, DT_INT64)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomPoisson"
    op.name = next_unique_name(node_name, "RandomPoisson")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(rate.tensor)
    op.input_desc.add().CopyFrom(rate.desc)
    op.input_desc[-1].name = "rate"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomShuffle
@auto_convert_to_tensor([False], [False])
def RandomShuffle(x: Tensor, *, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(RandomShuffle)\n
.INPUT(x, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_STRING, DT_RESOURCE}))\n
.OUTPUT(y, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_STRING, DT_RESOURCE}))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomShuffle"
    op.name = next_unique_name(node_name, "RandomShuffle")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomStandardNormal
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def RandomStandardNormal(shape: Tensor, *, dtype: int, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(RandomStandardNormal)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomStandardNormal"
    op.name = next_unique_name(node_name, "RandomStandardNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Normal
@auto_convert_to_tensor([False, False], [False, False])
def Normal(mean: Tensor, std: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Normal)\n
.INPUT(mean, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(std, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Normal"
    op.name = next_unique_name(node_name, "Normal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(std.tensor)
    op.input_desc.add().CopyFrom(std.desc)
    op.input_desc[-1].name = "std"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomUniformInt
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def RandomUniformInt(shape: Tensor, min: Tensor, max: Tensor, *, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(RandomUniformInt)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(min, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(max, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomUniformInt"
    op.name = next_unique_name(node_name, "RandomUniformInt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomUniform
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def RandomUniform(shape: Tensor, *, dtype: int, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(RandomUniform)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomUniform"
    op.name = next_unique_name(node_name, "RandomUniform")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TruncatedNormal
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def TruncatedNormal(shape: Tensor, *, seed: int=0, seed2: int=0, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(TruncatedNormal)\n
.INPUT(shape, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT16, DT_FLOAT, DT_DOUBLE }))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TruncatedNormal"
    op.name = next_unique_name(node_name, "TruncatedNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DropOutGenMask
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def DropOutGenMask(shape: Tensor, prob: Tensor, *, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(DropOutGenMask)\n
.INPUT(shape, TensorType({ DT_INT32, DT_INT64 }))\n
.INPUT(prob, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_UINT8 }))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropOutGenMask"
    op.name = next_unique_name(node_name, "DropOutGenMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(prob.tensor)
    op.input_desc.add().CopyFrom(prob.desc)
    op.input_desc[-1].name = "prob"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DropOutGenMaskV3
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def DropOutGenMaskV3(shape: Tensor, prob: Tensor, *, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(DropOutGenMaskV3)\n
.INPUT(shape, TensorType({ DT_INT32, DT_INT64 }))\n
.INPUT(prob, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_UINT8 }))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropOutGenMaskV3"
    op.name = next_unique_name(node_name, "DropOutGenMaskV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(prob.tensor)
    op.input_desc.add().CopyFrom(prob.desc)
    op.input_desc[-1].name = "prob"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessDropOutGenMask
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, True], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def StatelessDropOutGenMask(shape: Tensor, prob: Tensor, seed: Tensor, seed1: Tensor, offset: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(StatelessDropOutGenMask)\n
.INPUT(shape, TensorType({ DT_INT32, DT_INT64 }))\n
.INPUT(prob, TensorType({ DT_FLOAT16, DT_FLOAT, DT_BF16 }))\n
.INPUT(seed, TensorType({ DT_INT32, DT_INT64 }))\n
.INPUT(seed1, TensorType({ DT_INT32, DT_INT64 }))\n
.OPTIONAL_INPUT(offset, TensorType({ DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_UINT8 }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessDropOutGenMask"
    op.name = next_unique_name(node_name, "StatelessDropOutGenMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(prob.tensor)
    op.input_desc.add().CopyFrom(prob.desc)
    op.input_desc[-1].name = "prob"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(seed1.tensor)
    op.input_desc.add().CopyFrom(seed1.desc)
    op.input_desc[-1].name = "seed1"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessBernoulli
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessBernoulli(shape: Tensor, prob: Tensor, seed: Tensor, offset: Tensor, *, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(StatelessBernoulli)\n
.INPUT(shape, TensorType({ DT_INT32, DT_INT64}))\n
.INPUT(prob, TensorType({ DT_FLOAT16, DT_FLOAT, DT_DOUBLE }))\n
.INPUT(seed, TensorType({ DT_INT64 }))\n
.INPUT(offset, TensorType({ DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BF16}))\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessBernoulli"
    op.name = next_unique_name(node_name, "StatelessBernoulli")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(prob.tensor)
    op.input_desc.add().CopyFrom(prob.desc)
    op.input_desc[-1].name = "prob"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessBernoulliV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def StatelessBernoulliV2(x: Tensor, seed: Tensor, offset: Tensor, *, dtype: int=28, dependencies=[], node_name=None):
    """REG_OP(StatelessBernoulliV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(seed, TensorType({DT_INT64}))\n
.INPUT(offset, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BF16}))\n
.ATTR(dtype, Type, DT_UNDEFINED)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessBernoulliV2"
    op.name = next_unique_name(node_name, "StatelessBernoulliV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LinSpaceD
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def LinSpaceD(assist: Tensor, start: Tensor, stop: Tensor, num: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LinSpaceD)\n
.INPUT(assist, TensorType({DT_FLOAT}))\n
.INPUT(start, TensorType({DT_FLOAT}))\n
.INPUT(stop, TensorType({DT_FLOAT}))\n
.INPUT(num, TensorType::IndexNumberType())\n
.OUTPUT(output, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LinSpaceD"
    op.name = next_unique_name(node_name, "LinSpaceD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"
    op.input.append(start.tensor)
    op.input_desc.add().CopyFrom(start.desc)
    op.input_desc[-1].name = "start"
    op.input.append(stop.tensor)
    op.input_desc.add().CopyFrom(stop.desc)
    op.input_desc[-1].name = "stop"
    op.input.append(num.tensor)
    op.input_desc.add().CopyFrom(num.desc)
    op.input_desc[-1].name = "num"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR LinSpace
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def LinSpace(start: Tensor, stop: Tensor, num: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(LinSpace)\n
.INPUT(start, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(stop, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(num, TensorType::IndexNumberType())\n
.OUTPUT(output, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LinSpace"
    op.name = next_unique_name(node_name, "LinSpace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(start.tensor)
    op.input_desc.add().CopyFrom(start.desc)
    op.input_desc[-1].name = "start"
    op.input.append(stop.tensor)
    op.input_desc.add().CopyFrom(stop.desc)
    op.input_desc[-1].name = "stop"
    op.input.append(num.tensor)
    op.input_desc.add().CopyFrom(num.desc)
    op.input_desc[-1].name = "num"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR Dropout
@auto_convert_to_tensor([False], [False])
def Dropout(x: Tensor, *, dropout_ratio: float=0.500000, scale_train: bool=True, alpha: float=1.000000, beta: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(Dropout)\n
.INPUT(x, TensorType{DT_FLOAT})\n
.OUTPUT(y, TensorType{DT_FLOAT})\n
.ATTR(dropout_ratio, Float, 0.5)\n
.ATTR(scale_train, Bool, true)\n
.ATTR(alpha, Float, 1.0)\n
.ATTR(beta, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Dropout"
    op.name = next_unique_name(node_name, "Dropout")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dropout_ratio"].f = dropout_ratio
    op.attr["scale_train"].b = scale_train
    op.attr["alpha"].f = alpha
    op.attr["beta"].f = beta

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RandomChoiceWithMask
@auto_convert_to_tensor([False], [False])
def RandomChoiceWithMask(x: Tensor, *, count: int=0, seed: int=0, seed2: int=0, dependencies=[], node_name=None):
    """REG_OP(RandomChoiceWithMask)\n
.INPUT(x, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.OUTPUT(mask, TensorType({DT_BOOL}))\n
.ATTR(count, Int, 0)\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RandomChoiceWithMask"
    op.name = next_unique_name(node_name, "RandomChoiceWithMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["count"].i = count
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mask"
    mask = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mask


# This api is auto-generated from IR ShuffleChannel
@auto_convert_to_tensor([False], [False])
def ShuffleChannel(x: Tensor, *, group: int=1, dependencies=[], node_name=None):
    """REG_OP(ShuffleChannel)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32,DT_INT64,DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT,DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32,DT_INT64,DT_UINT64}))\n
.ATTR(group, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ShuffleChannel"
    op.name = next_unique_name(node_name, "ShuffleChannel")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["group"].i = group

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MultinomialFuss
@auto_convert_to_tensor([False], [False])
def MultinomialFuss(x: Tensor, *, dtype: int=6, sample_size: int=1, seed: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(MultinomialFuss)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_FLOAT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(dtype, Int, 6)\n
.ATTR(sample_size, Int, 1)\n
.ATTR(seed, Float, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultinomialFuss"
    op.name = next_unique_name(node_name, "MultinomialFuss")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dtype"].i = dtype
    op.attr["sample_size"].i = sample_size
    op.attr["seed"].f = seed

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DropoutV2
@auto_convert_to_tensor([False, False], [False, False])
def DropoutV2(x: Tensor, seed: Tensor, *, p: float, dependencies=[], node_name=None):
    """REG_OP(DropoutV2)\n
.INPUT(x, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.INPUT(seed, TensorType({ DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.OUTPUT(mask, TensorType({ DT_FLOAT }))\n
.OUTPUT(seed, TensorType({ DT_FLOAT }))\n
.REQUIRED_ATTR(p, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropoutV2"
    op.name = next_unique_name(node_name, "DropoutV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"

    # process attrs
    op.attr["p"].f = p

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mask"
    mask = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "seed"
    seed = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mask, seed


# This api is auto-generated from IR Uniform
@auto_convert_to_tensor([False], [False])
def Uniform(x: Tensor, *, from_changed_as_is_python_key: float=0.000000, to: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(Uniform)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(from, Float, 0.0)\n
.ATTR(to, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Uniform"
    op.name = next_unique_name(node_name, "Uniform")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["from"].f = from_changed_as_is_python_key
    op.attr["to"].f = to

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ContinuationIndicator
@auto_convert_to_tensor([], [])
def ContinuationIndicator(*, time_step: int, batch_size: int, dependencies=[], node_name=None):
    """REG_OP(ContinuationIndicator)\n
.REQUIRED_ATTR(time_step, Int)\n
.REQUIRED_ATTR(batch_size, Int)\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ContinuationIndicator"
    op.name = next_unique_name(node_name, "ContinuationIndicator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["time_step"].i = time_step
    op.attr["batch_size"].i = batch_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Exponential
@auto_convert_to_tensor([False], [False])
def Exponential(x: Tensor, *, lambda_changed_as_is_python_key: float=1.000000, seed: int=0, dependencies=[], node_name=None):
    """REG_OP(Exponential)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(lambda, Float, 1)\n
.ATTR(seed, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Exponential"
    op.name = next_unique_name(node_name, "Exponential")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["lambda"].f = lambda_changed_as_is_python_key
    op.attr["seed"].i = seed

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Geometric
@auto_convert_to_tensor([False], [False])
def Geometric(x: Tensor, *, p: float, seed: int=0, dependencies=[], node_name=None):
    """REG_OP(Geometric)\n
.INPUT(x, TensorType({ DT_FLOAT16,DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_FLOAT16,DT_FLOAT }))\n
.REQUIRED_ATTR(p, Float)\n
.ATTR(seed, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Geometric"
    op.name = next_unique_name(node_name, "Geometric")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["p"].f = p
    op.attr["seed"].i = seed

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MultinomialWithReplacement
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MultinomialWithReplacement(x: Tensor, seed: Tensor, offset: Tensor, *, numsamples: int, replacement: bool=False, dependencies=[], node_name=None):
    """REG_OP(MultinomialWithReplacement)\n
.INPUT(x, TensorType({ DT_FLOAT16,DT_FLOAT,DT_DOUBLE }))\n
.INPUT(seed, TensorType({ DT_INT64 }))\n
.INPUT(offset, TensorType({ DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_INT64 }))\n
.REQUIRED_ATTR(numsamples, Int)\n
.ATTR(replacement, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MultinomialWithReplacement"
    op.name = next_unique_name(node_name, "MultinomialWithReplacement")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["numsamples"].i = numsamples
    op.attr["replacement"].b = replacement

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandperm
@auto_convert_to_tensor([False, False, False], [False, False, False])
def StatelessRandperm(n: Tensor, seed: Tensor, offset: Tensor, *, layout: int=0, dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(StatelessRandperm)\n
.INPUT(n, TensorType({DT_INT64}))\n
.INPUT(seed, TensorType({DT_INT64}))\n
.INPUT(offset, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT64, DT_INT32, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(layout, Int, 0)\n
.ATTR(dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandperm"
    op.name = next_unique_name(node_name, "StatelessRandperm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(n.tensor)
    op.input_desc.add().CopyFrom(n.desc)
    op.input_desc[-1].name = "n"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["layout"].i = layout
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DropOutGenMaskV4
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def DropOutGenMaskV4(shape: Tensor, prob: Tensor, *, seed: int=0, seed2: int=0, dtype: int=DataType.DT_BOOL, dependencies=[], node_name=None):
    """REG_OP(DropOutGenMaskV4)\n
.INPUT(shape, TensorType({ DT_INT32, DT_INT64 }))\n
.INPUT(prob, TensorType({ DT_FLOAT16, DT_FLOAT }))\n
.OUTPUT(y, TensorType({ DT_BOOL, DT_UINT1 }))\n
.ATTR(seed, Int, 0)\n
.ATTR(seed2, Int, 0)\n
.ATTR(dtype, Type, DT_BOOL)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DropOutGenMaskV4"
    op.name = next_unique_name(node_name, "DropOutGenMaskV4")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(prob.tensor)
    op.input_desc.add().CopyFrom(prob.desc)
    op.input_desc[-1].name = "prob"

    # process attrs
    op.attr["seed"].i = seed
    op.attr["seed2"].i = seed2
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BNTrainingReduce
@auto_convert_to_tensor([False], [False])
def BNTrainingReduce(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BNTrainingReduce)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(sum, TensorType({DT_FLOAT}))\n
.OUTPUT(square_sum, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNTrainingReduce"
    op.name = next_unique_name(node_name, "BNTrainingReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sum"
    sum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "square_sum"
    square_sum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sum, square_sum


# This api is auto-generated from IR BN3DTrainingReduce
@auto_convert_to_tensor([False], [False])
def BN3DTrainingReduce(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BN3DTrainingReduce)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(sum, TensorType({DT_FLOAT}))\n
.OUTPUT(square_sum, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BN3DTrainingReduce"
    op.name = next_unique_name(node_name, "BN3DTrainingReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sum"
    sum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "square_sum"
    square_sum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sum, square_sum


# This api is auto-generated from IR BNTrainingReduceGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def BNTrainingReduceGrad(grads: Tensor, x: Tensor, diff_scale: Tensor, diff_offset: Tensor, scale: Tensor, batch_mean: Tensor, batch_variance: Tensor, *, epsilon: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(BNTrainingReduceGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(diff_scale, TensorType({DT_FLOAT}))\n
.INPUT(diff_offset, TensorType({DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(batch_mean, TensorType({DT_FLOAT}))\n
.INPUT(batch_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNTrainingReduceGrad"
    op.name = next_unique_name(node_name, "BNTrainingReduceGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(diff_scale.tensor)
    op.input_desc.add().CopyFrom(diff_scale.desc)
    op.input_desc[-1].name = "diff_scale"
    op.input.append(diff_offset.tensor)
    op.input_desc.add().CopyFrom(diff_offset.desc)
    op.input_desc[-1].name = "diff_offset"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(batch_mean.tensor)
    op.input_desc.add().CopyFrom(batch_mean.desc)
    op.input_desc[-1].name = "batch_mean"
    op.input.append(batch_variance.tensor)
    op.input_desc.add().CopyFrom(batch_variance.desc)
    op.input_desc[-1].name = "batch_variance"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BN3DTrainingReduceGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def BN3DTrainingReduceGrad(grads: Tensor, x: Tensor, diff_scale: Tensor, diff_offset: Tensor, scale: Tensor, batch_mean: Tensor, batch_variance: Tensor, *, epsilon: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(BN3DTrainingReduceGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(diff_scale, TensorType({DT_FLOAT}))\n
.INPUT(diff_offset, TensorType({DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(batch_mean, TensorType({DT_FLOAT}))\n
.INPUT(batch_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BN3DTrainingReduceGrad"
    op.name = next_unique_name(node_name, "BN3DTrainingReduceGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(diff_scale.tensor)
    op.input_desc.add().CopyFrom(diff_scale.desc)
    op.input_desc[-1].name = "diff_scale"
    op.input.append(diff_offset.tensor)
    op.input_desc.add().CopyFrom(diff_offset.desc)
    op.input_desc[-1].name = "diff_offset"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(batch_mean.tensor)
    op.input_desc.add().CopyFrom(batch_mean.desc)
    op.input_desc[-1].name = "batch_mean"
    op.input.append(batch_variance.tensor)
    op.input_desc.add().CopyFrom(batch_variance.desc)
    op.input_desc[-1].name = "batch_variance"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BNTrainingUpdate
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def BNTrainingUpdate(x: Tensor, sum: Tensor, square_sum: Tensor, scale: Tensor, offset: Tensor, mean: Tensor, variance: Tensor, *, factor: float, epsilon: float, dependencies=[], node_name=None):
    """REG_OP(BNTrainingUpdate)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(sum, TensorType({DT_FLOAT}))\n
.INPUT(square_sum, TensorType({DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(factor, Float)\n
.REQUIRED_ATTR(epsilon, Float)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(mean, TensorType({DT_FLOAT}))\n
.OUTPUT(variance, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNTrainingUpdate"
    op.name = next_unique_name(node_name, "BNTrainingUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(sum.tensor)
    op.input_desc.add().CopyFrom(sum.desc)
    op.input_desc[-1].name = "sum"
    op.input.append(square_sum.tensor)
    op.input_desc.add().CopyFrom(square_sum.desc)
    op.input_desc[-1].name = "square_sum"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["factor"].f = factor
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, variance, batch_mean, batch_variance


# This api is auto-generated from IR BN3DTrainingUpdate
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def BN3DTrainingUpdate(x: Tensor, sum: Tensor, square_sum: Tensor, scale: Tensor, offset: Tensor, mean: Tensor, variance: Tensor, *, factor: float, epsilon: float, dependencies=[], node_name=None):
    """REG_OP(BN3DTrainingUpdate)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(sum, TensorType({DT_FLOAT}))\n
.INPUT(square_sum, TensorType({DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(factor, Float)\n
.REQUIRED_ATTR(epsilon, Float)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(mean, TensorType({DT_FLOAT}))\n
.OUTPUT(variance, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BN3DTrainingUpdate"
    op.name = next_unique_name(node_name, "BN3DTrainingUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(sum.tensor)
    op.input_desc.add().CopyFrom(sum.desc)
    op.input_desc[-1].name = "sum"
    op.input.append(square_sum.tensor)
    op.input_desc.add().CopyFrom(square_sum.desc)
    op.input_desc[-1].name = "square_sum"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["factor"].f = factor
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, variance, batch_mean, batch_variance


# This api is auto-generated from IR BNInfer
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def BNInfer(x: Tensor, scale: Tensor, offset: Tensor, mean: Tensor, variance: Tensor, *, epsilon: float, dependencies=[], node_name=None):
    """REG_OP(BNInfer)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(epsilon, Float)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNInfer"
    op.name = next_unique_name(node_name, "BNInfer")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BNTrainingUpdateV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def BNTrainingUpdateV2(x: Tensor, sum: Tensor, square_sum: Tensor, scale: Tensor, offset: Tensor, *, epsilon: float, dependencies=[], node_name=None):
    """REG_OP(BNTrainingUpdateV2)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(sum, TensorType({DT_FLOAT}))\n
.INPUT(square_sum, TensorType({DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(epsilon, Float)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNTrainingUpdateV2"
    op.name = next_unique_name(node_name, "BNTrainingUpdateV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(sum.tensor)
    op.input_desc.add().CopyFrom(sum.desc)
    op.input_desc[-1].name = "sum"
    op.input.append(square_sum.tensor)
    op.input_desc.add().CopyFrom(square_sum.desc)
    op.input_desc[-1].name = "square_sum"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance


# This api is auto-generated from IR BNTrainingUpdateV3
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def BNTrainingUpdateV3(x: Tensor, sum: Tensor, square_sum: Tensor, scale: Tensor, offset: Tensor, *, epsilon: float, dependencies=[], node_name=None):
    """REG_OP(BNTrainingUpdateV3)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(sum, TensorType({DT_FLOAT}))\n
.INPUT(square_sum, TensorType({DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(offset, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(epsilon, Float)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_1, TensorType({DT_FLOAT}))\n
.OUTPUT(reserve_2, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNTrainingUpdateV3"
    op.name = next_unique_name(node_name, "BNTrainingUpdateV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(sum.tensor)
    op.input_desc.add().CopyFrom(sum.desc)
    op.input_desc[-1].name = "sum"
    op.input.append(square_sum.tensor)
    op.input_desc.add().CopyFrom(square_sum.desc)
    op.input_desc[-1].name = "square_sum"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_1"
    reserve_1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reserve_2"
    reserve_2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance, reserve_1, reserve_2


# This api is auto-generated from IR BNTrainingUpdateGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def BNTrainingUpdateGrad(grads: Tensor, x: Tensor, batch_mean: Tensor, batch_variance: Tensor, *, epsilon: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(BNTrainingUpdateGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(batch_mean, TensorType({DT_FLOAT}))\n
.INPUT(batch_variance, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.OUTPUT(diff_scale, TensorType({DT_FLOAT}))\n
.OUTPUT(diff_offset, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNTrainingUpdateGrad"
    op.name = next_unique_name(node_name, "BNTrainingUpdateGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(batch_mean.tensor)
    op.input_desc.add().CopyFrom(batch_mean.desc)
    op.input_desc[-1].name = "batch_mean"
    op.input.append(batch_variance.tensor)
    op.input_desc.add().CopyFrom(batch_variance.desc)
    op.input_desc[-1].name = "batch_variance"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "diff_scale"
    diff_scale = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "diff_offset"
    diff_offset = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return diff_scale, diff_offset


# This api is auto-generated from IR BN3DTrainingUpdateGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def BN3DTrainingUpdateGrad(grads: Tensor, x: Tensor, batch_mean: Tensor, batch_variance: Tensor, *, epsilon: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(BN3DTrainingUpdateGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(batch_mean, TensorType({DT_FLOAT}))\n
.INPUT(batch_variance, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
.OUTPUT(diff_scale, TensorType({DT_FLOAT}))\n
.OUTPUT(diff_offset, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BN3DTrainingUpdateGrad"
    op.name = next_unique_name(node_name, "BN3DTrainingUpdateGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(batch_mean.tensor)
    op.input_desc.add().CopyFrom(batch_mean.desc)
    op.input_desc[-1].name = "batch_mean"
    op.input.append(batch_variance.tensor)
    op.input_desc.add().CopyFrom(batch_variance.desc)
    op.input_desc[-1].name = "batch_variance"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "diff_scale"
    diff_scale = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "diff_offset"
    diff_offset = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return diff_scale, diff_offset


# This api is auto-generated from IR BNInferGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def BNInferGrad(grads: Tensor, scale: Tensor, batch_variance: Tensor, *, epsilon: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(BNInferGrad)\n
.INPUT(grads, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(scale, TensorType({DT_FLOAT}))\n
.INPUT(batch_variance, TensorType({DT_FLOAT}))\n
.OUTPUT(x_backprop, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.0001)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BNInferGrad"
    op.name = next_unique_name(node_name, "BNInferGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grads.tensor)
    op.input_desc.add().CopyFrom(grads.desc)
    op.input_desc[-1].name = "grads"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    op.input.append(batch_variance.tensor)
    op.input_desc.add().CopyFrom(batch_variance.desc)
    op.input_desc[-1].name = "batch_variance"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x_backprop"
    x_backprop = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x_backprop


# This api is auto-generated from IR ReduceSum
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ReduceSum(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceSum)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceSum"
    op.name = next_unique_name(node_name, "ReduceSum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceSumD
@auto_convert_to_tensor([False], [False])
def ReduceSumD(x: Tensor, *, axes: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceSumD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_BF16}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceSumD"
    op.name = next_unique_name(node_name, "ReduceSumD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMeanWithCount
@auto_convert_to_tensor([False, False, False], [False, False, False])
def ReduceMeanWithCount(x: Tensor, count: Tensor, count_sum: Tensor, *, axes: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceMeanWithCount)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(count, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(count_sum, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMeanWithCount"
    op.name = next_unique_name(node_name, "ReduceMeanWithCount")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(count.tensor)
    op.input_desc.add().CopyFrom(count.desc)
    op.input_desc[-1].name = "count"
    op.input.append(count_sum.tensor)
    op.input_desc.add().CopyFrom(count_sum.desc)
    op.input_desc[-1].name = "count_sum"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceAllD
@auto_convert_to_tensor([False], [False])
def ReduceAllD(x: Tensor, *, axes: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceAllD)\n
.INPUT(x, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceAllD"
    op.name = next_unique_name(node_name, "ReduceAllD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceAll
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ReduceAll(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceAll)\n
.INPUT(x, TensorType({DT_BOOL}))\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceAll"
    op.name = next_unique_name(node_name, "ReduceAll")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceProd
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ReduceProd(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceProd)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceProd"
    op.name = next_unique_name(node_name, "ReduceProd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceProdD
@auto_convert_to_tensor([False], [False])
def ReduceProdD(x: Tensor, *, axes: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceProdD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_UINT8, DT_INT8, DT_INT32, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_UINT8, DT_INT8, DT_INT32, DT_FLOAT16}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceProdD"
    op.name = next_unique_name(node_name, "ReduceProdD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMean
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ReduceMean(x: Tensor, axes: Tensor, *, keep_dims: bool=False, noop_with_empty_axes: bool=True, dependencies=[], node_name=None):
    """REG_OP(ReduceMean)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
.ATTR(noop_with_empty_axes, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMean"
    op.name = next_unique_name(node_name, "ReduceMean")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims
    op.attr["noop_with_empty_axes"].b = noop_with_empty_axes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMeanD
@auto_convert_to_tensor([False], [False])
def ReduceMeanD(x: Tensor, *, axes: List[int], keep_dims: bool=False, noop_with_empty_axes: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceMeanD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
.ATTR(noop_with_empty_axes, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMeanD"
    op.name = next_unique_name(node_name, "ReduceMeanD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims
    op.attr["noop_with_empty_axes"].b = noop_with_empty_axes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMax
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ReduceMax(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceMax)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMax"
    op.name = next_unique_name(node_name, "ReduceMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMaxD
@auto_convert_to_tensor([False], [False])
def ReduceMaxD(x: Tensor, *, axes: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceMaxD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_UINT8, DT_INT8, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_UINT8, DT_INT8, DT_FLOAT16, DT_INT32}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMaxD"
    op.name = next_unique_name(node_name, "ReduceMaxD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMin
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ReduceMin(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceMin)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMin"
    op.name = next_unique_name(node_name, "ReduceMin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMinD
@auto_convert_to_tensor([False], [False])
def ReduceMinD(x: Tensor, *, axes: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceMinD)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT8,DT_UINT8,DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT,DT_INT8,DT_UINT8,DT_INT32}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMinD"
    op.name = next_unique_name(node_name, "ReduceMinD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceAny
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ReduceAny(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceAny)\n
.INPUT(x, TensorType({DT_BOOL}))\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceAny"
    op.name = next_unique_name(node_name, "ReduceAny")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceAnyD
@auto_convert_to_tensor([False], [False])
def ReduceAnyD(x: Tensor, *, axes: List[int], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceAnyD)\n
.INPUT(x, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
.REQUIRED_ATTR(axes, ListInt)\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceAnyD"
    op.name = next_unique_name(node_name, "ReduceAnyD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Reduction
@auto_convert_to_tensor([False], [False])
def Reduction(x: Tensor, *, operation: int=1, axis: int=0, coeff: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(Reduction)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(operation, Int, 1)\n
.ATTR(axis, Int, 0)\n
.ATTR(coeff, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Reduction"
    op.name = next_unique_name(node_name, "Reduction")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["operation"].i = operation
    op.attr["axis"].i = axis
    op.attr["coeff"].f = coeff

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EuclideanNorm
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def EuclideanNorm(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(EuclideanNorm)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EuclideanNorm"
    op.name = next_unique_name(node_name, "EuclideanNorm")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EuclideanNormD
@auto_convert_to_tensor([False], [False])
def EuclideanNormD(x: Tensor, *, axes: List[int]=[], keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(EuclideanNormD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32, DT_FLOAT16}))\n
.ATTR(axes, ListInt, {})\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EuclideanNormD"
    op.name = next_unique_name(node_name, "EuclideanNormD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR INInferV2
@auto_convert_to_tensor([False, False, False, False, False], [False, True, True, True, True])
def INInferV2(x: Tensor, gamma: Optional[Tensor], beta: Optional[Tensor], mean: Optional[Tensor], variance: Optional[Tensor], *, epsilon: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(INInferV2)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OPTIONAL_INPUT(gamma, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(beta, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.ATTR(epsilon, Float, 0.00001)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "INInferV2"
    op.name = next_unique_name(node_name, "INInferV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if gamma is not None:
        op.input.append(gamma.tensor)
        op.input_desc.add().CopyFrom(gamma.desc)
        op.input_desc[-1].name = "gamma"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "gamma"
    if beta is not None:
        op.input.append(beta.tensor)
        op.input_desc.add().CopyFrom(beta.desc)
        op.input_desc[-1].name = "beta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "beta"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance


# This api is auto-generated from IR INTrainingReduceV2
@auto_convert_to_tensor([False], [False])
def INTrainingReduceV2(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(INTrainingReduceV2)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(sum, TensorType({DT_FLOAT}))\n
.OUTPUT(square_sum, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "INTrainingReduceV2"
    op.name = next_unique_name(node_name, "INTrainingReduceV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sum"
    sum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "square_sum"
    square_sum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sum, square_sum


# This api is auto-generated from IR INTrainingUpdateV2
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, True, True, True, True])
def INTrainingUpdateV2(x: Tensor, sum: Tensor, square_sum: Tensor, gamma: Optional[Tensor], beta: Optional[Tensor], mean: Optional[Tensor], variance: Optional[Tensor], *, momentum: float=0.100000, epsilon: float=0.000010, dependencies=[], node_name=None):
    """REG_OP(INTrainingUpdateV2)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(sum, TensorType({DT_FLOAT}))\n
.INPUT(square_sum, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(gamma, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(beta, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.ATTR(momentum, Float, 0.1)\n
.ATTR(epsilon, Float, 0.00001)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "INTrainingUpdateV2"
    op.name = next_unique_name(node_name, "INTrainingUpdateV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(sum.tensor)
    op.input_desc.add().CopyFrom(sum.desc)
    op.input_desc[-1].name = "sum"
    op.input.append(square_sum.tensor)
    op.input_desc.add().CopyFrom(square_sum.desc)
    op.input_desc[-1].name = "square_sum"
    if gamma is not None:
        op.input.append(gamma.tensor)
        op.input_desc.add().CopyFrom(gamma.desc)
        op.input_desc[-1].name = "gamma"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "gamma"
    if beta is not None:
        op.input.append(beta.tensor)
        op.input_desc.add().CopyFrom(beta.desc)
        op.input_desc[-1].name = "beta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "beta"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["momentum"].f = momentum
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance


# This api is auto-generated from IR INTrainingReduceGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def INTrainingReduceGrad(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, res_gamma: Tensor, res_beta: Tensor, gamma: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(INTrainingReduceGrad)\n
.INPUT(dy, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT}))\n
.INPUT(res_gamma, TensorType({DT_FLOAT}))\n
.INPUT(res_beta, TensorType({DT_FLOAT}))\n
.INPUT(gamma, TensorType({DT_FLOAT}))\n
.OUTPUT(pd_x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "INTrainingReduceGrad"
    op.name = next_unique_name(node_name, "INTrainingReduceGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"
    op.input.append(res_gamma.tensor)
    op.input_desc.add().CopyFrom(res_gamma.desc)
    op.input_desc[-1].name = "res_gamma"
    op.input.append(res_beta.tensor)
    op.input_desc.add().CopyFrom(res_beta.desc)
    op.input_desc[-1].name = "res_beta"
    op.input.append(gamma.tensor)
    op.input_desc.add().CopyFrom(gamma.desc)
    op.input_desc[-1].name = "gamma"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_x"
    pd_x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_x


# This api is auto-generated from IR INTrainingUpdateGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def INTrainingUpdateGrad(dy: Tensor, x: Tensor, variance: Tensor, mean: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(INTrainingUpdateGrad)\n
.INPUT(dy, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(variance, TensorType({DT_FLOAT}))\n
.INPUT(mean, TensorType({DT_FLOAT}))\n
.OUTPUT(res_gamma, TensorType({DT_FLOAT}))\n
.OUTPUT(res_beta, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "INTrainingUpdateGrad"
    op.name = next_unique_name(node_name, "INTrainingUpdateGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(variance.tensor)
    op.input_desc.add().CopyFrom(variance.desc)
    op.input_desc[-1].name = "variance"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "res_gamma"
    res_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "res_beta"
    res_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return res_gamma, res_beta


# This api is auto-generated from IR INTrainingUpdateGradGammaBeta
@auto_convert_to_tensor([False, False], [False, False])
def INTrainingUpdateGradGammaBeta(res_gamma: Tensor, res_beta: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(INTrainingUpdateGradGammaBeta)\n
.INPUT(res_gamma, TensorType({DT_FLOAT}))\n
.INPUT(res_beta, TensorType({DT_FLOAT}))\n
.OUTPUT(pd_gamma, TensorType({DT_FLOAT}))\n
.OUTPUT(pd_beta, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "INTrainingUpdateGradGammaBeta"
    op.name = next_unique_name(node_name, "INTrainingUpdateGradGammaBeta")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(res_gamma.tensor)
    op.input_desc.add().CopyFrom(res_gamma.desc)
    op.input_desc[-1].name = "res_gamma"
    op.input.append(res_beta.tensor)
    op.input_desc.add().CopyFrom(res_beta.desc)
    op.input_desc[-1].name = "res_beta"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "pd_gamma"
    pd_gamma = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pd_beta"
    pd_beta = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return pd_gamma, pd_beta


# This api is auto-generated from IR GNTrainingReduce
@auto_convert_to_tensor([False], [False])
def GNTrainingReduce(x: Tensor, *, num_groups: int=2, dependencies=[], node_name=None):
    """REG_OP(GNTrainingReduce)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(sum, TensorType({DT_FLOAT}))\n
.OUTPUT(square_sum, TensorType({DT_FLOAT}))\n
.ATTR(num_groups, Int, 2)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GNTrainingReduce"
    op.name = next_unique_name(node_name, "GNTrainingReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["num_groups"].i = num_groups

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sum"
    sum = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "square_sum"
    square_sum = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sum, square_sum


# This api is auto-generated from IR GNTrainingUpdate
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, True, True, True, True])
def GNTrainingUpdate(x: Tensor, sum: Tensor, square_sum: Tensor, scale: Optional[Tensor], offset: Optional[Tensor], mean: Optional[Tensor], variance: Optional[Tensor], *, num_groups: int=2, epsilon: float=0.000100, dependencies=[], node_name=None):
    """REG_OP(GNTrainingUpdate)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.INPUT(sum, TensorType({DT_FLOAT}))\n
.INPUT(square_sum, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(scale, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(offset, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(mean, TensorType({DT_FLOAT}))\n
.OPTIONAL_INPUT(variance, TensorType({DT_FLOAT}))\n
.ATTR(num_groups, Int, 2)\n
.ATTR(epsilon, Float, 0.0001)\n
.OUTPUT(y, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(batch_mean, TensorType({DT_FLOAT}))\n
.OUTPUT(batch_variance, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GNTrainingUpdate"
    op.name = next_unique_name(node_name, "GNTrainingUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(sum.tensor)
    op.input_desc.add().CopyFrom(sum.desc)
    op.input_desc[-1].name = "sum"
    op.input.append(square_sum.tensor)
    op.input_desc.add().CopyFrom(square_sum.desc)
    op.input_desc[-1].name = "square_sum"
    if scale is not None:
        op.input.append(scale.tensor)
        op.input_desc.add().CopyFrom(scale.desc)
        op.input_desc[-1].name = "scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"
    if mean is not None:
        op.input.append(mean.tensor)
        op.input_desc.add().CopyFrom(mean.desc)
        op.input_desc[-1].name = "mean"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mean"
    if variance is not None:
        op.input.append(variance.tensor)
        op.input_desc.add().CopyFrom(variance.desc)
        op.input_desc[-1].name = "variance"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "variance"

    # process attrs
    op.attr["num_groups"].i = num_groups
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_mean"
    batch_mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "batch_variance"
    batch_variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, batch_mean, batch_variance


# This api is auto-generated from IR ReduceJoin
@auto_convert_to_tensor([False, False], [False, False])
def ReduceJoin(input: Tensor, reduction_indices: Tensor, *, keep_dims: bool=True, separator: str="", dependencies=[], node_name=None):
    """REG_OP(ReduceJoin)\n
.INPUT(input, TensorType({DT_STRING}))\n
.INPUT(reduction_indices, TensorType({DT_INT32}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.ATTR(keep_dims, Bool, true)\n
.ATTR(separator, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceJoin"
    op.name = next_unique_name(node_name, "ReduceJoin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(reduction_indices.tensor)
    op.input_desc.add().CopyFrom(reduction_indices.desc)
    op.input_desc[-1].name = "reduction_indices"

    # process attrs
    op.attr["keep_dims"].b = keep_dims
    op.attr["separator"].s = compat_as_bytes(separator)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR ReduceStd
@auto_convert_to_tensor([False], [False])
def ReduceStd(x: Tensor, *, dim: List[int]=[], unbiased: bool=True, keepdim: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceStd)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y1, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y2, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(dim, ListInt, {})\n
.ATTR(unbiased, Bool, true)\n
.ATTR(keepdim, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceStd"
    op.name = next_unique_name(node_name, "ReduceStd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dim"].list.val_type = 2
    op.attr["dim"].list.i.extend(dim)
    op.attr["unbiased"].b = unbiased
    op.attr["keepdim"].b = keepdim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y1"
    y1 = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y2"
    y2 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y1, y2


# This api is auto-generated from IR ReduceStdWithMean
@auto_convert_to_tensor([False, False], [False, False])
def ReduceStdWithMean(x: Tensor, mean: Tensor, *, dim: List[int]=[], unbiased: bool=True, keepdim: bool=False, invert: bool=False, epsilon: float=0.001000, correction: int=1, dependencies=[], node_name=None):
    """REG_OP(ReduceStdWithMean)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.INPUT(mean, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
.ATTR(dim, ListInt, {})\n
.ATTR(unbiased, Bool, true)\n
.ATTR(keepdim, Bool, false)\n
.ATTR(invert, Bool, false)\n
.ATTR(epsilon, Float, 0.001)\n
.ATTR(correction, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceStdWithMean"
    op.name = next_unique_name(node_name, "ReduceStdWithMean")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"

    # process attrs
    op.attr["dim"].list.val_type = 2
    op.attr["dim"].list.i.extend(dim)
    op.attr["unbiased"].b = unbiased
    op.attr["keepdim"].b = keepdim
    op.attr["invert"].b = invert
    op.attr["epsilon"].f = epsilon
    op.attr["correction"].i = correction

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceMeanVariance
@auto_convert_to_tensor([False], [False])
def ReduceMeanVariance(x: Tensor, *, axes: List[int]=[], keep_dims: bool=True, dependencies=[], node_name=None):
    """REG_OP(ReduceMeanVariance)\n
.INPUT(x, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(mean, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.OUTPUT(variance, TensorType({DT_FLOAT16,DT_FLOAT}))\n
.ATTR(axes, ListInt, {})\n
.ATTR(keep_dims, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceMeanVariance"
    op.name = next_unique_name(node_name, "ReduceMeanVariance")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axes"].list.val_type = 2
    op.attr["axes"].list.i.extend(axes)
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "variance"
    variance = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return mean, variance


# This api is auto-generated from IR ReduceStdV2Update
@auto_convert_to_tensor([False, False], [False, False])
def ReduceStdV2Update(x: Tensor, mean: Tensor, *, dim: List[int], if_std: bool=False, unbiased: bool=True, keepdim: bool=False, correction: int=1, dependencies=[], node_name=None):
    """REG_OP(ReduceStdV2Update)\n
.INPUT(x, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.INPUT(mean, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.OUTPUT(output_var, TensorType({ DT_FLOAT, DT_FLOAT16 }))\n
.REQUIRED_ATTR(dim, ListInt)\n
.ATTR(if_std, Bool, false)\n
.ATTR(unbiased, Bool, true)\n
.ATTR(keepdim, Bool, false)\n
.ATTR(correction, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceStdV2Update"
    op.name = next_unique_name(node_name, "ReduceStdV2Update")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mean.tensor)
    op.input_desc.add().CopyFrom(mean.desc)
    op.input_desc[-1].name = "mean"

    # process attrs
    op.attr["dim"].list.val_type = 2
    op.attr["dim"].list.i.extend(dim)
    op.attr["if_std"].b = if_std
    op.attr["unbiased"].b = unbiased
    op.attr["keepdim"].b = keepdim
    op.attr["correction"].i = correction

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_var"
    output_var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_var


# This api is auto-generated from IR ReduceLogSumExp
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ReduceLogSumExp(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceLogSumExp)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceLogSumExp"
    op.name = next_unique_name(node_name, "ReduceLogSumExp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReduceLogSum
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def ReduceLogSum(x: Tensor, axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(ReduceLogSum)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axes, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReduceLogSum"
    op.name = next_unique_name(node_name, "ReduceLogSum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axes.tensor)
    op.input_desc.add().CopyFrom(axes.desc)
    op.input_desc[-1].name = "axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR VarHandleOp
@auto_convert_to_tensor([], [])
def VarHandleOp(*, dtype: int, container: str="", shared_name: str="", shape: List[int]=[-1], dependencies=[], node_name=None):
    """REG_OP(VarHandleOp)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(shape, ListInt, ge::UNKNOWN_SHAPE)\n
.OUTPUT(y, TensorType({DT_RESOURCE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "VarHandleOp"
    op.name = next_unique_name(node_name, "VarHandleOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AssignVariableOp
@auto_convert_to_tensor([False, False], [False, False])
def AssignVariableOp(resource: Tensor, value: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(AssignVariableOp)\n
.INPUT(resource, TensorType({DT_RESOURCE}))\n
.INPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AssignVariableOp"
    op.name = next_unique_name(node_name, "AssignVariableOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(resource.tensor)
    op.input_desc.add().CopyFrom(resource.desc)
    op.input_desc[-1].name = "resource"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR AssignAddVariableOp
@auto_convert_to_tensor([False, False], [False, False])
def AssignAddVariableOp(resource: Tensor, value: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(AssignAddVariableOp)\n
.INPUT(resource, TensorType({DT_RESOURCE}))\n
.INPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AssignAddVariableOp"
    op.name = next_unique_name(node_name, "AssignAddVariableOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(resource.tensor)
    op.input_desc.add().CopyFrom(resource.desc)
    op.input_desc[-1].name = "resource"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR AssignSubVariableOp
@auto_convert_to_tensor([False, False], [False, False])
def AssignSubVariableOp(resource: Tensor, value: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(AssignSubVariableOp)\n
.INPUT(resource, TensorType({DT_RESOURCE}))\n
.INPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AssignSubVariableOp"
    op.name = next_unique_name(node_name, "AssignSubVariableOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(resource.tensor)
    op.input_desc.add().CopyFrom(resource.desc)
    op.input_desc[-1].name = "resource"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR BasicLSTMCell
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, True])
def BasicLSTMCell(x: Tensor, h: Tensor, c: Tensor, w: Tensor, b: Tensor, mask: Optional[Tensor], *, keep_prob: float=1.000000, forget_bias: float=1.000000, state_is_tuple: bool=True, activation: str="tanh", dependencies=[], node_name=None):
    """REG_OP(BasicLSTMCell)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(h, TensorType({DT_FLOAT16}))\n
.INPUT(c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w, TensorType({DT_FLOAT16}))\n
.INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(ct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(ht, TensorType({DT_FLOAT16}))\n
.OUTPUT(it, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(jt, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(ft, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(ot, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(tanhct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(forget_bias, Float, 1.0)\n
.ATTR(state_is_tuple, Bool, true)\n
.ATTR(activation, String, "tanh")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BasicLSTMCell"
    op.name = next_unique_name(node_name, "BasicLSTMCell")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob
    op.attr["forget_bias"].f = forget_bias
    op.attr["state_is_tuple"].b = state_is_tuple
    op.attr["activation"].s = compat_as_bytes(activation)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "ct"
    ct = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ht"
    ht = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "it"
    it = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "jt"
    jt = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ft"
    ft = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ot"
    ot = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tanhct"
    tanhct = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return ct, ht, it, jt, ft, ot, tanhct


# This api is auto-generated from IR DynamicLSTM
@auto_convert_to_tensor([False, False, False], [False, False, False])
def DynamicLSTM(x: Tensor, w: Tensor, b: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DynamicLSTM)\n
.INPUT(x, TensorType({DT_FLOAT32}))\n
.INPUT(w, TensorType({DT_FLOAT32}))\n
.INPUT(b, TensorType({DT_FLOAT32}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicLSTM"
    op.name = next_unique_name(node_name, "DynamicLSTM")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_h


# This api is auto-generated from IR DynamicRNNGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True])
def _DynamicRNNGrad(x: Tensor, w: Tensor, b: Tensor, y: Tensor, init_h: Tensor, init_c: Tensor, h: Tensor, c: Tensor, dy: Tensor, dh: Tensor, dc: Tensor, i: Tensor, j: Tensor, f: Tensor, o: Tensor, tanhct: Optional[Tensor], seq_length: Optional[Tensor], mask: Optional[Tensor], wci: Optional[Tensor], wcf: Optional[Tensor], wco: Optional[Tensor], *, size_of_dwci: int, size_of_dwcf: int, size_of_dwco: int, cell_type: str="LSTM", direction: str="UNIDIRECTIONAL", cell_depth: int=0, use_peephole: bool=False, keep_prob: float=-1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, forget_bias: float=0.000000, gate_order: str="ijfo", dependencies=[], node_name=None):
    """REG_OP(DynamicRNNGrad)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dc, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(j, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(f, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(tanhct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OPTIONAL_INPUT(wci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dw, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(db, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dc_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_OUTPUT(dwci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_OUTPUT(dwcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_OUTPUT(dwco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(cell_type, String, "LSTM")\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 0)\n
.ATTR(use_peephole, Bool, false)\n
.ATTR(keep_prob, Float, -1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(forget_bias, Float, 0.0)\n
.ATTR(gate_order, String, "ijfo")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicRNNGrad"
    op.name = next_unique_name(node_name, "DynamicRNNGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(init_h.tensor)
    op.input_desc.add().CopyFrom(init_h.desc)
    op.input_desc[-1].name = "init_h"
    op.input.append(init_c.tensor)
    op.input_desc.add().CopyFrom(init_c.desc)
    op.input_desc[-1].name = "init_c"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(dc.tensor)
    op.input_desc.add().CopyFrom(dc.desc)
    op.input_desc[-1].name = "dc"
    op.input.append(i.tensor)
    op.input_desc.add().CopyFrom(i.desc)
    op.input_desc[-1].name = "i"
    op.input.append(j.tensor)
    op.input_desc.add().CopyFrom(j.desc)
    op.input_desc[-1].name = "j"
    op.input.append(f.tensor)
    op.input_desc.add().CopyFrom(f.desc)
    op.input_desc[-1].name = "f"
    op.input.append(o.tensor)
    op.input_desc.add().CopyFrom(o.desc)
    op.input_desc[-1].name = "o"
    if tanhct is not None:
        op.input.append(tanhct.tensor)
        op.input_desc.add().CopyFrom(tanhct.desc)
        op.input_desc[-1].name = "tanhct"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "tanhct"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"
    if wci is not None:
        op.input.append(wci.tensor)
        op.input_desc.add().CopyFrom(wci.desc)
        op.input_desc[-1].name = "wci"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wci"
    if wcf is not None:
        op.input.append(wcf.tensor)
        op.input_desc.add().CopyFrom(wcf.desc)
        op.input_desc[-1].name = "wcf"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wcf"
    if wco is not None:
        op.input.append(wco.tensor)
        op.input_desc.add().CopyFrom(wco.desc)
        op.input_desc[-1].name = "wco"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wco"

    # process attrs
    op.attr["cell_type"].s = compat_as_bytes(cell_type)
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["use_peephole"].b = use_peephole
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["forget_bias"].f = forget_bias
    op.attr["gate_order"].s = compat_as_bytes(gate_order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dw"
    dw = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "db"
    db = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dc_prev"
    dc_prev = Tensor(op, output_index)
    output_index += 1
    dwci = []
    for i in range(output_index, output_index + size_of_dwci):
        op.output_desc.add().name = "dwci" + str(i - output_index)
        dwci.append(Tensor(op, i))
    output_index += size_of_dwci
    dwcf = []
    for i in range(output_index, output_index + size_of_dwcf):
        op.output_desc.add().name = "dwcf" + str(i - output_index)
        dwcf.append(Tensor(op, i))
    output_index += size_of_dwcf
    dwco = []
    for i in range(output_index, output_index + size_of_dwco):
        op.output_desc.add().name = "dwco" + str(i - output_index)
        dwco.append(Tensor(op, i))
    output_index += size_of_dwco

    # return outputs
    return dw, db, dx, dh_prev, dc_prev, dwci, dwcf, dwco


# This api is auto-generated from IR DynamicRNN
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True, True, True])
def DynamicRNN(x: Tensor, w: Tensor, b: Tensor, seq_length: Optional[Tensor], init_h: Optional[Tensor], init_c: Optional[Tensor], wci: Optional[Tensor], wcf: Optional[Tensor], wco: Optional[Tensor], mask: Optional[Tensor], *, cell_type: str="LSTM", direction: str="UNIDIRECTIONAL", cell_depth: int=1, use_peephole: bool=False, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", forget_bias: float=0.000000, gate_order: str="ijfo", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicRNN)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(j, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(f, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(tanhc, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(cell_type, String, "LSTM")\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(use_peephole, Bool, false)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(forget_bias, Float, 0.0)\n
.ATTR(gate_order, String, "ijfo")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicRNN"
    op.name = next_unique_name(node_name, "DynamicRNN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"
    if init_c is not None:
        op.input.append(init_c.tensor)
        op.input_desc.add().CopyFrom(init_c.desc)
        op.input_desc[-1].name = "init_c"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_c"
    if wci is not None:
        op.input.append(wci.tensor)
        op.input_desc.add().CopyFrom(wci.desc)
        op.input_desc[-1].name = "wci"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wci"
    if wcf is not None:
        op.input.append(wcf.tensor)
        op.input_desc.add().CopyFrom(wcf.desc)
        op.input_desc[-1].name = "wcf"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wcf"
    if wco is not None:
        op.input.append(wco.tensor)
        op.input_desc.add().CopyFrom(wco.desc)
        op.input_desc[-1].name = "wco"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wco"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["cell_type"].s = compat_as_bytes(cell_type)
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["use_peephole"].b = use_peephole
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["forget_bias"].f = forget_bias
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_c"
    output_c = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "i"
    i = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "j"
    j = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "f"
    f = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "o"
    o = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tanhc"
    tanhc = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, output_c, i, j, f, o, tanhc


# This api is auto-generated from IR DynamicRNNV2
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True, True, True, True])
def DynamicRNNV2(x: Tensor, weight_input: Tensor, weight_hidden: Tensor, b: Optional[Tensor], seq_length: Optional[Tensor], init_h: Optional[Tensor], init_c: Optional[Tensor], wci: Optional[Tensor], wcf: Optional[Tensor], wco: Optional[Tensor], mask: Optional[Tensor], *, cell_type: str="LSTM", direction: str="UNIDIRECTIONAL", cell_depth: int=1, use_peephole: bool=False, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", recurrent_activation: str="sigmoid", forget_bias: float=0.000000, gate_order: str="ijfo", stateful: bool=False, merge_mode: str="concat", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicRNNV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weight_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weight_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(j, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(f, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(tanhc, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(cell_type, String, "LSTM")\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(use_peephole, Bool, false)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(recurrent_activation, String, "sigmoid")\n
.ATTR(forget_bias, Float, 0.0)\n
.ATTR(gate_order, String, "ijfo")\n
.ATTR(stateful, Bool, false)\n
.ATTR(merge_mode, String, "concat")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicRNNV2"
    op.name = next_unique_name(node_name, "DynamicRNNV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight_input.tensor)
    op.input_desc.add().CopyFrom(weight_input.desc)
    op.input_desc[-1].name = "weight_input"
    op.input.append(weight_hidden.tensor)
    op.input_desc.add().CopyFrom(weight_hidden.desc)
    op.input_desc[-1].name = "weight_hidden"
    if b is not None:
        op.input.append(b.tensor)
        op.input_desc.add().CopyFrom(b.desc)
        op.input_desc[-1].name = "b"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "b"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"
    if init_c is not None:
        op.input.append(init_c.tensor)
        op.input_desc.add().CopyFrom(init_c.desc)
        op.input_desc[-1].name = "init_c"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_c"
    if wci is not None:
        op.input.append(wci.tensor)
        op.input_desc.add().CopyFrom(wci.desc)
        op.input_desc[-1].name = "wci"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wci"
    if wcf is not None:
        op.input.append(wcf.tensor)
        op.input_desc.add().CopyFrom(wcf.desc)
        op.input_desc[-1].name = "wcf"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wcf"
    if wco is not None:
        op.input.append(wco.tensor)
        op.input_desc.add().CopyFrom(wco.desc)
        op.input_desc[-1].name = "wco"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wco"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["cell_type"].s = compat_as_bytes(cell_type)
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["use_peephole"].b = use_peephole
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["recurrent_activation"].s = compat_as_bytes(recurrent_activation)
    op.attr["forget_bias"].f = forget_bias
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["stateful"].b = stateful
    op.attr["merge_mode"].s = compat_as_bytes(merge_mode)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_c"
    output_c = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "i"
    i = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "j"
    j = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "f"
    f = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "o"
    o = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tanhc"
    tanhc = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, output_c, i, j, f, o, tanhc


# This api is auto-generated from IR DynamicRNNV2Grad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True])
def _DynamicRNNV2Grad(x: Tensor, w_x: Tensor, w_h: Tensor, y: Tensor, init_h: Tensor, init_c: Tensor, h: Tensor, c: Tensor, dy: Tensor, dh: Tensor, dc: Tensor, i: Tensor, j: Tensor, f: Tensor, o: Tensor, tanhct: Tensor, seq_length: Optional[Tensor], wci: Optional[Tensor], wcf: Optional[Tensor], wco: Optional[Tensor], mask: Optional[Tensor], *, size_of_dwci: int, size_of_dwcf: int, size_of_dwco: int, cell_type: str="LSTM", direction: str="UNIDIRECTIONAL", cell_depth: int=1, use_peephole: bool=False, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", recurrent_activation: str="sigmoid", gate_order: str="ijfo", stateful: bool=False, merge_mode: str="concat", dependencies=[], node_name=None):
    """REG_OP(DynamicRNNV2Grad)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dc, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(j, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(f, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(tanhct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(wci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(dw_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dw_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(db, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dc_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_OUTPUT(dwci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_OUTPUT(dwcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_OUTPUT(dwco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(cell_type, String, "LSTM")\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(use_peephole, Bool, false)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(recurrent_activation, String, "sigmoid")\n
.ATTR(gate_order, String, "ijfo")\n
.ATTR(stateful, Bool, false)\n
.ATTR(merge_mode, String, "concat")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicRNNV2Grad"
    op.name = next_unique_name(node_name, "DynamicRNNV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w_x.tensor)
    op.input_desc.add().CopyFrom(w_x.desc)
    op.input_desc[-1].name = "w_x"
    op.input.append(w_h.tensor)
    op.input_desc.add().CopyFrom(w_h.desc)
    op.input_desc[-1].name = "w_h"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(init_h.tensor)
    op.input_desc.add().CopyFrom(init_h.desc)
    op.input_desc[-1].name = "init_h"
    op.input.append(init_c.tensor)
    op.input_desc.add().CopyFrom(init_c.desc)
    op.input_desc[-1].name = "init_c"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(dc.tensor)
    op.input_desc.add().CopyFrom(dc.desc)
    op.input_desc[-1].name = "dc"
    op.input.append(i.tensor)
    op.input_desc.add().CopyFrom(i.desc)
    op.input_desc[-1].name = "i"
    op.input.append(j.tensor)
    op.input_desc.add().CopyFrom(j.desc)
    op.input_desc[-1].name = "j"
    op.input.append(f.tensor)
    op.input_desc.add().CopyFrom(f.desc)
    op.input_desc[-1].name = "f"
    op.input.append(o.tensor)
    op.input_desc.add().CopyFrom(o.desc)
    op.input_desc[-1].name = "o"
    op.input.append(tanhct.tensor)
    op.input_desc.add().CopyFrom(tanhct.desc)
    op.input_desc[-1].name = "tanhct"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if wci is not None:
        op.input.append(wci.tensor)
        op.input_desc.add().CopyFrom(wci.desc)
        op.input_desc[-1].name = "wci"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wci"
    if wcf is not None:
        op.input.append(wcf.tensor)
        op.input_desc.add().CopyFrom(wcf.desc)
        op.input_desc[-1].name = "wcf"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wcf"
    if wco is not None:
        op.input.append(wco.tensor)
        op.input_desc.add().CopyFrom(wco.desc)
        op.input_desc[-1].name = "wco"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wco"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["cell_type"].s = compat_as_bytes(cell_type)
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["use_peephole"].b = use_peephole
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["recurrent_activation"].s = compat_as_bytes(recurrent_activation)
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["stateful"].b = stateful
    op.attr["merge_mode"].s = compat_as_bytes(merge_mode)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dw_x"
    dw_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dw_h"
    dw_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "db"
    db = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dc_prev"
    dc_prev = Tensor(op, output_index)
    output_index += 1
    dwci = []
    for i in range(output_index, output_index + size_of_dwci):
        op.output_desc.add().name = "dwci" + str(i - output_index)
        dwci.append(Tensor(op, i))
    output_index += size_of_dwci
    dwcf = []
    for i in range(output_index, output_index + size_of_dwcf):
        op.output_desc.add().name = "dwcf" + str(i - output_index)
        dwcf.append(Tensor(op, i))
    output_index += size_of_dwcf
    dwco = []
    for i in range(output_index, output_index + size_of_dwco):
        op.output_desc.add().name = "dwco" + str(i - output_index)
        dwco.append(Tensor(op, i))
    output_index += size_of_dwco

    # return outputs
    return dw_x, dw_h, db, dx, dh_prev, dc_prev, dwci, dwcf, dwco


# This api is auto-generated from IR DynamicRNNV3
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True, True, True, True, True])
def DynamicRNNV3(x: Tensor, w: Tensor, b: Tensor, seq_length: Optional[Tensor], init_h: Optional[Tensor], init_c: Optional[Tensor], wci: Optional[Tensor], wcf: Optional[Tensor], wco: Optional[Tensor], mask: Optional[Tensor], real_mask: Optional[Tensor], project: Optional[Tensor], *, cell_type: str="LSTM", direction: str="UNIDIRECTIONAL", cell_depth: int=1, use_peephole: bool=False, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", forget_bias: float=0.000000, is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicRNNV3)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OPTIONAL_INPUT(real_mask, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(project, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(j, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(f, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(tanhc, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(cell_type, String, "LSTM")\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(use_peephole, Bool, false)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(forget_bias, Float, 0.0)\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicRNNV3"
    op.name = next_unique_name(node_name, "DynamicRNNV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"
    if init_c is not None:
        op.input.append(init_c.tensor)
        op.input_desc.add().CopyFrom(init_c.desc)
        op.input_desc[-1].name = "init_c"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_c"
    if wci is not None:
        op.input.append(wci.tensor)
        op.input_desc.add().CopyFrom(wci.desc)
        op.input_desc[-1].name = "wci"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wci"
    if wcf is not None:
        op.input.append(wcf.tensor)
        op.input_desc.add().CopyFrom(wcf.desc)
        op.input_desc[-1].name = "wcf"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wcf"
    if wco is not None:
        op.input.append(wco.tensor)
        op.input_desc.add().CopyFrom(wco.desc)
        op.input_desc[-1].name = "wco"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wco"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"
    if real_mask is not None:
        op.input.append(real_mask.tensor)
        op.input_desc.add().CopyFrom(real_mask.desc)
        op.input_desc[-1].name = "real_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "real_mask"
    if project is not None:
        op.input.append(project.tensor)
        op.input_desc.add().CopyFrom(project.desc)
        op.input_desc[-1].name = "project"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "project"

    # process attrs
    op.attr["cell_type"].s = compat_as_bytes(cell_type)
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["use_peephole"].b = use_peephole
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["forget_bias"].f = forget_bias
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_c"
    output_c = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "i"
    i = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "j"
    j = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "f"
    f = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "o"
    o = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "tanhc"
    tanhc = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, output_c, i, j, f, o, tanhc


# This api is auto-generated from IR DynamicLSTMV2
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, True, True, True, True, True, True, True])
def DynamicLSTMV2(x: Tensor, w: Tensor, b: Tensor, cont: Tensor, w_xc_x_static: Optional[Tensor], h0: Optional[Tensor], c0: Optional[Tensor], wci: Optional[Tensor], wcf: Optional[Tensor], wco: Optional[Tensor], mask: Optional[Tensor], *, num_output: int=0, expose_hidden: bool=False, need_output_last: bool=False, forget_bias: float=0.000000, dependencies=[], node_name=None):
    """REG_OP(DynamicLSTMV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(cont, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(w_xc_x_static, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(h0, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(c0, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wci, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wcf, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(wco, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(last_output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(last_output_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(num_output, Int, 0)\n
.ATTR(expose_hidden, Bool, false)\n
.ATTR(need_output_last, Bool, false)\n
.ATTR(forget_bias, Float, 0.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicLSTMV2"
    op.name = next_unique_name(node_name, "DynamicLSTMV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    op.input.append(cont.tensor)
    op.input_desc.add().CopyFrom(cont.desc)
    op.input_desc[-1].name = "cont"
    if w_xc_x_static is not None:
        op.input.append(w_xc_x_static.tensor)
        op.input_desc.add().CopyFrom(w_xc_x_static.desc)
        op.input_desc[-1].name = "w_xc_x_static"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "w_xc_x_static"
    if h0 is not None:
        op.input.append(h0.tensor)
        op.input_desc.add().CopyFrom(h0.desc)
        op.input_desc[-1].name = "h0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "h0"
    if c0 is not None:
        op.input.append(c0.tensor)
        op.input_desc.add().CopyFrom(c0.desc)
        op.input_desc[-1].name = "c0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "c0"
    if wci is not None:
        op.input.append(wci.tensor)
        op.input_desc.add().CopyFrom(wci.desc)
        op.input_desc[-1].name = "wci"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wci"
    if wcf is not None:
        op.input.append(wcf.tensor)
        op.input_desc.add().CopyFrom(wcf.desc)
        op.input_desc[-1].name = "wcf"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wcf"
    if wco is not None:
        op.input.append(wco.tensor)
        op.input_desc.add().CopyFrom(wco.desc)
        op.input_desc[-1].name = "wco"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "wco"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["num_output"].i = num_output
    op.attr["expose_hidden"].b = expose_hidden
    op.attr["need_output_last"].b = need_output_last
    op.attr["forget_bias"].f = forget_bias

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_c"
    output_c = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "last_output_h"
    last_output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "last_output_c"
    last_output_c = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, output_c, last_output_h, last_output_c


# This api is auto-generated from IR LSTMInputGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, True])
def LSTMInputGrad(w: Tensor, init_c: Tensor, c: Tensor, dy: Tensor, dh: Tensor, dc: Tensor, i: Tensor, j: Tensor, f: Tensor, o: Tensor, tanhct: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(LSTMInputGrad)\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dc, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(j, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(f, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(tanhct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dc_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgate, TensorType({DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LSTMInputGrad"
    op.name = next_unique_name(node_name, "LSTMInputGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(init_c.tensor)
    op.input_desc.add().CopyFrom(init_c.desc)
    op.input_desc[-1].name = "init_c"
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(dc.tensor)
    op.input_desc.add().CopyFrom(dc.desc)
    op.input_desc[-1].name = "dc"
    op.input.append(i.tensor)
    op.input_desc.add().CopyFrom(i.desc)
    op.input_desc[-1].name = "i"
    op.input.append(j.tensor)
    op.input_desc.add().CopyFrom(j.desc)
    op.input_desc[-1].name = "j"
    op.input.append(f.tensor)
    op.input_desc.add().CopyFrom(f.desc)
    op.input_desc[-1].name = "f"
    op.input.append(o.tensor)
    op.input_desc.add().CopyFrom(o.desc)
    op.input_desc[-1].name = "o"
    if tanhct is not None:
        op.input.append(tanhct.tensor)
        op.input_desc.add().CopyFrom(tanhct.desc)
        op.input_desc[-1].name = "tanhct"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "tanhct"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dc_prev"
    dc_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgate"
    dgate = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dx, dh_prev, dc_prev, dgate


# This api is auto-generated from IR DynamicLSTMGradCell
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False])
def DynamicLSTMGradCell(init_c: Tensor, c: Tensor, dy: Tensor, dh: Tensor, dc: Tensor, i: Tensor, j: Tensor, f: Tensor, o: Tensor, tanhct: Tensor, t_state: Tensor, mask: Tensor, *, forget_bias: float=1.000000, activation: str="tanh", direction: str="UNIDIRECTIONAL", gate_order: str="ijfo", dependencies=[], node_name=None):
    """REG_OP(DynamicLSTMGradCell)\n
.INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dc, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(j, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(f, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(tanhct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(t_state, TensorType({DT_INT32, DT_INT32}))\n
.INPUT(mask, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgate, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dct_1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(forget_bias, Float, 1.0)\n
.ATTR(activation, String, "tanh")\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(gate_order, String, "ijfo")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicLSTMGradCell"
    op.name = next_unique_name(node_name, "DynamicLSTMGradCell")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(init_c.tensor)
    op.input_desc.add().CopyFrom(init_c.desc)
    op.input_desc[-1].name = "init_c"
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(dc.tensor)
    op.input_desc.add().CopyFrom(dc.desc)
    op.input_desc[-1].name = "dc"
    op.input.append(i.tensor)
    op.input_desc.add().CopyFrom(i.desc)
    op.input_desc[-1].name = "i"
    op.input.append(j.tensor)
    op.input_desc.add().CopyFrom(j.desc)
    op.input_desc[-1].name = "j"
    op.input.append(f.tensor)
    op.input_desc.add().CopyFrom(f.desc)
    op.input_desc[-1].name = "f"
    op.input.append(o.tensor)
    op.input_desc.add().CopyFrom(o.desc)
    op.input_desc[-1].name = "o"
    op.input.append(tanhct.tensor)
    op.input_desc.add().CopyFrom(tanhct.desc)
    op.input_desc[-1].name = "tanhct"
    op.input.append(t_state.tensor)
    op.input_desc.add().CopyFrom(t_state.desc)
    op.input_desc[-1].name = "t_state"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["forget_bias"].f = forget_bias
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["gate_order"].s = compat_as_bytes(gate_order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dgate"
    dgate = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dct_1"
    dct_1 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dgate, dct_1


# This api is auto-generated from IR BasicLSTMCellInputGrad
@auto_convert_to_tensor([False, False, False], [False, False, True])
def BasicLSTMCellInputGrad(dgate: Tensor, w: Tensor, dropout_mask: Optional[Tensor], *, keep_prob: float=1.000000, dependencies=[], node_name=None):
    """REG_OP(BasicLSTMCellInputGrad)\n
.INPUT(dgate, TensorType({DT_FLOAT16}))\n
.INPUT(w, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(dropout_mask, TensorType({DT_UINT8}))\n
.OUTPUT(dxt, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.OUTPUT(dht, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
.ATTR(keep_prob, Float, 1.0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BasicLSTMCellInputGrad"
    op.name = next_unique_name(node_name, "BasicLSTMCellInputGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dgate.tensor)
    op.input_desc.add().CopyFrom(dgate.desc)
    op.input_desc[-1].name = "dgate"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    if dropout_mask is not None:
        op.input.append(dropout_mask.tensor)
        op.input_desc.add().CopyFrom(dropout_mask.desc)
        op.input_desc[-1].name = "dropout_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dropout_mask"

    # process attrs
    op.attr["keep_prob"].f = keep_prob

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dxt"
    dxt = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dht"
    dht = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dxt, dht


# This api is auto-generated from IR BasicLSTMCellWeightGrad
@auto_convert_to_tensor([False, False, False], [False, False, False])
def BasicLSTMCellWeightGrad(x: Tensor, h: Tensor, dgate: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BasicLSTMCellWeightGrad)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(h, TensorType({DT_FLOAT16}))\n
.INPUT(dgate, TensorType({DT_FLOAT16}))\n
.OUTPUT(dw, TensorType({DT_FLOAT16}))\n
.OUTPUT(db, TensorType({DT_FLOAT16, DT_FLOAT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BasicLSTMCellWeightGrad"
    op.name = next_unique_name(node_name, "BasicLSTMCellWeightGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(dgate.tensor)
    op.input_desc.add().CopyFrom(dgate.desc)
    op.input_desc[-1].name = "dgate"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dw"
    dw = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "db"
    db = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dw, db


# This api is auto-generated from IR BasicLSTMCellCStateGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False])
def BasicLSTMCellCStateGrad(c: Tensor, dht: Tensor, dct: Tensor, it: Tensor, jt: Tensor, ft: Tensor, ot: Tensor, tanhct: Tensor, *, forget_bias: float=1.000000, activation: str="tanh", dependencies=[], node_name=None):
    """REG_OP(BasicLSTMCellCStateGrad)\n
.INPUT(c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dht, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(it, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(jt, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(ft, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(ot, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(tanhct, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgate, TensorType({DT_FLOAT16}))\n
.OUTPUT(dct_1, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(forget_bias, Float, 1.0)\n
.ATTR(activation, String, "tanh")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BasicLSTMCellCStateGrad"
    op.name = next_unique_name(node_name, "BasicLSTMCellCStateGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(c.tensor)
    op.input_desc.add().CopyFrom(c.desc)
    op.input_desc[-1].name = "c"
    op.input.append(dht.tensor)
    op.input_desc.add().CopyFrom(dht.desc)
    op.input_desc[-1].name = "dht"
    op.input.append(dct.tensor)
    op.input_desc.add().CopyFrom(dct.desc)
    op.input_desc[-1].name = "dct"
    op.input.append(it.tensor)
    op.input_desc.add().CopyFrom(it.desc)
    op.input_desc[-1].name = "it"
    op.input.append(jt.tensor)
    op.input_desc.add().CopyFrom(jt.desc)
    op.input_desc[-1].name = "jt"
    op.input.append(ft.tensor)
    op.input_desc.add().CopyFrom(ft.desc)
    op.input_desc[-1].name = "ft"
    op.input.append(ot.tensor)
    op.input_desc.add().CopyFrom(ot.desc)
    op.input_desc[-1].name = "ot"
    op.input.append(tanhct.tensor)
    op.input_desc.add().CopyFrom(tanhct.desc)
    op.input_desc[-1].name = "tanhct"

    # process attrs
    op.attr["forget_bias"].f = forget_bias
    op.attr["activation"].s = compat_as_bytes(activation)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dgate"
    dgate = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dct_1"
    dct_1 = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dgate, dct_1


# This api is auto-generated from IR RNN
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False], [False, False, True, True, False, False, True, False, False, False])
def RNN(x: Tensor, cont: Tensor, x_static: Optional[Tensor], h_0: Optional[Tensor], w_xh: Tensor, bias_h: Tensor, w_sh: Optional[Tensor], w_hh: Tensor, w_ho: Tensor, bias_o: Tensor, *, num_output: int=0, expose_hidden: bool=False, dependencies=[], node_name=None):
    """REG_OP(RNN)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(cont, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(x_static, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(h_0, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w_xh, TensorType({DT_FLOAT16}))\n
.INPUT(bias_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(w_sh, TensorType({DT_FLOAT16}))\n
.INPUT(w_hh, TensorType({DT_FLOAT16}))\n
.INPUT(w_ho, TensorType({DT_FLOAT16}))\n
.INPUT(bias_o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(h_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(num_output, Int, 0)\n
.ATTR(expose_hidden, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RNN"
    op.name = next_unique_name(node_name, "RNN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(cont.tensor)
    op.input_desc.add().CopyFrom(cont.desc)
    op.input_desc[-1].name = "cont"
    if x_static is not None:
        op.input.append(x_static.tensor)
        op.input_desc.add().CopyFrom(x_static.desc)
        op.input_desc[-1].name = "x_static"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x_static"
    if h_0 is not None:
        op.input.append(h_0.tensor)
        op.input_desc.add().CopyFrom(h_0.desc)
        op.input_desc[-1].name = "h_0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "h_0"
    op.input.append(w_xh.tensor)
    op.input_desc.add().CopyFrom(w_xh.desc)
    op.input_desc[-1].name = "w_xh"
    op.input.append(bias_h.tensor)
    op.input_desc.add().CopyFrom(bias_h.desc)
    op.input_desc[-1].name = "bias_h"
    if w_sh is not None:
        op.input.append(w_sh.tensor)
        op.input_desc.add().CopyFrom(w_sh.desc)
        op.input_desc[-1].name = "w_sh"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "w_sh"
    op.input.append(w_hh.tensor)
    op.input_desc.add().CopyFrom(w_hh.desc)
    op.input_desc[-1].name = "w_hh"
    op.input.append(w_ho.tensor)
    op.input_desc.add().CopyFrom(w_ho.desc)
    op.input_desc[-1].name = "w_ho"
    op.input.append(bias_o.tensor)
    op.input_desc.add().CopyFrom(bias_o.desc)
    op.input_desc[-1].name = "bias_o"

    # process attrs
    op.attr["num_output"].i = num_output
    op.attr["expose_hidden"].b = expose_hidden

    # process outputs
    output_index = 0
    op.output_desc.add().name = "o"
    o = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "h_t"
    h_t = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return o, h_t


# This api is auto-generated from IR BasicRNNCell
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, True, True, True, False, False, True, False, False])
def BasicRNNCell(x: Tensor, cont: Optional[Tensor], w_xh_x_static: Optional[Tensor], h_0: Optional[Tensor], w_xh: Tensor, bias_h: Tensor, w_hh: Optional[Tensor], w_ho: Tensor, bias_o: Tensor, *, expose_hidden: bool=False, num_output: int=0, dependencies=[], node_name=None):
    """REG_OP(BasicRNNCell)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(cont, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(w_xh_x_static, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(h_0, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w_xh, TensorType({DT_FLOAT16}))\n
.INPUT(bias_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(w_hh, TensorType({DT_FLOAT16}))\n
.INPUT(w_ho, TensorType({DT_FLOAT16}))\n
.INPUT(bias_o, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(o_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(h_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(expose_hidden, Bool, false)\n
.ATTR(num_output, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BasicRNNCell"
    op.name = next_unique_name(node_name, "BasicRNNCell")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if cont is not None:
        op.input.append(cont.tensor)
        op.input_desc.add().CopyFrom(cont.desc)
        op.input_desc[-1].name = "cont"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "cont"
    if w_xh_x_static is not None:
        op.input.append(w_xh_x_static.tensor)
        op.input_desc.add().CopyFrom(w_xh_x_static.desc)
        op.input_desc[-1].name = "w_xh_x_static"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "w_xh_x_static"
    if h_0 is not None:
        op.input.append(h_0.tensor)
        op.input_desc.add().CopyFrom(h_0.desc)
        op.input_desc[-1].name = "h_0"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "h_0"
    op.input.append(w_xh.tensor)
    op.input_desc.add().CopyFrom(w_xh.desc)
    op.input_desc[-1].name = "w_xh"
    op.input.append(bias_h.tensor)
    op.input_desc.add().CopyFrom(bias_h.desc)
    op.input_desc[-1].name = "bias_h"
    if w_hh is not None:
        op.input.append(w_hh.tensor)
        op.input_desc.add().CopyFrom(w_hh.desc)
        op.input_desc[-1].name = "w_hh"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "w_hh"
    op.input.append(w_ho.tensor)
    op.input_desc.add().CopyFrom(w_ho.desc)
    op.input_desc[-1].name = "w_ho"
    op.input.append(bias_o.tensor)
    op.input_desc.add().CopyFrom(bias_o.desc)
    op.input_desc[-1].name = "bias_o"

    # process attrs
    op.attr["expose_hidden"].b = expose_hidden
    op.attr["num_output"].i = num_output

    # process outputs
    output_index = 0
    op.output_desc.add().name = "o_t"
    o_t = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "h_t"
    h_t = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return o_t, h_t


# This api is auto-generated from IR DynamicGRU
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, True, True])
def DynamicGRU(x: Tensor, w: Tensor, b: Tensor, cw: Tensor, cb: Tensor, seq_length: Optional[Tensor], init_h: Optional[Tensor], *, direction: str="UNIDIRECTIONAL", cell_depth: int=1, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicGRU)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(w, TensorType({DT_FLOAT16}))\n
.INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(cw, TensorType({DT_FLOAT16}))\n
.INPUT(cb, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(r, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(i, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(n, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicGRU"
    op.name = next_unique_name(node_name, "DynamicGRU")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(b.tensor)
    op.input_desc.add().CopyFrom(b.desc)
    op.input_desc[-1].name = "b"
    op.input.append(cw.tensor)
    op.input_desc.add().CopyFrom(cw.desc)
    op.input_desc[-1].name = "cw"
    op.input.append(cb.tensor)
    op.input_desc.add().CopyFrom(cb.desc)
    op.input_desc[-1].name = "cb"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"

    # process attrs
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "r"
    r = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "i"
    i = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "n"
    n = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, r, i, n


# This api is auto-generated from IR DynamicGRUV2
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, True, True, True, True])
def DynamicGRUV2(x: Tensor, weight_input: Tensor, weight_hidden: Tensor, bias_input: Optional[Tensor], bias_hidden: Optional[Tensor], seq_length: Optional[Tensor], init_h: Optional[Tensor], *, direction: str="UNIDIRECTIONAL", cell_depth: int=1, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", gate_order: str="zrh", reset_after: bool=True, is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicGRUV2)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(weight_input, TensorType({DT_FLOAT16}))\n
.INPUT(weight_hidden, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(bias_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(gate_order, String, "zrh")\n
.ATTR(reset_after, Bool, true)\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicGRUV2"
    op.name = next_unique_name(node_name, "DynamicGRUV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight_input.tensor)
    op.input_desc.add().CopyFrom(weight_input.desc)
    op.input_desc[-1].name = "weight_input"
    op.input.append(weight_hidden.tensor)
    op.input_desc.add().CopyFrom(weight_hidden.desc)
    op.input_desc[-1].name = "weight_hidden"
    if bias_input is not None:
        op.input.append(bias_input.tensor)
        op.input_desc.add().CopyFrom(bias_input.desc)
        op.input_desc[-1].name = "bias_input"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_input"
    if bias_hidden is not None:
        op.input.append(bias_hidden.tensor)
        op.input_desc.add().CopyFrom(bias_hidden.desc)
        op.input_desc[-1].name = "bias_hidden"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_hidden"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"

    # process attrs
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["reset_after"].b = reset_after
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "update"
    update = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reset"
    reset = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "new"
    new = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "hidden_new"
    hidden_new = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, update, reset, new, hidden_new


# This api is auto-generated from IR DynamicGRUV2Hidden
@auto_convert_to_tensor([False, False, False, False, False], [False, False, True, True, True])
def DynamicGRUV2Hidden(x_weight_input: Tensor, weight_hidden: Tensor, bias_hidden: Optional[Tensor], seq_length: Optional[Tensor], init_h: Optional[Tensor], *, direction: str="UNIDIRECTIONAL", cell_depth: int=1, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", gate_order: str="zrh", reset_after: bool=True, is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicGRUV2Hidden)\n
.INPUT(x_weight_input, TensorType({DT_FLOAT32}))\n
.INPUT(weight_hidden, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(gate_order, String, "zrh")\n
.ATTR(reset_after, Bool, true)\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicGRUV2Hidden"
    op.name = next_unique_name(node_name, "DynamicGRUV2Hidden")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_weight_input.tensor)
    op.input_desc.add().CopyFrom(x_weight_input.desc)
    op.input_desc[-1].name = "x_weight_input"
    op.input.append(weight_hidden.tensor)
    op.input_desc.add().CopyFrom(weight_hidden.desc)
    op.input_desc[-1].name = "weight_hidden"
    if bias_hidden is not None:
        op.input.append(bias_hidden.tensor)
        op.input_desc.add().CopyFrom(bias_hidden.desc)
        op.input_desc[-1].name = "bias_hidden"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_hidden"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"

    # process attrs
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["reset_after"].b = reset_after
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "update"
    update = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reset"
    reset = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "new"
    new = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "hidden_new"
    hidden_new = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, update, reset, new, hidden_new


# This api is auto-generated from IR DynamicAUGRU
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, True, True, True, True])
def DynamicAUGRU(x: Tensor, weight_input: Tensor, weight_hidden: Tensor, weight_att: Tensor, bias_input: Optional[Tensor], bias_hidden: Optional[Tensor], seq_length: Optional[Tensor], init_h: Optional[Tensor], *, direction: str="UNIDIRECTIONAL", cell_depth: int=1, keep_prob: float=1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, activation: str="tanh", gate_order: str="zrh", reset_after: bool=True, is_training: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicAUGRU)\n
.INPUT(x, TensorType({DT_FLOAT16}))\n
.INPUT(weight_input, TensorType({DT_FLOAT16}))\n
.INPUT(weight_hidden, TensorType({DT_FLOAT16}))\n
.INPUT(weight_att, TensorType({DT_FLOAT16}))\n
.OPTIONAL_INPUT(bias_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(bias_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32, DT_FLOAT16}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(update_att, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(keep_prob, Float, 1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(activation, String, "tanh")\n
.ATTR(gate_order, String, "zrh")\n
.ATTR(reset_after, Bool, true)\n
.ATTR(is_training, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicAUGRU"
    op.name = next_unique_name(node_name, "DynamicAUGRU")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight_input.tensor)
    op.input_desc.add().CopyFrom(weight_input.desc)
    op.input_desc[-1].name = "weight_input"
    op.input.append(weight_hidden.tensor)
    op.input_desc.add().CopyFrom(weight_hidden.desc)
    op.input_desc[-1].name = "weight_hidden"
    op.input.append(weight_att.tensor)
    op.input_desc.add().CopyFrom(weight_att.desc)
    op.input_desc[-1].name = "weight_att"
    if bias_input is not None:
        op.input.append(bias_input.tensor)
        op.input_desc.add().CopyFrom(bias_input.desc)
        op.input_desc[-1].name = "bias_input"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_input"
    if bias_hidden is not None:
        op.input.append(bias_hidden.tensor)
        op.input_desc.add().CopyFrom(bias_hidden.desc)
        op.input_desc[-1].name = "bias_hidden"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias_hidden"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"

    # process attrs
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["reset_after"].b = reset_after
    op.attr["is_training"].b = is_training

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "update"
    update = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "update_att"
    update_att = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reset"
    reset = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "new"
    new = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "hidden_new"
    hidden_new = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, update, update_att, reset, new, hidden_new


# This api is auto-generated from IR DynamicAUGRUGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True])
def DynamicAUGRUGrad(x: Tensor, weight_input: Tensor, weight_hidden: Tensor, weight_att: Tensor, y: Tensor, init_h: Tensor, h: Tensor, dy: Tensor, dh: Tensor, update: Tensor, update_att: Tensor, reset: Tensor, new: Tensor, hidden_new: Tensor, seq_length: Optional[Tensor], mask: Optional[Tensor], *, direction: str="UNIDIRECTIONAL", cell_depth: int=1, keep_prob: float=-1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, gate_order: str="zrh", reset_after: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicAUGRUGrad)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weight_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weight_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weight_att, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(update_att, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(dw_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dw_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(db_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(db_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dw_att, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 1)\n
.ATTR(keep_prob, Float, -1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(gate_order, String, "zrh")\n
.ATTR(reset_after, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicAUGRUGrad"
    op.name = next_unique_name(node_name, "DynamicAUGRUGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight_input.tensor)
    op.input_desc.add().CopyFrom(weight_input.desc)
    op.input_desc[-1].name = "weight_input"
    op.input.append(weight_hidden.tensor)
    op.input_desc.add().CopyFrom(weight_hidden.desc)
    op.input_desc[-1].name = "weight_hidden"
    op.input.append(weight_att.tensor)
    op.input_desc.add().CopyFrom(weight_att.desc)
    op.input_desc[-1].name = "weight_att"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(init_h.tensor)
    op.input_desc.add().CopyFrom(init_h.desc)
    op.input_desc[-1].name = "init_h"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(update.tensor)
    op.input_desc.add().CopyFrom(update.desc)
    op.input_desc[-1].name = "update"
    op.input.append(update_att.tensor)
    op.input_desc.add().CopyFrom(update_att.desc)
    op.input_desc[-1].name = "update_att"
    op.input.append(reset.tensor)
    op.input_desc.add().CopyFrom(reset.desc)
    op.input_desc[-1].name = "reset"
    op.input.append(new.tensor)
    op.input_desc.add().CopyFrom(new.desc)
    op.input_desc[-1].name = "new"
    op.input.append(hidden_new.tensor)
    op.input_desc.add().CopyFrom(hidden_new.desc)
    op.input_desc[-1].name = "hidden_new"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["reset_after"].b = reset_after

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dw_input"
    dw_input = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dw_hidden"
    dw_hidden = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "db_input"
    db_input = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "db_hidden"
    db_hidden = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dw_att"
    dw_att = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dw_input, dw_hidden, db_input, db_hidden, dx, dh_prev, dw_att


# This api is auto-generated from IR AUGRUHiddenGradCell
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, True])
def AUGRUHiddenGradCell(weight_att: Tensor, dh_pre_t: Tensor, h: Tensor, dy: Tensor, dh: Tensor, update: Tensor, update_att: Tensor, reset: Tensor, new: Tensor, hidden_new: Tensor, seq_length: Optional[Tensor], *, t_state: int=0, gate_order: str="zrh", dependencies=[], node_name=None):
    """REG_OP(AUGRUHiddenGradCell)\n
.INPUT(weight_att, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh_pre_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(update_att, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgate_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dnt_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dw_att_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(t_state, Int, 0)\n
.ATTR(gate_order, String, "zrh")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AUGRUHiddenGradCell"
    op.name = next_unique_name(node_name, "AUGRUHiddenGradCell")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(weight_att.tensor)
    op.input_desc.add().CopyFrom(weight_att.desc)
    op.input_desc[-1].name = "weight_att"
    op.input.append(dh_pre_t.tensor)
    op.input_desc.add().CopyFrom(dh_pre_t.desc)
    op.input_desc[-1].name = "dh_pre_t"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(update.tensor)
    op.input_desc.add().CopyFrom(update.desc)
    op.input_desc[-1].name = "update"
    op.input.append(update_att.tensor)
    op.input_desc.add().CopyFrom(update_att.desc)
    op.input_desc[-1].name = "update_att"
    op.input.append(reset.tensor)
    op.input_desc.add().CopyFrom(reset.desc)
    op.input_desc[-1].name = "reset"
    op.input.append(new.tensor)
    op.input_desc.add().CopyFrom(new.desc)
    op.input_desc[-1].name = "new"
    op.input.append(hidden_new.tensor)
    op.input_desc.add().CopyFrom(hidden_new.desc)
    op.input_desc[-1].name = "hidden_new"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"

    # process attrs
    op.attr["t_state"].i = t_state
    op.attr["gate_order"].s = compat_as_bytes(gate_order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgate_h"
    dgate_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dnt_x"
    dnt_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dw_att_t"
    dw_att_t = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dh_prev, dgate_h, dnt_x, dw_att_t


# This api is auto-generated from IR DynamicGRUV2Grad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, False, False, True, True])
def DynamicGRUV2Grad(x: Tensor, weight_input: Tensor, weight_hidden: Tensor, y: Tensor, init_h: Tensor, h: Tensor, dy: Tensor, dh: Tensor, update: Tensor, reset: Tensor, new: Tensor, hidden_new: Tensor, seq_length: Optional[Tensor], mask: Optional[Tensor], *, direction: str="UNIDIRECTIONAL", cell_depth: int=0, keep_prob: float=-1.000000, cell_clip: float=-1.000000, num_proj: int=0, time_major: bool=True, gate_order: str="zrh", reset_after: bool=True, dependencies=[], node_name=None):
    """REG_OP(DynamicGRUV2Grad)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weight_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(weight_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(mask, TensorType({DT_UINT8}))\n
.OUTPUT(dw_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dw_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(db_input, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(db_hidden, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dx, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(direction, String, "UNIDIRECTIONAL")\n
.ATTR(cell_depth, Int, 0)\n
.ATTR(keep_prob, Float, -1.0)\n
.ATTR(cell_clip, Float, -1.0)\n
.ATTR(num_proj, Int, 0)\n
.ATTR(time_major, Bool, true)\n
.ATTR(gate_order, String, "zrh")\n
.ATTR(reset_after, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicGRUV2Grad"
    op.name = next_unique_name(node_name, "DynamicGRUV2Grad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight_input.tensor)
    op.input_desc.add().CopyFrom(weight_input.desc)
    op.input_desc[-1].name = "weight_input"
    op.input.append(weight_hidden.tensor)
    op.input_desc.add().CopyFrom(weight_hidden.desc)
    op.input_desc[-1].name = "weight_hidden"
    op.input.append(y.tensor)
    op.input_desc.add().CopyFrom(y.desc)
    op.input_desc[-1].name = "y"
    op.input.append(init_h.tensor)
    op.input_desc.add().CopyFrom(init_h.desc)
    op.input_desc[-1].name = "init_h"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(update.tensor)
    op.input_desc.add().CopyFrom(update.desc)
    op.input_desc[-1].name = "update"
    op.input.append(reset.tensor)
    op.input_desc.add().CopyFrom(reset.desc)
    op.input_desc[-1].name = "reset"
    op.input.append(new.tensor)
    op.input_desc.add().CopyFrom(new.desc)
    op.input_desc[-1].name = "new"
    op.input.append(hidden_new.tensor)
    op.input_desc.add().CopyFrom(hidden_new.desc)
    op.input_desc[-1].name = "hidden_new"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"
    if mask is not None:
        op.input.append(mask.tensor)
        op.input_desc.add().CopyFrom(mask.desc)
        op.input_desc[-1].name = "mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "mask"

    # process attrs
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["cell_depth"].i = cell_depth
    op.attr["keep_prob"].f = keep_prob
    op.attr["cell_clip"].f = cell_clip
    op.attr["num_proj"].i = num_proj
    op.attr["time_major"].b = time_major
    op.attr["gate_order"].s = compat_as_bytes(gate_order)
    op.attr["reset_after"].b = reset_after

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dw_input"
    dw_input = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dw_hidden"
    dw_hidden = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "db_input"
    db_input = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "db_hidden"
    db_hidden = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dx"
    dx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dw_input, dw_hidden, db_input, db_hidden, dx, dh_prev


# This api is auto-generated from IR GRUV2HiddenGradCell
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, True])
def GRUV2HiddenGradCell(dh_pre_t: Tensor, h: Tensor, dy: Tensor, dh: Tensor, update: Tensor, reset: Tensor, new: Tensor, hidden_new: Tensor, seq_length: Optional[Tensor], *, t_state: int=0, gate_order: str="zrh", dependencies=[], node_name=None):
    """REG_OP(GRUV2HiddenGradCell)\n
.INPUT(dh_pre_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgate_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dnt_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(t_state, Int, 0)\n
.ATTR(gate_order, String, "zrh")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GRUV2HiddenGradCell"
    op.name = next_unique_name(node_name, "GRUV2HiddenGradCell")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dh_pre_t.tensor)
    op.input_desc.add().CopyFrom(dh_pre_t.desc)
    op.input_desc[-1].name = "dh_pre_t"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(update.tensor)
    op.input_desc.add().CopyFrom(update.desc)
    op.input_desc[-1].name = "update"
    op.input.append(reset.tensor)
    op.input_desc.add().CopyFrom(reset.desc)
    op.input_desc[-1].name = "reset"
    op.input.append(new.tensor)
    op.input_desc.add().CopyFrom(new.desc)
    op.input_desc[-1].name = "new"
    op.input.append(hidden_new.tensor)
    op.input_desc.add().CopyFrom(hidden_new.desc)
    op.input_desc[-1].name = "hidden_new"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"

    # process attrs
    op.attr["t_state"].i = t_state
    op.attr["gate_order"].s = compat_as_bytes(gate_order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgate_h"
    dgate_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dnt_x"
    dnt_x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dh_prev, dgate_h, dnt_x


# This api is auto-generated from IR DynamicGRUCellGrad
@auto_convert_to_tensor([False, False, False, False, False, False, False, False, False, False, False], [False, False, False, False, False, False, False, False, False, False, True])
def DynamicGRUCellGrad(dh_pre_t: Tensor, h: Tensor, dy: Tensor, dh: Tensor, update: Tensor, reset: Tensor, new: Tensor, hidden_new: Tensor, init_h: Tensor, t_state: Tensor, seq_length: Optional[Tensor], *, gate_order: str="zrh", dependencies=[], node_name=None):
    """REG_OP(DynamicGRUCellGrad)\n
.INPUT(dh_pre_t, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dy, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(dh, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(update, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(reset, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(hidden_new, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(t_state, TensorType({DT_INT32, DT_INT32}))\n
.OPTIONAL_INPUT(seq_length, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dh_prev, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dgate_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(dnt_x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(gate_order, String, "zrh")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynamicGRUCellGrad"
    op.name = next_unique_name(node_name, "DynamicGRUCellGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dh_pre_t.tensor)
    op.input_desc.add().CopyFrom(dh_pre_t.desc)
    op.input_desc[-1].name = "dh_pre_t"
    op.input.append(h.tensor)
    op.input_desc.add().CopyFrom(h.desc)
    op.input_desc[-1].name = "h"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"
    op.input.append(dh.tensor)
    op.input_desc.add().CopyFrom(dh.desc)
    op.input_desc[-1].name = "dh"
    op.input.append(update.tensor)
    op.input_desc.add().CopyFrom(update.desc)
    op.input_desc[-1].name = "update"
    op.input.append(reset.tensor)
    op.input_desc.add().CopyFrom(reset.desc)
    op.input_desc[-1].name = "reset"
    op.input.append(new.tensor)
    op.input_desc.add().CopyFrom(new.desc)
    op.input_desc[-1].name = "new"
    op.input.append(hidden_new.tensor)
    op.input_desc.add().CopyFrom(hidden_new.desc)
    op.input_desc[-1].name = "hidden_new"
    op.input.append(init_h.tensor)
    op.input_desc.add().CopyFrom(init_h.desc)
    op.input_desc[-1].name = "init_h"
    op.input.append(t_state.tensor)
    op.input_desc.add().CopyFrom(t_state.desc)
    op.input_desc[-1].name = "t_state"
    if seq_length is not None:
        op.input.append(seq_length.tensor)
        op.input_desc.add().CopyFrom(seq_length.desc)
        op.input_desc[-1].name = "seq_length"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "seq_length"

    # process attrs
    op.attr["gate_order"].s = compat_as_bytes(gate_order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dh_prev"
    dh_prev = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dgate_h"
    dgate_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "dnt_x"
    dnt_x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dh_prev, dgate_h, dnt_x


# This api is auto-generated from IR EmbeddingDenseGrad
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def EmbeddingDenseGrad(grad: Tensor, indices: Tensor, *, num_weights: int, padding_idx: int=-1, scale_grad_by_freq: bool=False, dependencies=[], node_name=None):
    """REG_OP(EmbeddingDenseGrad)\n
.INPUT(grad, TensorType({ DT_FLOAT32, DT_FLOAT16, DT_BF16 }))\n
.INPUT(indices, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT32, DT_FLOAT16, DT_BF16 }))\n
.REQUIRED_ATTR(num_weights, Int)\n
.ATTR(padding_idx, Int, -1)\n
.ATTR(scale_grad_by_freq, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingDenseGrad"
    op.name = next_unique_name(node_name, "EmbeddingDenseGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(grad.tensor)
    op.input_desc.add().CopyFrom(grad.desc)
    op.input_desc[-1].name = "grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["num_weights"].i = num_weights
    op.attr["padding_idx"].i = padding_idx
    op.attr["scale_grad_by_freq"].b = scale_grad_by_freq

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CommonLSTM
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True])
def CommonLSTM(x: Tensor, w: Tensor, r: Tensor, b: Optional[Tensor], sequence_lens: Optional[Tensor], initial_h: Optional[Tensor], initial_c: Optional[Tensor], p: Optional[Tensor], *, hidden_size: int, activation_alpha: List[float]=[], activation_beta: List[float]=[], activations: List[str]=[], clip: float=-1.000000, direction: str="forward", input_forget: int=0, dependencies=[], node_name=None):
    """REG_OP(CommonLSTM)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(r, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(sequence_lens, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(initial_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(initial_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(p, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(activation_alpha, ListFloat, {})\n
.ATTR(activation_beta, ListFloat, {})\n
.ATTR(activations, ListString, {})\n
.ATTR(clip, Float, -1.0)\n
.ATTR(direction, String, "forward")\n
.REQUIRED_ATTR(hidden_size, Int)\n
.ATTR(input_forget, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CommonLSTM"
    op.name = next_unique_name(node_name, "CommonLSTM")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(r.tensor)
    op.input_desc.add().CopyFrom(r.desc)
    op.input_desc[-1].name = "r"
    if b is not None:
        op.input.append(b.tensor)
        op.input_desc.add().CopyFrom(b.desc)
        op.input_desc[-1].name = "b"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "b"
    if sequence_lens is not None:
        op.input.append(sequence_lens.tensor)
        op.input_desc.add().CopyFrom(sequence_lens.desc)
        op.input_desc[-1].name = "sequence_lens"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "sequence_lens"
    if initial_h is not None:
        op.input.append(initial_h.tensor)
        op.input_desc.add().CopyFrom(initial_h.desc)
        op.input_desc[-1].name = "initial_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "initial_h"
    if initial_c is not None:
        op.input.append(initial_c.tensor)
        op.input_desc.add().CopyFrom(initial_c.desc)
        op.input_desc[-1].name = "initial_c"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "initial_c"
    if p is not None:
        op.input.append(p.tensor)
        op.input_desc.add().CopyFrom(p.desc)
        op.input_desc[-1].name = "p"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "p"

    # process attrs
    op.attr["hidden_size"].i = hidden_size
    op.attr["activation_alpha"].list.val_type = 3
    op.attr["activation_alpha"].list.f.extend(activation_alpha)
    op.attr["activation_beta"].list.val_type = 3
    op.attr["activation_beta"].list.f.extend(activation_beta)
    op.attr["activations"].list.val_type = 0
    op.attr["activations"].list.s.extend(compat_as_bytes_list(activations))
    op.attr["clip"].f = clip
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["input_forget"].i = input_forget

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_h"
    y_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_c"
    y_c = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, y_h, y_c


# This api is auto-generated from IR RnnGenMaskV2
@auto_convert_to_tensor([False, False], [False, False])
def RnnGenMaskV2(seq_length: Tensor, x: Tensor, *, hidden_size: int, dependencies=[], node_name=None):
    """REG_OP(RnnGenMaskV2)\n
.INPUT(seq_length, TensorType({DT_INT32}))\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(hidden_size, Int)\n
.OUTPUT(seq_mask, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RnnGenMaskV2"
    op.name = next_unique_name(node_name, "RnnGenMaskV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(seq_length.tensor)
    op.input_desc.add().CopyFrom(seq_length.desc)
    op.input_desc[-1].name = "seq_length"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["hidden_size"].i = hidden_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "seq_mask"
    seq_mask = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return seq_mask


# This api is auto-generated from IR CommonGRU
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, True, True, True])
def CommonGRU(x: Tensor, w: Tensor, r: Tensor, b: Optional[Tensor], sequence_lens: Optional[Tensor], initial_h: Optional[Tensor], *, hidden_size: int, activation_alpha: List[float]=[], activation_beta: List[float]=[], activations: List[str]=[], clip: float=-1.000000, direction: str="forward", linear_before_reset: int=0, dependencies=[], node_name=None):
    """REG_OP(CommonGRU)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(w, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(r, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(b, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(sequence_lens, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(initial_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(activation_alpha, ListFloat, {})\n
.ATTR(activation_beta, ListFloat, {})\n
.ATTR(activations, ListString, {})\n
.ATTR(clip, Float, -1.0)\n
.ATTR(direction, String, "forward")\n
.REQUIRED_ATTR(hidden_size, Int)\n
.ATTR(linear_before_reset, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CommonGRU"
    op.name = next_unique_name(node_name, "CommonGRU")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(w.tensor)
    op.input_desc.add().CopyFrom(w.desc)
    op.input_desc[-1].name = "w"
    op.input.append(r.tensor)
    op.input_desc.add().CopyFrom(r.desc)
    op.input_desc[-1].name = "r"
    if b is not None:
        op.input.append(b.tensor)
        op.input_desc.add().CopyFrom(b.desc)
        op.input_desc[-1].name = "b"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "b"
    if sequence_lens is not None:
        op.input.append(sequence_lens.tensor)
        op.input_desc.add().CopyFrom(sequence_lens.desc)
        op.input_desc[-1].name = "sequence_lens"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "sequence_lens"
    if initial_h is not None:
        op.input.append(initial_h.tensor)
        op.input_desc.add().CopyFrom(initial_h.desc)
        op.input_desc[-1].name = "initial_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "initial_h"

    # process attrs
    op.attr["hidden_size"].i = hidden_size
    op.attr["activation_alpha"].list.val_type = 3
    op.attr["activation_alpha"].list.f.extend(activation_alpha)
    op.attr["activation_beta"].list.val_type = 3
    op.attr["activation_beta"].list.f.extend(activation_beta)
    op.attr["activations"].list.val_type = 0
    op.attr["activations"].list.s.extend(compat_as_bytes_list(activations))
    op.attr["clip"].f = clip
    op.attr["direction"].s = compat_as_bytes(direction)
    op.attr["linear_before_reset"].i = linear_before_reset

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_h"
    y_h = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, y_h


# This api is auto-generated from IR EmbeddingBag
@auto_convert_to_tensor([False, False, False, False],
                        [False, False, True, True])
def EmbeddingBag(weight: Tensor,
                 indices: Tensor,
                 offsets: Optional[Tensor],
                 per_sample_weights: Optional[Tensor],
                 *,
                 mode: str = "mean",
                 scale_grad_by_freq: bool = False,
                 sparse: bool = False,
                 include_last_offset: bool = False,
                 padding_idx: int = -1,
                 dependencies=[],
                 node_name=None):
    """REG_OP(EmbeddingBag)
    .INPUT(weight, TensorType({DT_FLOAT16, DT_FLOAT}))
    .INPUT(indices, TensorType({DT_INT32, DT_INT64}))
    .OPTIONAL_INPUT(offsets, TensorType({DT_INT32, DT_INT64}))
    .OPTIONAL_INPUT(per_sample_weights, TensorType({DT_FLOAT16, DT_FLOAT}))
    .OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))
    .OUTPUT(offset2bag, TensorType({DT_INT32, DT_INT64}))
    .OUTPUT(bag_size, TensorType({DT_INT32, DT_INT64}))
    .OUTPUT(max_indices, TensorType({DT_INT32, DT_INT64}))
    .ATTR(mode, String, "mean")
    .ATTR(scale_grad_by_freq, Bool, false)
    .ATTR(sparse, Bool, false)
    .ATTR(include_last_offset, Bool, false)
    .ATTR(padding_idx, Int, -1)
    .OP_END_FACTORY_REG(EmbeddingBag)
"""

    op = get_default_ge_graph().op.add()
    op.type = "EmbeddingBag"
    op.name = next_unique_name(node_name, "EmbeddingBag")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    if offsets is not None:
        op.input.append(offsets.tensor)
        op.input_desc.add().CopyFrom(offsets.desc)
        op.input_desc[-1].name = "offsets"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offsets"
    if per_sample_weights is not None:
        op.input.append(per_sample_weights.tensor)
        op.input_desc.add().CopyFrom(per_sample_weights.desc)
        op.input_desc[-1].name = "per_sample_weights"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "per_sample_weights"

    # process attrs
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["scale_grad_by_freq"].b = scale_grad_by_freq
    op.attr["sparse"].b = sparse
    op.attr["include_last_offset"].b = include_last_offset
    op.attr["padding_idx"].i = padding_idx

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "offset2bag"
    offset2bag = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "bag_size"
    bag_size = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "max_indices"
    max_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, offset2bag, bag_size, max_indices


# This api is auto-generated from IR LSTMP
@auto_convert_to_tensor([False, False, False, False, False, False, False, False], [False, False, False, False, False, True, True, True])
def LSTMP(x: Tensor, wx: Tensor, bias: Tensor, wr: Tensor, project: Tensor, real_mask: Optional[Tensor], init_h: Optional[Tensor], init_c: Optional[Tensor], *, time_major: bool=False, dependencies=[], node_name=None):
    """REG_OP(LSTMP)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(wx, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(wr, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(project, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(real_mask, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(init_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OPTIONAL_INPUT(init_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_h, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(output_c, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(time_major, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LSTMP"
    op.name = next_unique_name(node_name, "LSTMP")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(wx.tensor)
    op.input_desc.add().CopyFrom(wx.desc)
    op.input_desc[-1].name = "wx"
    op.input.append(bias.tensor)
    op.input_desc.add().CopyFrom(bias.desc)
    op.input_desc[-1].name = "bias"
    op.input.append(wr.tensor)
    op.input_desc.add().CopyFrom(wr.desc)
    op.input_desc[-1].name = "wr"
    op.input.append(project.tensor)
    op.input_desc.add().CopyFrom(project.desc)
    op.input_desc[-1].name = "project"
    if real_mask is not None:
        op.input.append(real_mask.tensor)
        op.input_desc.add().CopyFrom(real_mask.desc)
        op.input_desc[-1].name = "real_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "real_mask"
    if init_h is not None:
        op.input.append(init_h.tensor)
        op.input_desc.add().CopyFrom(init_h.desc)
        op.input_desc[-1].name = "init_h"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_h"
    if init_c is not None:
        op.input.append(init_c.tensor)
        op.input_desc.add().CopyFrom(init_c.desc)
        op.input_desc[-1].name = "init_c"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "init_c"

    # process attrs
    op.attr["time_major"].b = time_major

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_h"
    output_h = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_c"
    output_c = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, output_h, output_c


# This api is auto-generated from IR NMSWithMask
@auto_convert_to_tensor([False], [False])
def NMSWithMask(box_scores: Tensor, *, iou_threshold: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(NMSWithMask)\n
.INPUT(box_scores, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(selected_boxes, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(selected_idx, TensorType({DT_INT32}))\n
.OUTPUT(selected_mask, TensorType({DT_UINT8}))\n
.ATTR(iou_threshold, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NMSWithMask"
    op.name = next_unique_name(node_name, "NMSWithMask")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(box_scores.tensor)
    op.input_desc.add().CopyFrom(box_scores.desc)
    op.input_desc[-1].name = "box_scores"

    # process attrs
    op.attr["iou_threshold"].f = iou_threshold

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_boxes"
    selected_boxes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "selected_idx"
    selected_idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "selected_mask"
    selected_mask = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_boxes, selected_idx, selected_mask


# This api is auto-generated from IR SortedNMS
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def SortedNMS(boxes: Tensor, sorted_scores: Tensor, input_indices: Tensor, max_output_size: Tensor, iou_threshold: Tensor, score_threshold: Tensor, *, offset: int=0, dependencies=[], node_name=None):
    """REG_OP(SortedNMS)\n
.INPUT(boxes, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(sorted_scores, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(input_indices, TensorType({DT_INT32}))\n
.INPUT(max_output_size, TensorType({DT_INT32}))\n
.INPUT(iou_threshold, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(score_threshold, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(selected_indices, TensorType({DT_INT32}))\n
.ATTR(offset, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SortedNMS"
    op.name = next_unique_name(node_name, "SortedNMS")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(sorted_scores.tensor)
    op.input_desc.add().CopyFrom(sorted_scores.desc)
    op.input_desc[-1].name = "sorted_scores"
    op.input.append(input_indices.tensor)
    op.input_desc.add().CopyFrom(input_indices.desc)
    op.input_desc[-1].name = "input_indices"
    op.input.append(max_output_size.tensor)
    op.input_desc.add().CopyFrom(max_output_size.desc)
    op.input_desc[-1].name = "max_output_size"
    op.input.append(iou_threshold.tensor)
    op.input_desc.add().CopyFrom(iou_threshold.desc)
    op.input_desc[-1].name = "iou_threshold"
    op.input.append(score_threshold.tensor)
    op.input_desc.add().CopyFrom(score_threshold.desc)
    op.input_desc[-1].name = "score_threshold"

    # process attrs
    op.attr["offset"].i = offset

    # process outputs
    output_index = 0
    op.output_desc.add().name = "selected_indices"
    selected_indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return selected_indices


# This api is auto-generated from IR Save
@auto_convert_to_tensor([True], [False], inputs_tensor_type=[TensorType.TT_ALL])
def Save(tensors: List[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(Save)\n
.DYNAMIC_INPUT(tensors, TensorType:ALL())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Save"
    op.name = next_unique_name(node_name, "Save")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(tensors, (tuple, list)):
        raise AssertionError("tensors must be a tuple or a list.")
    for i, v in enumerate(tensors):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "tensors" + str(i)

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR SdcaOptimizerV2
@auto_convert_to_tensor([True, True, True, True, False, False, True, True, True, False], [False, False, False, False, False, False, False, False, False, False])
def _SdcaOptimizerV2(sparse_example_indices: List[Tensor], sparse_feature_indices: List[Tensor], sparse_feature_values: List[Tensor], dense_features: List[Tensor], example_weights: Tensor, example_labels: Tensor, sparse_indices: List[Tensor], sparse_weights: List[Tensor], dense_weights: List[Tensor], example_state_data: Tensor, *, size_of_out_delta_sparse_weights: int, size_of_out_delta_dense_weights: int, adaptive: bool=False, num_sparse_features: int=0, num_sparse_features_with_values: int=0, num_dense_features: int=0, num_loss_partitions: int=1, num_inner_iterations: int=1, loss_type: str="logistic_loss", l1: float=0.500000, l2: float=0.500000, dependencies=[], node_name=None):
    """REG_OP(SdcaOptimizerV2)\n
.DYNAMIC_INPUT(sparse_example_indices, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(sparse_feature_indices, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(sparse_feature_values, TensorType({DT_FLOAT}))\n
.DYNAMIC_INPUT(dense_features, TensorType({DT_FLOAT}))\n
.INPUT(example_weights, TensorType({DT_FLOAT}))\n
.INPUT(example_labels, TensorType({DT_FLOAT}))\n
.DYNAMIC_INPUT(sparse_indices, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(sparse_weights, TensorType({DT_FLOAT}))\n
.DYNAMIC_INPUT(dense_weights, TensorType({DT_FLOAT}))\n
.INPUT(example_state_data, TensorType({DT_FLOAT}))\n
.OUTPUT(out_example_state_data, TensorType({DT_FLOAT}))\n
.DYNAMIC_OUTPUT(out_delta_sparse_weights, TensorType({DT_FLOAT}))\n
.DYNAMIC_OUTPUT(out_delta_dense_weights, TensorType({DT_FLOAT}))\n
.ATTR(adaptive, Bool, false)\n
.ATTR(num_sparse_features, Int, 0)\n
.ATTR(num_sparse_features_with_values, Int, 0)\n
.ATTR(num_dense_features, Int, 0)\n
.ATTR(num_loss_partitions, Int, 1)\n
.ATTR(num_inner_iterations, Int, 1)\n
.ATTR(loss_type, String, "logistic_loss")\n
.ATTR(l1, Float, 0.5)\n
.ATTR(l2, Float, 0.5)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SdcaOptimizerV2"
    op.name = next_unique_name(node_name, "SdcaOptimizerV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(sparse_example_indices, (tuple, list)):
        raise AssertionError("sparse_example_indices must be a tuple or a list.")
    for i, v in enumerate(sparse_example_indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "sparse_example_indices" + str(i)
    if not isinstance(sparse_feature_indices, (tuple, list)):
        raise AssertionError("sparse_feature_indices must be a tuple or a list.")
    for i, v in enumerate(sparse_feature_indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "sparse_feature_indices" + str(i)
    if not isinstance(sparse_feature_values, (tuple, list)):
        raise AssertionError("sparse_feature_values must be a tuple or a list.")
    for i, v in enumerate(sparse_feature_values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "sparse_feature_values" + str(i)
    if not isinstance(dense_features, (tuple, list)):
        raise AssertionError("dense_features must be a tuple or a list.")
    for i, v in enumerate(dense_features):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "dense_features" + str(i)
    op.input.append(example_weights.tensor)
    op.input_desc.add().CopyFrom(example_weights.desc)
    op.input_desc[-1].name = "example_weights"
    op.input.append(example_labels.tensor)
    op.input_desc.add().CopyFrom(example_labels.desc)
    op.input_desc[-1].name = "example_labels"
    if not isinstance(sparse_indices, (tuple, list)):
        raise AssertionError("sparse_indices must be a tuple or a list.")
    for i, v in enumerate(sparse_indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "sparse_indices" + str(i)
    if not isinstance(sparse_weights, (tuple, list)):
        raise AssertionError("sparse_weights must be a tuple or a list.")
    for i, v in enumerate(sparse_weights):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "sparse_weights" + str(i)
    if not isinstance(dense_weights, (tuple, list)):
        raise AssertionError("dense_weights must be a tuple or a list.")
    for i, v in enumerate(dense_weights):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "dense_weights" + str(i)
    op.input.append(example_state_data.tensor)
    op.input_desc.add().CopyFrom(example_state_data.desc)
    op.input_desc[-1].name = "example_state_data"

    # process attrs
    op.attr["adaptive"].b = adaptive
    op.attr["num_sparse_features"].i = num_sparse_features
    op.attr["num_sparse_features_with_values"].i = num_sparse_features_with_values
    op.attr["num_dense_features"].i = num_dense_features
    op.attr["num_loss_partitions"].i = num_loss_partitions
    op.attr["num_inner_iterations"].i = num_inner_iterations
    op.attr["loss_type"].s = compat_as_bytes(loss_type)
    op.attr["l1"].f = l1
    op.attr["l2"].f = l2

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out_example_state_data"
    out_example_state_data = Tensor(op, output_index)
    output_index += 1
    out_delta_sparse_weights = []
    for i in range(output_index, output_index + size_of_out_delta_sparse_weights):
        op.output_desc.add().name = "out_delta_sparse_weights" + str(i - output_index)
        out_delta_sparse_weights.append(Tensor(op, i))
    output_index += size_of_out_delta_sparse_weights
    out_delta_dense_weights = []
    for i in range(output_index, output_index + size_of_out_delta_dense_weights):
        op.output_desc.add().name = "out_delta_dense_weights" + str(i - output_index)
        out_delta_dense_weights.append(Tensor(op, i))
    output_index += size_of_out_delta_dense_weights

    # return outputs
    return out_example_state_data, out_delta_sparse_weights, out_delta_dense_weights


# This api is auto-generated from IR Range
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Range(start: Tensor, limit: Tensor, delta: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Range)\n
.INPUT(start, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_DOUBLE, DT_INT64, DT_BF16}))\n
.INPUT(limit, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_DOUBLE, DT_INT64, DT_BF16}))\n
.INPUT(delta, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_DOUBLE, DT_INT64, DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_DOUBLE, DT_INT64, DT_BF16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Range"
    op.name = next_unique_name(node_name, "Range")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(start.tensor)
    op.input_desc.add().CopyFrom(start.desc)
    op.input_desc[-1].name = "start"
    op.input.append(limit.tensor)
    op.input_desc.add().CopyFrom(limit.desc)
    op.input_desc[-1].name = "limit"
    op.input.append(delta.tensor)
    op.input_desc.add().CopyFrom(delta.desc)
    op.input_desc[-1].name = "delta"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RangeD
@auto_convert_to_tensor([False], [False])
def RangeD(x: Tensor, *, start: float, limit: float, delta: float, dependencies=[], node_name=None):
    """REG_OP(RangeD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_INT32}))\n
.REQUIRED_ATTR(start, Float)\n
.REQUIRED_ATTR(limit, Float)\n
.REQUIRED_ATTR(delta, Float)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RangeD"
    op.name = next_unique_name(node_name, "RangeD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["start"].f = start
    op.attr["limit"].f = limit
    op.attr["delta"].f = delta

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Tile
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def Tile(x: Tensor, multiples: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Tile)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(multiples, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Tile"
    op.name = next_unique_name(node_name, "Tile")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(multiples.tensor)
    op.input_desc.add().CopyFrom(multiples.desc)
    op.input_desc[-1].name = "multiples"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TileD
@auto_convert_to_tensor([False], [False])
def TileD(x: Tensor, *, multiples: List[int], dependencies=[], node_name=None):
    """REG_OP(TileD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.REQUIRED_ATTR(multiples, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TileD"
    op.name = next_unique_name(node_name, "TileD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["multiples"].list.val_type = 2
    op.attr["multiples"].list.i.extend(multiples)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GatherNd
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def GatherNd(x: Tensor, indices: Tensor, *, negative_index_support: bool=False, dependencies=[], node_name=None):
    """REG_OP(GatherNd)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(negative_index_support, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GatherNd"
    op.name = next_unique_name(node_name, "GatherNd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["negative_index_support"].b = negative_index_support

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GatherV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def GatherV2(x: Tensor, indices: Tensor, axis: Tensor, *, batch_dims: int=0, is_preprocessed: bool=False, negative_index_support: bool=False, dependencies=[], node_name=None):
    """REG_OP(GatherV2)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(axis, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(batch_dims, Int, 0)\n
.ATTR(is_preprocessed, Bool, false)\n
.ATTR(negative_index_support, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GatherV2"
    op.name = next_unique_name(node_name, "GatherV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs
    op.attr["batch_dims"].i = batch_dims
    op.attr["is_preprocessed"].b = is_preprocessed
    op.attr["negative_index_support"].b = negative_index_support

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GatherV2D
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def GatherV2D(x: Tensor, indices: Tensor, *, axis: int, dependencies=[], node_name=None):
    """REG_OP(GatherV2D)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_UINT32, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT64, DT_UINT64}))\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_UINT32, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT64, DT_UINT64}))\n
.REQUIRED_ATTR(axis, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GatherV2D"
    op.name = next_unique_name(node_name, "GatherV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GatherElements
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def GatherElements(x: Tensor, index: Tensor, *, dim: int=0, dependencies=[], node_name=None):
    """REG_OP(GatherElements)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.INPUT(index, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_BOOL}))\n
.ATTR(dim, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GatherElements"
    op.name = next_unique_name(node_name, "GatherElements")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"

    # process attrs
    op.attr["dim"].i = dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR GatherD
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def GatherD(x: Tensor, dim: Tensor, index: Tensor, *, dim_changed_as_duplicate_with_input: int=0, dependencies=[], node_name=None):
    """REG_OP(GatherD)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32 DT_INT64, DT_UINT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(dim, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(index, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT64}))\n
.ATTR(dim, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GatherD"
    op.name = next_unique_name(node_name, "GatherD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(dim.tensor)
    op.input_desc.add().CopyFrom(dim.desc)
    op.input_desc[-1].name = "dim"
    op.input.append(index.tensor)
    op.input_desc.add().CopyFrom(index.desc)
    op.input_desc[-1].name = "index"

    # process attrs
    op.attr["dim"].i = dim_changed_as_duplicate_with_input

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StridedSlice
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def StridedSlice(x: Tensor, begin: Tensor, end: Tensor, strides: Tensor, *, begin_mask: int=0, end_mask: int=0, ellipsis_mask: int=0, new_axis_mask: int=0, shrink_axis_mask: int=0, dependencies=[], node_name=None):
    """REG_OP(StridedSlice)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(begin, TensorType::IndexNumberType())\n
.INPUT(end, TensorType::IndexNumberType())\n
.INPUT(strides, TensorType::IndexNumberType())\n
.ATTR(begin_mask, Int, 0)\n
.ATTR(end_mask, Int, 0)\n
.ATTR(ellipsis_mask, Int, 0)\n
.ATTR(new_axis_mask, Int, 0)\n
.ATTR(shrink_axis_mask, Int, 0)\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSlice"
    op.name = next_unique_name(node_name, "StridedSlice")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(begin.tensor)
    op.input_desc.add().CopyFrom(begin.desc)
    op.input_desc[-1].name = "begin"
    op.input.append(end.tensor)
    op.input_desc.add().CopyFrom(end.desc)
    op.input_desc[-1].name = "end"
    op.input.append(strides.tensor)
    op.input_desc.add().CopyFrom(strides.desc)
    op.input_desc[-1].name = "strides"

    # process attrs
    op.attr["begin_mask"].i = begin_mask
    op.attr["end_mask"].i = end_mask
    op.attr["ellipsis_mask"].i = ellipsis_mask
    op.attr["new_axis_mask"].i = new_axis_mask
    op.attr["shrink_axis_mask"].i = shrink_axis_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StridedSliceD
@auto_convert_to_tensor([False], [False])
def StridedSliceD(x: Tensor, *, begin: List[int], end: List[int], strides: List[int], begin_mask: int=0, end_mask: int=0, ellipsis_mask: int=0, new_axis_mask: int=0, shrink_axis_mask: int=0, dependencies=[], node_name=None):
    """REG_OP(StridedSliceD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT64, DT_UINT8, DT_INT8, DT_BOOL, DT_BF16, DT_COMPLEX32, DT_COMPLEX64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT64, DT_UINT8, DT_INT8, DT_BOOL, DT_BF16, DT_COMPLEX32, DT_COMPLEX64}))\n
.REQUIRED_ATTR(begin, ListInt)\n
.REQUIRED_ATTR(end, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(begin_mask, Int, 0)\n
.ATTR(end_mask, Int, 0)\n
.ATTR(ellipsis_mask, Int, 0)\n
.ATTR(new_axis_mask, Int, 0)\n
.ATTR(shrink_axis_mask, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSliceD"
    op.name = next_unique_name(node_name, "StridedSliceD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["begin"].list.val_type = 2
    op.attr["begin"].list.i.extend(begin)
    op.attr["end"].list.val_type = 2
    op.attr["end"].list.i.extend(end)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["begin_mask"].i = begin_mask
    op.attr["end_mask"].i = end_mask
    op.attr["ellipsis_mask"].i = ellipsis_mask
    op.attr["new_axis_mask"].i = new_axis_mask
    op.attr["shrink_axis_mask"].i = shrink_axis_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StridedSliceGradD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def StridedSliceGradD(dy: Tensor, *, shape: List[int], begin: List[int], end: List[int], strides: List[int], begin_mask: int=0, end_mask: int=0, ellipsis_mask: int=0, new_axis_mask: int=0, shrink_axis_mask: int=0, dependencies=[], node_name=None):
    """REG_OP(StridedSliceGradD)\n
.INPUT(dy, TensorType::BasicType())\n
.OUTPUT(output, TensorType::BasicType())\n
.REQUIRED_ATTR(shape, ListInt)\n
.REQUIRED_ATTR(begin, ListInt)\n
.REQUIRED_ATTR(end, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(begin_mask, Int, 0)\n
.ATTR(end_mask, Int, 0)\n
.ATTR(ellipsis_mask, Int, 0)\n
.ATTR(new_axis_mask, Int, 0)\n
.ATTR(shrink_axis_mask, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSliceGradD"
    op.name = next_unique_name(node_name, "StridedSliceGradD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["begin"].list.val_type = 2
    op.attr["begin"].list.i.extend(begin)
    op.attr["end"].list.val_type = 2
    op.attr["end"].list.i.extend(end)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["begin_mask"].i = begin_mask
    op.attr["end_mask"].i = end_mask
    op.attr["ellipsis_mask"].i = ellipsis_mask
    op.attr["new_axis_mask"].i = new_axis_mask
    op.attr["shrink_axis_mask"].i = shrink_axis_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR StridedSliceGrad
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def StridedSliceGrad(shape: Tensor, begin: Tensor, end: Tensor, strides: Tensor, dy: Tensor, *, begin_mask: int=0, end_mask: int=0, ellipsis_mask: int=0, new_axis_mask: int=0, shrink_axis_mask: int=0, dependencies=[], node_name=None):
    """REG_OP(StridedSliceGrad)\n
.INPUT(shape, TensorType::IndexNumberType())\n
.INPUT(begin, TensorType::IndexNumberType())\n
.INPUT(end, TensorType::IndexNumberType())\n
.INPUT(strides, TensorType::IndexNumberType())\n
.INPUT(dy, TensorType::BasicType())\n
.OUTPUT(output, TensorType::BasicType())\n
.ATTR(begin_mask, Int, 0)\n
.ATTR(end_mask, Int, 0)\n
.ATTR(ellipsis_mask, Int, 0)\n
.ATTR(new_axis_mask, Int, 0)\n
.ATTR(shrink_axis_mask, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSliceGrad"
    op.name = next_unique_name(node_name, "StridedSliceGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(begin.tensor)
    op.input_desc.add().CopyFrom(begin.desc)
    op.input_desc[-1].name = "begin"
    op.input.append(end.tensor)
    op.input_desc.add().CopyFrom(end.desc)
    op.input_desc[-1].name = "end"
    op.input.append(strides.tensor)
    op.input_desc.add().CopyFrom(strides.desc)
    op.input_desc[-1].name = "strides"
    op.input.append(dy.tensor)
    op.input_desc.add().CopyFrom(dy.desc)
    op.input_desc[-1].name = "dy"

    # process attrs
    op.attr["begin_mask"].i = begin_mask
    op.attr["end_mask"].i = end_mask
    op.attr["ellipsis_mask"].i = ellipsis_mask
    op.attr["new_axis_mask"].i = new_axis_mask
    op.attr["shrink_axis_mask"].i = shrink_axis_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR UnsortedSegmentSum
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def UnsortedSegmentSum(x: Tensor, segment_ids: Tensor, num_segments: Tensor, *, is_preprocessed: bool=False, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentSum)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(segment_ids, TensorType::IndexNumberType())\n
.INPUT(num_segments, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(is_preprocessed, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentSum"
    op.name = next_unique_name(node_name, "UnsortedSegmentSum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"
    op.input.append(num_segments.tensor)
    op.input_desc.add().CopyFrom(num_segments.desc)
    op.input_desc[-1].name = "num_segments"

    # process attrs
    op.attr["is_preprocessed"].b = is_preprocessed

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR LogSpaceD
@auto_convert_to_tensor([False], [False])
def LogSpaceD(assist: Tensor, *, start: float, end: float, steps: int=100, base: float=10.000000, dtype: int=1, dependencies=[], node_name=None):
    """REG_OP(LogSpaceD)\n
.INPUT(assist, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(start, Float)\n
.REQUIRED_ATTR(end, Float)\n
.ATTR(steps, Int, 100)\n
.ATTR(base, Float, 10.0)\n
.ATTR(dtype, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "LogSpaceD"
    op.name = next_unique_name(node_name, "LogSpaceD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(assist.tensor)
    op.input_desc.add().CopyFrom(assist.desc)
    op.input_desc[-1].name = "assist"

    # process attrs
    op.attr["start"].f = start
    op.attr["end"].f = end
    op.attr["steps"].i = steps
    op.attr["base"].f = base
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsortedSegmentSumD
@auto_convert_to_tensor([False, False], [False, False])
def UnsortedSegmentSumD(x: Tensor, segment_ids: Tensor, *, num_segments: int, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentSumD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_UINT8}))\n
.INPUT(segment_ids, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT8, DT_UINT8}))\n
.REQUIRED_ATTR(num_segments, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentSumD"
    op.name = next_unique_name(node_name, "UnsortedSegmentSumD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs
    op.attr["num_segments"].i = num_segments

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReverseV2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ReverseV2(x: Tensor, axis: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ReverseV2)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_STRING}))\n
.INPUT(axis, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReverseV2"
    op.name = next_unique_name(node_name, "ReverseV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReverseV2D
@auto_convert_to_tensor([False], [False])
def ReverseV2D(x: Tensor, *, axis: List[int], dependencies=[], node_name=None):
    """REG_OP(ReverseV2D)\n
.INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_STRING}))\n
.REQUIRED_ATTR(axis, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReverseV2D"
    op.name = next_unique_name(node_name, "ReverseV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].list.val_type = 2
    op.attr["axis"].list.i.extend(axis)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Select
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Select(condition: Tensor, x1: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Select)\n
.INPUT(condition, TensorType({DT_BOOL}))\n
.INPUT(x1, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.INPUT(x2, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Select"
    op.name = next_unique_name(node_name, "Select")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(condition.tensor)
    op.input_desc.add().CopyFrom(condition.desc)
    op.input_desc[-1].name = "condition"
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SelectV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_BASIC, TensorType.TT_BASIC])
def SelectV2(condition: Tensor, then: Tensor, else_changed_as_is_python_key: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SelectV2)\n
.INPUT(condition, TensorType({DT_BOOL}))\n
.INPUT(then, TensorType::BasicType())\n
.INPUT(else, TensorType::BasicType())\n
.OUTPUT(result, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SelectV2"
    op.name = next_unique_name(node_name, "SelectV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(condition.tensor)
    op.input_desc.add().CopyFrom(condition.desc)
    op.input_desc[-1].name = "condition"
    op.input.append(then.tensor)
    op.input_desc.add().CopyFrom(then.desc)
    op.input_desc[-1].name = "then"
    op.input.append(else_changed_as_is_python_key.tensor)
    op.input_desc.add().CopyFrom(else_changed_as_is_python_key.desc)
    op.input_desc[-1].name = "else"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "result"
    result = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return result


# This api is auto-generated from IR SegmentMax
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_INDEX_NUMBER])
def SegmentMax(x: Tensor, segment_ids: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SegmentMax)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(segment_ids, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SegmentMax"
    op.name = next_unique_name(node_name, "SegmentMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SegmentSum
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def SegmentSum(x: Tensor, segment_ids: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SegmentSum)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(segment_ids, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SegmentSum"
    op.name = next_unique_name(node_name, "SegmentSum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SegmentMaxD
@auto_convert_to_tensor([False], [False])
def SegmentMaxD(x: Tensor, *, segment_ids: List[int], dependencies=[], node_name=None):
    """REG_OP(SegmentMaxD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.REQUIRED_ATTR(segment_ids, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SegmentMaxD"
    op.name = next_unique_name(node_name, "SegmentMaxD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["segment_ids"].list.val_type = 2
    op.attr["segment_ids"].list.i.extend(segment_ids)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR OneHot
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC, TensorType.TT_BASIC])
def OneHot(x: Tensor, depth: Tensor, on_value: Tensor, off_value: Tensor, *, axis: int=-1, dependencies=[], node_name=None):
    """REG_OP(OneHot)\n
.INPUT(x, TensorType({DT_UINT8, DT_INT32, DT_INT64}))\n
.INPUT(depth, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(on_value, TensorType::BasicType())\n
.INPUT(off_value, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(axis, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OneHot"
    op.name = next_unique_name(node_name, "OneHot")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(depth.tensor)
    op.input_desc.add().CopyFrom(depth.desc)
    op.input_desc[-1].name = "depth"
    op.input.append(on_value.tensor)
    op.input_desc.add().CopyFrom(on_value.desc)
    op.input_desc[-1].name = "on_value"
    op.input.append(off_value.tensor)
    op.input_desc.add().CopyFrom(off_value.desc)
    op.input_desc[-1].name = "off_value"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR OneHotD
@auto_convert_to_tensor([False, False, False], [False, False, False])
def OneHotD(x: Tensor, on_value: Tensor, off_value: Tensor, *, depth: int, axis: int=-1, dependencies=[], node_name=None):
    """REG_OP(OneHotD)\n
.INPUT(x, TensorType({DT_UINT8, DT_INT32}))\n
.INPUT(on_value, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT8, DT_INT8}))\n
.INPUT(off_value, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT8, DT_INT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT8, DT_INT8}))\n
.REQUIRED_ATTR(depth, Int)\n
.ATTR(axis, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "OneHotD"
    op.name = next_unique_name(node_name, "OneHotD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(on_value.tensor)
    op.input_desc.add().CopyFrom(on_value.desc)
    op.input_desc[-1].name = "on_value"
    op.input.append(off_value.tensor)
    op.input_desc.add().CopyFrom(off_value.desc)
    op.input_desc[-1].name = "off_value"

    # process attrs
    op.attr["depth"].i = depth
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Slice
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def Slice(x: Tensor, offsets: Tensor, size: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Slice)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(offsets, TensorType::IndexNumberType())\n
.INPUT(size, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Slice"
    op.name = next_unique_name(node_name, "Slice")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SliceD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def SliceD(x: Tensor, *, offsets: List[int], size: List[int], dependencies=[], node_name=None):
    """REG_OP(SliceD)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(offsets, ListInt)\n
.REQUIRED_ATTR(size, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SliceD"
    op.name = next_unique_name(node_name, "SliceD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["offsets"].list.val_type = 2
    op.attr["offsets"].list.i.extend(offsets)
    op.attr["size"].list.val_type = 2
    op.attr["size"].list.i.extend(size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SliceDV2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def SliceDV2(x: Tensor, offsets: Tensor, *, size: List[int], dependencies=[], node_name=None):
    """REG_OP(SliceDV2)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(offsets, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(size, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SliceDV2"
    op.name = next_unique_name(node_name, "SliceDV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(offsets.tensor)
    op.input_desc.add().CopyFrom(offsets.desc)
    op.input_desc[-1].name = "offsets"

    # process attrs
    op.attr["size"].list.val_type = 2
    op.attr["size"].list.i.extend(size)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TopKD
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_UNKNOWN])
def TopKD(x: Tensor, assist_seq: Tensor, *, k: int, sorted: bool=True, dim: int=-1, largest: bool=True, dependencies=[], node_name=None):
    """REG_OP(TopKD)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(assist_seq, TensorType({DT_FLOAT16}))\n
.OUTPUT(values, TensorType::RealNumberType())\n
.OUTPUT(indices, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(k, Int)\n
.ATTR(sorted, Bool, true)\n
.ATTR(dim, Int, -1)\n
.ATTR(largest, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TopKD"
    op.name = next_unique_name(node_name, "TopKD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(assist_seq.tensor)
    op.input_desc.add().CopyFrom(assist_seq.desc)
    op.input_desc[-1].name = "assist_seq"

    # process attrs
    op.attr["k"].i = k
    op.attr["sorted"].b = sorted
    op.attr["dim"].i = dim
    op.attr["largest"].b = largest

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values, indices


# This api is auto-generated from IR TopKV2D
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def TopKV2D(x: Tensor, k: Tensor, assist_seq: Tensor, *, sorted: bool=True, dim: int=-1, largest: bool=True, dependencies=[], node_name=None):
    """REG_OP(TopKV2D)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.INPUT(assist_seq, TensorType({DT_FLOAT16}))\n
.OUTPUT(values, TensorType::RealNumberType())\n
.OUTPUT(indices, TensorType({DT_INT32}))\n
.ATTR(sorted, Bool, true)\n
.ATTR(dim, Int, -1)\n
.ATTR(largest, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TopKV2D"
    op.name = next_unique_name(node_name, "TopKV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"
    op.input.append(assist_seq.tensor)
    op.input_desc.add().CopyFrom(assist_seq.desc)
    op.input_desc[-1].name = "assist_seq"

    # process attrs
    op.attr["sorted"].b = sorted
    op.attr["dim"].i = dim
    op.attr["largest"].b = largest

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values, indices


# This api is auto-generated from IR TopKV2
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_UNKNOWN])
def TopKV2(x: Tensor, k: Tensor, *, sorted: bool=True, dim: int=-1, largest: bool=True, dependencies=[], node_name=None):
    """REG_OP(TopKV2)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.OUTPUT(values, TensorType::RealNumberType())\n
.OUTPUT(indices, TensorType({DT_INT32}))\n
.ATTR(sorted, Bool, true)\n
.ATTR(dim, Int, -1)\n
.ATTR(largest, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TopKV2"
    op.name = next_unique_name(node_name, "TopKV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"

    # process attrs
    op.attr["sorted"].b = sorted
    op.attr["dim"].i = dim
    op.attr["largest"].b = largest

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values, indices


# This api is auto-generated from IR TopK
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_UNKNOWN])
def TopK(x: Tensor, k: Tensor, *, sorted: bool=True, largest: bool=True, dim: int=-1, dependencies=[], node_name=None):
    """REG_OP(TopK)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(k, TensorType({DT_INT32}))\n
.OUTPUT(values, TensorType::RealNumberType())\n
.OUTPUT(indices, TensorType({DT_INT32}))\n
.ATTR(sorted, Bool, true)\n
.ATTR(largest, Bool, true)\n
.ATTR(dim, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TopK"
    op.name = next_unique_name(node_name, "TopK")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"

    # process attrs
    op.attr["sorted"].b = sorted
    op.attr["largest"].b = largest
    op.attr["dim"].i = dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return values, indices


# This api is auto-generated from IR ScatterNd
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def ScatterNd(indices: Tensor, x: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ScatterNd)\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(shape, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNd"
    op.name = next_unique_name(node_name, "ScatterNd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScatterNdD
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def ScatterNdD(indices: Tensor, x: Tensor, *, shape: List[int], dependencies=[], node_name=None):
    """REG_OP(ScatterNdD)\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_UINT8}))\n
.REQUIRED_ATTR(shape, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNdD"
    op.name = next_unique_name(node_name, "ScatterNdD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InTopKD
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def InTopKD(x1: Tensor, x2: Tensor, *, k: int, dependencies=[], node_name=None):
    """REG_OP(InTopKD)\n
.INPUT(x1, TensorType({DT_FLOAT}))\n
.INPUT(x2, TensorType({IndexNumberType}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
.REQUIRED_ATTR(k, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InTopKD"
    op.name = next_unique_name(node_name, "InTopKD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["k"].i = k

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InTopK
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def InTopK(x1: Tensor, x2: Tensor, k: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InTopK)\n
.INPUT(x1, TensorType({DT_FLOAT}))\n
.INPUT(x2, TensorType(IndexNumberType))\n
.INPUT(k, TensorType({IndexNumberType}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InTopK"
    op.name = next_unique_name(node_name, "InTopK")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(k.tensor)
    op.input_desc.add().CopyFrom(k.desc)
    op.input_desc[-1].name = "k"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StridedSliceAssign
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def StridedSliceAssign(var: Tensor, begin: Tensor, end: Tensor, strides: Tensor, input_value: Tensor, *, begin_mask: int=0, end_mask: int=0, ellipsis_mask: int=0, new_axis_mask: int=0, shrink_axis_mask: int=0, dependencies=[], node_name=None):
    """REG_OP(StridedSliceAssign)\n
.INPUT(var, TensorType(BasicType))\n
.INPUT(begin, TensorType(IndexNumberType))\n
.INPUT(end, TensorType(IndexNumberType))\n
.INPUT(strides, TensorType(IndexNumberType))\n
.INPUT(input_value, TensorType(BasicType))\n
.OUTPUT(var, TensorType(BasicType))\n
.ATTR(begin_mask, Int, 0)\n
.ATTR(end_mask, Int, 0)\n
.ATTR(ellipsis_mask, Int, 0)\n
.ATTR(new_axis_mask, Int, 0)\n
.ATTR(shrink_axis_mask, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSliceAssign"
    op.name = next_unique_name(node_name, "StridedSliceAssign")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(begin.tensor)
    op.input_desc.add().CopyFrom(begin.desc)
    op.input_desc[-1].name = "begin"
    op.input.append(end.tensor)
    op.input_desc.add().CopyFrom(end.desc)
    op.input_desc[-1].name = "end"
    op.input.append(strides.tensor)
    op.input_desc.add().CopyFrom(strides.desc)
    op.input_desc[-1].name = "strides"
    op.input.append(input_value.tensor)
    op.input_desc.add().CopyFrom(input_value.desc)
    op.input_desc[-1].name = "input_value"

    # process attrs
    op.attr["begin_mask"].i = begin_mask
    op.attr["end_mask"].i = end_mask
    op.attr["ellipsis_mask"].i = ellipsis_mask
    op.attr["new_axis_mask"].i = new_axis_mask
    op.attr["shrink_axis_mask"].i = shrink_axis_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR StridedSliceAssignD
@auto_convert_to_tensor([False, False], [False, False])
def StridedSliceAssignD(var: Tensor, input_value: Tensor, *, begin: List[int], end: List[int], strides: List[int], begin_mask: int=0, end_mask: int=0, ellipsis_mask: int=0, new_axis_mask: int=0, shrink_axis_mask: int=0, dependencies=[], node_name=None):
    """REG_OP(StridedSliceAssignD)\n
.INPUT(var, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT16}))\n
.INPUT(input_value, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT16}))\n
.OUTPUT(var, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT16}))\n
.REQUIRED_ATTR(begin, ListInt)\n
.REQUIRED_ATTR(end, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.ATTR(begin_mask, Int, 0)\n
.ATTR(end_mask, Int, 0)\n
.ATTR(ellipsis_mask, Int, 0)\n
.ATTR(new_axis_mask, Int, 0)\n
.ATTR(shrink_axis_mask, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSliceAssignD"
    op.name = next_unique_name(node_name, "StridedSliceAssignD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(input_value.tensor)
    op.input_desc.add().CopyFrom(input_value.desc)
    op.input_desc[-1].name = "input_value"

    # process attrs
    op.attr["begin"].list.val_type = 2
    op.attr["begin"].list.i.extend(begin)
    op.attr["end"].list.val_type = 2
    op.attr["end"].list.i.extend(end)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["begin_mask"].i = begin_mask
    op.attr["end_mask"].i = end_mask
    op.attr["ellipsis_mask"].i = ellipsis_mask
    op.attr["new_axis_mask"].i = new_axis_mask
    op.attr["shrink_axis_mask"].i = shrink_axis_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR Gather
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def Gather(x: Tensor, indices: Tensor, *, validate_indices: bool=True, batch_dims: int=0, is_preprocessed: bool=False, negative_index_support: bool=False, dependencies=[], node_name=None):
    """REG_OP(Gather)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(validate_indices, Bool, true)\n
.ATTR(batch_dims, Int, 0)\n
.ATTR(is_preprocessed, Bool, false)\n
.ATTR(negative_index_support, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Gather"
    op.name = next_unique_name(node_name, "Gather")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs
    op.attr["validate_indices"].b = validate_indices
    op.attr["batch_dims"].i = batch_dims
    op.attr["is_preprocessed"].b = is_preprocessed
    op.attr["negative_index_support"].b = negative_index_support

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Cumprod
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def Cumprod(x: Tensor, axis: Tensor, *, exclusive: bool=False, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(Cumprod)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axis, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(exclusive, Bool, false)\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cumprod"
    op.name = next_unique_name(node_name, "Cumprod")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs
    op.attr["exclusive"].b = exclusive
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CumprodD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_NUMBER])
def CumprodD(x: Tensor, *, axis: int, exclusive: bool=False, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(CumprodD)\n
.INPUT(x, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.REQUIRED_ATTR(axis, Int)\n
.ATTR(exclusive, Bool, false)\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CumprodD"
    op.name = next_unique_name(node_name, "CumprodD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["exclusive"].b = exclusive
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Cumsum
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER])
def Cumsum(x: Tensor, axis: Tensor, *, exclusive: bool=False, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(Cumsum)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(axis, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.ATTR(exclusive, Bool, false)\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cumsum"
    op.name = next_unique_name(node_name, "Cumsum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs
    op.attr["exclusive"].b = exclusive
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CumsumD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_NUMBER])
def CumsumD(x: Tensor, *, axis: int, exclusive: bool=False, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(CumsumD)\n
.INPUT(x, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
.REQUIRED_ATTR(axis, Int)\n
.ATTR(exclusive, Bool, false)\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CumsumD"
    op.name = next_unique_name(node_name, "CumsumD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["exclusive"].b = exclusive
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceUpdate
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_BASIC])
def InplaceUpdate(x: Tensor, indices: Tensor, v: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InplaceUpdate)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(v, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceUpdate"
    op.name = next_unique_name(node_name, "InplaceUpdate")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceUpdateD
@auto_convert_to_tensor([False, False], [False, False])
def InplaceUpdateD(x: Tensor, v: Tensor, *, indices: List[int], dependencies=[], node_name=None):
    """REG_OP(InplaceUpdateD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(v, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.REQUIRED_ATTR(indices, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceUpdateD"
    op.name = next_unique_name(node_name, "InplaceUpdateD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"

    # process attrs
    op.attr["indices"].list.val_type = 2
    op.attr["indices"].list.i.extend(indices)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceAdd
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_BASIC])
def InplaceAdd(x: Tensor, indices: Tensor, v: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InplaceAdd)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(v, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceAdd"
    op.name = next_unique_name(node_name, "InplaceAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceAddD
@auto_convert_to_tensor([False, False], [False, False])
def InplaceAddD(x: Tensor, v: Tensor, *, indices: List[int], dependencies=[], node_name=None):
    """REG_OP(InplaceAddD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(v, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.REQUIRED_ATTR(indices, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceAddD"
    op.name = next_unique_name(node_name, "InplaceAddD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"

    # process attrs
    op.attr["indices"].list.val_type = 2
    op.attr["indices"].list.i.extend(indices)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceSub
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_BASIC])
def InplaceSub(x: Tensor, indices: Tensor, v: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(InplaceSub)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(indices, TensorType({DT_INT32}))\n
.INPUT(v, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceSub"
    op.name = next_unique_name(node_name, "InplaceSub")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceSubD
@auto_convert_to_tensor([False, False], [False, False])
def InplaceSubD(x: Tensor, v: Tensor, *, indices: List[int], dependencies=[], node_name=None):
    """REG_OP(InplaceSubD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.INPUT(v, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32}))\n
.REQUIRED_ATTR(indices, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceSubD"
    op.name = next_unique_name(node_name, "InplaceSubD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(v.tensor)
    op.input_desc.add().CopyFrom(v.desc)
    op.input_desc[-1].name = "v"

    # process attrs
    op.attr["indices"].list.val_type = 2
    op.attr["indices"].list.i.extend(indices)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ScatterNonAliasingAdd
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_NUMBER])
def ScatterNonAliasingAdd(x: Tensor, indices: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(ScatterNonAliasingAdd)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(indices, TensorType::IndexNumberType())\n
.INPUT(updates, TensorType::NumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScatterNonAliasingAdd"
    op.name = next_unique_name(node_name, "ScatterNonAliasingAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsortedSegmentMin
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def UnsortedSegmentMin(x: Tensor, segment_ids: Tensor, num_segments: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentMin)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(segment_ids, TensorType::IndexNumberType())\n
.INPUT(num_segments, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentMin"
    op.name = next_unique_name(node_name, "UnsortedSegmentMin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"
    op.input.append(num_segments.tensor)
    op.input_desc.add().CopyFrom(num_segments.desc)
    op.input_desc[-1].name = "num_segments"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsortedSegmentMinD
@auto_convert_to_tensor([False, False], [False, False])
def UnsortedSegmentMinD(x: Tensor, segment_ids: Tensor, *, num_segments: int, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentMinD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.INPUT(segment_ids, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.REQUIRED_ATTR(num_segments, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentMinD"
    op.name = next_unique_name(node_name, "UnsortedSegmentMinD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs
    op.attr["num_segments"].i = num_segments

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsortedSegmentMax
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def UnsortedSegmentMax(x: Tensor, segment_ids: Tensor, num_segments: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentMax)\n
.INPUT(x, TensorType::RealNumberType())\n
.INPUT(segment_ids, TensorType::IndexNumberType())\n
.INPUT(num_segments, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentMax"
    op.name = next_unique_name(node_name, "UnsortedSegmentMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"
    op.input.append(num_segments.tensor)
    op.input_desc.add().CopyFrom(num_segments.desc)
    op.input_desc[-1].name = "num_segments"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsortedSegmentMaxD
@auto_convert_to_tensor([False, False], [False, False])
def UnsortedSegmentMaxD(x: Tensor, segment_ids: Tensor, *, num_segments: int, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentMaxD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.INPUT(segment_ids, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.REQUIRED_ATTR(num_segments, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentMaxD"
    op.name = next_unique_name(node_name, "UnsortedSegmentMaxD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs
    op.attr["num_segments"].i = num_segments

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsortedSegmentProd
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def UnsortedSegmentProd(x: Tensor, segment_ids: Tensor, num_segments: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentProd)\n
.INPUT(x, TensorType::NumberType())\n
.INPUT(segment_ids, TensorType::IndexNumberType())\n
.INPUT(num_segments, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::NumberType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentProd"
    op.name = next_unique_name(node_name, "UnsortedSegmentProd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"
    op.input.append(num_segments.tensor)
    op.input_desc.add().CopyFrom(num_segments.desc)
    op.input_desc[-1].name = "num_segments"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR UnsortedSegmentProdD
@auto_convert_to_tensor([False, False], [False, False])
def UnsortedSegmentProdD(x: Tensor, segment_ids: Tensor, *, num_segments: int, dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentProdD)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.INPUT(segment_ids, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT32, DT_INT16}))\n
.REQUIRED_ATTR(num_segments, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentProdD"
    op.name = next_unique_name(node_name, "UnsortedSegmentProdD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"

    # process attrs
    op.attr["num_segments"].i = num_segments

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Proposal
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Proposal(cls_prob: Tensor, bbox_delta: Tensor, im_info: Tensor, *, feat_stride: float=16.000000, base_size: float=16.000000, min_size: float=16.000000, ratio: List[float]=[0.500000, 1.000000, 2.000000], scale: List[float]=[8.000000, 16.000000, 32.000000], pre_nms_topn: int=3000, post_nms_topn: int=304, iou_threshold: float=0.700000, output_actual_rois_num: bool=False, dependencies=[], node_name=None):
    """REG_OP(Proposal)\n
.INPUT(cls_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bbox_delta, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(im_info, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(actual_rois_num, TensorType({DT_INT32}))\n
.ATTR(feat_stride, Float, 16)\n
.ATTR(base_size, Float, 16)\n
.ATTR(min_size, Float, 16)\n
.ATTR(ratio, ListFloat, {0.5, 1, 2})\n
.ATTR(scale, ListFloat, {8, 16, 32})\n
.ATTR(pre_nms_topn, Int, 3000)\n
.ATTR(post_nms_topn, Int, 304)\n
.ATTR(iou_threshold, Float, 0.7)\n
.ATTR(output_actual_rois_num, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Proposal"
    op.name = next_unique_name(node_name, "Proposal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(cls_prob.tensor)
    op.input_desc.add().CopyFrom(cls_prob.desc)
    op.input_desc[-1].name = "cls_prob"
    op.input.append(bbox_delta.tensor)
    op.input_desc.add().CopyFrom(bbox_delta.desc)
    op.input_desc[-1].name = "bbox_delta"
    op.input.append(im_info.tensor)
    op.input_desc.add().CopyFrom(im_info.desc)
    op.input_desc[-1].name = "im_info"

    # process attrs
    op.attr["feat_stride"].f = feat_stride
    op.attr["base_size"].f = base_size
    op.attr["min_size"].f = min_size
    op.attr["ratio"].list.val_type = 3
    op.attr["ratio"].list.f.extend(ratio)
    op.attr["scale"].list.val_type = 3
    op.attr["scale"].list.f.extend(scale)
    op.attr["pre_nms_topn"].i = pre_nms_topn
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["output_actual_rois_num"].b = output_actual_rois_num

    # process outputs
    output_index = 0
    op.output_desc.add().name = "rois"
    rois = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "actual_rois_num"
    actual_rois_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return rois, actual_rois_num


# This api is auto-generated from IR ProposalD
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def ProposalD(cls_prob: Tensor, bbox_delta: Tensor, im_info: Tensor, rpn_bbox: Tensor, *, feat_stride: float=16.000000, base_size: float=16.000000, min_size: float=16.000000, ratio: List[float]=[0.500000, 1.000000, 2.000000], scale: List[float]=[8.000000, 16.000000, 32.000000], pre_nms_topn: int=3000, post_nms_topn: int=304, iou_threshold: float=0.700000, output_actual_rois_num: bool=False, dependencies=[], node_name=None):
    """REG_OP(ProposalD)\n
.INPUT(cls_prob, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bbox_delta, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(im_info, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(rpn_bbox, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(rois, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(actual_rois_num, TensorType({DT_INT32}))\n
.ATTR(feat_stride, Float, 16)\n
.ATTR(base_size, Float, 16)\n
.ATTR(min_size, Float, 16)\n
.ATTR(ratio, ListFloat, {0.5, 1, 2})\n
.ATTR(scale, ListFloat, {8, 16, 32})\n
.ATTR(pre_nms_topn, Int, 3000)\n
.ATTR(post_nms_topn, Int, 304)\n
.ATTR(iou_threshold, Float, 0.7)\n
.ATTR(output_actual_rois_num, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ProposalD"
    op.name = next_unique_name(node_name, "ProposalD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(cls_prob.tensor)
    op.input_desc.add().CopyFrom(cls_prob.desc)
    op.input_desc[-1].name = "cls_prob"
    op.input.append(bbox_delta.tensor)
    op.input_desc.add().CopyFrom(bbox_delta.desc)
    op.input_desc[-1].name = "bbox_delta"
    op.input.append(im_info.tensor)
    op.input_desc.add().CopyFrom(im_info.desc)
    op.input_desc[-1].name = "im_info"
    op.input.append(rpn_bbox.tensor)
    op.input_desc.add().CopyFrom(rpn_bbox.desc)
    op.input_desc[-1].name = "rpn_bbox"

    # process attrs
    op.attr["feat_stride"].f = feat_stride
    op.attr["base_size"].f = base_size
    op.attr["min_size"].f = min_size
    op.attr["ratio"].list.val_type = 3
    op.attr["ratio"].list.f.extend(ratio)
    op.attr["scale"].list.val_type = 3
    op.attr["scale"].list.f.extend(scale)
    op.attr["pre_nms_topn"].i = pre_nms_topn
    op.attr["post_nms_topn"].i = post_nms_topn
    op.attr["iou_threshold"].f = iou_threshold
    op.attr["output_actual_rois_num"].b = output_actual_rois_num

    # process outputs
    output_index = 0
    op.output_desc.add().name = "rois"
    rois = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "actual_rois_num"
    actual_rois_num = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return rois, actual_rois_num


# This api is auto-generated from IR PassThrough
@auto_convert_to_tensor([False], [False])
def PassThrough(x: Tensor, *, stride: int=2, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(PassThrough)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.ATTR(stride, Int, 2)\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "PassThrough"
    op.name = next_unique_name(node_name, "PassThrough")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["stride"].i = stride
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Crop
@auto_convert_to_tensor([False, False], [False, False])
def Crop(x: Tensor, size: Tensor, *, offsets: List[int], axis: int=2, dependencies=[], node_name=None):
    """REG_OP(Crop)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.INPUT(size, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.ATTR(axis, Int, 2)\n
.REQUIRED_ATTR(offsets, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Crop"
    op.name = next_unique_name(node_name, "Crop")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs
    op.attr["offsets"].list.val_type = 2
    op.attr["offsets"].list.i.extend(offsets)
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Cummin
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def Cummin(x: Tensor, *, axis: int, dependencies=[], node_name=None):
    """REG_OP(Cummin)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.OUTPUT(indices, TensorType::BasicType())\n
.REQUIRED_ATTR(axis, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cummin"
    op.name = next_unique_name(node_name, "Cummin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, indices


# This api is auto-generated from IR Cummax
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def Cummax(x: Tensor, *, dim: int, dependencies=[], node_name=None):
    """REG_OP(Cummax)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.OUTPUT(indices, TensorType::BasicType())\n
.REQUIRED_ATTR(dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cummax"
    op.name = next_unique_name(node_name, "Cummax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["dim"].i = dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, indices


# This api is auto-generated from IR TileWithAxis
@auto_convert_to_tensor([False], [False])
def TileWithAxis(x: Tensor, *, tiles: int, axis: int=1, dependencies=[], node_name=None):
    """REG_OP(TileWithAxis)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT64, DT_INT32, DT_INT16, DT_INT8, DT_UINT64, DT_UINT32, DT_UINT16, DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT64, DT_INT32, DT_INT16, DT_INT8, DT_UINT64, DT_UINT32, DT_UINT16, DT_UINT8}))\n
.ATTR(axis, Int, 1)\n
.REQUIRED_ATTR(tiles, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TileWithAxis"
    op.name = next_unique_name(node_name, "TileWithAxis")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["tiles"].i = tiles
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ReadSelect
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def ReadSelect(x: Tensor, *, stride_list: List[int]=[1, 1, 1, 1, 1], dependencies=[], node_name=None):
    """REG_OP(ReadSelect)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(stride_list, ListInt, {1,1,1,1,1})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ReadSelect"
    op.name = next_unique_name(node_name, "ReadSelect")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["stride_list"].list.val_type = 2
    op.attr["stride_list"].list.i.extend(stride_list)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR WriteSelect
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def WriteSelect(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(WriteSelect)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "WriteSelect"
    op.name = next_unique_name(node_name, "WriteSelect")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StridedRead
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def StridedRead(x: Tensor, *, axis: int=1, stride: int=1, dependencies=[], node_name=None):
    """REG_OP(StridedRead)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(axis, Int, 1)\n
.ATTR(stride, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedRead"
    op.name = next_unique_name(node_name, "StridedRead")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["stride"].i = stride

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StridedWrite
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def StridedWrite(x: Tensor, *, axis: int=1, stride: int=1, dependencies=[], node_name=None):
    """REG_OP(StridedWrite)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(axis, Int, 1)\n
.ATTR(stride, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedWrite"
    op.name = next_unique_name(node_name, "StridedWrite")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["stride"].i = stride

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CumulativeLogsumexp
@auto_convert_to_tensor([False, False], [False, False])
def CumulativeLogsumexp(x: Tensor, axis: Tensor, *, exclusive: bool=False, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(CumulativeLogsumexp)\n
.INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.INPUT(axis, TensorType({DT_INT32, DT_INT16}))\n
.OUTPUT(y, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.ATTR(exclusive, Bool, false)\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CumulativeLogsumexp"
    op.name = next_unique_name(node_name, "CumulativeLogsumexp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(axis.tensor)
    op.input_desc.add().CopyFrom(axis.desc)
    op.input_desc[-1].name = "axis"

    # process attrs
    op.attr["exclusive"].b = exclusive
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CumulativeLogsumexpD
@auto_convert_to_tensor([False], [False])
def CumulativeLogsumexpD(x: Tensor, *, axis: int, exclusive: bool=False, reverse: bool=False, dependencies=[], node_name=None):
    """REG_OP(CumulativeLogsumexpD)\n
.INPUT(x, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(axis, Int)\n
.ATTR(exclusive, Bool, false)\n
.ATTR(reverse, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CumulativeLogsumexpD"
    op.name = next_unique_name(node_name, "CumulativeLogsumexpD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["exclusive"].b = exclusive
    op.attr["reverse"].b = reverse

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceIndexAdd
@auto_convert_to_tensor([False, False, False, False], [False, False, False, True], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def InplaceIndexAdd(var: Tensor, indices: Tensor, updates: Tensor, alpha: Optional[Tensor], *, axis: int, dependencies=[], node_name=None):
    """REG_OP(InplaceIndexAdd)\n
.INPUT(var, TensorType({DT_INT16, DT_INT32, DT_INT8, DT_UINT8, DT_FLOAT32, DT_FLOAT16, DT_DOUBLE, DT_INT64, DT_BOOL}))\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(updates, TensorType({DT_INT16, DT_INT32, DT_INT8, DT_UINT8, DT_FLOAT32, DT_FLOAT16, DT_DOUBLE, DT_INT64, DT_BOOL}))\n
.OPTIONAL_INPUT(alpha, TensorType({DT_INT16, DT_INT32, DT_INT8, DT_UINT8, DT_FLOAT32, DT_FLOAT16, DT_DOUBLE, DT_INT64, DT_BOOL}))\n
.OUTPUT(var, TensorType({DT_INT16, DT_INT32, DT_INT8, DT_UINT8, DT_FLOAT32, DT_FLOAT16, DT_DOUBLE, DT_INT64, DT_BOOL}))\n
.REQUIRED_ATTR(axis, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceIndexAdd"
    op.name = next_unique_name(node_name, "InplaceIndexAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(var.tensor)
    op.input_desc.add().CopyFrom(var.desc)
    op.input_desc[-1].name = "var"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"
    if alpha is not None:
        op.input.append(alpha.tensor)
        op.input_desc.add().CopyFrom(alpha.desc)
        op.input_desc[-1].name = "alpha"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "alpha"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "var"
    var = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return var


# This api is auto-generated from IR MaskedFill
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MaskedFill(x: Tensor, mask: Tensor, value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MaskedFill)\n
.INPUT(x, TensorType({DT_FLOAT, DT_BF16, DT_FLOAT16, DT_INT8, DT_INT32, DT_INT64}))\n
.INPUT(mask, TensorType({DT_BOOL}))\n
.INPUT(value, TensorType({DT_FLOAT, DT_BF16, DT_FLOAT16, DT_INT8, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_BF16, DT_FLOAT16, DT_INT8, DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaskedFill"
    op.name = next_unique_name(node_name, "MaskedFill")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaskedSelectV2
@auto_convert_to_tensor([False, False], [False, False])
def MaskedSelectV2(x: Tensor, mask: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MaskedSelectV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(mask, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaskedSelectV2"
    op.name = next_unique_name(node_name, "MaskedSelectV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaskedScatter
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MaskedScatter(x: Tensor, mask: Tensor, updates: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MaskedScatter)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_BOOL}))\n
.INPUT(mask, TensorType({DT_BOOL}))\n
.INPUT(updates, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaskedScatter"
    op.name = next_unique_name(node_name, "MaskedScatter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"
    op.input.append(updates.tensor)
    op.input_desc.add().CopyFrom(updates.desc)
    op.input_desc[-1].name = "updates"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SliceLastDim
@auto_convert_to_tensor([False], [False])
def SliceLastDim(x: Tensor, *, start: int, end: int, stride: int=1, dependencies=[], node_name=None):
    """REG_OP(SliceLastDim)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(start, Int)\n
.REQUIRED_ATTR(end, Int)\n
.ATTR(stride, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SliceLastDim"
    op.name = next_unique_name(node_name, "SliceLastDim")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["start"].i = start
    op.attr["end"].i = end
    op.attr["stride"].i = stride

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StridedSliceV2
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def StridedSliceV2(x: Tensor, begin: Tensor, end: Tensor, axes: Optional[Tensor], strides: Optional[Tensor], *, begin_mask: int=0, end_mask: int=0, ellipsis_mask: int=0, new_axis_mask: int=0, shrink_axis_mask: int=0, dependencies=[], node_name=None):
    """REG_OP(StridedSliceV2)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(begin, TensorType::IndexNumberType())\n
.INPUT(end, TensorType::IndexNumberType())\n
.OPTIONAL_INPUT(axes, TensorType::IndexNumberType())\n
.OPTIONAL_INPUT(strides, TensorType::IndexNumberType())\n
.ATTR(begin_mask, Int, 0)\n
.ATTR(end_mask, Int, 0)\n
.ATTR(ellipsis_mask, Int, 0)\n
.ATTR(new_axis_mask, Int, 0)\n
.ATTR(shrink_axis_mask, Int, 0)\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSliceV2"
    op.name = next_unique_name(node_name, "StridedSliceV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(begin.tensor)
    op.input_desc.add().CopyFrom(begin.desc)
    op.input_desc[-1].name = "begin"
    op.input.append(end.tensor)
    op.input_desc.add().CopyFrom(end.desc)
    op.input_desc[-1].name = "end"
    if axes is not None:
        op.input.append(axes.tensor)
        op.input_desc.add().CopyFrom(axes.desc)
        op.input_desc[-1].name = "axes"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "axes"
    if strides is not None:
        op.input.append(strides.tensor)
        op.input_desc.add().CopyFrom(strides.desc)
        op.input_desc[-1].name = "strides"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "strides"

    # process attrs
    op.attr["begin_mask"].i = begin_mask
    op.attr["end_mask"].i = end_mask
    op.attr["ellipsis_mask"].i = ellipsis_mask
    op.attr["new_axis_mask"].i = new_axis_mask
    op.attr["shrink_axis_mask"].i = shrink_axis_mask

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IndexFillD
@auto_convert_to_tensor([False, False, False], [False, False, False])
def IndexFillD(x: Tensor, assist1: Tensor, assist2: Tensor, *, dim: int, dependencies=[], node_name=None):
    """REG_OP(IndexFillD)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.INPUT(assist1, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.INPUT(assist2, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.REQUIRED_ATTR(dim, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IndexFillD"
    op.name = next_unique_name(node_name, "IndexFillD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(assist1.tensor)
    op.input_desc.add().CopyFrom(assist1.desc)
    op.input_desc[-1].name = "assist1"
    op.input.append(assist2.tensor)
    op.input_desc.add().CopyFrom(assist2.desc)
    op.input_desc[-1].name = "assist2"

    # process attrs
    op.attr["dim"].i = dim

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AddRowRanges
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AddRowRanges(x: Tensor, src: Tensor, indices: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AddRowRanges)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(src, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(indices, TensorType({DT_INT32}))\n
.OUTPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AddRowRanges"
    op.name = next_unique_name(node_name, "AddRowRanges")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(src.tensor)
    op.input_desc.add().CopyFrom(src.desc)
    op.input_desc[-1].name = "src"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x"
    x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x


# This api is auto-generated from IR MaskedFillRange
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def MaskedFillRange(x: Tensor, start: Tensor, end: Tensor, value: Tensor, *, axis: int, dependencies=[], node_name=None):
    """REG_OP(MaskedFillRange)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32}))\n
.INPUT(start, TensorType({DT_INT32}))\n
.INPUT(end, TensorType({DT_INT32}))\n
.INPUT(value, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32}))\n
.REQUIRED_ATTR(axis, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaskedFillRange"
    op.name = next_unique_name(node_name, "MaskedFillRange")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(start.tensor)
    op.input_desc.add().CopyFrom(start.desc)
    op.input_desc[-1].name = "start"
    op.input.append(end.tensor)
    op.input_desc.add().CopyFrom(end.desc)
    op.input_desc[-1].name = "end"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR InplaceTopKDistance
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def InplaceTopKDistance(topk_pq_distance: Tensor, topk_pq_index: Tensor, topk_pq_ivf: Tensor, pq_distance: Tensor, pq_index: Tensor, pq_ivf: Tensor, *, order: str="asc", dependencies=[], node_name=None):
    """REG_OP(InplaceTopKDistance)\n
.INPUT(topk_pq_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(topk_pq_index, TensorType({DT_INT32}))\n
.INPUT(topk_pq_ivf, TensorType({DT_INT32}))\n
.INPUT(pq_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(pq_index, TensorType({DT_INT32}))\n
.INPUT(pq_ivf, TensorType({DT_INT32}))\n
.ATTR(order, String, "asc")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "InplaceTopKDistance"
    op.name = next_unique_name(node_name, "InplaceTopKDistance")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(topk_pq_distance.tensor)
    op.input_desc.add().CopyFrom(topk_pq_distance.desc)
    op.input_desc[-1].name = "topk_pq_distance"
    op.input.append(topk_pq_index.tensor)
    op.input_desc.add().CopyFrom(topk_pq_index.desc)
    op.input_desc[-1].name = "topk_pq_index"
    op.input.append(topk_pq_ivf.tensor)
    op.input_desc.add().CopyFrom(topk_pq_ivf.desc)
    op.input_desc[-1].name = "topk_pq_ivf"
    op.input.append(pq_distance.tensor)
    op.input_desc.add().CopyFrom(pq_distance.desc)
    op.input_desc[-1].name = "pq_distance"
    op.input.append(pq_index.tensor)
    op.input_desc.add().CopyFrom(pq_index.desc)
    op.input_desc[-1].name = "pq_index"
    op.input.append(pq_ivf.tensor)
    op.input_desc.add().CopyFrom(pq_ivf.desc)
    op.input_desc[-1].name = "pq_ivf"

    # process attrs
    op.attr["order"].s = compat_as_bytes(order)

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR TopKPQDistanceMerge
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TopKPQDistanceMerge(sorted_distance: Tensor, pq_ivf: Tensor, pq_index: Tensor, *, k: int, dependencies=[], node_name=None):
    """REG_OP(TopKPQDistanceMerge)\n
.INPUT(sorted_distance, TensorType({DT_FLOAT16}))\n
.INPUT(pq_ivf, TensorType({DT_INT32}))\n
.INPUT(pq_index, TensorType({DT_INT32}))\n
.OUTPUT(topk_distance, TensorType({DT_FLOAT16}))\n
.OUTPUT(topk_ivf, TensorType({DT_INT32}))\n
.OUTPUT(topk_index, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(k, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TopKPQDistanceMerge"
    op.name = next_unique_name(node_name, "TopKPQDistanceMerge")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(sorted_distance.tensor)
    op.input_desc.add().CopyFrom(sorted_distance.desc)
    op.input_desc[-1].name = "sorted_distance"
    op.input.append(pq_ivf.tensor)
    op.input_desc.add().CopyFrom(pq_ivf.desc)
    op.input_desc[-1].name = "pq_ivf"
    op.input.append(pq_index.tensor)
    op.input_desc.add().CopyFrom(pq_index.desc)
    op.input_desc[-1].name = "pq_index"

    # process attrs
    op.attr["k"].i = k

    # process outputs
    output_index = 0
    op.output_desc.add().name = "topk_distance"
    topk_distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "topk_ivf"
    topk_ivf = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "topk_index"
    topk_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return topk_distance, topk_ivf, topk_index


# This api is auto-generated from IR StridedSliceV3
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, True, True], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def StridedSliceV3(x: Tensor, begin: Tensor, end: Tensor, axes: Optional[Tensor], strides: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(StridedSliceV3)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(begin, TensorType::IndexNumberType())\n
.INPUT(end, TensorType::IndexNumberType())\n
.OPTIONAL_INPUT(axes, TensorType::IndexNumberType())\n
.OPTIONAL_INPUT(strides, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StridedSliceV3"
    op.name = next_unique_name(node_name, "StridedSliceV3")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(begin.tensor)
    op.input_desc.add().CopyFrom(begin.desc)
    op.input_desc[-1].name = "begin"
    op.input.append(end.tensor)
    op.input_desc.add().CopyFrom(end.desc)
    op.input_desc[-1].name = "end"
    if axes is not None:
        op.input.append(axes.tensor)
        op.input_desc.add().CopyFrom(axes.desc)
        op.input_desc[-1].name = "axes"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "axes"
    if strides is not None:
        op.input.append(strides.tensor)
        op.input_desc.add().CopyFrom(strides.desc)
        op.input_desc[-1].name = "strides"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "strides"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MovingSumWithSigmoid
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_UNKNOWN])
def MovingSumWithSigmoid(alpha: Tensor, energy: Tensor, offset: Tensor, *, ksize: int, dependencies=[], node_name=None):
    """REG_OP(MovingSumWithSigmoid)\n
.INPUT(alpha, TensorType::BasicType())\n
.INPUT(energy, TensorType::BasicType())\n
.INPUT(offset, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(ksize, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MovingSumWithSigmoid"
    op.name = next_unique_name(node_name, "MovingSumWithSigmoid")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"
    op.input.append(energy.tensor)
    op.input_desc.add().CopyFrom(energy.desc)
    op.input_desc[-1].name = "energy"
    op.input.append(offset.tensor)
    op.input_desc.add().CopyFrom(offset.desc)
    op.input_desc[-1].name = "offset"

    # process attrs
    op.attr["ksize"].i = ksize

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MaskedSelect
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_UNKNOWN])
def MaskedSelect(x: Tensor, mask: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(MaskedSelect)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(mask, TensorType({DT_BOOL}))\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "MaskedSelect"
    op.name = next_unique_name(node_name, "MaskedSelect")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(mask.tensor)
    op.input_desc.add().CopyFrom(mask.desc)
    op.input_desc[-1].name = "mask"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DynSeqOuter
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_BASIC, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def DynSeqOuter(x1: Tensor, x2: Tensor, seq_len1: Tensor, seq_len2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DynSeqOuter)\n
.INPUT(x1, TensorType::BasicType())\n
.INPUT(x2, TensorType::BasicType())\n
.INPUT(seq_len1, TensorType({DT_INT32}))\n
.INPUT(seq_len2, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DynSeqOuter"
    op.name = next_unique_name(node_name, "DynSeqOuter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(seq_len1.tensor)
    op.input_desc.add().CopyFrom(seq_len1.desc)
    op.input_desc[-1].name = "seq_len1"
    op.input.append(seq_len2.tensor)
    op.input_desc.add().CopyFrom(seq_len2.desc)
    op.input_desc[-1].name = "seq_len2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NonMaxSuppressionBucketize
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def NonMaxSuppressionBucketize(input_nmsed_boxes: Tensor, input_nmsed_score: Tensor, input_nmsed_class: Tensor, input_nmsed_num: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(NonMaxSuppressionBucketize)\n
.INPUT(input_nmsed_boxes, TensorType({DT_FLOAT16}))\n
.INPUT(input_nmsed_score, TensorType({DT_FLOAT16}))\n
.INPUT(input_nmsed_class, TensorType({DT_FLOAT16}))\n
.INPUT(input_nmsed_num, TensorType({DT_INT32}))\n
.OUTPUT(output_nmsed_boxes, TensorType({DT_FLOAT}))\n
.OUTPUT(output_nmsed_score, TensorType({DT_FLOAT}))\n
.OUTPUT(output_nmsed_class, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonMaxSuppressionBucketize"
    op.name = next_unique_name(node_name, "NonMaxSuppressionBucketize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_nmsed_boxes.tensor)
    op.input_desc.add().CopyFrom(input_nmsed_boxes.desc)
    op.input_desc[-1].name = "input_nmsed_boxes"
    op.input.append(input_nmsed_score.tensor)
    op.input_desc.add().CopyFrom(input_nmsed_score.desc)
    op.input_desc[-1].name = "input_nmsed_score"
    op.input.append(input_nmsed_class.tensor)
    op.input_desc.add().CopyFrom(input_nmsed_class.desc)
    op.input_desc[-1].name = "input_nmsed_class"
    op.input.append(input_nmsed_num.tensor)
    op.input_desc.add().CopyFrom(input_nmsed_num.desc)
    op.input_desc[-1].name = "input_nmsed_num"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_nmsed_boxes"
    output_nmsed_boxes = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_nmsed_score"
    output_nmsed_score = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_nmsed_class"
    output_nmsed_class = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_nmsed_boxes, output_nmsed_score, output_nmsed_class


# This api is auto-generated from IR SearchSorted
@auto_convert_to_tensor([False, False, False], [False, False, True])
def SearchSorted(sorted_sequence: Tensor, values: Tensor, sorter: Optional[Tensor], *, dtype: int=DataType.DT_INT64, right: bool=False, dependencies=[], node_name=None):
    """REG_OP(SearchSorted)\n
.INPUT(sorted_sequence, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT16, DT_INT8, DT_UINT8, DT_INT32, DT_INT64}))\n
.INPUT(values, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT16, DT_INT8, DT_UINT8, DT_INT32, DT_INT64}))\n
.OPTIONAL_INPUT(sorter, TensorType({DT_INT64}))\n
.OUTPUT(out, TensorType(DT_INT32, DT_INT64))\n
.ATTR(dtype, Type, DT_INT64)\n
.ATTR(right, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SearchSorted"
    op.name = next_unique_name(node_name, "SearchSorted")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(sorted_sequence.tensor)
    op.input_desc.add().CopyFrom(sorted_sequence.desc)
    op.input_desc[-1].name = "sorted_sequence"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    if sorter is not None:
        op.input.append(sorter.tensor)
        op.input_desc.add().CopyFrom(sorter.desc)
        op.input_desc[-1].name = "sorter"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "sorter"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["right"].b = right

    # process outputs
    output_index = 0
    op.output_desc.add().name = "out"
    out = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return out


# This api is auto-generated from IR RepeatInterleave
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def RepeatInterleave(x: Tensor, repeats: Tensor, *, axis: int=1000, dependencies=[], node_name=None):
    """REG_OP(RepeatInterleave)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(repeats, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(axis, Int, 1000)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RepeatInterleave"
    op.name = next_unique_name(node_name, "RepeatInterleave")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(repeats.tensor)
    op.input_desc.add().CopyFrom(repeats.desc)
    op.input_desc[-1].name = "repeats"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DenseToDenseSetOperation
@auto_convert_to_tensor([False, False], [False, False])
def DenseToDenseSetOperation(x1: Tensor, x2: Tensor, *, set_operation: str="", validate_indices: bool=True, dependencies=[], node_name=None):
    """REG_OP(DenseToDenseSetOperation)\n
.INPUT(x1, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(x2, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(y_shape, TensorType({DT_INT64}))\n
.ATTR(set_operation, String, "")\n
.ATTR(validate_indices, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DenseToDenseSetOperation"
    op.name = next_unique_name(node_name, "DenseToDenseSetOperation")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["set_operation"].s = compat_as_bytes(set_operation)
    op.attr["validate_indices"].b = validate_indices

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR DenseToSparseSetOperation
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def DenseToSparseSetOperation(x1: Tensor, x2_indices: Tensor, x2_values: Tensor, x2_shape: Tensor, *, set_operation: str="", validate_indices: bool=True, dependencies=[], node_name=None):
    """REG_OP(DenseToSparseSetOperation)\n
.INPUT(x1, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(x2_indices, TensorType({DT_INT64}))\n
.INPUT(x2_values, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(x2_shape, TensorType({DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(y_shape, TensorType({DT_INT64}))\n
.ATTR(set_operation, String, "")\n
.ATTR(validate_indices, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DenseToSparseSetOperation"
    op.name = next_unique_name(node_name, "DenseToSparseSetOperation")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2_indices.tensor)
    op.input_desc.add().CopyFrom(x2_indices.desc)
    op.input_desc[-1].name = "x2_indices"
    op.input.append(x2_values.tensor)
    op.input_desc.add().CopyFrom(x2_values.desc)
    op.input_desc[-1].name = "x2_values"
    op.input.append(x2_shape.tensor)
    op.input_desc.add().CopyFrom(x2_shape.desc)
    op.input_desc[-1].name = "x2_shape"

    # process attrs
    op.attr["set_operation"].s = compat_as_bytes(set_operation)
    op.attr["validate_indices"].b = validate_indices

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR SparseToSparseSetOperation
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def SparseToSparseSetOperation(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2_indices: Tensor, x2_values: Tensor, x2_shape: Tensor, *, set_operation: str="", validate_indices: bool=True, dependencies=[], node_name=None):
    """REG_OP(SparseToSparseSetOperation)\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2_indices, TensorType({DT_INT64}))\n
.INPUT(x2_values, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(x2_shape, TensorType({DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(y_shape, TensorType({DT_INT64}))\n
.ATTR(set_operation, String, "")\n
.ATTR(validate_indices, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseToSparseSetOperation"
    op.name = next_unique_name(node_name, "SparseToSparseSetOperation")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2_indices.tensor)
    op.input_desc.add().CopyFrom(x2_indices.desc)
    op.input_desc[-1].name = "x2_indices"
    op.input.append(x2_values.tensor)
    op.input_desc.add().CopyFrom(x2_values.desc)
    op.input_desc[-1].name = "x2_values"
    op.input.append(x2_shape.tensor)
    op.input_desc.add().CopyFrom(x2_shape.desc)
    op.input_desc[-1].name = "x2_shape"

    # process attrs
    op.attr["set_operation"].s = compat_as_bytes(set_operation)
    op.attr["validate_indices"].b = validate_indices

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR SetSize
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SetSize(set_indices: Tensor, set_values: Tensor, set_shape: Tensor, *, validate_indices: bool=True, dependencies=[], node_name=None):
    """REG_OP(SetSize)\n
.INPUT(set_indices, TensorType({DT_INT64}))\n
.INPUT(set_values, TensorType({DT_INT8, DT_INT16, DT_UINT8, DT_UINT16, DT_INT32, DT_INT64, DT_STRING}))\n
.INPUT(set_shape, TensorType({DT_INT64}))\n
.OUTPUT(size, TensorType({DT_INT32}))\n
.ATTR(validate_indices, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SetSize"
    op.name = next_unique_name(node_name, "SetSize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(set_indices.tensor)
    op.input_desc.add().CopyFrom(set_indices.desc)
    op.input_desc[-1].name = "set_indices"
    op.input.append(set_values.tensor)
    op.input_desc.add().CopyFrom(set_values.desc)
    op.input_desc[-1].name = "set_values"
    op.input.append(set_shape.tensor)
    op.input_desc.add().CopyFrom(set_shape.desc)
    op.input_desc[-1].name = "set_shape"

    # process attrs
    op.attr["validate_indices"].b = validate_indices

    # process outputs
    output_index = 0
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return size


# This api is auto-generated from IR SparseSoftmax
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SparseSoftmax(indices: Tensor, values: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSoftmax)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_FLOAT, DT_DOUBLE}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSoftmax"
    op.name = next_unique_name(node_name, "SparseSoftmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseTensorDenseAdd
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def SparseTensorDenseAdd(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseTensorDenseAdd)\n
.INPUT(x1_indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x1_shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(x2, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseTensorDenseAdd"
    op.name = next_unique_name(node_name, "SparseTensorDenseAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseReorder
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SparseReorder(indices: Tensor, values: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseReorder)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseReorder"
    op.name = next_unique_name(node_name, "SparseReorder")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values


# This api is auto-generated from IR SparseReshape
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def SparseReshape(indices: Tensor, shape: Tensor, new_shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseReshape)\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(new_shape, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y_shape, TensorType({DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseReshape"
    op.name = next_unique_name(node_name, "SparseReshape")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(new_shape.tensor)
    op.input_desc.add().CopyFrom(new_shape.desc)
    op.input_desc[-1].name = "new_shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_shape


# This api is auto-generated from IR SparseDenseCwiseAdd
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseDenseCwiseAdd(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseDenseCwiseAdd)\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseDenseCwiseAdd"
    op.name = next_unique_name(node_name, "SparseDenseCwiseAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseDenseCwiseDiv
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseDenseCwiseDiv(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseDenseCwiseDiv)\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseDenseCwiseDiv"
    op.name = next_unique_name(node_name, "SparseDenseCwiseDiv")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseDenseCwiseMul
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseDenseCwiseMul(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseDenseCwiseMul)\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseDenseCwiseMul"
    op.name = next_unique_name(node_name, "SparseDenseCwiseMul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AddSparseToTensorsMap
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AddSparseToTensorsMap(indices: Tensor, values: Tensor, shape: Tensor, *, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(AddSparseToTensorsMap)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.OUTPUT(handle, TensorType({DT_INT64}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AddSparseToTensorsMap"
    op.name = next_unique_name(node_name, "AddSparseToTensorsMap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR SparseSliceGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseSliceGrad(backprop_val_grad: Tensor, indices: Tensor, start: Tensor, new_indices: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSliceGrad)\n
.INPUT(backprop_val_grad, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(start, TensorType({DT_INT64}))\n
.INPUT(new_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_grad, TensorType({ DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128 }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSliceGrad"
    op.name = next_unique_name(node_name, "SparseSliceGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(backprop_val_grad.tensor)
    op.input_desc.add().CopyFrom(backprop_val_grad.desc)
    op.input_desc[-1].name = "backprop_val_grad"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(start.tensor)
    op.input_desc.add().CopyFrom(start.desc)
    op.input_desc[-1].name = "start"
    op.input.append(new_indices.tensor)
    op.input_desc.add().CopyFrom(new_indices.desc)
    op.input_desc[-1].name = "new_indices"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_grad"
    y_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_grad


# This api is auto-generated from IR SparseSlice
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def SparseSlice(indices: Tensor, values: Tensor, shape: Tensor, start: Tensor, size: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSlice)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_STRING, DT_RESOURCE}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.INPUT(start, TensorType({DT_INT64}))\n
.INPUT(size, TensorType({DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_STRING, DT_RESOURCE}))\n
.OUTPUT(y_shape, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSlice"
    op.name = next_unique_name(node_name, "SparseSlice")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(start.tensor)
    op.input_desc.add().CopyFrom(start.desc)
    op.input_desc[-1].name = "start"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR SparseAddGrad
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseAddGrad(backprop_val_grad: Tensor, x1_indices: Tensor, x2_indices: Tensor, sum_indices: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseAddGrad)\n
.INPUT(backprop_val_grad, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x2_indices, TensorType({DT_INT64}))\n
.INPUT(sum_indices, TensorType({DT_INT64}))\n
.OUTPUT(x1_val_grad, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(x2_val_grad, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseAddGrad"
    op.name = next_unique_name(node_name, "SparseAddGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(backprop_val_grad.tensor)
    op.input_desc.add().CopyFrom(backprop_val_grad.desc)
    op.input_desc[-1].name = "backprop_val_grad"
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x2_indices.tensor)
    op.input_desc.add().CopyFrom(x2_indices.desc)
    op.input_desc[-1].name = "x2_indices"
    op.input.append(sum_indices.tensor)
    op.input_desc.add().CopyFrom(sum_indices.desc)
    op.input_desc[-1].name = "sum_indices"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x1_val_grad"
    x1_val_grad = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "x2_val_grad"
    x2_val_grad = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x1_val_grad, x2_val_grad


# This api is auto-generated from IR SparseFillEmptyRowsGrad
@auto_convert_to_tensor([False, False], [False, False])
def SparseFillEmptyRowsGrad(reverse_index_map: Tensor, grad_values: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseFillEmptyRowsGrad)\n
.INPUT(reverse_index_map, TensorType({DT_INT64}))\n
.INPUT(grad_values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.OUTPUT(y_value, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.OUTPUT(y_default_value, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseFillEmptyRowsGrad"
    op.name = next_unique_name(node_name, "SparseFillEmptyRowsGrad")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(reverse_index_map.tensor)
    op.input_desc.add().CopyFrom(reverse_index_map.desc)
    op.input_desc[-1].name = "reverse_index_map"
    op.input.append(grad_values.tensor)
    op.input_desc.add().CopyFrom(grad_values.desc)
    op.input_desc[-1].name = "grad_values"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_value"
    y_value = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_default_value"
    y_default_value = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_value, y_default_value


# This api is auto-generated from IR SparseTensorDenseMatMul
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def SparseTensorDenseMatMul(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2: Tensor, *, adjoint_a: bool=False, adjoint_b: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseTensorDenseMatMul)\n
.INPUT(x1_indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEXT64, DT_COMPLEX128, DT_FLOAT16, DT_INT64}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_COMPLEXT64, DT_COMPLEX128, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_INT64, DT_INT32, DT_COMPLEXT64, DT_COMPLEX128, DT_FLOAT16}))\n
.ATTR(adjoint_a, Bool, false)\n
.ATTR(adjoint_b, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseTensorDenseMatMul"
    op.name = next_unique_name(node_name, "SparseTensorDenseMatMul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"

    # process attrs
    op.attr["adjoint_a"].b = adjoint_a
    op.attr["adjoint_b"].b = adjoint_b

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseToDense
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def SparseToDense(indices: Tensor, output_shape: Tensor, values: Tensor, default_value: Tensor, *, validate_indices: bool=True, dependencies=[], node_name=None):
    """REG_OP(SparseToDense)\n
.INPUT(indices, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(output_shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_BOOL, DT_DOUBLE}))\n
.INPUT(default_value, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_BOOL, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_BOOL, DT_DOUBLE}))\n
.ATTR(validate_indices, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseToDense"
    op.name = next_unique_name(node_name, "SparseToDense")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(output_shape.tensor)
    op.input_desc.add().CopyFrom(output_shape.desc)
    op.input_desc[-1].name = "output_shape"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(default_value.tensor)
    op.input_desc.add().CopyFrom(default_value.desc)
    op.input_desc[-1].name = "default_value"

    # process attrs
    op.attr["validate_indices"].b = validate_indices

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseConcat
@auto_convert_to_tensor([True, True, True], [False, False, False])
def SparseConcat(indices: List[Tensor], values: List[Tensor], shapes: List[Tensor], *, concat_dim: int=0, N: int=1, dependencies=[], node_name=None):
    """REG_OP(SparseConcat)\n
.DYNAMIC_INPUT(indices, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.DYNAMIC_INPUT(shapes, TensorType({DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.OUTPUT(y_shape, TensorType({DT_INT64}))\n
.ATTR(concat_dim, Int, 0)\n
.ATTR(N, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseConcat"
    op.name = next_unique_name(node_name, "SparseConcat")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(indices, (tuple, list)):
        raise AssertionError("indices must be a tuple or a list.")
    for i, v in enumerate(indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "indices" + str(i)
    if not isinstance(values, (tuple, list)):
        raise AssertionError("values must be a tuple or a list.")
    for i, v in enumerate(values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "values" + str(i)
    if not isinstance(shapes, (tuple, list)):
        raise AssertionError("shapes must be a tuple or a list.")
    for i, v in enumerate(shapes):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "shapes" + str(i)

    # process attrs
    op.attr["concat_dim"].i = concat_dim
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR SparseAdd
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, False, False, False, False, False])
def SparseAdd(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2_indices: Tensor, x2_values: Tensor, x2_shape: Tensor, thresh: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseAdd)\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2_indices, TensorType({DT_INT64}))\n
.INPUT(x2_values, TensorType({DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2_shape, TensorType({DT_INT64}))\n
.INPUT(thresh, TensorType({DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(sum_indices, TensorType({DT_INT64}))\n
.OUTPUT(sum_values, TensorType({DT_FLOAT, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(sum_shape, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseAdd"
    op.name = next_unique_name(node_name, "SparseAdd")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2_indices.tensor)
    op.input_desc.add().CopyFrom(x2_indices.desc)
    op.input_desc[-1].name = "x2_indices"
    op.input.append(x2_values.tensor)
    op.input_desc.add().CopyFrom(x2_values.desc)
    op.input_desc[-1].name = "x2_values"
    op.input.append(x2_shape.tensor)
    op.input_desc.add().CopyFrom(x2_shape.desc)
    op.input_desc[-1].name = "x2_shape"
    op.input.append(thresh.tensor)
    op.input_desc.add().CopyFrom(thresh.desc)
    op.input_desc[-1].name = "thresh"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "sum_indices"
    sum_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sum_values"
    sum_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "sum_shape"
    sum_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return sum_indices, sum_values, sum_shape


# This api is auto-generated from IR SparseFillEmptyRows
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseFillEmptyRows(indices: Tensor, values: Tensor, dense_shape: Tensor, default_value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseFillEmptyRows)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.INPUT(dense_shape, TensorType({DT_INT64}))\n
.INPUT(default_value, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_BOOL, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.OUTPUT(empty_row_indicator, TensorType({DT_BOOL}))\n
.OUTPUT(reverse_index_map, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseFillEmptyRows"
    op.name = next_unique_name(node_name, "SparseFillEmptyRows")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(dense_shape.tensor)
    op.input_desc.add().CopyFrom(dense_shape.desc)
    op.input_desc[-1].name = "dense_shape"
    op.input.append(default_value.tensor)
    op.input_desc.add().CopyFrom(default_value.desc)
    op.input_desc[-1].name = "default_value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "empty_row_indicator"
    empty_row_indicator = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "reverse_index_map"
    reverse_index_map = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, empty_row_indicator, reverse_index_map


# This api is auto-generated from IR SparseSparseMaximum
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def SparseSparseMaximum(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2_indices: Tensor, x2_values: Tensor, x2_shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSparseMaximum)\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2_indices, TensorType({DT_INT64}))\n
.INPUT(x2_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(x2_shape, TensorType({DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSparseMaximum"
    op.name = next_unique_name(node_name, "SparseSparseMaximum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2_indices.tensor)
    op.input_desc.add().CopyFrom(x2_indices.desc)
    op.input_desc[-1].name = "x2_indices"
    op.input.append(x2_values.tensor)
    op.input_desc.add().CopyFrom(x2_values.desc)
    op.input_desc[-1].name = "x2_values"
    op.input.append(x2_shape.tensor)
    op.input_desc.add().CopyFrom(x2_shape.desc)
    op.input_desc[-1].name = "x2_shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values


# This api is auto-generated from IR SparseSparseMinimum
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False])
def SparseSparseMinimum(x1_indices: Tensor, x1_values: Tensor, x1_shape: Tensor, x2_indices: Tensor, x2_values: Tensor, x2_shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SparseSparseMinimum)\n
.INPUT(x1_indices, TensorType({DT_INT64}))\n
.INPUT(x1_values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x1_shape, TensorType({DT_INT64}))\n
.INPUT(x2_indices, TensorType({DT_INT64}))\n
.INPUT(x2_values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x2_shape, TensorType({DT_INT64}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSparseMinimum"
    op.name = next_unique_name(node_name, "SparseSparseMinimum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1_indices.tensor)
    op.input_desc.add().CopyFrom(x1_indices.desc)
    op.input_desc[-1].name = "x1_indices"
    op.input.append(x1_values.tensor)
    op.input_desc.add().CopyFrom(x1_values.desc)
    op.input_desc[-1].name = "x1_values"
    op.input.append(x1_shape.tensor)
    op.input_desc.add().CopyFrom(x1_shape.desc)
    op.input_desc[-1].name = "x1_shape"
    op.input.append(x2_indices.tensor)
    op.input_desc.add().CopyFrom(x2_indices.desc)
    op.input_desc[-1].name = "x2_indices"
    op.input.append(x2_values.tensor)
    op.input_desc.add().CopyFrom(x2_values.desc)
    op.input_desc[-1].name = "x2_values"
    op.input.append(x2_shape.tensor)
    op.input_desc.add().CopyFrom(x2_shape.desc)
    op.input_desc[-1].name = "x2_shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values


# This api is auto-generated from IR SparseReduceMax
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseReduceMax(x_indices: Tensor, x_values: Tensor, x_shape: Tensor, reduction_axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseReduceMax)\n
.INPUT(x_indices, TensorType({DT_INT64}))\n
.INPUT(x_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(x_shape, TensorType({DT_INT64}))\n
.INPUT(reduction_axes, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseReduceMax"
    op.name = next_unique_name(node_name, "SparseReduceMax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_indices.tensor)
    op.input_desc.add().CopyFrom(x_indices.desc)
    op.input_desc[-1].name = "x_indices"
    op.input.append(x_values.tensor)
    op.input_desc.add().CopyFrom(x_values.desc)
    op.input_desc[-1].name = "x_values"
    op.input.append(x_shape.tensor)
    op.input_desc.add().CopyFrom(x_shape.desc)
    op.input_desc[-1].name = "x_shape"
    op.input.append(reduction_axes.tensor)
    op.input_desc.add().CopyFrom(reduction_axes.desc)
    op.input_desc[-1].name = "reduction_axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseReduceMaxSparse
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseReduceMaxSparse(x_indices: Tensor, x_values: Tensor, x_shape: Tensor, reduction_axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseReduceMaxSparse)\n
.INPUT(x_indices, TensorType({DT_INT64}))\n
.INPUT(x_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.INPUT(x_shape, TensorType({DT_INT64}))\n
.INPUT(reduction_axes, TensorType({DT_INT32}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE}))\n
.OUTPUT(y_shape, TensorType({DT_INT64}))\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseReduceMaxSparse"
    op.name = next_unique_name(node_name, "SparseReduceMaxSparse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_indices.tensor)
    op.input_desc.add().CopyFrom(x_indices.desc)
    op.input_desc[-1].name = "x_indices"
    op.input.append(x_values.tensor)
    op.input_desc.add().CopyFrom(x_values.desc)
    op.input_desc[-1].name = "x_values"
    op.input.append(x_shape.tensor)
    op.input_desc.add().CopyFrom(x_shape.desc)
    op.input_desc[-1].name = "x_shape"
    op.input.append(reduction_axes.tensor)
    op.input_desc.add().CopyFrom(reduction_axes.desc)
    op.input_desc[-1].name = "reduction_axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR SparseReduceSum
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseReduceSum(x_indices: Tensor, x_values: Tensor, x_shape: Tensor, reduction_axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseReduceSum)\n
.INPUT(x_indices, TensorType({DT_INT64}))\n
.INPUT(x_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x_shape, TensorType({DT_INT64}))\n
.INPUT(reduction_axes, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseReduceSum"
    op.name = next_unique_name(node_name, "SparseReduceSum")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_indices.tensor)
    op.input_desc.add().CopyFrom(x_indices.desc)
    op.input_desc[-1].name = "x_indices"
    op.input.append(x_values.tensor)
    op.input_desc.add().CopyFrom(x_values.desc)
    op.input_desc[-1].name = "x_values"
    op.input.append(x_shape.tensor)
    op.input_desc.add().CopyFrom(x_shape.desc)
    op.input_desc[-1].name = "x_shape"
    op.input.append(reduction_axes.tensor)
    op.input_desc.add().CopyFrom(reduction_axes.desc)
    op.input_desc[-1].name = "reduction_axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SparseReduceSumSparse
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def SparseReduceSumSparse(x_indices: Tensor, x_values: Tensor, x_shape: Tensor, reduction_axes: Tensor, *, keep_dims: bool=False, dependencies=[], node_name=None):
    """REG_OP(SparseReduceSumSparse)\n
.INPUT(x_indices, TensorType({DT_INT64}))\n
.INPUT(x_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.INPUT(x_shape, TensorType({DT_INT64}))\n
.INPUT(reduction_axes, TensorType({DT_INT32}))\n
.OUTPUT(y_indices, TensorType({DT_INT64}))\n
.OUTPUT(y_values, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y_shape, TensorType({DT_INT64}))\n
.ATTR(keep_dims, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseReduceSumSparse"
    op.name = next_unique_name(node_name, "SparseReduceSumSparse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x_indices.tensor)
    op.input_desc.add().CopyFrom(x_indices.desc)
    op.input_desc[-1].name = "x_indices"
    op.input.append(x_values.tensor)
    op.input_desc.add().CopyFrom(x_values.desc)
    op.input_desc[-1].name = "x_values"
    op.input.append(x_shape.tensor)
    op.input_desc.add().CopyFrom(x_shape.desc)
    op.input_desc[-1].name = "x_shape"
    op.input.append(reduction_axes.tensor)
    op.input_desc.add().CopyFrom(reduction_axes.desc)
    op.input_desc[-1].name = "reduction_axes"

    # process attrs
    op.attr["keep_dims"].b = keep_dims

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y_indices"
    y_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_values"
    y_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "y_shape"
    y_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR SparseSplit
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def _SparseSplit(split_dim: Tensor, indices: Tensor, values: Tensor, shape: Tensor, *, size_of_y_indices: int, size_of_y_values: int, size_of_y_shape: int, num_split: int=1, dependencies=[], node_name=None):
    """REG_OP(SparseSplit)\n
.INPUT(split_dim, TensorType({DT_INT64}))\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_STRING, DT_RESOURCE}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(y_indices, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(y_values, TensorType({DT_INT64, DT_INT32, DT_UINT16, DT_INT16, DT_UINT8, DT_INT8, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_STRING, DT_RESOURCE}))\n
.DYNAMIC_OUTPUT(y_shape, TensorType({DT_INT64}))\n
.ATTR(num_split, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseSplit"
    op.name = next_unique_name(node_name, "SparseSplit")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(split_dim.tensor)
    op.input_desc.add().CopyFrom(split_dim.desc)
    op.input_desc[-1].name = "split_dim"
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["num_split"].i = num_split

    # process outputs
    output_index = 0
    y_indices = []
    for i in range(output_index, output_index + size_of_y_indices):
        op.output_desc.add().name = "y_indices" + str(i - output_index)
        y_indices.append(Tensor(op, i))
    output_index += size_of_y_indices
    y_values = []
    for i in range(output_index, output_index + size_of_y_values):
        op.output_desc.add().name = "y_values" + str(i - output_index)
        y_values.append(Tensor(op, i))
    output_index += size_of_y_values
    y_shape = []
    for i in range(output_index, output_index + size_of_y_shape):
        op.output_desc.add().name = "y_shape" + str(i - output_index)
        y_shape.append(Tensor(op, i))
    output_index += size_of_y_shape

    # return outputs
    return y_indices, y_values, y_shape


# This api is auto-generated from IR SparseCross
@auto_convert_to_tensor([True, True, True, True], [False, False, False, False])
def SparseCross(indices: List[Tensor], values: List[Tensor], shapes: List[Tensor], dense_inputs: List[Tensor], *, hashed_output: bool, hash_key: int, out_type: int, internal_type: int, N: int=0, num_buckets: int=0, dependencies=[], node_name=None):
    """REG_OP(SparseCross)\n
.DYNAMIC_INPUT(indices, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(values, TensorType({DT_INT64, DT_STRING}))\n
.DYNAMIC_INPUT(shapes, TensorType({DT_INT64}))\n
.DYNAMIC_INPUT(dense_inputs, TensorType({DT_INT64, DT_STRING}))\n
.OUTPUT(output_indices, TensorType({DT_INT64}))\n
.OUTPUT(output_values, TensorType({DT_INT64, DT_STRING}))\n
.OUTPUT(output_shape, TensorType({DT_INT64}))\n
.ATTR(N, Int, 0)\n
.REQUIRED_ATTR(hashed_output, Bool)\n
.ATTR(num_buckets, Int, 0)\n
.REQUIRED_ATTR(hash_key, Int)\n
.REQUIRED_ATTR(out_type, Type)\n
.REQUIRED_ATTR(internal_type, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SparseCross"
    op.name = next_unique_name(node_name, "SparseCross")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(indices, (tuple, list)):
        raise AssertionError("indices must be a tuple or a list.")
    for i, v in enumerate(indices):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "indices" + str(i)
    if not isinstance(values, (tuple, list)):
        raise AssertionError("values must be a tuple or a list.")
    for i, v in enumerate(values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "values" + str(i)
    if not isinstance(shapes, (tuple, list)):
        raise AssertionError("shapes must be a tuple or a list.")
    for i, v in enumerate(shapes):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "shapes" + str(i)
    if not isinstance(dense_inputs, (tuple, list)):
        raise AssertionError("dense_inputs must be a tuple or a list.")
    for i, v in enumerate(dense_inputs):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "dense_inputs" + str(i)

    # process attrs
    op.attr["hashed_output"].b = hashed_output
    op.attr["hash_key"].i = hash_key
    op.attr["out_type"].dt = out_type
    op.attr["internal_type"].dt = internal_type
    op.attr["N"].i = N
    op.attr["num_buckets"].i = num_buckets

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_indices"
    output_indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_values"
    output_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "output_shape"
    output_shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_indices, output_values, output_shape


# This api is auto-generated from IR AddManySparseToTensorsMap
@auto_convert_to_tensor([False, False, False], [False, False, False])
def AddManySparseToTensorsMap(indices: Tensor, values: Tensor, shape: Tensor, *, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(AddManySparseToTensorsMap)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.OUTPUT(handles, TensorType({DT_INT64}))\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AddManySparseToTensorsMap"
    op.name = next_unique_name(node_name, "AddManySparseToTensorsMap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handles"
    handles = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handles


# This api is auto-generated from IR TakeManySparseFromTensorsMap
@auto_convert_to_tensor([False], [False])
def TakeManySparseFromTensorsMap(handles: Tensor, *, dtype: int, container: str="", shared_name: str="", dependencies=[], node_name=None):
    """REG_OP(TakeManySparseFromTensorsMap)\n
.INPUT(handles, TensorType({DT_INT64}))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(shape, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
.ATTR(container, String, "")\n
.ATTR(shared_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TakeManySparseFromTensorsMap"
    op.name = next_unique_name(node_name, "TakeManySparseFromTensorsMap")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handles.tensor)
    op.input_desc.add().CopyFrom(handles.desc)
    op.input_desc[-1].name = "handles"

    # process attrs
    op.attr["dtype"].dt = dtype
    op.attr["container"].s = compat_as_bytes(container)
    op.attr["shared_name"].s = compat_as_bytes(shared_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "shape"
    shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, values, shape


# This api is auto-generated from IR SerializeSparse
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SerializeSparse(indices: Tensor, values: Tensor, shape: Tensor, *, out_type: int=DataType.DT_STRING, dependencies=[], node_name=None):
    """REG_OP(SerializeSparse)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.OUTPUT(serialized_sparse, TensorType({DT_STRING, DT_VARIANT}))\n
.ATTR(out_type, Type, DT_STRING)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SerializeSparse"
    op.name = next_unique_name(node_name, "SerializeSparse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["out_type"].dt = out_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "serialized_sparse"
    serialized_sparse = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return serialized_sparse


# This api is auto-generated from IR SerializeManySparse
@auto_convert_to_tensor([False, False, False], [False, False, False])
def SerializeManySparse(indices: Tensor, values: Tensor, shape: Tensor, *, out_type: int=DataType.DT_STRING, dependencies=[], node_name=None):
    """REG_OP(SerializeManySparse)\n
.INPUT(indices, TensorType({DT_INT64}))\n
.INPUT(values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.INPUT(shape, TensorType({DT_INT64}))\n
.OUTPUT(serialized_sparse, TensorType({DT_STRING, DT_VARIANT}))\n
.ATTR(out_type, Type, DT_STRING)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SerializeManySparse"
    op.name = next_unique_name(node_name, "SerializeManySparse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(indices.tensor)
    op.input_desc.add().CopyFrom(indices.desc)
    op.input_desc[-1].name = "indices"
    op.input.append(values.tensor)
    op.input_desc.add().CopyFrom(values.desc)
    op.input_desc[-1].name = "values"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["out_type"].dt = out_type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "serialized_sparse"
    serialized_sparse = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return serialized_sparse


# This api is auto-generated from IR DeserializeSparse
@auto_convert_to_tensor([False], [False])
def DeserializeSparse(serialized_sparse: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(DeserializeSparse)\n
.INPUT(serialized_sparse, TensorType({DT_STRING, DT_VARIANT}))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.OUTPUT(shape, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeserializeSparse"
    op.name = next_unique_name(node_name, "DeserializeSparse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(serialized_sparse.tensor)
    op.input_desc.add().CopyFrom(serialized_sparse.desc)
    op.input_desc[-1].name = "serialized_sparse"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "shape"
    shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, values, shape


# This api is auto-generated from IR DeserializeManySparse
@auto_convert_to_tensor([False], [False])
def DeserializeManySparse(serialized_sparse: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(DeserializeManySparse)\n
.INPUT(serialized_sparse, TensorType({DT_STRING}))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_BOOL, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_COMPLEX64, DT_COMPLEX128, DT_RESOURCE, DT_STRING}))\n
.OUTPUT(shape, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeserializeManySparse"
    op.name = next_unique_name(node_name, "DeserializeManySparse")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(serialized_sparse.tensor)
    op.input_desc.add().CopyFrom(serialized_sparse.desc)
    op.input_desc[-1].name = "serialized_sparse"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "shape"
    shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, values, shape


# This api is auto-generated from IR IFFT
@auto_convert_to_tensor([False], [False])
def IFFT(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IFFT)\n
.INPUT(x, TensorType({DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_COMPLEX64,DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IFFT"
    op.name = next_unique_name(node_name, "IFFT")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RFFT
@auto_convert_to_tensor([False, False], [False, False])
def RFFT(input: Tensor, fft_length: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RFFT)\n
.INPUT(input, TensorType({DT_FLOAT}))\n
.INPUT(fft_length, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_COMPLEX64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RFFT"
    op.name = next_unique_name(node_name, "RFFT")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(fft_length.tensor)
    op.input_desc.add().CopyFrom(fft_length.desc)
    op.input_desc[-1].name = "fft_length"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IRFFT
@auto_convert_to_tensor([False, False], [False, False])
def IRFFT(x: Tensor, fft_length: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IRFFT)\n
.INPUT(x, TensorType({DT_COMPLEX64}))\n
.INPUT(fft_length, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IRFFT"
    op.name = next_unique_name(node_name, "IRFFT")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(fft_length.tensor)
    op.input_desc.add().CopyFrom(fft_length.desc)
    op.input_desc[-1].name = "fft_length"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FFT2D
@auto_convert_to_tensor([False], [False])
def FFT2D(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FFT2D)\n
.INPUT(x, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_COMPLEX64, DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FFT2D"
    op.name = next_unique_name(node_name, "FFT2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FFT
@auto_convert_to_tensor([False], [False])
def FFT(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(FFT)\n
.INPUT(x, TensorType({DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_COMPLEX64,DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FFT"
    op.name = next_unique_name(node_name, "FFT")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IFFT2D
@auto_convert_to_tensor([False], [False])
def IFFT2D(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IFFT2D)\n
.INPUT(x, TensorType({DT_COMPLEX64,DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_COMPLEX64,DT_COMPLEX128}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IFFT2D"
    op.name = next_unique_name(node_name, "IFFT2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR STFT
@auto_convert_to_tensor([False, False], [False, True])
def STFT(x: Tensor, window: Optional[Tensor], *, n_fft: int, hop_length: int=0, win_length: int=0, normalized: bool=False, onesided: bool=True, return_complex: bool=True, dependencies=[], node_name=None):
    """REG_OP(STFT)\n
.INPUT(x, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OPTIONAL_INPUT(window, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128}))\n
.ATTR(hop_length, Int, 0)\n
.ATTR(win_length, Int, 0)\n
.ATTR(normalized, Bool, false)\n
.ATTR(onesided, Bool, true)\n
.ATTR(return_complex, Bool, true)\n
.REQUIRED_ATTR(n_fft, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "STFT"
    op.name = next_unique_name(node_name, "STFT")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if window is not None:
        op.input.append(window.tensor)
        op.input_desc.add().CopyFrom(window.desc)
        op.input_desc[-1].name = "window"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "window"

    # process attrs
    op.attr["n_fft"].i = n_fft
    op.attr["hop_length"].i = hop_length
    op.attr["win_length"].i = win_length
    op.attr["normalized"].b = normalized
    op.attr["onesided"].b = onesided
    op.attr["return_complex"].b = return_complex

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Split
@auto_convert_to_tensor([False, False], [False, False])
def _Split(split_dim: Tensor, x: Tensor, *, size_of_y: int, num_split: int, dependencies=[], node_name=None):
    """REG_OP(Split)\n
.INPUT(split_dim, TensorType({DT_INT32}))\n
.INPUT(x, TensorType({DT_COMPLEX128, DT_COMPLEX64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT16, DT_INT32, DT_INT64, DT_INT8, DT_QINT16, DT_QINT32, DT_QINT8, DT_QUINT16, DT_QUINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_UINT8, DT_BF16, DT_BOOL}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_COMPLEX128, DT_COMPLEX64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT16, DT_INT32, DT_INT64, DT_INT8, DT_QINT16, DT_QINT32, DT_QINT8, DT_QUINT16, DT_QUINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_UINT8, DT_BF16, DT_BOOL}))\n
.REQUIRED_ATTR(num_split, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Split"
    op.name = next_unique_name(node_name, "Split")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(split_dim.tensor)
    op.input_desc.add().CopyFrom(split_dim.desc)
    op.input_desc[-1].name = "split_dim"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["num_split"].i = num_split

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR SplitD
@auto_convert_to_tensor([False], [False])
def _SplitD(x: Tensor, *, size_of_y: int, split_dim: int, num_split: int, dependencies=[], node_name=None):
    """REG_OP(SplitD)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16, DT_BOOL}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16, DT_BOOL}))\n
.REQUIRED_ATTR(split_dim, Int)\n
.REQUIRED_ATTR(num_split, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SplitD"
    op.name = next_unique_name(node_name, "SplitD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["split_dim"].i = split_dim
    op.attr["num_split"].i = num_split

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR SplitV
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def _SplitV(x: Tensor, size_splits: Tensor, split_dim: Tensor, *, size_of_y: int, num_split: int, dependencies=[], node_name=None):
    """REG_OP(SplitV)\n
.INPUT(x, TensorType({DT_COMPLEX128, DT_COMPLEX64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT16, DT_INT32, DT_INT64, DT_INT8, DT_QINT16, DT_QINT32, DT_QINT8, DT_QUINT16, DT_QUINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_UINT8, DT_BF16, DT_BOOL, DT_STRING}))\n
.INPUT(size_splits, TensorType::IndexNumberType())\n
.INPUT(split_dim, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_COMPLEX128, DT_COMPLEX64, DT_DOUBLE, DT_FLOAT, DT_FLOAT16, DT_INT16, DT_INT32, DT_INT64, DT_INT8, DT_QINT16, DT_QINT32, DT_QINT8, DT_QUINT16, DT_QUINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_UINT8, DT_BF16, DT_BOOL, DT_STRING}))\n
.REQUIRED_ATTR(num_split, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SplitV"
    op.name = next_unique_name(node_name, "SplitV")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size_splits.tensor)
    op.input_desc.add().CopyFrom(size_splits.desc)
    op.input_desc[-1].name = "size_splits"
    op.input.append(split_dim.tensor)
    op.input_desc.add().CopyFrom(split_dim.desc)
    op.input_desc[-1].name = "split_dim"

    # process attrs
    op.attr["num_split"].i = num_split

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR SplitVD
@auto_convert_to_tensor([False], [False])
def _SplitVD(x: Tensor, *, size_of_y: int, size_splits: List[int], split_dim: int, num_split: int, dependencies=[], node_name=None):
    """REG_OP(SplitVD)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16, DT_BOOL}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16, DT_BOOL}))\n
.REQUIRED_ATTR(size_splits, ListInt)\n
.REQUIRED_ATTR(split_dim, Int)\n
.REQUIRED_ATTR(num_split, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SplitVD"
    op.name = next_unique_name(node_name, "SplitVD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["size_splits"].list.val_type = 2
    op.attr["size_splits"].list.i.extend(size_splits)
    op.attr["split_dim"].i = split_dim
    op.attr["num_split"].i = num_split

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ParallelConcat
@auto_convert_to_tensor([True], [False])
def ParallelConcat(values: List[Tensor], *, shape: List[int], N: int, dependencies=[], node_name=None):
    """REG_OP(ParallelConcat)\n
.DYNAMIC_INPUT(values, TensorType({DT_FLOAT,DT_FLOAT16,DT_INT8,DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_UINT32,DT_UINT64}))\n
.OUTPUT(output_data, TensorType({DT_FLOAT,DT_FLOAT16,DT_INT8,DT_INT16,DT_INT32,DT_INT64,DT_UINT8,DT_UINT16,DT_UINT32,DT_UINT64}))\n
.REQUIRED_ATTR(shape, ListInt)\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ParallelConcat"
    op.name = next_unique_name(node_name, "ParallelConcat")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(values, (tuple, list)):
        raise AssertionError("values must be a tuple or a list.")
    for i, v in enumerate(values):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "values" + str(i)

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output_data"
    output_data = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output_data


# This api is auto-generated from IR ConcatV2D
@auto_convert_to_tensor([True], [False])
def ConcatV2D(x: List[Tensor], *, concat_dim: int, N: int=1, dependencies=[], node_name=None):
    """REG_OP(ConcatV2D)\n
.DYNAMIC_INPUT(x, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_INT64, DT_UINT64, DT_UINT32, DT_INT16, DT_UINT16, DT_UINT8}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT16, DT_FLOAT, DT_INT32, DT_INT8, DT_INT64, DT_UINT64, DT_UINT32, DT_INT16, DT_UINT16, DT_UINT8}))\n
.REQUIRED_ATTR(concat_dim, Int)\n
.ATTR(N, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConcatV2D"
    op.name = next_unique_name(node_name, "ConcatV2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["concat_dim"].i = concat_dim
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConcatV2
@auto_convert_to_tensor([True, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def ConcatV2(x: List[Tensor], concat_dim: Tensor, *, N: int=1, dependencies=[], node_name=None):
    """REG_OP(ConcatV2)\n
.DYNAMIC_INPUT(x, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.INPUT(concat_dim, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.ATTR(N, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConcatV2"
    op.name = next_unique_name(node_name, "ConcatV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    op.input.append(concat_dim.tensor)
    op.input_desc.add().CopyFrom(concat_dim.desc)
    op.input_desc[-1].name = "concat_dim"

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConcatD
@auto_convert_to_tensor([True], [False])
def ConcatD(x: List[Tensor], *, concat_dim: int, N: int=1, dependencies=[], node_name=None):
    """REG_OP(ConcatD)\n
.DYNAMIC_INPUT(x, TensorType({DT_BF16, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_BF16, DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64}))\n
.REQUIRED_ATTR(concat_dim, Int)\n
.ATTR(N, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConcatD"
    op.name = next_unique_name(node_name, "ConcatD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["concat_dim"].i = concat_dim
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Concat
@auto_convert_to_tensor([False, True], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_BASIC])
def Concat(concat_dim: Tensor, x: List[Tensor], *, N: int=1, dependencies=[], node_name=None):
    """REG_OP(Concat)\n
.INPUT(concat_dim, TensorType::IndexNumberType())\n
.DYNAMIC_INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.ATTR(N, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Concat"
    op.name = next_unique_name(node_name, "Concat")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(concat_dim.tensor)
    op.input_desc.add().CopyFrom(concat_dim.desc)
    op.input_desc[-1].name = "concat_dim"
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Pack
@auto_convert_to_tensor([True], [False])
def Pack(x: List[Tensor], *, N: int, axis: int=0, dependencies=[], node_name=None):
    """REG_OP(Pack)\n
.DYNAMIC_INPUT(x, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.OUTPUT(y, TensorType({BasicType(), DT_BOOL, DT_STRING}))\n
.ATTR(axis, Int, 0)\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Pack"
    op.name = next_unique_name(node_name, "Pack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConcatOffset
@auto_convert_to_tensor([False, True], [False, False])
def _ConcatOffset(concat_dim: Tensor, x: List[Tensor], *, size_of_y: int, N: int, dependencies=[], node_name=None):
    """REG_OP(ConcatOffset)\n
.INPUT(concat_dim, TensorType({DT_INT32}))\n
.DYNAMIC_INPUT(x, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConcatOffset"
    op.name = next_unique_name(node_name, "ConcatOffset")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(concat_dim.tensor)
    op.input_desc.add().CopyFrom(concat_dim.desc)
    op.input_desc[-1].name = "concat_dim"
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ConcatOffsetD
@auto_convert_to_tensor([True], [False])
def _ConcatOffsetD(x: List[Tensor], *, size_of_y: int, concat_dim: int, N: int, dependencies=[], node_name=None):
    """REG_OP(ConcatOffsetD)\n
.DYNAMIC_INPUT(x, TensorType({DT_INT32}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(concat_dim, Int)\n
.REQUIRED_ATTR(N, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConcatOffsetD"
    op.name = next_unique_name(node_name, "ConcatOffsetD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["concat_dim"].i = concat_dim
    op.attr["N"].i = N

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR Combinations
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def Combinations(x: Tensor, *, r: int=2, with_replacement: bool=False, dependencies=[], node_name=None):
    """REG_OP(Combinations)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(r, Int, 2)\n
.ATTR(with_replacement, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Combinations"
    op.name = next_unique_name(node_name, "Combinations")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["r"].i = r
    op.attr["with_replacement"].b = with_replacement

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR NonDeterministicInts
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def NonDeterministicInts(shape: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(NonDeterministicInts)\n
.INPUT(shape, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32,DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "NonDeterministicInts"
    op.name = next_unique_name(node_name, "NonDeterministicInts")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RngSkip
@auto_convert_to_tensor([False, False, False], [False, False, False])
def RngSkip(x: Tensor, algorithm: Tensor, delta: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RngSkip)\n
.INPUT(x, TensorType({DT_RESOURCE}))\n
.INPUT(algorithm, TensorType({DT_INT64}))\n
.INPUT(delta, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RngSkip"
    op.name = next_unique_name(node_name, "RngSkip")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(delta.tensor)
    op.input_desc.add().CopyFrom(delta.desc)
    op.input_desc[-1].name = "delta"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR StatefulRandomBinomial
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def StatefulRandomBinomial(x: Tensor, algorithm: Tensor, shape: Tensor, counts: Tensor, probs: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(StatefulRandomBinomial)\n
.INPUT(x, TensorType({DT_RESOURCE}))\n
.INPUT(algorithm, TensorType({DT_INT64}))\n
.INPUT(shape, TensorType({DT_INT32}))\n
.INPUT(counts, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(probs, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatefulRandomBinomial"
    op.name = next_unique_name(node_name, "StatefulRandomBinomial")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(counts.tensor)
    op.input_desc.add().CopyFrom(counts.desc)
    op.input_desc[-1].name = "counts"
    op.input.append(probs.tensor)
    op.input_desc.add().CopyFrom(probs.desc)
    op.input_desc[-1].name = "probs"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatefulStandardNormalV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def StatefulStandardNormalV2(x: Tensor, algorithm: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatefulStandardNormalV2)\n
.INPUT(x, TensorType({DT_RESOURCE}))\n
.INPUT(algorithm, TensorType({DT_INT64}))\n
.INPUT(shape, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatefulStandardNormalV2"
    op.name = next_unique_name(node_name, "StatefulStandardNormalV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatefulTruncatedNormal
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def StatefulTruncatedNormal(x: Tensor, algorithm: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatefulTruncatedNormal)\n
.INPUT(x, TensorType({DT_RESOURCE}))\n
.INPUT(algorithm, TensorType({DT_INT64}))\n
.INPUT(shape, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatefulTruncatedNormal"
    op.name = next_unique_name(node_name, "StatefulTruncatedNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatefulUniform
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def StatefulUniform(x: Tensor, algorithm: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatefulUniform)\n
.INPUT(x, TensorType({DT_RESOURCE}))\n
.INPUT(algorithm, TensorType({DT_INT64}))\n
.INPUT(shape, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatefulUniform"
    op.name = next_unique_name(node_name, "StatefulUniform")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatefulUniformFullInt
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def StatefulUniformFullInt(x: Tensor, algorithm: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatefulUniformFullInt)\n
.INPUT(x, TensorType({DT_RESOURCE}))\n
.INPUT(algorithm, TensorType({DT_INT64}))\n
.INPUT(shape, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(y, TensorType({DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatefulUniformFullInt"
    op.name = next_unique_name(node_name, "StatefulUniformFullInt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatefulUniformInt
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatefulUniformInt(x: Tensor, algorithm: Tensor, shape: Tensor, minval: Tensor, maxval: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatefulUniformInt)\n
.INPUT(x, TensorType({DT_RESOURCE}))\n
.INPUT(algorithm, TensorType({DT_INT64}))\n
.INPUT(shape, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(minval, TensorType({DT_INT64}))\n
.INPUT(maxval, TensorType({DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatefulUniformInt"
    op.name = next_unique_name(node_name, "StatefulUniformInt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(minval.tensor)
    op.input_desc.add().CopyFrom(minval.desc)
    op.input_desc[-1].name = "minval"
    op.input.append(maxval.tensor)
    op.input_desc.add().CopyFrom(maxval.desc)
    op.input_desc[-1].name = "maxval"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RngReadAndSkipV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def RngReadAndSkipV2(value: Tensor, algorithm: Tensor, delta: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RngReadAndSkipV2)\n
.INPUT(value, TensorType({DT_INT64}))\n
.INPUT(algorithm, TensorType({DT_INT32}))\n
.INPUT(delta, TensorType({DT_UINT64}))\n
.OUTPUT(value, TensorType({DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RngReadAndSkipV2"
    op.name = next_unique_name(node_name, "RngReadAndSkipV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"
    op.input.append(algorithm.tensor)
    op.input_desc.add().CopyFrom(algorithm.desc)
    op.input_desc[-1].name = "algorithm"
    op.input.append(delta.tensor)
    op.input_desc.add().CopyFrom(delta.desc)
    op.input_desc[-1].name = "delta"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "value"
    value = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return value


# This api is auto-generated from IR StatelessMultinomial
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def StatelessMultinomial(logits: Tensor, num_samples: Tensor, seed: Tensor, *, output_dtype: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(StatelessMultinomial)\n
.INPUT(logits, TensorType({DT_FLOAT16,DT_FLOAT,DT_DOUBLE}))\n
.INPUT(num_samples, TensorType({DT_INT32}))\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(output_dtype, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessMultinomial"
    op.name = next_unique_name(node_name, "StatelessMultinomial")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(logits.tensor)
    op.input_desc.add().CopyFrom(logits.desc)
    op.input_desc[-1].name = "logits"
    op.input.append(num_samples.tensor)
    op.input_desc.add().CopyFrom(num_samples.desc)
    op.input_desc[-1].name = "num_samples"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"

    # process attrs
    op.attr["output_dtype"].dt = output_dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomUniformInt
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def StatelessRandomUniformInt(shape: Tensor, seed: Tensor, minval: Tensor, maxval: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomUniformInt)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(minval, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(maxval, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomUniformInt"
    op.name = next_unique_name(node_name, "StatelessRandomUniformInt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(minval.tensor)
    op.input_desc.add().CopyFrom(minval.desc)
    op.input_desc[-1].name = "minval"
    op.input.append(maxval.tensor)
    op.input_desc.add().CopyFrom(maxval.desc)
    op.input_desc[-1].name = "maxval"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessParameterizedTruncatedNormal
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessParameterizedTruncatedNormal(shape: Tensor, seed: Tensor, means: Tensor, stdevs: Tensor, min: Tensor, max: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatelessParameterizedTruncatedNormal)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(means, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(stdevs, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(min, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.INPUT(max, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessParameterizedTruncatedNormal"
    op.name = next_unique_name(node_name, "StatelessParameterizedTruncatedNormal")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(means.tensor)
    op.input_desc.add().CopyFrom(means.desc)
    op.input_desc[-1].name = "means"
    op.input.append(stdevs.tensor)
    op.input_desc.add().CopyFrom(stdevs.desc)
    op.input_desc[-1].name = "stdevs"
    op.input.append(min.tensor)
    op.input_desc.add().CopyFrom(min.desc)
    op.input_desc[-1].name = "min"
    op.input.append(max.tensor)
    op.input_desc.add().CopyFrom(max.desc)
    op.input_desc[-1].name = "max"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessSampleDistortedBoundingBox
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def StatelessSampleDistortedBoundingBox(image_size: Tensor, bounding_boxes: Tensor, min_object_covered: Tensor, seed: Tensor, *, aspect_ratio_range: List[float]=[0.750000, 1.330000], area_range: List[float]=[0.050000, 1.000000], max_attempts: int=100, use_image_if_no_bounding_boxes: bool=False, dependencies=[], node_name=None):
    """REG_OP(StatelessSampleDistortedBoundingBox)\n
.INPUT(image_size, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.INPUT(bounding_boxes, TensorType({ DT_FLOAT }))\n
.INPUT(min_object_covered, TensorType({ DT_FLOAT }))\n
.INPUT(seed, TensorType({ DT_INT32, DT_INT64 }))\n
.OUTPUT(begin, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.OUTPUT(size, TensorType({ DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64 }))\n
.OUTPUT(bboxes, TensorType({ DT_FLOAT }))\n
.ATTR(aspect_ratio_range, ListFloat, { 0.75f, 1.33f })\n
.ATTR(area_range, ListFloat, { 0.05f, 1.0f })\n
.ATTR(max_attempts, Int, 100)\n
.ATTR(use_image_if_no_bounding_boxes, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessSampleDistortedBoundingBox"
    op.name = next_unique_name(node_name, "StatelessSampleDistortedBoundingBox")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(image_size.tensor)
    op.input_desc.add().CopyFrom(image_size.desc)
    op.input_desc[-1].name = "image_size"
    op.input.append(bounding_boxes.tensor)
    op.input_desc.add().CopyFrom(bounding_boxes.desc)
    op.input_desc[-1].name = "bounding_boxes"
    op.input.append(min_object_covered.tensor)
    op.input_desc.add().CopyFrom(min_object_covered.desc)
    op.input_desc[-1].name = "min_object_covered"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"

    # process attrs
    op.attr["aspect_ratio_range"].list.val_type = 3
    op.attr["aspect_ratio_range"].list.f.extend(aspect_ratio_range)
    op.attr["area_range"].list.val_type = 3
    op.attr["area_range"].list.f.extend(area_range)
    op.attr["max_attempts"].i = max_attempts
    op.attr["use_image_if_no_bounding_boxes"].b = use_image_if_no_bounding_boxes

    # process outputs
    output_index = 0
    op.output_desc.add().name = "begin"
    begin = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "size"
    size = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "bboxes"
    bboxes = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return begin, size, bboxes


# This api is auto-generated from IR StatelessTruncatedNormalV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessTruncatedNormalV2(shape: Tensor, key: Tensor, counter: Tensor, alg: Tensor, *, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(StatelessTruncatedNormalV2)\n
.INPUT(shape, TensorType({ DT_INT32, DT_INT64 }))\n
.INPUT(key, TensorType({ DT_UINT64 }))\n
.INPUT(counter, TensorType({ DT_UINT64 }))\n
.INPUT(alg, TensorType({ DT_INT32 }))\n
.OUTPUT(y, TensorType({ DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE }))\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessTruncatedNormalV2"
    op.name = next_unique_name(node_name, "StatelessTruncatedNormalV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"
    op.input.append(alg.tensor)
    op.input_desc.add().CopyFrom(alg.desc)
    op.input_desc[-1].name = "alg"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomGammaV2
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def StatelessRandomGammaV2(shape: Tensor, seed: Tensor, alpha: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomGammaV2)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(alpha, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomGammaV2"
    op.name = next_unique_name(node_name, "StatelessRandomGammaV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(alpha.tensor)
    op.input_desc.add().CopyFrom(alpha.desc)
    op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomUniformFullInt
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def StatelessRandomUniformFullInt(shape: Tensor, seed: Tensor, *, dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomUniformFullInt)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.ATTR(dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomUniformFullInt"
    op.name = next_unique_name(node_name, "StatelessRandomUniformFullInt")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomUniformFullIntV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessRandomUniformFullIntV2(shape: Tensor, key: Tensor, counter: Tensor, alg: Tensor, *, dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomUniformFullIntV2)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(key, TensorType({DT_UINT64}))\n
.INPUT(counter, TensorType({DT_UINT64}))\n
.INPUT(alg, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.ATTR(dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomUniformFullIntV2"
    op.name = next_unique_name(node_name, "StatelessRandomUniformFullIntV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"
    op.input.append(alg.tensor)
    op.input_desc.add().CopyFrom(alg.desc)
    op.input_desc[-1].name = "alg"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomUniformIntV2
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessRandomUniformIntV2(shape: Tensor, key: Tensor, counter: Tensor, alg: Tensor, minval: Tensor, maxval: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomUniformIntV2)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(key, TensorType({DT_UINT64}))\n
.INPUT(counter, TensorType({DT_UINT64}))\n
.INPUT(alg, TensorType({DT_INT32}))\n
.INPUT(minval, TensorType({DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.INPUT(maxval, TensorType({DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64, DT_UINT32, DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomUniformIntV2"
    op.name = next_unique_name(node_name, "StatelessRandomUniformIntV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"
    op.input.append(alg.tensor)
    op.input_desc.add().CopyFrom(alg.desc)
    op.input_desc[-1].name = "alg"
    op.input.append(minval.tensor)
    op.input_desc.add().CopyFrom(minval.desc)
    op.input_desc[-1].name = "minval"
    op.input.append(maxval.tensor)
    op.input_desc.add().CopyFrom(maxval.desc)
    op.input_desc[-1].name = "maxval"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomBinomial
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessRandomBinomial(shape: Tensor, seed: Tensor, counts: Tensor, probs: Tensor, *, dtype: int=DataType.DT_INT32, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomBinomial)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(counts, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(probs, TensorType({DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64, DT_FLOAT16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(dtype, Type, DT_INT32)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomBinomial"
    op.name = next_unique_name(node_name, "StatelessRandomBinomial")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(counts.tensor)
    op.input_desc.add().CopyFrom(counts.desc)
    op.input_desc[-1].name = "counts"
    op.input.append(probs.tensor)
    op.input_desc.add().CopyFrom(probs.desc)
    op.input_desc[-1].name = "probs"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomPoisson
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def StatelessRandomPoisson(shape: Tensor, seed: Tensor, lam: Tensor, *, dtype: int, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomPoisson)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(lam, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(dtype, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomPoisson"
    op.name = next_unique_name(node_name, "StatelessRandomPoisson")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(lam.tensor)
    op.input_desc.add().CopyFrom(lam.desc)
    op.input_desc[-1].name = "lam"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomGetAlg
@auto_convert_to_tensor([], [])
def StatelessRandomGetAlg(*, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomGetAlg)\n
.OUTPUT(alg, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomGetAlg"
    op.name = next_unique_name(node_name, "StatelessRandomGetAlg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "alg"
    alg = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return alg


# This api is auto-generated from IR StatelessRandomGetKeyCounter
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def StatelessRandomGetKeyCounter(seed: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomGetKeyCounter)\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(key, TensorType({DT_UINT64}))\n
.OUTPUT(counter, TensorType({DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomGetKeyCounter"
    op.name = next_unique_name(node_name, "StatelessRandomGetKeyCounter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "key"
    key = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "counter"
    counter = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return key, counter


# This api is auto-generated from IR StatelessRandomGetKeyCounterAlg
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def StatelessRandomGetKeyCounterAlg(seed: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomGetKeyCounterAlg)\n
.INPUT(seed, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(key, TensorType({DT_UINT64}))\n
.OUTPUT(counter, TensorType({DT_UINT64}))\n
.OUTPUT(alg, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomGetKeyCounterAlg"
    op.name = next_unique_name(node_name, "StatelessRandomGetKeyCounterAlg")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "key"
    key = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "counter"
    counter = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "alg"
    alg = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return key, counter, alg


# This api is auto-generated from IR StatelessRandomNormalV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessRandomNormalV2(shape: Tensor, key: Tensor, counter: Tensor, alg: Tensor, *, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomNormalV2)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(key, TensorType({DT_UINT64}))\n
.INPUT(counter, TensorType({DT_UINT64}))\n
.INPUT(alg, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT, DT_DOUBLE}))\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomNormalV2"
    op.name = next_unique_name(node_name, "StatelessRandomNormalV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"
    op.input.append(alg.tensor)
    op.input_desc.add().CopyFrom(alg.desc)
    op.input_desc[-1].name = "alg"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StatelessRandomUniformV2
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def StatelessRandomUniformV2(shape: Tensor, key: Tensor, counter: Tensor, alg: Tensor, *, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(StatelessRandomUniformV2)\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(key, TensorType({DT_UINT64}))\n
.INPUT(counter, TensorType({DT_UINT64}))\n
.INPUT(alg, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_BF16, DT_FLOAT16, DT_DOUBLE}))\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StatelessRandomUniformV2"
    op.name = next_unique_name(node_name, "StatelessRandomUniformV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"
    op.input.append(key.tensor)
    op.input_desc.add().CopyFrom(key.desc)
    op.input_desc[-1].name = "key"
    op.input.append(counter.tensor)
    op.input_desc.add().CopyFrom(counter.desc)
    op.input_desc[-1].name = "counter"
    op.input.append(alg.tensor)
    op.input_desc.add().CopyFrom(alg.desc)
    op.input_desc[-1].name = "alg"

    # process attrs
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AnonymousSeedGenerator
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def AnonymousSeedGenerator(seed: Tensor, seed2: Tensor, reshuffle: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AnonymousSeedGenerator)\n
.INPUT(seed, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(seed2, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(reshuffle, TensorType({DT_BOOL}))\n
.OUTPUT(handle, TensorType({DT_RESOURSE}))\n
.OUTPUT(deleter, TensorType({DT_VARIANT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AnonymousSeedGenerator"
    op.name = next_unique_name(node_name, "AnonymousSeedGenerator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(seed.tensor)
    op.input_desc.add().CopyFrom(seed.desc)
    op.input_desc[-1].name = "seed"
    op.input.append(seed2.tensor)
    op.input_desc.add().CopyFrom(seed2.desc)
    op.input_desc[-1].name = "seed2"
    op.input.append(reshuffle.tensor)
    op.input_desc.add().CopyFrom(reshuffle.desc)
    op.input_desc[-1].name = "reshuffle"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "deleter"
    deleter = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle, deleter


# This api is auto-generated from IR DeleteSeedGenerator
@auto_convert_to_tensor([False, False], [False, False])
def DeleteSeedGenerator(handle: Tensor, deleter: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DeleteSeedGenerator)\n
.INPUT(handle, TensorType({DT_RESOURCE}))\n
.INPUT(deleter, TensorType({DT_VARIANT}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DeleteSeedGenerator"
    op.name = next_unique_name(node_name, "DeleteSeedGenerator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(handle.tensor)
    op.input_desc.add().CopyFrom(handle.desc)
    op.input_desc[-1].name = "handle"
    op.input.append(deleter.tensor)
    op.input_desc.add().CopyFrom(deleter.desc)
    op.input_desc[-1].name = "deleter"

    # process attrs

    # process outputs
    output_index = 0

    # return outputs
    return


# This api is auto-generated from IR DummySeedGenerator
@auto_convert_to_tensor([], [])
def DummySeedGenerator(*, dependencies=[], node_name=None):
    """REG_OP(DummySeedGenerator)\n
.OUTPUT(handle, TensorType({ DT_RESOURCE }))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DummySeedGenerator"
    op.name = next_unique_name(node_name, "DummySeedGenerator")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "handle"
    handle = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return handle


# This api is auto-generated from IR TemporaryVariable
@auto_convert_to_tensor([], [])
def TemporaryVariable(*, shape: List[int], dtype: int, var_name: str="", dependencies=[], node_name=None):
    """REG_OP(TemporaryVariable)\n
.OUTPUT(y, TensorType::ALL())\n
.REQUIRED_ATTR(shape, ListInt)\n
.REQUIRED_ATTR(dtype, Int)\n
.ATTR(var_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TemporaryVariable"
    op.name = next_unique_name(node_name, "TemporaryVariable")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs

    # process attrs
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["dtype"].i = dtype
    op.attr["var_name"].s = compat_as_bytes(var_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DestroyTemporaryVariable
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_ALL])
def DestroyTemporaryVariable(x: Tensor, *, var_name: str="", dependencies=[], node_name=None):
    """REG_OP(DestroyTemporaryVariable)\n
.INPUT(x, TensorType::ALL())\n
.OUTPUT(y, TensorType::ALL())\n
.ATTR(var_name, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DestroyTemporaryVariable"
    op.name = next_unique_name(node_name, "DestroyTemporaryVariable")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["var_name"].s = compat_as_bytes(var_name)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR IsVariableInitialized
@auto_convert_to_tensor([False], [False])
def IsVariableInitialized(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(IsVariableInitialized)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IsVariableInitialized"
    op.name = next_unique_name(node_name, "IsVariableInitialized")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR VarIsInitializedOp
@auto_convert_to_tensor([False], [False])
def VarIsInitializedOp(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(VarIsInitializedOp)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_INT8, DT_INT16, DT_UINT16, DT_UINT8, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_BOOL, DT_DOUBLE}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "VarIsInitializedOp"
    op.name = next_unique_name(node_name, "VarIsInitializedOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR CountUpTo
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_INDEX_NUMBER])
def CountUpTo(ref: Tensor, *, limit: int=0, dependencies=[], node_name=None):
    """REG_OP(CountUpTo)\n
.INPUT(ref, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(y, TensorType({DT_INT32, DT_INT64}))\n
.ATTR(limit, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CountUpTo"
    op.name = next_unique_name(node_name, "CountUpTo")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(ref.tensor)
    op.input_desc.add().CopyFrom(ref.desc)
    op.input_desc[-1].name = "ref"

    # process attrs
    op.attr["limit"].i = limit

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringNGrams
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def StringNGrams(data: Tensor, data_splits: Tensor, *, separator: str, left_pad: str, right_pad: str, pad_width: int, preserve_short_sequences: bool, ngram_widths: List[int]=[], dependencies=[], node_name=None):
    """REG_OP(StringNGrams)\n
.INPUT(data, TensorType({DT_STRING}))\n
.INPUT(data_splits, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(ngrams, TensorType({DT_STRING}))\n
.OUTPUT(ngrams_splits, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(separator, String)\n
.ATTR(ngram_widths, ListInt, {})\n
.REQUIRED_ATTR(left_pad, String)\n
.REQUIRED_ATTR(right_pad, String)\n
.REQUIRED_ATTR(pad_width, Int)\n
.REQUIRED_ATTR(preserve_short_sequences, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringNGrams"
    op.name = next_unique_name(node_name, "StringNGrams")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(data.tensor)
    op.input_desc.add().CopyFrom(data.desc)
    op.input_desc[-1].name = "data"
    op.input.append(data_splits.tensor)
    op.input_desc.add().CopyFrom(data_splits.desc)
    op.input_desc[-1].name = "data_splits"

    # process attrs
    op.attr["separator"].s = compat_as_bytes(separator)
    op.attr["left_pad"].s = compat_as_bytes(left_pad)
    op.attr["right_pad"].s = compat_as_bytes(right_pad)
    op.attr["pad_width"].i = pad_width
    op.attr["preserve_short_sequences"].b = preserve_short_sequences
    op.attr["ngram_widths"].list.val_type = 2
    op.attr["ngram_widths"].list.i.extend(ngram_widths)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "ngrams"
    ngrams = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "ngrams_splits"
    ngrams_splits = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return ngrams, ngrams_splits


# This api is auto-generated from IR UnicodeDecodeWithOffsets
@auto_convert_to_tensor([False], [False])
def UnicodeDecodeWithOffsets(input: Tensor, *, input_encoding: str, errors: str="replace", replacement_char: int=65533, replace_control_characters: bool=False, Tsplits: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(UnicodeDecodeWithOffsets)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(row_splits, TensorType({DT_INT64}))\n
.OUTPUT(char_values, TensorType({DT_INT32}))\n
.OUTPUT(char_to_byte_starts, TensorType({DT_INT64}))\n
.REQUIRED_ATTR(input_encoding, String)\n
.ATTR(errors, String, "replace")\n
.ATTR(replacement_char, Int, 65533)\n
.ATTR(replace_control_characters, Bool, false)\n
.ATTR(Tsplits, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnicodeDecodeWithOffsets"
    op.name = next_unique_name(node_name, "UnicodeDecodeWithOffsets")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["input_encoding"].s = compat_as_bytes(input_encoding)
    op.attr["errors"].s = compat_as_bytes(errors)
    op.attr["replacement_char"].i = replacement_char
    op.attr["replace_control_characters"].b = replace_control_characters
    op.attr["Tsplits"].dt = Tsplits

    # process outputs
    output_index = 0
    op.output_desc.add().name = "row_splits"
    row_splits = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "char_values"
    char_values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "char_to_byte_starts"
    char_to_byte_starts = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return row_splits, char_values, char_to_byte_starts


# This api is auto-generated from IR UnicodeDecode
@auto_convert_to_tensor([False], [False])
def UnicodeDecode(input: Tensor, *, input_encoding: str, errors: str="replace", replacement_char: int=65533, replace_control_characters: bool=False, Tsplits: int=DataType.DT_INT64, dependencies=[], node_name=None):
    """REG_OP(UnicodeDecode)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(row_splits, TensorType({DT_INT64}))\n
.OUTPUT(char_values, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(input_encoding, String)\n
.ATTR(errors, String, "replace")\n
.ATTR(replacement_char, Int, 65533)\n
.ATTR(replace_control_characters, Bool, false)\n
.ATTR(Tsplits, Type, DT_INT64)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnicodeDecode"
    op.name = next_unique_name(node_name, "UnicodeDecode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["input_encoding"].s = compat_as_bytes(input_encoding)
    op.attr["errors"].s = compat_as_bytes(errors)
    op.attr["replacement_char"].i = replacement_char
    op.attr["replace_control_characters"].b = replace_control_characters
    op.attr["Tsplits"].dt = Tsplits

    # process outputs
    output_index = 0
    op.output_desc.add().name = "row_splits"
    row_splits = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "char_values"
    char_values = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return row_splits, char_values


# This api is auto-generated from IR UnicodeTranscode
@auto_convert_to_tensor([False], [False])
def UnicodeTranscode(input: Tensor, *, input_encoding: str, output_encoding: str="UTF-8", errors: str="replace", replacement_char: int=65533, replace_control_characters: bool=False, dependencies=[], node_name=None):
    """REG_OP(UnicodeTranscode)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.REQUIRED_ATTR(input_encoding, String)\n
.ATTR(output_encoding, String, "UTF-8")\n
.ATTR(errors, String, "replace")\n
.ATTR(replacement_char, Int, 65533)\n
.ATTR(replace_control_characters, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnicodeTranscode"
    op.name = next_unique_name(node_name, "UnicodeTranscode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["input_encoding"].s = compat_as_bytes(input_encoding)
    op.attr["output_encoding"].s = compat_as_bytes(output_encoding)
    op.attr["errors"].s = compat_as_bytes(errors)
    op.attr["replacement_char"].i = replacement_char
    op.attr["replace_control_characters"].b = replace_control_characters

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR UnicodeEncode
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def UnicodeEncode(input_values: Tensor, input_splits: Tensor, *, errors: str="replace", output_encoding: str="UTF-8", replacement_char: int=65533, dependencies=[], node_name=None):
    """REG_OP(UnicodeEncode)\n
.INPUT(input_values, TensorType({DT_INT32}))\n
.INPUT(input_splits, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.ATTR(errors, String, "replace")\n
.ATTR(output_encoding, String, "UTF-8")\n
.ATTR(replacement_char, Int, 65533)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnicodeEncode"
    op.name = next_unique_name(node_name, "UnicodeEncode")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input_values.tensor)
    op.input_desc.add().CopyFrom(input_values.desc)
    op.input_desc[-1].name = "input_values"
    op.input.append(input_splits.tensor)
    op.input_desc.add().CopyFrom(input_splits.desc)
    op.input_desc[-1].name = "input_splits"

    # process attrs
    op.attr["errors"].s = compat_as_bytes(errors)
    op.attr["output_encoding"].s = compat_as_bytes(output_encoding)
    op.attr["replacement_char"].i = replacement_char

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR StringSplit
@auto_convert_to_tensor([False, False], [False, False])
def StringSplit(input: Tensor, delimiter: Tensor, *, skip_empty: bool=True, dependencies=[], node_name=None):
    """REG_OP(StringSplit)\n
.INPUT(input, TensorType({DT_STRING}))\n
.INPUT(delimiter, TensorType({DT_STRING}))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_STRING}))\n
.OUTPUT(shape, TensorType({DT_INT64}))\n
.ATTR(skip_empty, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringSplit"
    op.name = next_unique_name(node_name, "StringSplit")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(delimiter.tensor)
    op.input_desc.add().CopyFrom(delimiter.desc)
    op.input_desc[-1].name = "delimiter"

    # process attrs
    op.attr["skip_empty"].b = skip_empty

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "shape"
    shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, values, shape


# This api is auto-generated from IR StaticRegexReplace
@auto_convert_to_tensor([False], [False])
def StaticRegexReplace(input: Tensor, *, pattern: str="", rewrite: str="", replace_global: bool=True, dependencies=[], node_name=None):
    """REG_OP(StaticRegexReplace)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.ATTR(pattern, String, "")\n
.ATTR(rewrite, String, "")\n
.ATTR(replace_global, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StaticRegexReplace"
    op.name = next_unique_name(node_name, "StaticRegexReplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["pattern"].s = compat_as_bytes(pattern)
    op.attr["rewrite"].s = compat_as_bytes(rewrite)
    op.attr["replace_global"].b = replace_global

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR StaticRegexFullMatch
@auto_convert_to_tensor([False], [False])
def StaticRegexFullMatch(input: Tensor, *, pattern: str="", dependencies=[], node_name=None):
    """REG_OP(StaticRegexFullMatch)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType({DT_BOOL}))\n
.ATTR(pattern, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StaticRegexFullMatch"
    op.name = next_unique_name(node_name, "StaticRegexFullMatch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["pattern"].s = compat_as_bytes(pattern)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR UnsortedSegmentJoin
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def UnsortedSegmentJoin(input: Tensor, segment_ids: Tensor, num_segments: Tensor, *, separator: str="", dependencies=[], node_name=None):
    """REG_OP(UnsortedSegmentJoin)\n
.INPUT(input, TensorType({DT_STRING}))\n
.INPUT(segment_ids, TensorType({DT_INT32,DT_INT64}))\n
.INPUT(num_segments, TensorType({DT_INT32,DT_INT64}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.ATTR(separator, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnsortedSegmentJoin"
    op.name = next_unique_name(node_name, "UnsortedSegmentJoin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(segment_ids.tensor)
    op.input_desc.add().CopyFrom(segment_ids.desc)
    op.input_desc[-1].name = "segment_ids"
    op.input.append(num_segments.tensor)
    op.input_desc.add().CopyFrom(num_segments.desc)
    op.input_desc[-1].name = "num_segments"

    # process attrs
    op.attr["separator"].s = compat_as_bytes(separator)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR StringLower
@auto_convert_to_tensor([False], [False])
def StringLower(input: Tensor, *, encoding: str="", dependencies=[], node_name=None):
    """REG_OP(StringLower)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.ATTR(encoding, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringLower"
    op.name = next_unique_name(node_name, "StringLower")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["encoding"].s = compat_as_bytes(encoding)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR StringUpper
@auto_convert_to_tensor([False], [False])
def StringUpper(input: Tensor, *, encoding: str="", dependencies=[], node_name=None):
    """REG_OP(StringUpper)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.ATTR(encoding, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringUpper"
    op.name = next_unique_name(node_name, "StringUpper")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["encoding"].s = compat_as_bytes(encoding)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR StringSplitV2
@auto_convert_to_tensor([False, False], [False, False])
def StringSplitV2(input: Tensor, sep: Tensor, *, maxsplit: int=-1, dependencies=[], node_name=None):
    """REG_OP(StringSplitV2)\n
.INPUT(input, TensorType({DT_STRING}))\n
.INPUT(sep, TensorType({DT_STRING}))\n
.OUTPUT(indices, TensorType({DT_INT64}))\n
.OUTPUT(values, TensorType({DT_STRING}))\n
.OUTPUT(shape, TensorType({DT_INT64}))\n
.ATTR(maxsplit, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringSplitV2"
    op.name = next_unique_name(node_name, "StringSplitV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(sep.tensor)
    op.input_desc.add().CopyFrom(sep.desc)
    op.input_desc[-1].name = "sep"

    # process attrs
    op.attr["maxsplit"].i = maxsplit

    # process outputs
    output_index = 0
    op.output_desc.add().name = "indices"
    indices = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "values"
    values = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "shape"
    shape = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return indices, values, shape


# This api is auto-generated from IR UnicodeScript
@auto_convert_to_tensor([False], [False])
def UnicodeScript(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(UnicodeScript)\n
.INPUT(x, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "UnicodeScript"
    op.name = next_unique_name(node_name, "UnicodeScript")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Substr
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def Substr(input: Tensor, pos: Tensor, len: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Substr)\n
.INPUT(input, TensorType({DT_STRING}))\n
.INPUT(pos, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(len, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Substr"
    op.name = next_unique_name(node_name, "Substr")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"
    op.input.append(pos.tensor)
    op.input_desc.add().CopyFrom(pos.desc)
    op.input_desc[-1].name = "pos"
    op.input.append(len.tensor)
    op.input_desc.add().CopyFrom(len.desc)
    op.input_desc[-1].name = "len"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR StringToHashBucketFast
@auto_convert_to_tensor([False], [False])
def StringToHashBucketFast(x: Tensor, *, num_buckets: int=1, dependencies=[], node_name=None):
    """REG_OP(StringToHashBucketFast)\n
.INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT64}))\n
.ATTR(num_buckets, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringToHashBucketFast"
    op.name = next_unique_name(node_name, "StringToHashBucketFast")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["num_buckets"].i = num_buckets

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringToHashBucketStrong
@auto_convert_to_tensor([False], [False])
def StringToHashBucketStrong(x: Tensor, *, key: List[int], num_buckets: int=1, dependencies=[], node_name=None):
    """REG_OP(StringToHashBucketStrong)\n
.INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT64}))\n
.ATTR(num_buckets, Int, 1)\n
.REQUIRED_ATTR(key, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringToHashBucketStrong"
    op.name = next_unique_name(node_name, "StringToHashBucketStrong")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["key"].list.val_type = 2
    op.attr["key"].list.i.extend(key)
    op.attr["num_buckets"].i = num_buckets

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringToHashBucket
@auto_convert_to_tensor([False], [False])
def StringToHashBucket(string_tensor: Tensor, *, num_buckets: int=1, dependencies=[], node_name=None):
    """REG_OP(StringToHashBucket)\n
.INPUT(string_tensor, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT64}))\n
.ATTR(num_buckets, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringToHashBucket"
    op.name = next_unique_name(node_name, "StringToHashBucket")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(string_tensor.tensor)
    op.input_desc.add().CopyFrom(string_tensor.desc)
    op.input_desc[-1].name = "string_tensor"

    # process attrs
    op.attr["num_buckets"].i = num_buckets

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringStrip
@auto_convert_to_tensor([False], [False])
def StringStrip(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(StringStrip)\n
.INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringStrip"
    op.name = next_unique_name(node_name, "StringStrip")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringLength
@auto_convert_to_tensor([False], [False])
def StringLength(x: Tensor, *, unit: str="BYTE", dependencies=[], node_name=None):
    """REG_OP(StringLength)\n
.INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.ATTR(unit, String, "BYTE")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringLength"
    op.name = next_unique_name(node_name, "StringLength")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["unit"].s = compat_as_bytes(unit)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringJoin
@auto_convert_to_tensor([True], [False])
def StringJoin(x: List[Tensor], *, N: int, separator: str="", dependencies=[], node_name=None):
    """REG_OP(StringJoin)\n
.DYNAMIC_INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_STRING}))\n
.REQUIRED_ATTR(N, Int)\n
.ATTR(separator, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringJoin"
    op.name = next_unique_name(node_name, "StringJoin")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["N"].i = N
    op.attr["separator"].s = compat_as_bytes(separator)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringFormat
@auto_convert_to_tensor([True], [False])
def StringFormat(x: List[Tensor], *, template: str="%s", placeholder: str="%s", summarize: int=3, dependencies=[], node_name=None):
    """REG_OP(StringFormat)\n
.DYNAMIC_INPUT(x, TensorType({DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_INT64, DT_UINT32, DT_UINT64, DT_STRING, DT_FLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL}))\n
.OUTPUT(y, TensorType({DT_STRING}))\n
.ATTR(template, String, "%s")\n
.ATTR(placeholder, String, "%s")\n
.ATTR(summarize, Int, 3)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringFormat"
    op.name = next_unique_name(node_name, "StringFormat")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)

    # process attrs
    op.attr["template"].s = compat_as_bytes(template)
    op.attr["placeholder"].s = compat_as_bytes(placeholder)
    op.attr["summarize"].i = summarize

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RegexFullMatch
@auto_convert_to_tensor([False, False], [False, False])
def RegexFullMatch(x: Tensor, pattern: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(RegexFullMatch)\n
.INPUT(x, TensorType({DT_STRING}))\n
.INPUT(pattern, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_BOOL}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RegexFullMatch"
    op.name = next_unique_name(node_name, "RegexFullMatch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(pattern.tensor)
    op.input_desc.add().CopyFrom(pattern.desc)
    op.input_desc[-1].name = "pattern"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR RegexReplace
@auto_convert_to_tensor([False, False, False], [False, False, False])
def RegexReplace(x: Tensor, pattern: Tensor, rewrite: Tensor, *, replace_global: bool=True, dependencies=[], node_name=None):
    """REG_OP(RegexReplace)\n
.INPUT(x, TensorType({DT_STRING}))\n
.INPUT(pattern, TensorType({DT_STRING}))\n
.INPUT(rewrite, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_STRING}))\n
.ATTR(replace_global, Bool, true)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "RegexReplace"
    op.name = next_unique_name(node_name, "RegexReplace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(pattern.tensor)
    op.input_desc.add().CopyFrom(pattern.desc)
    op.input_desc[-1].name = "pattern"
    op.input.append(rewrite.tensor)
    op.input_desc.add().CopyFrom(rewrite.desc)
    op.input_desc[-1].name = "rewrite"

    # process attrs
    op.attr["replace_global"].b = replace_global

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AsString
@auto_convert_to_tensor([False], [False])
def AsString(x: Tensor, *, precision: int=-1, scientific: bool=False, shortest: bool=False, width: int=-1, fill: str="", dependencies=[], node_name=None):
    """REG_OP(AsString)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_COMPLEX128}))\n
.OUTPUT(y, TensorType({DT_STRING}))\n
.ATTR(precision, Int, -1)\n
.ATTR(scientific, Bool, false)\n
.ATTR(shortest, Bool, false)\n
.ATTR(width, Int, -1)\n
.ATTR(fill, String, "")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AsString"
    op.name = next_unique_name(node_name, "AsString")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["precision"].i = precision
    op.attr["scientific"].b = scientific
    op.attr["shortest"].b = shortest
    op.attr["width"].i = width
    op.attr["fill"].s = compat_as_bytes(fill)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR EncodeBase64
@auto_convert_to_tensor([False], [False])
def EncodeBase64(x: Tensor, *, pad: bool=False, dependencies=[], node_name=None):
    """REG_OP(EncodeBase64)\n
.INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_STRING}))\n
.ATTR(pad, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "EncodeBase64"
    op.name = next_unique_name(node_name, "EncodeBase64")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["pad"].b = pad

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DecodeBase64
@auto_convert_to_tensor([False], [False])
def DecodeBase64(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DecodeBase64)\n
.INPUT(x, TensorType({DT_STRING}))\n
.OUTPUT(y, TensorType({DT_STRING}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DecodeBase64"
    op.name = next_unique_name(node_name, "DecodeBase64")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR StringNormalizer
@auto_convert_to_tensor([False], [False])
def StringNormalizer(input: Tensor, *, stopwords: List[str]=[], is_case_sensitive: bool=False, case_change_action: str="NONE", locale: str="C", dependencies=[], node_name=None):
    """REG_OP(StringNormalizer)\n
.INPUT(input, TensorType({DT_STRING}))\n
.OUTPUT(output, TensorType({DT_STRING}))\n
.ATTR(stopwords, ListString, {})\n
.ATTR(is_case_sensitive, Bool, false)\n
.ATTR(case_change_action, String, "NONE")\n
.ATTR(locale, String, "C")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "StringNormalizer"
    op.name = next_unique_name(node_name, "StringNormalizer")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["stopwords"].list.val_type = 0
    op.attr["stopwords"].list.s.extend(compat_as_bytes_list(stopwords))
    op.attr["is_case_sensitive"].b = is_case_sensitive
    op.attr["case_change_action"].s = compat_as_bytes(case_change_action)
    op.attr["locale"].s = compat_as_bytes(locale)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR SwapCo
@auto_convert_to_tensor([False], [False])
def SwapCo(x: Tensor, *, output_dim: int=0, group_size: int=0, dependencies=[], node_name=None):
    """REG_OP(SwapCo)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.ATTR(output_dim, Int, 0)\n
.ATTR(group_size, Int, 0)\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SwapCo"
    op.name = next_unique_name(node_name, "SwapCo")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["output_dim"].i = output_dim
    op.attr["group_size"].i = group_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TargetCropAndResize
@auto_convert_to_tensor([False, False, False], [False, False, False])
def TargetCropAndResize(x: Tensor, boxes: Tensor, box_index: Tensor, *, output_h: int=224, output_w: int=224, input_format: str="YUV420SP_U8", dependencies=[], node_name=None):
    """REG_OP(TargetCropAndResize)\n
.INPUT(x, TensorType({DT_UINT8}))\n
.INPUT(boxes, TensorType({DT_INT32}))\n
.INPUT(box_index, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_UINT8}))\n
.ATTR(output_h, Int, 224)\n
.ATTR(output_w, Int, 224)\n
.ATTR(input_format, String, "YUV420SP_U8")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TargetCropAndResize"
    op.name = next_unique_name(node_name, "TargetCropAndResize")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(boxes.tensor)
    op.input_desc.add().CopyFrom(boxes.desc)
    op.input_desc[-1].name = "boxes"
    op.input.append(box_index.tensor)
    op.input_desc.add().CopyFrom(box_index.desc)
    op.input_desc[-1].name = "box_index"

    # process attrs
    op.attr["output_h"].i = output_h
    op.attr["output_w"].i = output_w
    op.attr["input_format"].s = compat_as_bytes(input_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Bitcast
@auto_convert_to_tensor([False], [False])
def Bitcast(x: Tensor, *, type: int, dependencies=[], node_name=None):
    """REG_OP(Bitcast)\n
.INPUT(x, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT32, DT_UINT8, DT_INT64, DT_UINT64, DT_INT16, DT_UINT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32}))\n
.OUTPUT(y, TensorType({DT_BOOL, DT_FLOAT16, DT_FLOAT, DT_INT8, DT_INT32, DT_UINT32, DT_UINT8, DT_INT64, DT_UINT64, DT_INT16, DT_UINT16, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_QINT8, DT_QUINT8, DT_QINT16, DT_QUINT16, DT_QINT32}))\n
.REQUIRED_ATTR(type, Type)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Bitcast"
    op.name = next_unique_name(node_name, "Bitcast")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["type"].dt = type

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DepthwiseWeight4DTo6D
@auto_convert_to_tensor([False], [False])
def DepthwiseWeight4DTo6D(x: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(DepthwiseWeight4DTo6D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT16}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthwiseWeight4DTo6D"
    op.name = next_unique_name(node_name, "DepthwiseWeight4DTo6D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DepthwiseWeight6DTo4D
@auto_convert_to_tensor([False], [False])
def DepthwiseWeight6DTo4D(x: Tensor, *, channel_size: int=16, dependencies=[], node_name=None):
    """REG_OP(DepthwiseWeight6DTo4D)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT16}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32, DT_UINT16}))\n
.ATTR(channel_size, Int, 16)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthwiseWeight6DTo4D"
    op.name = next_unique_name(node_name, "DepthwiseWeight6DTo4D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["channel_size"].i = channel_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TransposeD
@auto_convert_to_tensor([False], [False])
def TransposeD(x: Tensor, *, perm: List[int], dependencies=[], node_name=None):
    """REG_OP(TransposeD)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(perm, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TransposeD"
    op.name = next_unique_name(node_name, "TransposeD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["perm"].list.val_type = 2
    op.attr["perm"].list.i.extend(perm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Transpose
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def Transpose(x: Tensor, perm: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Transpose)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(perm, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Transpose"
    op.name = next_unique_name(node_name, "Transpose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(perm.tensor)
    op.input_desc.add().CopyFrom(perm.desc)
    op.input_desc[-1].name = "perm"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TransData
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def TransData(src: Tensor, *, src_format: str, dst_format: str, src_subformat: int=0, dst_subformat: int=0, groups: int=1, dependencies=[], node_name=None):
    """REG_OP(TransData)\n
.INPUT(src, TensorType::BasicType())\n
.OUTPUT(dst, TensorType::BasicType())\n
.REQUIRED_ATTR(src_format, String)\n
.REQUIRED_ATTR(dst_format, String)\n
.ATTR(src_subformat, Int, 0)\n
.ATTR(dst_subformat, Int, 0)\n
.ATTR(groups, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TransData"
    op.name = next_unique_name(node_name, "TransData")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(src.tensor)
    op.input_desc.add().CopyFrom(src.desc)
    op.input_desc[-1].name = "src"

    # process attrs
    op.attr["src_format"].s = compat_as_bytes(src_format)
    op.attr["dst_format"].s = compat_as_bytes(dst_format)
    op.attr["src_subformat"].i = src_subformat
    op.attr["dst_subformat"].i = dst_subformat
    op.attr["groups"].i = groups

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dst"
    dst = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dst


# This api is auto-generated from IR TransDataRNN
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def TransDataRNN(src: Tensor, *, src_format: str, dst_format: str, input_size: int, hidden_size: int, dependencies=[], node_name=None):
    """REG_OP(TransDataRNN)\n
.INPUT(src, TensorType::BasicType())\n
.OUTPUT(dst, TensorType::BasicType())\n
.REQUIRED_ATTR(src_format, String)\n
.REQUIRED_ATTR(dst_format, String)\n
.REQUIRED_ATTR(input_size, Int)\n
.REQUIRED_ATTR(hidden_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TransDataRNN"
    op.name = next_unique_name(node_name, "TransDataRNN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(src.tensor)
    op.input_desc.add().CopyFrom(src.desc)
    op.input_desc[-1].name = "src"

    # process attrs
    op.attr["src_format"].s = compat_as_bytes(src_format)
    op.attr["dst_format"].s = compat_as_bytes(dst_format)
    op.attr["input_size"].i = input_size
    op.attr["hidden_size"].i = hidden_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "dst"
    dst = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return dst


# This api is auto-generated from IR Permute
@auto_convert_to_tensor([False], [False])
def Permute(x: Tensor, *, order: List[int]=[0], dependencies=[], node_name=None):
    """REG_OP(Permute)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(order, ListInt, {0})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Permute"
    op.name = next_unique_name(node_name, "Permute")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["order"].list.val_type = 2
    op.attr["order"].list.i.extend(order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Flatten
@auto_convert_to_tensor([False], [False])
def Flatten(x: Tensor, *, axis: int=1, dependencies=[], node_name=None):
    """REG_OP(Flatten)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT, DT_FLOAT16}))\n
.ATTR(axis, Int, 1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Flatten"
    op.name = next_unique_name(node_name, "Flatten")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BatchToSpaceND
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def BatchToSpaceND(x: Tensor, block_shape: Tensor, crops: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(BatchToSpaceND)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(block_shape, TensorType::IndexNumberType())\n
.INPUT(crops, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchToSpaceND"
    op.name = next_unique_name(node_name, "BatchToSpaceND")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(block_shape.tensor)
    op.input_desc.add().CopyFrom(block_shape.desc)
    op.input_desc[-1].name = "block_shape"
    op.input.append(crops.tensor)
    op.input_desc.add().CopyFrom(crops.desc)
    op.input_desc[-1].name = "crops"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BatchToSpaceNDD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def BatchToSpaceNDD(x: Tensor, *, block_shape: List[int], crops: List[int], dependencies=[], node_name=None):
    """REG_OP(BatchToSpaceNDD)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_shape, ListInt)\n
.REQUIRED_ATTR(crops, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchToSpaceNDD"
    op.name = next_unique_name(node_name, "BatchToSpaceNDD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["block_shape"].list.val_type = 2
    op.attr["block_shape"].list.i.extend(block_shape)
    op.attr["crops"].list.val_type = 2
    op.attr["crops"].list.i.extend(crops)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SpaceToBatchND
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def SpaceToBatchND(x: Tensor, block_shape: Tensor, paddings: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SpaceToBatchND)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(block_shape, TensorType::IndexNumberType())\n
.INPUT(paddings, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SpaceToBatchND"
    op.name = next_unique_name(node_name, "SpaceToBatchND")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(block_shape.tensor)
    op.input_desc.add().CopyFrom(block_shape.desc)
    op.input_desc[-1].name = "block_shape"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SpaceToBatchNDD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def SpaceToBatchNDD(x: Tensor, *, block_shape: List[int], paddings: List[int], dependencies=[], node_name=None):
    """REG_OP(SpaceToBatchNDD)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_shape, ListInt)\n
.REQUIRED_ATTR(paddings, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SpaceToBatchNDD"
    op.name = next_unique_name(node_name, "SpaceToBatchNDD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["block_shape"].list.val_type = 2
    op.attr["block_shape"].list.i.extend(block_shape)
    op.attr["paddings"].list.val_type = 2
    op.attr["paddings"].list.i.extend(paddings)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SpaceToDepth
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def SpaceToDepth(x: Tensor, *, block_size: int, data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(SpaceToDepth)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_size, Int)\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SpaceToDepth"
    op.name = next_unique_name(node_name, "SpaceToDepth")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["block_size"].i = block_size
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR DepthToSpace
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def DepthToSpace(x: Tensor, *, block_size: int, mode: str="DCR", data_format: str="NHWC", dependencies=[], node_name=None):
    """REG_OP(DepthToSpace)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_size, Int)\n
.ATTR(mode, String, "DCR")\n
.ATTR(data_format, String, "NHWC")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "DepthToSpace"
    op.name = next_unique_name(node_name, "DepthToSpace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["block_size"].i = block_size
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BatchToSpace
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def BatchToSpace(x: Tensor, crops: Tensor, *, block_size: int, dependencies=[], node_name=None):
    """REG_OP(BatchToSpace)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(crops, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchToSpace"
    op.name = next_unique_name(node_name, "BatchToSpace")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(crops.tensor)
    op.input_desc.add().CopyFrom(crops.desc)
    op.input_desc[-1].name = "crops"

    # process attrs
    op.attr["block_size"].i = block_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR BatchToSpaceD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def BatchToSpaceD(x: Tensor, *, block_size: int, crops: List[int], dependencies=[], node_name=None):
    """REG_OP(BatchToSpaceD)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_size, Int)\n
.REQUIRED_ATTR(crops, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "BatchToSpaceD"
    op.name = next_unique_name(node_name, "BatchToSpaceD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["block_size"].i = block_size
    op.attr["crops"].list.val_type = 2
    op.attr["crops"].list.i.extend(crops)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SpaceToBatch
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def SpaceToBatch(x: Tensor, paddings: Tensor, *, block_size: int, dependencies=[], node_name=None):
    """REG_OP(SpaceToBatch)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(paddings, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SpaceToBatch"
    op.name = next_unique_name(node_name, "SpaceToBatch")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(paddings.tensor)
    op.input_desc.add().CopyFrom(paddings.desc)
    op.input_desc[-1].name = "paddings"

    # process attrs
    op.attr["block_size"].i = block_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SpaceToBatchD
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def SpaceToBatchD(x: Tensor, *, block_size: int, paddings: List[int], dependencies=[], node_name=None):
    """REG_OP(SpaceToBatchD)\n
.INPUT(x, TensorType::BasicType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(block_size, Int)\n
.REQUIRED_ATTR(paddings, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SpaceToBatchD"
    op.name = next_unique_name(node_name, "SpaceToBatchD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["block_size"].i = block_size
    op.attr["paddings"].list.val_type = 2
    op.attr["paddings"].list.i.extend(paddings)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Unpack
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_BASIC])
def _Unpack(x: Tensor, *, size_of_y: int, num: int, axis: int=0, dependencies=[], node_name=None):
    """REG_OP(Unpack)\n
.INPUT(x, TensorType::BasicType())\n
.DYNAMIC_OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(num, Int)\n
.ATTR(axis, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Unpack"
    op.name = next_unique_name(node_name, "Unpack")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["num"].i = num
    op.attr["axis"].i = axis

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR ExtractImagePatches
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def ExtractImagePatches(x: Tensor, *, ksizes: List[int], strides: List[int], rates: List[int], padding: str, dependencies=[], node_name=None):
    """REG_OP(ExtractImagePatches)\n
.INPUT(x, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksizes, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(rates, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ExtractImagePatches"
    op.name = next_unique_name(node_name, "ExtractImagePatches")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksizes"].list.val_type = 2
    op.attr["ksizes"].list.i.extend(ksizes)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["rates"].list.val_type = 2
    op.attr["rates"].list.i.extend(rates)
    op.attr["padding"].s = compat_as_bytes(padding)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ExtractVolumePatches
@auto_convert_to_tensor([False], [False])
def ExtractVolumePatches(x: Tensor, *, ksizes: List[int], strides: List[int], padding: str, dependencies=[], node_name=None):
    """REG_OP(ExtractVolumePatches)\n
.INPUT(x, TensorType::REALNUMBERTYPE())\n
.OUTPUT(y, TensorType::REALNUMBERTYPE())\n
.REQUIRED_ATTR(ksizes, ListInt)\n
.REQUIRED_ATTR(strides, ListInt)\n
.REQUIRED_ATTR(padding, String)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ExtractVolumePatches"
    op.name = next_unique_name(node_name, "ExtractVolumePatches")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksizes"].list.val_type = 2
    op.attr["ksizes"].list.i.extend(ksizes)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["padding"].s = compat_as_bytes(padding)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConfusionTransposeD
@auto_convert_to_tensor([False], [False])
def ConfusionTransposeD(x: Tensor, *, perm: List[int], shape: List[int], transpose_first: bool, dependencies=[], node_name=None):
    """REG_OP(ConfusionTransposeD)\n
.INPUT(x, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT8, DT_UINT16, DT_UINT32, DT_UINT64, DT_FLOAT16, DT_FLOAT}))\n
.REQUIRED_ATTR(perm, ListInt)\n
.REQUIRED_ATTR(shape, ListInt)\n
.REQUIRED_ATTR(transpose_first, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConfusionTransposeD"
    op.name = next_unique_name(node_name, "ConfusionTransposeD")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["perm"].list.val_type = 2
    op.attr["perm"].list.i.extend(perm)
    op.attr["shape"].list.val_type = 2
    op.attr["shape"].list.i.extend(shape)
    op.attr["transpose_first"].b = transpose_first

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConfusionTranspose
@auto_convert_to_tensor([False, False], [False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER])
def ConfusionTranspose(x: Tensor, shape: Tensor, *, perm: List[int], transpose_first: bool, dependencies=[], node_name=None):
    """REG_OP(ConfusionTranspose)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(shape, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
.REQUIRED_ATTR(perm, ListInt)\n
.REQUIRED_ATTR(transpose_first, Bool)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ConfusionTranspose"
    op.name = next_unique_name(node_name, "ConfusionTranspose")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs
    op.attr["perm"].list.val_type = 2
    op.attr["perm"].list.i.extend(perm)
    op.attr["transpose_first"].b = transpose_first

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR FlattenV2
@auto_convert_to_tensor([False], [False])
def FlattenV2(x: Tensor, *, axis: int=1, end_axis: int=-1, dependencies=[], node_name=None):
    """REG_OP(FlattenV2)\n
.INPUT(x, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT8, DT_UINT8, DT_INT16, DT_UINT16, DT_INT32, DT_UINT32, DT_INT64, DT_UINT64}))\n
.ATTR(axis, Int, 1)\n
.ATTR(end_axis, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FlattenV2"
    op.name = next_unique_name(node_name, "FlattenV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["axis"].i = axis
    op.attr["end_axis"].i = end_axis

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Compress
@auto_convert_to_tensor([False], [False])
def Compress(weight: Tensor, *, compress_parameters: List[int], dependencies=[], node_name=None):
    """REG_OP(Compress)\n
.INPUT(weight, TensorType({DT_INT8, DT_FLOAT16}))\n
.OUTPUT(weight_compress, TensorType({DT_INT8, DT_FLOAT16}))\n
.OUTPUT(compress_index, TensorType({DT_INT8}))\n
.REQUIRED_ATTR(compress_parameters, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Compress"
    op.name = next_unique_name(node_name, "Compress")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["compress_parameters"].list.val_type = 2
    op.attr["compress_parameters"].list.i.extend(compress_parameters)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "weight_compress"
    weight_compress = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "compress_index"
    compress_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return weight_compress, compress_index


# This api is auto-generated from IR CompressFcOp
@auto_convert_to_tensor([False], [False])
def CompressFcOp(weight: Tensor, *, compress_parameters: List[int], dependencies=[], node_name=None):
    """REG_OP(CompressFcOp)\n
.INPUT(weight, TensorType({DT_INT8}))\n
.OUTPUT(weight_compress, TensorType({DT_INT8}))\n
.OUTPUT(compress_index, TensorType({DT_INT8}))\n
.REQUIRED_ATTR(compress_parameters, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CompressFcOp"
    op.name = next_unique_name(node_name, "CompressFcOp")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"

    # process attrs
    op.attr["compress_parameters"].list.val_type = 2
    op.attr["compress_parameters"].list.i.extend(compress_parameters)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "weight_compress"
    weight_compress = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "compress_index"
    compress_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return weight_compress, compress_index


# This api is auto-generated from IR Col2im
@auto_convert_to_tensor([False, False], [False, False])
def Col2im(x: Tensor, output_size: Tensor, *, kernel_size: List[int], dilation: List[int], padding: List[int], stride: List[int], dependencies=[], node_name=None):
    """REG_OP(Col2im)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(output_size, TensorType({DT_INT32, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(kernel_size, ListInt)\n
.REQUIRED_ATTR(dilation, ListInt)\n
.REQUIRED_ATTR(padding, ListInt)\n
.REQUIRED_ATTR(stride, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Col2im"
    op.name = next_unique_name(node_name, "Col2im")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(output_size.tensor)
    op.input_desc.add().CopyFrom(output_size.desc)
    op.input_desc[-1].name = "output_size"

    # process attrs
    op.attr["kernel_size"].list.val_type = 2
    op.attr["kernel_size"].list.i.extend(kernel_size)
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["padding"].list.val_type = 2
    op.attr["padding"].list.i.extend(padding)
    op.attr["stride"].list.val_type = 2
    op.attr["stride"].list.i.extend(stride)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Col2ImV2
@auto_convert_to_tensor([False, False, False], [False, False, False])
def Col2ImV2(x: Tensor, output_size: Tensor, kernel_size: Tensor, *, dilation: List[int], padding: List[int], stride: List[int], dependencies=[], node_name=None):
    """REG_OP(Col2ImV2)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.INPUT(output_size, TensorType({DT_INT32, DT_INT32}))\n
.INPUT(kernel_size, TensorType({DT_INT32, DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16}))\n
.REQUIRED_ATTR(dilation, ListInt)\n
.REQUIRED_ATTR(padding, ListInt)\n
.REQUIRED_ATTR(stride, ListInt)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Col2ImV2"
    op.name = next_unique_name(node_name, "Col2ImV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(output_size.tensor)
    op.input_desc.add().CopyFrom(output_size.desc)
    op.input_desc[-1].name = "output_size"
    op.input.append(kernel_size.tensor)
    op.input_desc.add().CopyFrom(kernel_size.desc)
    op.input_desc[-1].name = "kernel_size"

    # process attrs
    op.attr["dilation"].list.val_type = 2
    op.attr["dilation"].list.i.extend(dilation)
    op.attr["padding"].list.val_type = 2
    op.attr["padding"].list.i.extend(padding)
    op.attr["stride"].list.val_type = 2
    op.attr["stride"].list.i.extend(stride)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR Im2col
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_REAL_NUMBER])
def Im2col(x: Tensor, *, ksizes: List[int], strides: List[int]=[1], dilations: List[int]=[1], padding_mode: str="CALCULATED", pads: List[int]=[0], dependencies=[], node_name=None):
    """REG_OP(Im2col)\n
.INPUT(x, TensorType::RealNumberType())\n
.OUTPUT(y, TensorType::RealNumberType())\n
.REQUIRED_ATTR(ksizes, ListInt)\n
.ATTR(strides, ListInt, {1})\n
.ATTR(dilations, ListInt, {1})\n
.ATTR(padding_mode, String, "CALCULATED")\n
.ATTR(pads, ListInt, {0})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Im2col"
    op.name = next_unique_name(node_name, "Im2col")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ksizes"].list.val_type = 2
    op.attr["ksizes"].list.i.extend(ksizes)
    op.attr["strides"].list.val_type = 2
    op.attr["strides"].list.i.extend(strides)
    op.attr["dilations"].list.val_type = 2
    op.attr["dilations"].list.i.extend(dilations)
    op.attr["padding_mode"].s = compat_as_bytes(padding_mode)
    op.attr["pads"].list.val_type = 2
    op.attr["pads"].list.i.extend(pads)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AffineGrid
@auto_convert_to_tensor([False, False], [False, False])
def AffineGrid(theta: Tensor, output_size: Tensor, *, align_corners: bool=False, dependencies=[], node_name=None):
    """REG_OP(AffineGrid)\n
.INPUT(theta, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(output_size, TensorType({DT_INT32}))\n
.OUTPUT(y, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(align_corners, Bool, false)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AffineGrid"
    op.name = next_unique_name(node_name, "AffineGrid")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(theta.tensor)
    op.input_desc.add().CopyFrom(theta.desc)
    op.input_desc[-1].name = "theta"
    op.input.append(output_size.tensor)
    op.input_desc.add().CopyFrom(output_size.desc)
    op.input_desc[-1].name = "output_size"

    # process attrs
    op.attr["align_corners"].b = align_corners

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR AsStrided
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_BASIC, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER, TensorType.TT_INDEX_NUMBER])
def AsStrided(x: Tensor, size: Tensor, stride: Tensor, storage_offset: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(AsStrided)\n
.INPUT(x, TensorType::BasicType())\n
.INPUT(size, TensorType::IndexNumberType())\n
.INPUT(stride, TensorType::IndexNumberType())\n
.INPUT(storage_offset, TensorType::IndexNumberType())\n
.OUTPUT(y, TensorType::BasicType())\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "AsStrided"
    op.name = next_unique_name(node_name, "AsStrided")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(size.tensor)
    op.input_desc.add().CopyFrom(size.desc)
    op.input_desc[-1].name = "size"
    op.input.append(stride.tensor)
    op.input_desc.add().CopyFrom(stride.desc)
    op.input_desc[-1].name = "stride"
    op.input.append(storage_offset.tensor)
    op.input_desc.add().CopyFrom(storage_offset.desc)
    op.input_desc[-1].name = "storage_offset"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TfIdfVectorizer
@auto_convert_to_tensor([False], [False])
def TfIdfVectorizer(input: Tensor, *, max_gram_length: int, max_skip_count: int, min_gram_length: int, mode: str, ngram_counts: List[int], ngram_indexes: List[int], pool_int64s: List[int]=[], pool_strings: List[str]=[], weights: List[float]=[], dependencies=[], node_name=None):
    """REG_OP(TfIdfVectorizer)\n
.INPUT(input, TensorType({DT_INT32, DT_INT64, DT_STRING}))\n
.OUTPUT(output, TensorType({DT_FLOAT}))\n
.REQUIRED_ATTR(max_gram_length, Int)\n
.REQUIRED_ATTR(max_skip_count, Int)\n
.REQUIRED_ATTR(min_gram_length, Int)\n
.REQUIRED_ATTR(mode, String)\n
.REQUIRED_ATTR(ngram_counts, ListInt)\n
.REQUIRED_ATTR(ngram_indexes, ListInt)\n
.ATTR(pool_int64s, ListInt, {})\n
.ATTR(pool_strings, ListString, {})\n
.ATTR(weights, ListFloat, {})\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TfIdfVectorizer"
    op.name = next_unique_name(node_name, "TfIdfVectorizer")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(input.tensor)
    op.input_desc.add().CopyFrom(input.desc)
    op.input_desc[-1].name = "input"

    # process attrs
    op.attr["max_gram_length"].i = max_gram_length
    op.attr["max_skip_count"].i = max_skip_count
    op.attr["min_gram_length"].i = min_gram_length
    op.attr["mode"].s = compat_as_bytes(mode)
    op.attr["ngram_counts"].list.val_type = 2
    op.attr["ngram_counts"].list.i.extend(ngram_counts)
    op.attr["ngram_indexes"].list.val_type = 2
    op.attr["ngram_indexes"].list.i.extend(ngram_indexes)
    op.attr["pool_int64s"].list.val_type = 2
    op.attr["pool_int64s"].list.i.extend(pool_int64s)
    op.attr["pool_strings"].list.val_type = 0
    op.attr["pool_strings"].list.s.extend(compat_as_bytes_list(pool_strings))
    op.attr["weights"].list.val_type = 3
    op.attr["weights"].list.f.extend(weights)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "output"
    output = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return output


# This api is auto-generated from IR GenADC
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def GenADC(query: Tensor, code_book: Tensor, centroids: Tensor, bucket_list: Tensor, *, distance_type: str="l2sqr", dependencies=[], node_name=None):
    """REG_OP(GenADC)\n
.INPUT(query, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(code_book, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(centroids, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bucket_list, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(adc_tables, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.ATTR(distance_type, String, "l2sqr")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GenADC"
    op.name = next_unique_name(node_name, "GenADC")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    op.input.append(code_book.tensor)
    op.input_desc.add().CopyFrom(code_book.desc)
    op.input_desc[-1].name = "code_book"
    op.input.append(centroids.tensor)
    op.input_desc.add().CopyFrom(centroids.desc)
    op.input_desc[-1].name = "centroids"
    op.input.append(bucket_list.tensor)
    op.input_desc.add().CopyFrom(bucket_list.desc)
    op.input_desc[-1].name = "bucket_list"

    # process attrs
    op.attr["distance_type"].s = compat_as_bytes(distance_type)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "adc_tables"
    adc_tables = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return adc_tables


# This api is auto-generated from IR TopKPQDistance
@auto_convert_to_tensor([True, True, True, True, True], [False, False, False, False, False])
def TopKPQDistance(actual_count: List[Tensor], pq_distance: List[Tensor], grouped_extreme_distance: List[Tensor], pq_ivf: List[Tensor], pq_index: List[Tensor], *, k: int, group_size: int, order: str="ASC", dependencies=[], node_name=None):
    """REG_OP(TopKPQDistance)\n
.DYNAMIC_INPUT(actual_count, TensorType({DT_INT32}))\n
.DYNAMIC_INPUT(pq_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_INPUT(grouped_extreme_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.DYNAMIC_INPUT(pq_ivf, TensorType({DT_INT32}))\n
.DYNAMIC_INPUT(pq_index, TensorType({DT_INT32}))\n
.OUTPUT(topk_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(topk_ivf, TensorType({DT_INT32}))\n
.OUTPUT(topk_index, TensorType({DT_INT32}))\n
.ATTR(order, String, "ASC")\n
.REQUIRED_ATTR(k, Int)\n
.REQUIRED_ATTR(group_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TopKPQDistance"
    op.name = next_unique_name(node_name, "TopKPQDistance")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(actual_count, (tuple, list)):
        raise AssertionError("actual_count must be a tuple or a list.")
    for i, v in enumerate(actual_count):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "actual_count" + str(i)
    if not isinstance(pq_distance, (tuple, list)):
        raise AssertionError("pq_distance must be a tuple or a list.")
    for i, v in enumerate(pq_distance):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "pq_distance" + str(i)
    if not isinstance(grouped_extreme_distance, (tuple, list)):
        raise AssertionError("grouped_extreme_distance must be a tuple or a list.")
    for i, v in enumerate(grouped_extreme_distance):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "grouped_extreme_distance" + str(i)
    if not isinstance(pq_ivf, (tuple, list)):
        raise AssertionError("pq_ivf must be a tuple or a list.")
    for i, v in enumerate(pq_ivf):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "pq_ivf" + str(i)
    if not isinstance(pq_index, (tuple, list)):
        raise AssertionError("pq_index must be a tuple or a list.")
    for i, v in enumerate(pq_index):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "pq_index" + str(i)

    # process attrs
    op.attr["k"].i = k
    op.attr["group_size"].i = group_size
    op.attr["order"].s = compat_as_bytes(order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "topk_distance"
    topk_distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "topk_ivf"
    topk_ivf = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "topk_index"
    topk_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return topk_distance, topk_ivf, topk_index


# This api is auto-generated from IR TopKPQDistanceV2
@auto_convert_to_tensor([False, False], [False, False])
def TopKPQDistanceV2(pq_distance: Tensor, grouped_extreme_distance: Tensor, *, k: int, group_size: int, order: str="ASC", dependencies=[], node_name=None):
    """REG_OP(TopKPQDistanceV2)\n
.INPUT(pq_distance, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.INPUT(grouped_extreme_distance, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(topk_distance, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.OUTPUT(topk_index, TensorType({DT_INT32}))\n
.ATTR(order, String, "ASC")\n
.REQUIRED_ATTR(k, Int)\n
.REQUIRED_ATTR(group_size, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "TopKPQDistanceV2"
    op.name = next_unique_name(node_name, "TopKPQDistanceV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(pq_distance.tensor)
    op.input_desc.add().CopyFrom(pq_distance.desc)
    op.input_desc[-1].name = "pq_distance"
    op.input.append(grouped_extreme_distance.tensor)
    op.input_desc.add().CopyFrom(grouped_extreme_distance.desc)
    op.input_desc[-1].name = "grouped_extreme_distance"

    # process attrs
    op.attr["k"].i = k
    op.attr["group_size"].i = group_size
    op.attr["order"].s = compat_as_bytes(order)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "topk_distance"
    topk_distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "topk_index"
    topk_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return topk_distance, topk_index


# This api is auto-generated from IR ScanPQCodes
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN])
def ScanPQCodes(ivf: Tensor, bucket_list: Tensor, bucket_base_distance: Tensor, bucket_limits: Tensor, bucket_offsets: Tensor, adc_tables: Tensor, *, total_limit: int, group_size: int=64, extreme_mode: int=0, split_count: int=1, split_index: int=0, dependencies=[], node_name=None):
    """REG_OP(ScanPQCodes)\n
.INPUT(ivf, TensorType({DT_UINT8}))\n
.INPUT(bucket_list, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(bucket_base_distance, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.INPUT(bucket_limits, TensorType({DT_INT32}))\n
.INPUT(bucket_offsets, TensorType({DT_INT64}))\n
.INPUT(adc_tables, TensorType({DT_FLOAT16, DT_FLOAT}))\n
.OUTPUT(actual_count, TensorType({DT_INT32}))\n
.OUTPUT(pq_distance, TensorType({DT_FLOAT16}))\n
.OUTPUT(grouped_extreme_distance, TensorType({DT_FLOAT16}))\n
.OUTPUT(pq_ivf, TensorType({DT_INT32}))\n
.OUTPUT(pq_index, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(total_limit, Int)\n
.ATTR(group_size, Int, 64)\n
.ATTR(extreme_mode, Int, 0)\n
.ATTR(split_count, Int, 1)\n
.ATTR(split_index, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "ScanPQCodes"
    op.name = next_unique_name(node_name, "ScanPQCodes")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(ivf.tensor)
    op.input_desc.add().CopyFrom(ivf.desc)
    op.input_desc[-1].name = "ivf"
    op.input.append(bucket_list.tensor)
    op.input_desc.add().CopyFrom(bucket_list.desc)
    op.input_desc[-1].name = "bucket_list"
    op.input.append(bucket_base_distance.tensor)
    op.input_desc.add().CopyFrom(bucket_base_distance.desc)
    op.input_desc[-1].name = "bucket_base_distance"
    op.input.append(bucket_limits.tensor)
    op.input_desc.add().CopyFrom(bucket_limits.desc)
    op.input_desc[-1].name = "bucket_limits"
    op.input.append(bucket_offsets.tensor)
    op.input_desc.add().CopyFrom(bucket_offsets.desc)
    op.input_desc[-1].name = "bucket_offsets"
    op.input.append(adc_tables.tensor)
    op.input_desc.add().CopyFrom(adc_tables.desc)
    op.input_desc[-1].name = "adc_tables"

    # process attrs
    op.attr["total_limit"].i = total_limit
    op.attr["group_size"].i = group_size
    op.attr["extreme_mode"].i = extreme_mode
    op.attr["split_count"].i = split_count
    op.attr["split_index"].i = split_index

    # process outputs
    output_index = 0
    op.output_desc.add().name = "actual_count"
    actual_count = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pq_distance"
    pq_distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "grouped_extreme_distance"
    grouped_extreme_distance = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pq_ivf"
    pq_ivf = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "pq_index"
    pq_index = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return actual_count, pq_distance, grouped_extreme_distance, pq_ivf, pq_index


# This api is auto-generated from IR CalcBucketsLimitAndOffset
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER])
def CalcBucketsLimitAndOffset(bucket_list: Tensor, ivf_counts: Tensor, ivf_offset: Tensor, *, total_limit: int, dependencies=[], node_name=None):
    """REG_OP(CalcBucketsLimitAndOffset)\n
.INPUT(bucket_list, TensorType({DT_INT32}))\n
.INPUT(ivf_counts, TensorType({DT_INT32}))\n
.INPUT(ivf_offset, TensorType({DT_INT32, DT_INT64}))\n
.OUTPUT(buckets_limit, TensorType({DT_INT32}))\n
.OUTPUT(buckets_offset, TensorType({DT_INT32, DT_INT64}))\n
.REQUIRED_ATTR(total_limit, Int)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CalcBucketsLimitAndOffset"
    op.name = next_unique_name(node_name, "CalcBucketsLimitAndOffset")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(bucket_list.tensor)
    op.input_desc.add().CopyFrom(bucket_list.desc)
    op.input_desc[-1].name = "bucket_list"
    op.input.append(ivf_counts.tensor)
    op.input_desc.add().CopyFrom(ivf_counts.desc)
    op.input_desc[-1].name = "ivf_counts"
    op.input.append(ivf_offset.tensor)
    op.input_desc.add().CopyFrom(ivf_offset.desc)
    op.input_desc[-1].name = "ivf_offset"

    # process attrs
    op.attr["total_limit"].i = total_limit

    # process outputs
    output_index = 0
    op.output_desc.add().name = "buckets_limit"
    buckets_limit = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "buckets_offset"
    buckets_offset = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return buckets_limit, buckets_offset


# This api is auto-generated from IR IndexToAddr
@auto_convert_to_tensor([False, False], [False, False])
def IndexToAddr(base_addr: Tensor, x: Tensor, *, ori_shape: List[int], block_size: List[int], ori_storage_mode: str="Matrix", block_storage_mode: str="Matrix", rank_id: int=0, dtype: int=DataType.DT_FLOAT, dependencies=[], node_name=None):
    """REG_OP(IndexToAddr)\n
.INPUT(base_addr, TensorType({DT_INT64, DT_UINT64}))\n
.INPUT(x, TensorType({DT_INT64, DT_UINT64}))\n
.OUTPUT(addrs_table, TensorType({DT_INT64, DT_UINT64}))\n
.REQUIRED_ATTR(ori_shape, ListInt)\n
.REQUIRED_ATTR(block_size, ListInt)\n
.ATTR(ori_storage_mode, String, "Matrix")\n
.ATTR(block_storage_mode, String, "Matrix")\n
.ATTR(rank_id, Int, 0)\n
.ATTR(dtype, Type, DT_FLOAT)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "IndexToAddr"
    op.name = next_unique_name(node_name, "IndexToAddr")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(base_addr.tensor)
    op.input_desc.add().CopyFrom(base_addr.desc)
    op.input_desc[-1].name = "base_addr"
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["ori_shape"].list.val_type = 2
    op.attr["ori_shape"].list.i.extend(ori_shape)
    op.attr["block_size"].list.val_type = 2
    op.attr["block_size"].list.i.extend(block_size)
    op.attr["ori_storage_mode"].s = compat_as_bytes(ori_storage_mode)
    op.attr["block_storage_mode"].s = compat_as_bytes(block_storage_mode)
    op.attr["rank_id"].i = rank_id
    op.attr["dtype"].dt = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "addrs_table"
    addrs_table = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return addrs_table


# This api is auto-generated from IR Coordinates1DTo2D
@auto_convert_to_tensor([False, False], [False, False])
def Coordinates1DTo2D(x: Tensor, shape: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(Coordinates1DTo2D)\n
.INPUT(x, TensorType({DT_INT32, DT_INT64, DT_UINT64}))\n
.INPUT(shape, TensorType({DT_INT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(row, TensorType({DT_INT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(col, TensorType({DT_INT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(n, TensorType({DT_INT32, DT_INT64, DT_UINT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Coordinates1DTo2D"
    op.name = next_unique_name(node_name, "Coordinates1DTo2D")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(shape.tensor)
    op.input_desc.add().CopyFrom(shape.desc)
    op.input_desc[-1].name = "shape"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "row"
    row = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "col"
    col = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "n"
    n = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return row, col, n


# This api is auto-generated from IR CaseCondition
@auto_convert_to_tensor([False], [False])
def CaseCondition(x: Tensor, *, algorithm: str="LU", dependencies=[], node_name=None):
    """REG_OP(CaseCondition)\n
.INPUT(x, TensorType({DT_INT32, DT_INT64, DT_UINT64}))\n
.OUTPUT(y, TensorType({DT_INT32}))\n
.ATTR(algorithm, String, "LU")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "CaseCondition"
    op.name = next_unique_name(node_name, "CaseCondition")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"

    # process attrs
    op.attr["algorithm"].s = compat_as_bytes(algorithm)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR SliceWrite
@auto_convert_to_tensor([False, False, False], [False, False, False], inputs_tensor_type=[TensorType.TT_UNKNOWN, TensorType.TT_INDEX_NUMBER, TensorType.TT_UNKNOWN])
def SliceWrite(x: Tensor, begin: Tensor, value: Tensor, *, dependencies=[], node_name=None):
    """REG_OP(SliceWrite)\n
.INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.INPUT(begin, TensorType({DT_INT32, DT_INT64}))\n
.INPUT(value, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
.OUTPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_DOUBLE, DT_INT32, DT_INT64}))\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "SliceWrite"
    op.name = next_unique_name(node_name, "SliceWrite")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(begin.tensor)
    op.input_desc.add().CopyFrom(begin.desc)
    op.input_desc[-1].name = "begin"
    op.input.append(value.tensor)
    op.input_desc.add().CopyFrom(value.desc)
    op.input_desc[-1].name = "value"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "x"
    x = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return x


# This api is auto-generated from IR WarpPerspective
@auto_convert_to_tensor([False, False], [False, False])
def WarpPerspective(x: Tensor, matrix: Tensor, *, out_height: int, out_width: int, interpolation_mode: str="bilinear", border_type: str="BORDER_CONSTANT", constant: float=0.000000, data_format: str="CHW", dependencies=[], node_name=None):
    """REG_OP(WarpPerspective)\n
.INPUT(x, TensorType({DT_FLOAT, DT_UINT8}))\n
.INPUT(matrix, TensorType({DT_DOUBLE, DT_FLOAT}))\n
.OUTPUT(y, TensorType({DT_FLOAT, DT_UINT8}))\n
.REQUIRED_ATTR(out_height, Int)\n
.REQUIRED_ATTR(out_width, Int)\n
.ATTR(interpolation_mode, String, "bilinear")\n
.ATTR(border_type, String, "BORDER_CONSTANT")\n
.ATTR(constant, Float, 0)\n
.ATTR(data_format, String, "CHW")\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "WarpPerspective"
    op.name = next_unique_name(node_name, "WarpPerspective")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(matrix.tensor)
    op.input_desc.add().CopyFrom(matrix.desc)
    op.input_desc[-1].name = "matrix"

    # process attrs
    op.attr["out_height"].i = out_height
    op.attr["out_width"].i = out_width
    op.attr["interpolation_mode"].s = compat_as_bytes(interpolation_mode)
    op.attr["border_type"].s = compat_as_bytes(border_type)
    op.attr["constant"].f = constant
    op.attr["data_format"].s = compat_as_bytes(data_format)

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    
    return y

# This api is auto-generated from IR ReduceMeanWithCast
@auto_convert_to_tensor([False, False], [False, False])
def ReduceMeanWithCast(x: Tensor, axes: Tensor, *, keep_dims: bool=False, noop_with_empty_axes: bool=True, dtype: int=28, dependencies=[], node_name=None):
  """REG_OP(ReduceMeanWithCast)\n
.INPUT(x, "T1")\n
.INPUT(axes, "T2")\n
.OUTPUT(y, "T3")\n
.ATTR(keep_dims, Bool, false)\n
.ATTR(noop_with_empty_axes, Bool, true)\n
.ATTR(dtype, Type, DT_UNDEFINED)\n
"""
  
  op = get_default_ge_graph().op.add()
  op.type = "ReduceMeanWithCast"
  op.name = next_unique_name(node_name, "ReduceMeanWithCast")
  
  # process dependices
  for dependency in dependencies:
    op.input.append(dependency.controller)
  
  # process inputs
  op.input.append(x.tensor)
  op.input_desc.add().CopyFrom(x.desc)
  op.input_desc[-1].name = "x"
  op.input.append(axes.tensor)
  op.input_desc.add().CopyFrom(axes.desc)
  op.input_desc[-1].name = "axes"
  
  # process attrs
  op.attr["keep_dims"].b = keep_dims
  op.attr["noop_with_empty_axes"].b = noop_with_empty_axes
  op.attr["dtype"].dt = dtype
  
  # process outputs
  output_index = 0
  op.output_desc.add().name = "y"
  y = Tensor(op, output_index)
  output_index += 1
  
  
  return y


#This api is auto-generated from IR MaskedSoftmaxWithRelPosBias
@auto_convert_to_tensor([False, False, False], [False, True, False])
def MaskedSoftmaxWithRelPosBias(x: Tensor, atten_mask: Optional[Tensor], relative_pos_bias: Tensor, *, scale_value: float=1.000000, inner_precision_mode: int=0, dependencies=[], node_name=None):
  """REG_OP(MaskedSoftmaxWithRelPosBias)\n
.INPUT(x, TensorType({DT_FLOAT,DT_FLOAT16,DT_BF16}))\n
.OPTIONAL_INPUT(atten_mask, TensorType({DT_FLOAT,DT_FLOAT16,DT_BF16}))\n
.INPUT(relative_pos_bias, TensorType({DT_FLOAT,DT_FLOAT16,DT_BF16}))\n
.OUTPUT(y, TensorType({DT_FLOAT,DT_FLOAT16,DT_BF16}))\n
.ATTR(scale_value, Float, 1.0)\n
.ATTR(inner_precision_mode, Int, 0)\n
"""

  op = get_default_ge_graph().op.add()
  op.type = "MaskedSoftmaxWithRelPosBias"
  op.name = next_unique_name(node_name, "MaskedSoftmaxWithRelPosBias")

  # process dependices
  for dependency in dependencies:
    op.input.append(dependency.controller)

  # process inputs
  op.input.append(x.tensor)
  op.input_desc.add().CopyFrom(x.desc)
  op.input_desc[-1].name = "x"
  if atten_mask is not None:
    op.input.append(atten_mask.tensor)
    op.input_desc.add().CopyFrom(atten_mask.desc)
    op.input_desc[-1].name = "atten_mask"
  else:
    op.input.append('')
    op.input_desc.add().CopyFrom(get_invalid_desc())
    op.input_desc[-1].name = "atten_mask"
  op.input.append(relative_pos_bias.tensor)
  op.input_desc.add().CopyFrom(relative_pos_bias.desc)
  op.input_desc[-1].name = "relative_pos_bias"

  # process attrs
  op.attr["scale_value"].f = scale_value
  op.attr["inner_precision_mode"].i = inner_precision_mode

  # process outputs
  output_index = 0
  op.output_desc.add().name = "y"
  y = Tensor(op, output_index)
  output_index += 1


  return y


# This api is auto-generated from IR FFN
@auto_convert_to_tensor(
    [False, False, False, False, False, False, False, False, False, False, False, False, False, False],
    [False, False, False, True, True, True, True, True, True, True, True, True, True, True])
def FFN(x: Tensor,
        weight1: Tensor,
        weight2: Tensor,
        expert_tokens: Optional[Tensor],
        bias1: Optional[Tensor],
        bias2: Optional[Tensor],
        scale: Optional[Tensor],
        offset: Optional[Tensor],
        deq_scale1: Optional[Tensor],
        deq_scale2: Optional[Tensor],
        antiquant_scale1: Optional[Tensor],
        antiquant_scale2: Optional[Tensor],
        antiquant_offset1: Optional[Tensor],
        antiquant_offset2: Optional[Tensor],
        *,
        activation: str,
        inner_precise: int = 0,
        output_dtype: int = -1,
        tokens_index_flag: bool = False,
        dependencies = [],
        node_name = None):
    """REG_OP(FFN)\n
    .INPUT(x, TensorType({DT_INT8, DT_FLOAT16, DT_BF16}))\n
    .INPUT(weight1, TensorType({DT_INT8, DT_FLOAT16, DT_BF16, DT_INT4}))\n
    .INPUT(weight2, TensorType({DT_INT8, DT_FLOAT16, DT_BF16, DT_INT4}))\n
    .OPTIONAL_INPUT(expert_tokens, TensorType({DT_INT64}))\n
    .OPTIONAL_INPUT(bias1, TensorType({DT_INT32, DT_FLOAT16, DT_FLOAT}))\n
    .OPTIONAL_INPUT(bias2, TensorType({DT_INT32, DT_FLOAT16, DT_FLOAT}))\n
    .OPTIONAL_INPUT(scale, TensorType({DT_FLOAT}))\n
    .OPTIONAL_INPUT(offset, TensorType({DT_FLOAT}))\n
    .OPTIONAL_INPUT(deq_scale1, TensorType({DT_UINT64, DT_BF16}))\n
    .OPTIONAL_INPUT(deq_scale2, TensorType({DT_UINT64, DT_BF16}))\n
    .OPTIONAL_INPUT(antiquant_scale1, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(antiquant_scale2, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(antiquant_offset1, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(antiquant_offset2, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16}))\n
    .REQUIRED_ATTR(activation, String)\n
    .ATTR(inner_precise, Int, 0)\n
    .ATTR(output_dtype, Int, -1)\n
    .ATTR(tokens_index_flag, Bool, false)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "FFN"
    op.name = next_unique_name(node_name, "FFN")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight1.tensor)
    op.input_desc.add().CopyFrom(weight1.desc)
    op.input_desc[-1].name = "weight1"
    op.input.append(weight2.tensor)
    op.input_desc.add().CopyFrom(weight2.desc)
    op.input_desc[-1].name = "weight2"
    if expert_tokens is not None:
        op.input.append(expert_tokens.tensor)
        op.input_desc.add().CopyFrom(expert_tokens.desc)
        op.input_desc[-1].name = "expert_tokens"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "expert_tokens"
    if bias1 is not None:
        op.input.append(bias1.tensor)
        op.input_desc.add().CopyFrom(bias1.desc)
        op.input_desc[-1].name = "bias1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias1"
    if bias2 is not None:
        op.input.append(bias2.tensor)
        op.input_desc.add().CopyFrom(bias2.desc)
        op.input_desc[-1].name = "bias2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias2"
    if scale is not None:
        op.input.append(scale.tensor)
        op.input_desc.add().CopyFrom(scale.desc)
        op.input_desc[-1].name = "scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"
    if deq_scale1 is not None:
        op.input.append(deq_scale1.tensor)
        op.input_desc.add().CopyFrom(deq_scale1.desc)
        op.input_desc[-1].name = "deq_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "deq_scale1"
    if deq_scale2 is not None:
        op.input.append(deq_scale2.tensor)
        op.input_desc.add().CopyFrom(deq_scale2.desc)
        op.input_desc[-1].name = "deq_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "deq_scale2"
    if antiquant_scale1 is not None:
        op.input.append(antiquant_scale1.tensor)
        op.input_desc.add().CopyFrom(antiquant_scale1.desc)
        op.input_desc[-1].name = "antiquant_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_scale1"
    if antiquant_scale2 is not None:
        op.input.append(antiquant_scale2.tensor)
        op.input_desc.add().CopyFrom(antiquant_scale2.desc)
        op.input_desc[-1].name = "antiquant_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_scale2"
    if antiquant_offset1 is not None:
        op.input.append(antiquant_offset1.tensor)
        op.input_desc.add().CopyFrom(antiquant_offset1.desc)
        op.input_desc[-1].name = "antiquant_offset1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_offset1"
    if antiquant_offset2 is not None:
        op.input.append(antiquant_offset2.tensor)
        op.input_desc.add().CopyFrom(antiquant_offset2.desc)
        op.input_desc[-1].name = "antiquant_offset2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_offset2"

    # process attrs
    op.attr["activation"].s = compat_as_bytes(activation)
    op.attr["inner_precise"].i = inner_precise
    op.attr["output_dtype"].i = output_dtype
    op.attr["tokens_index_flag"].b = tokens_index_flag

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1


    return y


# This api is auto-generated from IR MatmulAllReduce
@auto_convert_to_tensor(
    [False, False, False, False, False, False, False, False, False, False],
    [False, False, True, True, True, True, True, True, True, True])
def MatmulAllReduce(x1: Tensor,
                    x2: Tensor,
                    bias: Optional[Tensor],
                    x3: Optional[Tensor],
                    antiquant_scale: Optional[Tensor],
                    antiquant_offset: Optional[Tensor],
                    dequant_scale: Optional[Tensor],
                    pertoken_scale: Optional[Tensor],
                    comm_quant_scale_1: Optional[Tensor],
                    comm_quant_scale_2: Optional[Tensor],
                    *,
                    group: str,
                    reduce_op: str="sum",
                    is_trans_a: bool=False,
                    is_trans_b: bool=False,
                    comm_turn: int=0,
                    antiquant_group_size: int=0,
                    dependencies=[],
                    node_name=None):
    """REG_OP(MatmulAllReduce)
    .INPUT(x1, TensorType({DT_FLOAT16, DT_BF16, DT_INT8}))\n
    .INPUT(x2, TensorType({DT_FLOAT16, DT_BF16, DT_INT8, DT_INT4}))\n
    .OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_BF16, DT_INT32}))\n
    .OPTIONAL_INPUT(x3, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(antiquant_scale, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(antiquant_offset, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(dequant_scale, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16, DT_UINT64, DT_INT64}))\n
    .OPTIONAL_INPUT(pertoken_scale, TensorType({DT_FLOAT}))\n
    .OPTIONAL_INPUT(comm_quant_scale_1, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(comm_quant_scale_2, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16}))\n
    .REQUIRED_ATTR(group, String)\n
    .ATTR(reduce_op, String, "sum")\n
    .ATTR(is_trans_a, Bool, false)\n
    .ATTR(is_trans_b, Bool, false)\n
    .ATTR(comm_turn, Int, 0)\n
    .ATTR(antiquant_group_size, Int, 0)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "MatmulAllReduce"
    op.name = next_unique_name(node_name, "MatmulAllReduce")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if x3 is not None:
        op.input.append(x3.tensor)
        op.input_desc.add().CopyFrom(x3.desc)
        op.input_desc[-1].name = "x3"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x3"
    if antiquant_scale is not None:
        op.input.append(antiquant_scale.tensor)
        op.input_desc.add().CopyFrom(antiquant_scale.desc)
        op.input_desc[-1].name = "antiquant_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_scale"
    if antiquant_offset is not None:
        op.input.append(antiquant_offset.tensor)
        op.input_desc.add().CopyFrom(antiquant_offset.desc)
        op.input_desc[-1].name = "antiquant_offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_offset"
    if dequant_scale is not None:
        op.input.append(dequant_scale.tensor)
        op.input_desc.add().CopyFrom(dequant_scale.desc)
        op.input_desc[-1].name = "dequant_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dequant_scale"
    if pertoken_scale is not None:
        op.input.append(pertoken_scale.tensor)
        op.input_desc.add().CopyFrom(pertoken_scale.desc)
        op.input_desc[-1].name = "pertoken_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pertoken_scale"
    if comm_quant_scale_1 is not None:
        op.input.append(comm_quant_scale_1.tensor)
        op.input_desc.add().CopyFrom(comm_quant_scale_1.desc)
        op.input_desc[-1].name = "comm_quant_scale_1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "comm_quant_scale_1"
    if comm_quant_scale_2 is not None:
        op.input.append(comm_quant_scale_2.tensor)
        op.input_desc.add().CopyFrom(comm_quant_scale_2.desc)
        op.input_desc[-1].name = "comm_quant_scale_2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "comm_quant_scale_2"
        
    # process attrs
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["reduce_op"].s = compat_as_bytes(reduce_op)
    op.attr["is_trans_a"].b = is_trans_a
    op.attr["is_trans_b"].b = is_trans_b
    op.attr["comm_turn"].i = comm_turn
    op.attr["antiquant_group_size"].i = antiquant_group_size

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR MatmulReduceScatter
@auto_convert_to_tensor([False, False, False], [False, False, True])
def MatmulReduceScatter(x1: Tensor,
                        x2: Tensor,
                        bias: Optional[Tensor],
                        *,
                        group: str,
                        reduce_op: str="sum",
                        is_trans_a: bool=False,
                        is_trans_b: bool=False,
                        comm_turn: int=0,
                        dependencies=[],
                        node_name=None):
    """
    REG_OP(MatmulReduceScatter)\n
    .INPUT(x1, TensorType({DT_FLOAT16, DT_BF16}))\n
    .INPUT(x2, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16}))\n
    .REQUIRED_ATTR(group, String)\n
    .ATTR(reduce_op, String, "sum")\n
    .ATTR(is_trans_a, Bool, false)\n
    .ATTR(is_trans_b, Bool, false)\n
    .ATTR(comm_turn, Int, 0)\n
    """
    op = get_default_ge_graph().op.add()
    op.type = "MatmulReduceScatter"
    op.name = next_unique_name(node_name, "MatmulReduceScatter")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["reduce_op"].s = compat_as_bytes(reduce_op)
    op.attr["is_trans_a"].b = is_trans_a
    op.attr["is_trans_b"].b = is_trans_b
    op.attr["comm_turn"].i = comm_turn

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1


    return y


# This api is auto-generated from IR AllGatherMatmul
@auto_convert_to_tensor([False, False, False], [False, False, True])
def AllGatherMatmul(x1: Tensor,
                    x2: Tensor,
                    bias: Optional[Tensor],
                    *,
                    group: str,
                    is_trans_a: bool=False,
                    is_trans_b: bool=False,
                    gather_index: int=0,
                    comm_turn: int=0,
                    dependencies=[],
                    node_name=None):
    """
    REG_OP(AllGatherMatmul)\n
    .INPUT(x1, TensorType({DT_FLOAT16, DT_BF16}))\n
    .INPUT(x2, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OUTPUT(gather_out, TensorType({DT_FLOAT16, DT_BF16}))\n
    .REQUIRED_ATTR(group, String)\n
    .ATTR(is_trans_a, Bool, false)\n
    .ATTR(is_trans_b, Bool, false)\n
    .ATTR(gather_index, Int, 0)\n
    .ATTR(comm_turn, Int, 0)\n
    """
    op = get_default_ge_graph().op.add()
    op.type = "AllGatherMatmul"
    op.name = next_unique_name(node_name, "AllGatherMatmul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["group"].s = compat_as_bytes(group)
    op.attr["is_trans_a"].b = is_trans_a
    op.attr["is_trans_b"].b = is_trans_b
    op.attr["gather_index"].i = gather_index
    op.attr["comm_turn"].i = comm_turn

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "gather_out"
    gather_out = Tensor(op, output_index)
    output_index += 1


    return y, gather_out


# This api is auto-generated from IR WeightQuantBatchMatmulV2
@auto_convert_to_tensor([False, False, False, False, False, False, False],
                        [False, False, False, True, True, True, True])
def WeightQuantBatchMatmulV2(x: Tensor,
                             weight: Tensor,
                             antiquant_scale: Tensor,
                             antiquant_offset: Optional[Tensor],
                             quant_scale: Optional[Tensor],
                             quant_offset: Optional[Tensor],
                             bias: Optional[Tensor],
                             *,
                             transpose_x: bool = False,
                             transpose_weight: bool = False,
                             antiquant_group_size: int = 0,
                             dtype: int = -1,
                             dependencies=[],
                             node_name=None):
    """REG_OP(WeightQuantBatchMatmulV2)\n
    .INPUT(x, TensorType({DT_FLOAT16,DT_BF16}))\n
    .INPUT(weight, TensorType({DT_INT8,DT_INT4}))\n
    .INPUT(antiquant_scale, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(antiquant_offset, TensorType({DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(quant_scale, TensorType({DT_FLOAT, DT_UINT64}))\n
    .OPTIONAL_INPUT(quant_offset, TensorType({DT_FLOAT}))\n
    .OPTIONAL_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT}))\n
    .OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16, DT_INT8}))\n
    .ATTR(transpose_x, Bool, false)\n
    .ATTR(transpose_weight, Bool, false)\n
    .ATRR(antiquant_group_size, Int, 0)\n
    .ATRR(dtype, Int, -1)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "WeightQuantBatchMatmulV2"
    op.name = next_unique_name(node_name, "WeightQuantBatchMatmulV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(weight.tensor)
    op.input_desc.add().CopyFrom(weight.desc)
    op.input_desc[-1].name = "weight"
    op.input.append(antiquant_scale.tensor)
    op.input_desc.add().CopyFrom(antiquant_scale.desc)
    op.input_desc[-1].name = "antiquant_scale"
    if antiquant_offset is not None:
        op.input.append(antiquant_offset.tensor)
        op.input_desc.add().CopyFrom(antiquant_offset.desc)
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
    op.input_desc[-1].name = "antiquant_offset"
    if quant_scale is not None:
        op.input.append(quant_scale.tensor)
        op.input_desc.add().CopyFrom(quant_scale.desc)
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
    op.input_desc[-1].name = "quant_scale"
    if quant_offset is not None:
        op.input.append(quant_offset.tensor)
        op.input_desc.add().CopyFrom(quant_offset.desc)
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
    op.input_desc[-1].name = "quant_offset"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
    op.input_desc[-1].name = "bias"

    # process attrs
    op.attr["transpose_x"].b = transpose_x
    op.attr["transpose_weight"].b = transpose_weight
    op.attr["antiquant_group_size"].i = antiquant_group_size
    op.attr["dtype"].i = dtype

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1


    return y


# This api is auto-generated from IR LayerNormV4
@auto_convert_to_tensor([False, False, False, False], [False, False, True, True])
def LayerNormV4(x: Tensor, 
                normalized_shape: Tensor, 
                gamma: Optional[Tensor], 
                beta: Optional[Tensor], 
                *, 
                epsilon: float=0.000010, 
                dependencies=[], 
                node_name=None):
    """REG_OP(LayerNormV4)\n
    .INPUT(x, "T1")\n
    .INPUT(normalized_shape, "T2")\n
    .OPTIONAL_INPUT(gamma, "T3")\n
    .OPTIONAL_INPUT(beta, "T4")\n
    .OUTPUT(y, "T5")\n
    .OUTPUT(mean, "T6")\n
    .OUTPUT(rstd, "T6")\n
    .ATTR(epsilon, Float, 0.00001)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "LayerNormV4"
    op.name = next_unique_name(node_name, "LayerNormV4")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(normalized_shape.tensor)
    op.input_desc.add().CopyFrom(normalized_shape.desc)
    op.input_desc[-1].name = "normalized_shape"
    if gamma is not None:
        op.input.append(gamma.tensor)
        op.input_desc.add().CopyFrom(gamma.desc)
        op.input_desc[-1].name = "gamma"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "gamma"
    if beta is not None:
        op.input.append(beta.tensor)
        op.input_desc.add().CopyFrom(beta.desc)
        op.input_desc[-1].name = "beta"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "beta"

    # process attrs
    op.attr["epsilon"].f = epsilon

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "mean"
    mean = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "rstd"
    rstd = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y, mean, rstd


# This api is auto-generated from IR Sxpy
@auto_convert_to_tensor([False, False, False], [False, False, True])
def Sxpy(x1: Tensor, 
         x2: Tensor, 
         alpha: Optional[Tensor], 
         *, 
         dependencies=[], 
         node_name=None):
    """REG_OP(Sxpy)\n
    .INPUT(x1, "T1")\n
    .INPUT(x2, "T2")\n
    .OPTIONAL_INPUT(alpha, "T3")\n
    .OUTPUT(y, "T4")\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "Sxpy"
    op.name = next_unique_name(node_name, "Sxpy")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    if alpha is not None:
        op.input.append(alpha.tensor)
        op.input_desc.add().CopyFrom(alpha.desc)
        op.input_desc[-1].name = "alpha"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "alpha"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR ConstPlaceHolder
@auto_convert_to_tensor([], [])
def ConstPlaceHolder(*,
                     origin_shape: List[int],
                     origin_format: int,
                     storage_shape: List[int],
                     storage_format: int,
                     expand_dim_rules: str,
                     dtype: int,
                     addr: int,
                     size: int,
                     placement: int=1,
                     dependencies=[],
                     node_name=None):
    """REG_OP(ConstPlaceHolder)\n
    .OUTPUT(y, TensorType:ALL())\n
    .REQUIRED_ATTR(origin_shape, ListInt)\n
    .REQUIRED_ATTR(origin_format, Int)\n
    .REQUIRED_ATTR(storage_shape, ListInt)\n
    .REQUIRED_ATTR(storage_format, Int)\n
    .REQUIRED_ATTR(expand_dim_rules, String)\n
    .REQUIRED_ATTR(dtype, Int)\n
    .REQUIRED_ATTR(addr, Int)\n
    .REQUIRED_ATTR(size, Int)\n
    .ATTR(placement, Int, 1)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "ConstPlaceHolder"
    op.name = next_unique_name(node_name, "ConstPlaceHolder")
    
    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)
    
    # process inputs

    # process attrs
    op.attr["origin_shape"].list.val_type = 2
    op.attr["origin_shape"].list.i.extend(origin_shape)
    op.attr["origin_format"].i = origin_format
    op.attr["storage_shape"].list.val_type = 2
    op.attr["storage_shape"].list.i.extend(storage_shape)
    op.attr["storage_format"].i = storage_format
    op.attr["expand_dim_rules"].s = compat_as_bytes(expand_dim_rules)
    op.attr["dtype"].dt = dtype
    op.attr["addr"].i = addr
    op.attr["size"].i = size
    op.attr["placement"].i = placement

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR QuantBatchMatmulV3
@auto_convert_to_tensor([False, False, False, False, False, False], [False, False, False, True, True, True])
def QuantBatchMatmulV3(x1: Tensor, x2: Tensor, scale: Tensor, offset: Optional[Tensor], bias: Optional[Tensor],
                       pertoken_scale: Optional[Tensor], *, dtype: int, transpose_x1: bool = False, transpose_x2: bool = False,
                       dependencies=[], node_name=None):
    """REG_OP(QuantBatchMatmulV3)\n
    .INPUT(x1, TensorType({DT_INT8, DT_INT4}))\n
    .INPUT(x2, TensorType({DT_INT8, DT_INT4}))\n
    .INPUT(scale, TensorType({DT_UINT64, DT_FLOAT, DT_BF16}))\n
    .OPTIONAL_INPUT(offset, TensorType({DT_FLOAT}))\n
    .OPTIONAL_INPUT(bias, TensorType({DT_INT32}))\n
    .OPTIONAL_INPUT(pertoken_scale, TensorType({DT_FLOAT}))\n
    .OUTPUT(y, TensorType({DT_FLOAT16, DT_INT8, DT_BF16}))\n
    .REQUIRED_ATTR(dtype, Int)\n
    .ATTR(transpose_x1, Bool, false)\n
    .ATTR(transpose_x2, Bool, false)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "QuantBatchMatmulV3"
    op.name = next_unique_name(node_name, "QuantBatchMatmulV3")
  
    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)
  
    # process inputs
    op.input.append(x1.tensor)
    op.input_desc.add().CopyFrom(x1.desc)
    op.input_desc[-1].name = "x1"
    op.input.append(x2.tensor)
    op.input_desc.add().CopyFrom(x2.desc)
    op.input_desc[-1].name = "x2"
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if pertoken_scale is not None:
        op.input.append(pertoken_scale.tensor)
        op.input_desc.add().CopyFrom(pertoken_scale.desc)
        op.input_desc[-1].name = "pertoken_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pertoken_scale"
  
    # process attrs
    op.attr["dtype"].i = dtype
    op.attr["transpose_x1"].b = transpose_x1
    op.attr["transpose_x2"].b = transpose_x2
  
    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y


# This api is auto-generated from IR TomeMerge
@auto_convert_to_tensor([False, False, False, False], [False, False, False, False])
def TomeMerge(token_a: Tensor, 
              token_b: Tensor, 
              topk_indice: Tensor, 
              arg_max: Tensor, 
              topRate: float=0.500000, 
              dependencies=[], 
              node_name=None):
    """    REG_OP(TomeMerge)\n
    .INPUT(token_a, TensorType({DT_FLOAT16}))\n
    .INPUT(token_b, TensorType({DT_FLOAT16}))\n
    .INPUT(topk_indice, TensorType({DT_INT64}))\n
    .INPUT(arg_max, TensorType({DT_INT64}))\n
    .OUTPUT(unmerge_token_a, TensorType({DT_FLOAT16}))\n
    .OUTPUT(unreduce_token_b, TensorType({DT_FLOAT16}))\n
    .OUTPUT(unreduce_count, TensorType({DT_FLOAT}))\n
    .ATTR(top_rate, Float, 0.5)\n
    .OP_END_FACTORY_REG(TomeMerge)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "TomeMerge"
    op.name = next_unique_name(node_name, "TomeMerge")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(token_a.tensor)
    op.input_desc.add().CopyFrom(token_a.desc)
    op.input_desc[-1].name = "token_a"
    op.input.append(token_b.tensor)
    op.input_desc.add().CopyFrom(token_b.desc)
    op.input_desc[-1].name = "token_b"
    op.input.append(topk_indice.tensor)
    op.input_desc.add().CopyFrom(topk_indice.desc)
    op.input_desc[-1].name = "topk_indice"
    op.input.append(arg_max.tensor)
    op.input_desc.add().CopyFrom(arg_max.desc)
    op.input_desc[-1].name = "arg_max"

    # process attrs
    op.attr["topRate"].f = topRate

    # process outputs
    output_index = 0
    op.output_desc.add().name = "unmerge_token_a"
    unMergeTokenA = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "unreduce_token_b"
    unReduceTokenB = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "unreduce_count"
    unReduceCount = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return unMergeTokenA, unReduceTokenB, unReduceCount


# This api is auto-generated from IR TomeUnmerge
@auto_convert_to_tensor([False, False, False, False, False], [False, False, False, False, False])
def TomeUnmerge(atten_out: Tensor, 
                Ori_IndiceA: Tensor, 
                Ori_IndiceB: Tensor, 
                TOPK_Indice: Tensor, 
                Arg_Max: Tensor, *, 
                top_r_rate: float=0.500000, 
                dependencies=[], 
                node_name=None):
    """REG_OP(TomeUnmerge)\n
    .INPUT(atten_out, TensorType({DT_FLOAT16}))\n
    .INPUT(Ori_IndiceA, TensorType({DT_INT64}))\n
    .INPUT(Ori_IndiceB, TensorType({DT_INT64}))\n
    .INPUT(TOPK_Indice, TensorType({DT_INT64}))\n
    .INPUT(Arg_Max, TensorType({DT_FLOAT16}))\n
    .OUTPUT(unZipToken, TensorType({DT_FLOAT16}))\n
    .ATTR(topRRate, Float, 0.5)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "TomeUnmerge"
    op.name = next_unique_name(node_name, "TomeUnmerge")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(atten_out.tensor)
    op.input_desc.add().CopyFrom(atten_out.desc)
    op.input_desc[-1].name = "atten_out"
    op.input.append(Ori_IndiceA.tensor)
    op.input_desc.add().CopyFrom(Ori_IndiceA.desc)
    op.input_desc[-1].name = "Ori_IndiceA"
    op.input.append(Ori_IndiceB.tensor)
    op.input_desc.add().CopyFrom(Ori_IndiceB.desc)
    op.input_desc[-1].name = "Ori_IndiceB"
    op.input.append(TOPK_Indice.tensor)
    op.input_desc.add().CopyFrom(TOPK_Indice.desc)
    op.input_desc[-1].name = "TOPK_Indice"
    op.input.append(Arg_Max.tensor)
    op.input_desc.add().CopyFrom(Arg_Max.desc)
    op.input_desc[-1].name = "Arg_Max"

    # process attrs
    op.attr["top_r_rate"].f = top_r_rate

    # process outputs
    output_index = 0
    op.output_desc.add().name = "unZipToken"
    unZipToken = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return unZipToken


# This api is auto-generated from IR TransQuantParamV2
@auto_convert_to_tensor([False, False], [False, True])
def TransQuantParamV2(scale: Tensor, offset: Optional[Tensor], *, dependencies=[], node_name=None):
    """REG_OP(TransQuantParamV2)\n
    .INPUT(scale, TensorType({DT_UINT64, DT_FLOAT}))\n
    .OPTIONAL_INPUT(offset, TensorType({DT_FLOAT}))\n
    .OUTPUT(y, TensorType({DT_INT64}))\n
    """
  
    op = get_default_ge_graph().op.add()
    op.type = "TransQuantParamV2"
    op.name = next_unique_name(node_name, "TransQuantParamV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)
  
    # process inputs
    op.input.append(scale.tensor)
    op.input_desc.add().CopyFrom(scale.desc)
    op.input_desc[-1].name = "scale"
    if offset is not None:
        op.input.append(offset.tensor)
        op.input_desc.add().CopyFrom(offset.desc)
        op.input_desc[-1].name = "offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "offset"

    # process attrs

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return y

#This api is auto-generated from IR MoeComputeExpertTokens
@auto_convert_to_tensor([False], [False])
def MoeComputeExpertTokens(sorted_experts: Tensor, *, num_experts: int=1, dependencies=[], node_name=None):
  """REG_OP(MoeComputeExpertTokens)\n
.INPUT(sorted_experts, TensorType({DT_INT32}))\n
.OUTPUT(total_rows_before_expert, TensorType({DT_INT32}))\n
.REQUIRED_ATTR(num_experts, Int)\n
"""

  op = get_default_ge_graph().op.add()
  op.type = "MoeComputeExpertTokens"
  op.name = next_unique_name(node_name, "MoeComputeExpertTokens")
  # process dependices
  for dependency in dependencies:
    op.input.append(dependency.controller)

  # process inputs
  op.input.append(sorted_experts.tensor)
  op.input_desc.add().CopyFrom(sorted_experts.desc)
  op.input_desc[-1].name = "sorted_experts"

  # process attrs
  op.attr["num_experts"].i = num_experts

  # process outputs
  output_index = 0
  op.output_desc.add().name = "total_rows_before_expert"
  y = Tensor(op, output_index)
  output_index += 1


  return y

# This api is auto-generated from IR GroupedMatmul
@auto_convert_to_tensor([True, True, True, True, True, True, True, False], [False, False, False, False, False, False, False, True])
def _GroupedMatmul(x: List[Tensor], weight: List[Tensor], bias: List[Tensor], scale: List[Tensor], offset: List[Tensor], antiquant_scale: List[Tensor], antiquant_offset: List[Tensor], group_list: Optional[Tensor], *, size_of_y: int, split_item: int=0, dtype: int=0, transpose_weight: bool=False, transpose_x: bool=False, group_type: int=-1, dependencies=[], node_name=None):
    """REG_OP(GroupedMatmul)\n
.DYNAMIC_INPUT(x, TensorType({DT_FLOAT16, DT_BF16, DT_INT8}))\n
.DYNAMIC_INPUT(weight, TensorType({DT_FLOAT16, DT_BF16, DT_INT8}))\n
.DYNAMIC_INPUT(bias, TensorType({DT_FLOAT16, DT_FLOAT, DT_INT32}))\n
.DYNAMIC_INPUT(scale, TensorType({DT_UINT64}))\n
.DYNAMIC_INPUT(offset, TensorType({DT_FLOAT32}))\n
.DYNAMIC_INPUT(antiquant_scale, TensorType({DT_FLOAT16, DT_BF16}))\n
.DYNAMIC_INPUT(antiquant_offset, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(group_list, TensorType({DT_INT64}))\n
.DYNAMIC_OUTPUT(y, TensorType({DT_FLOAT16, DT_BF16}))\n
.ATTR(split_item, Int, 0)\n
.ATTR(dtype, Int, 0)\n
.ATTR(transpose_weight, Bool, false)\n
.ATTR(transpose_x, Bool, false)\n
.ATTR(group_type, Int, -1)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "GroupedMatmul"
    op.name = next_unique_name(node_name, "GroupedMatmul")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    if not isinstance(x, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(x):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "x" + str(i)
    if not isinstance(weight, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(weight):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "weight" + str(i)
    if not isinstance(bias, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(bias):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "bias" + str(i)
    if not isinstance(scale, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(scale):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "scale" + str(i)
    if not isinstance(offset, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(offset):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "offset" + str(i)
    if not isinstance(antiquant_scale, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(antiquant_scale):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "antiquant_scale" + str(i)
    if not isinstance(antiquant_offset, (tuple, list)):
        raise AssertionError
    for i, v in enumerate(antiquant_offset):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "antiquant_offset" + str(i)
    if group_list is not None:
        op.input.append(group_list.tensor)
        op.input_desc.add().CopyFrom(group_list.desc)
        op.input_desc[-1].name = "group_list"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "group_list"

    # process attrs
    op.attr["split_item"].i = split_item
    op.attr["dtype"].i = dtype
    op.attr["transpose_weight"].b = transpose_weight
    op.attr["transpose_x"].b = transpose_x
    op.attr["group_type"].i = group_type

    # process outputs
    output_index = 0
    y = []
    for i in range(output_index, output_index + size_of_y):
        op.output_desc.add().name = "y" + str(i - output_index)
        y.append(Tensor(op, i))
    output_index += size_of_y

    # return outputs
    return y


# This api is auto-generated from IR MoeInitRouting
@auto_convert_to_tensor([False, False, False], [False, False, False])
def MoeInitRouting(x: Tensor,
                   row_idx: Tensor,
                   expert_idx: Tensor,
                   *,
                   active_num: int, 
                   dependencies=[], 
                   node_name=None):
    """REG_OP(MoeInitRouting)\n
    .INPUT(x, "T1")\n
    .INPUT(row_idx, "T2")\n
    .INPUT(expert_idx, "T2")\n
    .OUTPUT(expanded_x, "T1")\n
    .OUTPUT(expanded_row_idx, "T2")\n
    .OUTPUT(expanded_expert_idx, "T2")\n
    .DATATYPE(T1, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .DATATYPE(T2, TensorType({DT_INT32}))\n
    .REQUIRED_ATTR(active_num, Int)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "MoeInitRouting"
    op.name = next_unique_name(node_name, "MoeInitRouting")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    op.input.append(row_idx.tensor)
    op.input_desc.add().CopyFrom(row_idx.desc)
    op.input_desc[-1].name = "row_idx"
    op.input.append(expert_idx.tensor)
    op.input_desc.add().CopyFrom(expert_idx.desc)
    op.input_desc[-1].name = "expert_idx"

    # process attrs
    op.attr["active_num"].i = active_num

    # process outputs
    output_index = 0
    op.output_desc.add().name = "expanded_x"
    expanded_x = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "expanded_row_idx"
    expanded_row_idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "expanded_expert_idx"
    expanded_expert_idx = Tensor(op, output_index)

    # return outputs
    return expanded_x, expanded_row_idx, expanded_expert_idx

# This api is auto-generated from IR MoeGatingTopKSoftmax
@auto_convert_to_tensor([False, False], [False, True])
def MoeGatingTopKSoftmax(x: Tensor,
                   finished: Optional[Tensor],
                   *,
                   k: int,
                   dependencies=[],
                   node_name=None):
    """REG_OP(MoeGatingTopKSoftmax)\n
    .INPUT(x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(finished, TensorType({DT_BOOL}))\n
    .OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .OUTPUT(expert_idx, TensorType({DT_INT32}))\n
    .OUTPUT(row_idx, TensorType({DT_INT32}))\n
    .REQUIRED_ATTR(k, Int)\n
    .OP_END_FACTORY_REG(MoeGatingTopKSoftmax)\n
    """

    op = get_default_ge_graph().op.add()
    op.type = "MoeGatingTopKSoftmax"
    op.name = next_unique_name(node_name, "MoeGatingTopKSoftmax")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(x.tensor)
    op.input_desc.add().CopyFrom(x.desc)
    op.input_desc[-1].name = "x"
    if finished is not None:
        op.input.append(finished.tensor)
        op.input_desc.add().CopyFrom(finished.desc)
        op.input_desc[-1].name = "finished"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "finished"

    # process attrs
    op.attr["k"].i = k

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "expert_idx"
    expert_idx = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "row_idx"
    row_idx = Tensor(op, output_index)

    # return outputs
    return y, expert_idx, row_idx

# This api is auto-generated from IR FusedInferAttentionScore
@auto_convert_to_tensor([False, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True])
def FusedInferAttentionScore(query: Tensor, key: List[Tensor], value: List[Tensor], pse_shift: Optional[Tensor], atten_mask: Optional[Tensor], actual_seq_lengths: Optional[Tensor], actual_seq_lengths_kv: Optional[Tensor], dequant_scale1: Optional[Tensor], quant_scale1: Optional[Tensor], dequant_scale2: Optional[Tensor], quant_scale2: Optional[Tensor], quant_offset2: Optional[Tensor], antiquant_scale: Optional[Tensor], antiquant_offset: Optional[Tensor], block_table: Optional[Tensor], query_padding_size: Optional[Tensor], kv_padding_size: Optional[Tensor], key_antiquant_scale: Optional[Tensor], key_antiquant_offset: Optional[Tensor], value_antiquant_scale: Optional[Tensor], value_antiquant_offset: Optional[Tensor], key_shared_prefix: Optional[Tensor], value_shared_prefix: Optional[Tensor], actual_shared_prefix_len: Optional[Tensor], *, num_heads: int, scale: float=1.000000, pre_tokens: int=2147483647, next_tokens: int=2147483647, input_layout: str="BSH", num_key_value_heads: int=0, sparse_mode: int=0, inner_precise: int=0, block_size: int=0, antiquant_mode: int=0, softmax_lse_flag: bool=False, key_antiquant_mode: int=0, value_antiquant_mode: int=0, dependencies=[], node_name=None):
    """REG_OP(FusedInferAttentionScore)\n
.INPUT(query, TensorType({DT_INT8, DT_FLOAT16,DT_BF16}))\n
.DYNAMIC_INPUT(key, TensorType({DT_INT8, DT_FLOAT16,DT_BF16}))\n
.DYNAMIC_INPUT(value, TensorType({DT_INT8, DT_FLOAT16,DT_BF16}))\n
.OPTIONAL_INPUT(pse_shift, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(atten_mask, TensorType({DT_FLOAT16, DT_BOOL, DT_UINT8, DT_INT8}))\n
.OPTIONAL_INPUT(actual_seq_lengths, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(actual_seq_lengths_kv, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(dequant_scale1, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(quant_scale1, TensorType({DT_FLOAT32}))\n
.OPTIONAL_INPUT(dequant_scale2, TensorType({DT_UINT64}))\n
.OPTIONAL_INPUT(quant_scale2, TensorType({DT_FLOAT32, DT_BF16}))\n
.OPTIONAL_INPUT(quant_offset2, TensorType({DT_FLOAT32, DT_BF16}))\n
.OPTIONAL_INPUT(antiquant_scale, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(antiquant_offset, TensorType({DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(block_table, TensorType({DT_INT32}))\n
.OPTIONAL_INPUT(query_padding_size, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(kv_padding_size, TensorType({DT_INT64}))\n
.OPTIONAL_INPUT(key_antiquant_scale, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(key_antiquant_offset, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(value_antiquant_scale, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(value_antiquant_offset, TensorType({DT_FLOAT16, DT_BF16, DT_FLOAT32}))\n
.OPTIONAL_INPUT(key_shared_prefix, TensorType({DT_INT8, DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(value_shared_prefix, TensorType({DT_INT8, DT_FLOAT16, DT_BF16}))\n
.OPTIONAL_INPUT(actual_shared_prefix_len, TensorType({DT_INT64}))\n
.OUTPUT(attention_out, TensorType({DT_FLOAT16, DT_FLOAT32, DT_INT8, DT_BF16}))\n
.OUTPUT(softmax_lse, TensorType({DT_FLOAT16, DT_FLOAT32, DT_BF16}))\n
.REQUIRED_ATTR(num_heads, Int)\n
.ATTR(scale, Float, 1.0)\n
.ATTR(pre_tokens, Int, 2147483647)\n
.ATTR(next_tokens, Int, 2147483647)\n
.ATTR(input_layout, String, "BSH")\n
.ATTR(num_key_value_heads, Int, 0)\n
.ATTR(sparse_mode, Int, 0)\n
.ATTR(inner_precise, Int, 0)\n
.ATTR(block_size, Int, 0)\n
.ATTR(antiquant_mode, Int, 0)\n
.ATTR(softmax_lse_flag, Bool, false)\n
.ATTR(key_antiquant_mode, Int, 0)\n
.ATTR(value_antiquant_mode, Int, 0)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "FusedInferAttentionScore"
    op.name = next_unique_name(node_name, "FusedInferAttentionScore")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(query.tensor)
    op.input_desc.add().CopyFrom(query.desc)
    op.input_desc[-1].name = "query"
    if not isinstance(key, (tuple, list)):
        raise AssertionError("key must be a tuple or a list.")
    for i, v in enumerate(key):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "key" + str(i)
    if not isinstance(value, (tuple, list)):
        raise AssertionError("value must be a tuple or a list.")
    for i, v in enumerate(value):
        op.input.append(v.tensor)
        op.input_desc.add().CopyFrom(v.desc)
        op.input_desc[-1].name = "value" + str(i)
    if pse_shift is not None:
        op.input.append(pse_shift.tensor)
        op.input_desc.add().CopyFrom(pse_shift.desc)
        op.input_desc[-1].name = "pse_shift"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "pse_shift"
    if atten_mask is not None:
        op.input.append(atten_mask.tensor)
        op.input_desc.add().CopyFrom(atten_mask.desc)
        op.input_desc[-1].name = "atten_mask"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "atten_mask"
    if actual_seq_lengths is not None:
        op.input.append(actual_seq_lengths.tensor)
        op.input_desc.add().CopyFrom(actual_seq_lengths.desc)
        op.input_desc[-1].name = "actual_seq_lengths"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_lengths"
    if actual_seq_lengths_kv is not None:
        op.input.append(actual_seq_lengths_kv.tensor)
        op.input_desc.add().CopyFrom(actual_seq_lengths_kv.desc)
        op.input_desc[-1].name = "actual_seq_lengths_kv"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_seq_lengths_kv"
    if dequant_scale1 is not None:
        op.input.append(dequant_scale1.tensor)
        op.input_desc.add().CopyFrom(dequant_scale1.desc)
        op.input_desc[-1].name = "dequant_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dequant_scale1"
    if quant_scale1 is not None:
        op.input.append(quant_scale1.tensor)
        op.input_desc.add().CopyFrom(quant_scale1.desc)
        op.input_desc[-1].name = "quant_scale1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale1"
    if dequant_scale2 is not None:
        op.input.append(dequant_scale2.tensor)
        op.input_desc.add().CopyFrom(dequant_scale2.desc)
        op.input_desc[-1].name = "dequant_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "dequant_scale2"
    if quant_scale2 is not None:
        op.input.append(quant_scale2.tensor)
        op.input_desc.add().CopyFrom(quant_scale2.desc)
        op.input_desc[-1].name = "quant_scale2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_scale2"
    if quant_offset2 is not None:
        op.input.append(quant_offset2.tensor)
        op.input_desc.add().CopyFrom(quant_offset2.desc)
        op.input_desc[-1].name = "quant_offset2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "quant_offset2"
    if antiquant_scale is not None:
        op.input.append(antiquant_scale.tensor)
        op.input_desc.add().CopyFrom(antiquant_scale.desc)
        op.input_desc[-1].name = "antiquant_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_scale"
    if antiquant_offset is not None:
        op.input.append(antiquant_offset.tensor)
        op.input_desc.add().CopyFrom(antiquant_offset.desc)
        op.input_desc[-1].name = "antiquant_offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "antiquant_offset"
    if block_table is not None:
        op.input.append(block_table.tensor)
        op.input_desc.add().CopyFrom(block_table.desc)
        op.input_desc[-1].name = "block_table"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "block_table"
    if query_padding_size is not None:
        op.input.append(query_padding_size.tensor)
        op.input_desc.add().CopyFrom(query_padding_size.desc)
        op.input_desc[-1].name = "query_padding_size"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "query_padding_size"
    if kv_padding_size is not None:
        op.input.append(kv_padding_size.tensor)
        op.input_desc.add().CopyFrom(kv_padding_size.desc)
        op.input_desc[-1].name = "kv_padding_size"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "kv_padding_size"
    if key_antiquant_scale is not None:
        op.input.append(key_antiquant_scale.tensor)
        op.input_desc.add().CopyFrom(key_antiquant_scale.desc)
        op.input_desc[-1].name = "key_antiquant_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "key_antiquant_scale"
    if key_antiquant_offset is not None:
        op.input.append(key_antiquant_offset.tensor)
        op.input_desc.add().CopyFrom(key_antiquant_offset.desc)
        op.input_desc[-1].name = "key_antiquant_offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "key_antiquant_offset"
    if value_antiquant_scale is not None:
        op.input.append(value_antiquant_scale.tensor)
        op.input_desc.add().CopyFrom(value_antiquant_scale.desc)
        op.input_desc[-1].name = "value_antiquant_scale"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "value_antiquant_scale"
    if value_antiquant_offset is not None:
        op.input.append(value_antiquant_offset.tensor)
        op.input_desc.add().CopyFrom(value_antiquant_offset.desc)
        op.input_desc[-1].name = "value_antiquant_offset"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "value_antiquant_offset"
    if key_shared_prefix is not None:
        op.input.append(key_shared_prefix.tensor)
        op.input_desc.add().CopyFrom(key_shared_prefix.desc)
        op.input_desc[-1].name = "key_shared_prefix"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "key_shared_prefix"
    if value_shared_prefix is not None:
        op.input.append(value_shared_prefix.tensor)
        op.input_desc.add().CopyFrom(value_shared_prefix.desc)
        op.input_desc[-1].name = "value_shared_prefix"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "value_shared_prefix"
    if actual_shared_prefix_len is not None:
        op.input.append(actual_shared_prefix_len.tensor)
        op.input_desc.add().CopyFrom(actual_shared_prefix_len.desc)
        op.input_desc[-1].name = "actual_shared_prefix_len"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "actual_shared_prefix_len"
    # process attrs
    op.attr["num_heads"].i = num_heads
    op.attr["scale"].f = scale
    op.attr["pre_tokens"].i = pre_tokens
    op.attr["next_tokens"].i = next_tokens
    op.attr["input_layout"].s = compat_as_bytes(input_layout)
    op.attr["num_key_value_heads"].i = num_key_value_heads
    op.attr["sparse_mode"].i = sparse_mode
    op.attr["inner_precise"].i = inner_precise
    op.attr["block_size"].i = block_size
    op.attr["antiquant_mode"].i = antiquant_mode
    op.attr["softmax_lse_flag"].b = softmax_lse_flag
    op.attr["key_antiquant_mode"].i = key_antiquant_mode
    op.attr["value_antiquant_mode"].i = value_antiquant_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "attention_out"
    attention_out = Tensor(op, output_index)
    output_index += 1
    op.output_desc.add().name = "softmax_lse"
    softmax_lse = Tensor(op, output_index)
    output_index += 1

    # return outputs
    return attention_out, softmax_lse


# This api is auto-generated from IR MoeFinalizeRoutingV2
@auto_convert_to_tensor([False, False, False, False, False, False, False], [False, False, True, True, True, True, True])
def MoeFinalizeRoutingV2(expanded_x: Tensor, expanded_row_idx: Tensor, x1: Optional[Tensor], x2: Optional[Tensor],
                         bias: Optional[Tensor], scales: Optional[Tensor], expert_idx: Optional[Tensor], *,
                         drop_pad_mode: int=0, dependencies=[], node_name=None):
    """REG_OP(MoeFinalizeRoutingV2)\n
    .INPUT(expanded_x, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .INPUT(expanded_row_idx, TensorType({DT_INT32}))\n
    .OPTIONAL_INPUT(x1, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(x2, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(bias, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(scales, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .OPTIONAL_INPUT(expert_idx, TensorType({DT_INT32}))\n
    .OUTPUT(y, TensorType({DT_FLOAT, DT_FLOAT16, DT_BF16}))\n
    .ATTR(drop_pad_mode, Int, 0)\n
    """
    
    op = get_default_ge_graph().op.add()
    op.type = "MoeFinalizeRoutingV2"
    op.name = next_unique_name(node_name, "MoeFinalizeRoutingV2")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(expanded_x.tensor)
    op.input_desc.add().CopyFrom(expanded_x.desc)
    op.input_desc[-1].name = "expanded_x"
    op.input.append(expanded_row_idx.tensor)
    op.input_desc.add().CopyFrom(expanded_row_idx.desc)
    op.input_desc[-1].name = "expanded_row_idx"
    if x1 is not None:
        op.input.append(x1.tensor)
        op.input_desc.add().CopyFrom(x1.desc)
        op.input_desc[-1].name = "x1"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x1"
    if x2 is not None:
        op.input.append(x2.tensor)
        op.input_desc.add().CopyFrom(x2.desc)
        op.input_desc[-1].name = "x2"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "x2"
    if bias is not None:
        op.input.append(bias.tensor)
        op.input_desc.add().CopyFrom(bias.desc)
        op.input_desc[-1].name = "bias"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "bias"
    if scales is not None:
        op.input.append(scales.tensor)
        op.input_desc.add().CopyFrom(scales.desc)
        op.input_desc[-1].name = "scales"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "scales"
    if expert_idx is not None:
        op.input.append(expert_idx.tensor)
        op.input_desc.add().CopyFrom(expert_idx.desc)
        op.input_desc[-1].name = "expert_idx"
    else:
        op.input.append('')
        op.input_desc.add().CopyFrom(get_invalid_desc())
        op.input_desc[-1].name = "expert_idx"

    # process attrs
    op.attr["drop_pad_mode"].i = drop_pad_mode

    # process outputs
    output_index = 0
    op.output_desc.add().name = "y"
    y = Tensor(op, output_index)
    output_index += 1

    return y


#This api is auto-generated from IR Cmo
@auto_convert_to_tensor([False], [False], inputs_tensor_type=[TensorType.TT_NUMBER])
def Cmo(src: Tensor, *, max_size: int, type: int=6, dependencies=[], node_name=None):
    """REG_OP(Cmo)\n
.INPUT(src, TensorType::NumberType())\n
.REQUIRED_ATTR(max_size, Int)\n
.ATTR(type, Int, 6)\n
"""

    op = get_default_ge_graph().op.add()
    op.type = "Cmo"
    op.name = next_unique_name(node_name, "Cmo")

    # process dependices
    for dependency in dependencies:
        op.input.append(dependency.controller)

    # process inputs
    op.input.append(src.tensor)
    op.input_desc.add().CopyFrom(src.desc)
    op.input_desc[-1].name = "src"

    # process attrs
    op.attr["max_size"].i = max_size
    op.attr["type"].i = type

    # process outputs
    output_index = 0

    # return outputs
    return