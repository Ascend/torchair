diff -urN /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/attention.py /home/ascend/diffusers/models/attention.py
--- /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/attention.py	2024-05-27 11:21:55.209469400 +0800
+++ /home/ascend/diffusers/models/attention.py	2024-05-27 19:35:20.209469400 +0800
@@ -285,6 +285,8 @@
         cross_attention_kwargs = cross_attention_kwargs.copy() if cross_attention_kwargs is not None else {}
         gligen_kwargs = cross_attention_kwargs.pop("gligen", None)
 
+        cross_attention_kwargs["unnormed_hidden_states"] = hidden_states
+
         attn_output = self.attn1(
             norm_hidden_states,
             encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None,
diff -urN /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/attention_processor.py /home/ascend/diffusers/models/attention_processor.py
--- /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/attention_processor.py	2024-05-27 11:21:55.209469400 +0800
+++ /home/ascend/diffusers/models/attention_processor.py	2024-05-27 19:42:14.739469400 +0800
@@ -704,6 +704,7 @@
         attention_mask: Optional[torch.FloatTensor] = None,
         temb: Optional[torch.FloatTensor] = None,
         scale: float = 1.0,
+        unnormed_hidden_states: torch.FloatTensor = None,
     ) -> torch.Tensor:
         residual = hidden_states
 
@@ -1180,6 +1181,7 @@
         attention_mask: Optional[torch.FloatTensor] = None,
         temb: Optional[torch.FloatTensor] = None,
         scale: float = 1.0,
+        unnormed_hidden_states: torch.FloatTensor = None,
     ) -> torch.FloatTensor:
         residual = hidden_states
 
diff -urN /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py /home/ascend/diffusers/models/unet_2d_blocks.py
--- /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_blocks.py	2024-05-27 11:21:55.219469400 +0800
+++ /home/ascend/diffusers/models/unet_2d_blocks.py	2024-05-27 19:35:20.209469400 +0800
@@ -1120,6 +1120,7 @@
         attention_mask: Optional[torch.FloatTensor] = None,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        exist_block_number: Optional[torch.FloatTensor] = None,
         additional_residuals: Optional[torch.FloatTensor] = None,
     ) -> Tuple[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]:
         output_states = ()
@@ -1172,6 +1173,9 @@
 
             output_states = output_states + (hidden_states,)
 
+            if exist_block_number is not None and len(output_states) == exist_block_number + 1:
+                return hidden_states, output_states
+
         if self.downsamplers is not None:
             for downsampler in self.downsamplers:
                 hidden_states = downsampler(hidden_states, scale=lora_scale)
@@ -2295,6 +2299,7 @@
         upsample_size: Optional[int] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        enter_block_number: Optional[torch.FloatTensor] = None,
     ) -> torch.FloatTensor:
         lora_scale = cross_attention_kwargs.get("scale", 1.0) if cross_attention_kwargs is not None else 1.0
         is_freeu_enabled = (
@@ -2304,7 +2309,12 @@
             and getattr(self, "b2", None)
         )
 
-        for resnet, attn in zip(self.resnets, self.attentions):
+        prv_f = []
+        for i, (resnet, attn) in enumerate(zip(self.resnets, self.attentions)):
+            if enter_block_number is not None and i < len(self.resnets) - enter_block_number - 1:
+                continue
+            prv_f.append(hidden_states)
+
             # pop res hidden states
             res_hidden_states = res_hidden_states_tuple[-1]
             res_hidden_states_tuple = res_hidden_states_tuple[:-1]
@@ -2364,7 +2374,7 @@
             for upsampler in self.upsamplers:
                 hidden_states = upsampler(hidden_states, upsample_size, scale=lora_scale)
 
-        return hidden_states
+        return hidden_states, prv_f
 
 
 class UpBlock2D(nn.Module):
diff -urN /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_condition.py /home/ascend/diffusers/models/unet_2d_condition.py
--- /home/ascend/.local/lib/python3.8/site-packages/diffusers/models/unet_2d_condition.py	2024-05-27 11:21:55.219469400 +0800
+++ /home/ascend/diffusers/models/unet_2d_condition.py	2024-05-27 19:40:15.699469400 +0800
@@ -1137,7 +1137,7 @@
                 upsample_size = down_block_res_samples[-1].shape[2:]
 
             if hasattr(upsample_block, "has_cross_attention") and upsample_block.has_cross_attention:
-                sample = upsample_block(
+                sample, _ = upsample_block(
                     hidden_states=sample,
                     temb=emb,
                     res_hidden_states_tuple=res_samples,
@@ -1170,3 +1170,419 @@
             return (sample,)
 
         return UNet2DConditionOutput(sample=sample)
+
+    def forward_deepcache(
+            self,
+            sample: torch.FloatTensor,
+            timestep: Union[torch.Tensor, float, int],
+            encoder_hidden_states: torch.Tensor,
+            class_labels: Optional[torch.Tensor] = None,
+            timestep_cond: Optional[torch.Tensor] = None,
+            attention_mask: Optional[torch.Tensor] = None,
+            cross_attention_kwargs: Optional[Dict[str, Any]] = None,
+            added_cond_kwargs: Optional[Dict[str, torch.Tensor]] = None,
+            down_block_additional_residuals: Optional[Tuple[torch.Tensor]] = None,
+            mid_block_additional_residual: Optional[torch.Tensor] = None,
+            encoder_attention_mask: Optional[torch.Tensor] = None,
+            quick_replicate: bool = False,
+            replicate_prv_feature: Optional[List[torch.Tensor]] = None,
+            cache_layer_id: Optional[int] = None,
+            cache_block_id: Optional[int] = None,
+            return_dict: bool = True,
+    ) -> Union[UNet2DConditionOutput, Tuple]:
+        r"""
+        The [`UNet2DConditionModel`] forward method.
+
+        Args:
+            sample (`torch.FloatTensor`):
+                The noisy input tensor with the following shape `(batch, channel, height, width)`.
+            timestep (`torch.FloatTensor` or `float` or `int`): The number of timesteps to denoise an input.
+            encoder_hidden_states (`torch.FloatTensor`):
+                The encoder hidden states with shape `(batch, sequence_length, feature_dim)`.
+            encoder_attention_mask (`torch.Tensor`):
+                A cross-attention mask of shape `(batch, sequence_length)` is applied to `encoder_hidden_states`. If
+                `True` the mask is kept, otherwise if `False` it is discarded. Mask will be converted into a bias,
+                which adds large negative values to the attention scores corresponding to "discard" tokens.
+            return_dict (`bool`, *optional*, defaults to `True`):
+                Whether or not to return a [`~models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain
+                tuple.
+            cross_attention_kwargs (`dict`, *optional*):
+                A kwargs dictionary that if specified is passed along to the [`AttnProcessor`].
+            added_cond_kwargs: (`dict`, *optional*):
+                A kwargs dictionary containin additional embeddings that if specified are added to the embeddings that
+                are passed along to the UNet blocks.
+
+        Returns:
+            [`~models.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:
+                If `return_dict` is True, an [`~models.unet_2d_condition.UNet2DConditionOutput`] is returned, otherwise
+                a `tuple` is returned where the first element is the sample tensor.
+        """
+        # By default samples have to be AT least a multiple of the overall upsampling factor.
+        # The overall upsampling factor is equal to 2 ** (# num of upsampling layers).
+        # However, the upsampling interpolation output size can be forced to fit any upsampling size
+        # on the fly if necessary.
+        default_overall_up_factor = 2 ** self.num_upsamplers
+
+        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`
+        forward_upsample_size = False
+        upsample_size = None
+
+        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):
+            logger.info("Forward upsample size to force interpolation output size.")
+            forward_upsample_size = True
+
+        # ensure attention_mask is a bias, and give it a singleton query_tokens dimension
+        # expects mask of shape:
+        #   [batch, key_tokens]
+        # adds singleton query_tokens dimension:
+        #   [batch,                    1, key_tokens]
+        # this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:
+        #   [batch,  heads, query_tokens, key_tokens] (e.g. torch sdp attn)
+        #   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)
+        if attention_mask is not None:
+            # assume that mask is expressed as:
+            #   (1 = keep,      0 = discard)
+            # convert mask into a bias that can be added to attention scores:
+            #       (keep = +0,     discard = -10000.0)
+            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0
+            attention_mask = attention_mask.unsqueeze(1)
+
+        # convert encoder_attention_mask to a bias the same way we do for attention_mask
+        if encoder_attention_mask is not None:
+            encoder_attention_mask = (1 - encoder_attention_mask.to(sample.dtype)) * -10000.0
+            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)
+
+        # 0. center input if necessary
+        if self.config.center_input_sample:
+            sample = 2 * sample - 1.0
+
+        # 1. time
+        timesteps = timestep
+        if not torch.is_tensor(timesteps):
+            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can
+            # This would be a good case for the `match` statement (Python 3.10+)
+            is_mps = sample.device.type == "mps"
+            if isinstance(timestep, float):
+                dtype = torch.float32 if is_mps else torch.float64
+            else:
+                dtype = torch.int32 if is_mps else torch.int64
+            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)
+        elif len(timesteps.shape) == 0:
+            timesteps = timesteps[None].to(sample.device)
+
+        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML
+        timesteps = timesteps.expand(sample.shape[0])
+
+        t_emb = self.time_proj(timesteps)
+
+        # `Timesteps` does not contain any weights and will always return f32 tensors
+        # but time_embedding might actually be running in fp16. so we need to cast here.
+        # there might be better ways to encapsulate this.
+        t_emb = t_emb.to(dtype=sample.dtype)
+
+        emb = self.time_embedding(t_emb, timestep_cond)
+        aug_emb = None
+
+        if self.class_embedding is not None:
+            if class_labels is None:
+                raise ValueError("class_labels should be provided when num_class_embeds > 0")
+
+            if self.config.class_embed_type == "timestep":
+                class_labels = self.time_proj(class_labels)
+
+                # `Timesteps` does not contain any weights and will always return f32 tensors
+                # there might be better ways to encapsulate this.
+                class_labels = class_labels.to(dtype=sample.dtype)
+
+            class_emb = self.class_embedding(class_labels).to(dtype=sample.dtype)
+
+            if self.config.class_embeddings_concat:
+                emb = torch.cat([emb, class_emb], dim=-1)
+            else:
+                emb = emb + class_emb
+
+        if self.config.addition_embed_type == "text":
+            aug_emb = self.add_embedding(encoder_hidden_states)
+        elif self.config.addition_embed_type == "text_image":
+            # Kandinsky 2.1 - style
+            if "image_embeds" not in added_cond_kwargs:
+                raise ValueError(
+                    f"{self.__class__} has the config param `addition_embed_type` set to 'text_image' which requires the keyword argument `image_embeds` to be passed in `added_cond_kwargs`"
+                )
+
+            image_embs = added_cond_kwargs.get("image_embeds")
+            text_embs = added_cond_kwargs.get("text_embeds", encoder_hidden_states)
+            aug_emb = self.add_embedding(text_embs, image_embs)
+        elif self.config.addition_embed_type == "text_time":
+            # SDXL - style
+            if "text_embeds" not in added_cond_kwargs:
+                raise ValueError(
+                    f"{self.__class__} has the config param `addition_embed_type` set to 'text_time' which requires the keyword argument `text_embeds` to be passed in `added_cond_kwargs`"
+                )
+            text_embeds = added_cond_kwargs.get("text_embeds")
+            if "time_ids" not in added_cond_kwargs:
+                raise ValueError(
+                    f"{self.__class__} has the config param `addition_embed_type` set to 'text_time' which requires the keyword argument `time_ids` to be passed in `added_cond_kwargs`"
+                )
+            time_ids = added_cond_kwargs.get("time_ids")
+            time_embeds = self.add_time_proj(time_ids.flatten())
+            time_embeds = time_embeds.reshape((text_embeds.shape[0], -1))
+
+            add_embeds = torch.concat([text_embeds, time_embeds], dim=-1)
+            add_embeds = add_embeds.to(emb.dtype)
+            aug_emb = self.add_embedding(add_embeds)
+        elif self.config.addition_embed_type == "image":
+            # Kandinsky 2.2 - style
+            if "image_embeds" not in added_cond_kwargs:
+                raise ValueError(
+                    f"{self.__class__} has the config param `addition_embed_type` set to 'image' which requires the keyword argument `image_embeds` to be passed in `added_cond_kwargs`"
+                )
+            image_embs = added_cond_kwargs.get("image_embeds")
+            aug_emb = self.add_embedding(image_embs)
+        elif self.config.addition_embed_type == "image_hint":
+            # Kandinsky 2.2 - style
+            if "image_embeds" not in added_cond_kwargs or "hint" not in added_cond_kwargs:
+                raise ValueError(
+                    f"{self.__class__} has the config param `addition_embed_type` set to 'image_hint' which requires the keyword arguments `image_embeds` and `hint` to be passed in `added_cond_kwargs`"
+                )
+            image_embs = added_cond_kwargs.get("image_embeds")
+            hint = added_cond_kwargs.get("hint")
+            aug_emb, hint = self.add_embedding(image_embs, hint)
+            sample = torch.cat([sample, hint], dim=1)
+
+        emb = emb + aug_emb if aug_emb is not None else emb
+
+        if self.time_embed_act is not None:
+            emb = self.time_embed_act(emb)
+
+        if self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_proj":
+            encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states)
+        elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "text_image_proj":
+            # Kadinsky 2.1 - style
+            if "image_embeds" not in added_cond_kwargs:
+                raise ValueError(
+                    f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'text_image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`"
+                )
+
+            image_embeds = added_cond_kwargs.get("image_embeds")
+            encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states, image_embeds)
+        elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == "image_proj":
+            # Kandinsky 2.2 - style
+            if "image_embeds" not in added_cond_kwargs:
+                raise ValueError(
+                    f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`"
+                )
+            image_embeds = added_cond_kwargs.get("image_embeds")
+            encoder_hidden_states = self.encoder_hid_proj(image_embeds)
+        # 2. pre-process
+        sample = self.conv_in(sample)
+
+        # 2.5 GLIGEN position net
+        if cross_attention_kwargs is not None and cross_attention_kwargs.get("gligen", None) is not None:
+            cross_attention_kwargs = cross_attention_kwargs.copy()
+            gligen_args = cross_attention_kwargs.pop("gligen")
+            cross_attention_kwargs["gligen"] = {"objs": self.position_net(**gligen_args)}
+
+        # 3. down
+        lora_scale = cross_attention_kwargs.get("scale", 1.0) if cross_attention_kwargs is not None else 1.0
+
+        is_controlnet = mid_block_additional_residual is not None and down_block_additional_residuals is not None
+        is_adapter = mid_block_additional_residual is None and down_block_additional_residuals is not None
+
+        down_block_res_samples = (sample,)
+        if quick_replicate and replicate_prv_feature is not None:
+            # Down
+            for i, downsample_block in enumerate(self.down_blocks):
+                if i > cache_layer_id:
+                    break
+
+                if hasattr(downsample_block, "has_cross_attention") and downsample_block.has_cross_attention:
+                    # For t2i-adapter CrossAttnDownBlock2D
+                    additional_residuals = {}
+                    if is_adapter and len(down_block_additional_residuals) > 0:
+                        additional_residuals["additional_residuals"] = down_block_additional_residuals.pop(0)
+
+                    sample, res_samples = downsample_block(
+                        hidden_states=sample,
+                        temb=emb,
+                        encoder_hidden_states=encoder_hidden_states,
+                        attention_mask=attention_mask,
+                        cross_attention_kwargs=cross_attention_kwargs,
+                        encoder_attention_mask=encoder_attention_mask,
+                        exist_block_number=cache_block_id if i == cache_layer_id else None,
+                        **additional_residuals,
+                    )
+                else:
+                    sample, res_samples = downsample_block(hidden_states=sample, temb=emb, scale=lora_scale)
+
+                    if is_adapter and len(down_block_additional_residuals) > 0:
+                        sample += down_block_additional_residuals.pop(0)
+
+                down_block_res_samples += res_samples
+
+            # No Middle
+            # Up
+            # print("down_block_res_samples:", [res_sample.shape for res_sample in down_block_res_samples])
+            sample = replicate_prv_feature
+            # down_block_res_samples = down_block_res_samples[:-1]
+            if cache_block_id == len(self.down_blocks[cache_layer_id].attentions):
+                cache_block_id = 0
+                cache_layer_id += 1
+            else:
+                cache_block_id += 1
+
+            for i, upsample_block in enumerate(self.up_blocks):
+                if i < len(self.up_blocks) - 1 - cache_layer_id:
+                    continue
+
+                if i == len(self.up_blocks) - 1 - cache_layer_id:
+                    trunc_upsample_block = cache_block_id + 1
+                else:
+                    trunc_upsample_block = len(upsample_block.resnets)
+
+                is_final_block = i == len(self.up_blocks) - 1
+
+                res_samples = down_block_res_samples[-trunc_upsample_block:]
+                down_block_res_samples = down_block_res_samples[: -trunc_upsample_block]
+
+                # if we have not reached the final block and need to forward the
+                # upsample size, we do it here
+                if not is_final_block and forward_upsample_size:
+                    upsample_size = down_block_res_samples[-1].shape[2:]
+
+                if hasattr(upsample_block, "has_cross_attention") and upsample_block.has_cross_attention:
+                    # print(sample.shape, [res_sample.shape for res_sample in res_samples])
+                    sample, _ = upsample_block(
+                        hidden_states=sample,
+                        temb=emb,
+                        res_hidden_states_tuple=res_samples,
+                        encoder_hidden_states=encoder_hidden_states,
+                        cross_attention_kwargs=cross_attention_kwargs,
+                        upsample_size=upsample_size,
+                        attention_mask=attention_mask,
+                        encoder_attention_mask=encoder_attention_mask,
+                        enter_block_number=cache_block_id if i == len(self.up_blocks) - 1 - cache_layer_id else None,
+                    )
+                else:
+                    sample = upsample_block(
+                        hidden_states=sample,
+                        temb=emb,
+                        res_hidden_states_tuple=res_samples,
+                        upsample_size=upsample_size,
+                        scale=lora_scale,
+                    )
+
+            prv_f = replicate_prv_feature
+        else:
+            for i, downsample_block in enumerate(self.down_blocks):
+                if hasattr(downsample_block, "has_cross_attention") and downsample_block.has_cross_attention:
+                    # For t2i-adapter CrossAttnDownBlock2D
+                    additional_residuals = {}
+                    if is_adapter and len(down_block_additional_residuals) > 0:
+                        additional_residuals["additional_residuals"] = down_block_additional_residuals.pop(0)
+
+                    sample, res_samples = downsample_block(
+                        hidden_states=sample,
+                        temb=emb,
+                        encoder_hidden_states=encoder_hidden_states,
+                        attention_mask=attention_mask,
+                        cross_attention_kwargs=cross_attention_kwargs,
+                        encoder_attention_mask=encoder_attention_mask,
+                        **additional_residuals,
+                    )
+                else:
+                    sample, res_samples = downsample_block(hidden_states=sample, temb=emb, scale=lora_scale)
+
+                    if is_adapter and len(down_block_additional_residuals) > 0:
+                        sample += down_block_additional_residuals.pop(0)
+
+                down_block_res_samples += res_samples
+
+            if is_controlnet:
+                new_down_block_res_samples = ()
+
+                for down_block_res_sample, down_block_additional_residual in zip(
+                        down_block_res_samples, down_block_additional_residuals
+                ):
+                    down_block_res_sample = down_block_res_sample + down_block_additional_residual
+                    new_down_block_res_samples = new_down_block_res_samples + (down_block_res_sample,)
+
+                down_block_res_samples = new_down_block_res_samples
+
+            # 4. mid
+            if self.mid_block is not None:
+                sample = self.mid_block(
+                    sample,
+                    emb,
+                    encoder_hidden_states=encoder_hidden_states,
+                    attention_mask=attention_mask,
+                    cross_attention_kwargs=cross_attention_kwargs,
+                    encoder_attention_mask=encoder_attention_mask,
+                )
+                # To support T2I-Adapter-XL
+                if (
+                        is_adapter
+                        and len(down_block_additional_residuals) > 0
+                        and sample.shape == down_block_additional_residuals[0].shape
+                ):
+                    sample += down_block_additional_residuals.pop(0)
+
+            if is_controlnet:
+                sample = sample + mid_block_additional_residual
+
+            # 5. up
+            if cache_block_id is not None:
+                if cache_block_id == len(self.down_blocks[cache_layer_id].attentions):
+                    cache_block_id = 0
+                    cache_layer_id += 1
+                else:
+                    cache_block_id += 1
+            # print("down_block_res_samples:", [res_sample.shape for res_sample in down_block_res_samples])
+            # print(cache_block_id, cache_layer_id)
+            prv_f = None
+            for i, upsample_block in enumerate(self.up_blocks):
+                is_final_block = i == len(self.up_blocks) - 1
+
+                res_samples = down_block_res_samples[-len(upsample_block.resnets):]
+                down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]
+                # print(sample.shape, [res_sample.shape for res_sample in res_samples])
+                # if we have not reached the final block and need to forward the
+                # upsample size, we do it here
+                if not is_final_block and forward_upsample_size:
+                    upsample_size = down_block_res_samples[-1].shape[2:]
+
+                if hasattr(upsample_block, "has_cross_attention") and upsample_block.has_cross_attention:
+                    sample, current_record_f = upsample_block(
+                        hidden_states=sample,
+                        temb=emb,
+                        res_hidden_states_tuple=res_samples,
+                        encoder_hidden_states=encoder_hidden_states,
+                        cross_attention_kwargs=cross_attention_kwargs,
+                        upsample_size=upsample_size,
+                        attention_mask=attention_mask,
+                        encoder_attention_mask=encoder_attention_mask,
+                    )
+                else:
+                    sample = upsample_block(
+                        hidden_states=sample,
+                        temb=emb,
+                        res_hidden_states_tuple=res_samples,
+                        upsample_size=upsample_size,
+                        scale=lora_scale,
+                    )
+                    current_record_f = None
+
+                # print("Append prv_feature with shape:", sample.shape)
+                if cache_layer_id is not None and current_record_f is not None and i == len(
+                        self.up_blocks) - cache_layer_id - 1:
+                    prv_f = current_record_f[-cache_block_id - 1]
+
+        # 6. post-process
+        if self.conv_norm_out:
+            sample = self.conv_norm_out(sample)
+            sample = self.conv_act(sample)
+        sample = self.conv_out(sample)
+        if not return_dict:
+            return (sample, prv_f,)
+
+        return UNet2DConditionOutput(sample=sample)
\ No newline at end of file
diff -urN /home/ascend/.local/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py /home/ascend/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py
--- /home/ascend/.local/lib/python3.8/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py	2024-05-27 11:21:55.259469400 +0800
+++ /home/ascend/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py	2024-05-27 19:59:02.759469400 +0800
@@ -16,6 +16,7 @@
 from typing import Any, Callable, Dict, List, Optional, Union
 
 import torch
+import numpy as np
 from packaging import version
 from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection
 
@@ -717,6 +718,12 @@
         return_dict: bool = True,
         cross_attention_kwargs: Optional[Dict[str, Any]] = None,
         guidance_rescale: float = 0.0,
+        cache_interval: int = 1,
+        cache_layer_id: int = None,
+        cache_block_id: int = None,
+        uniform: bool = True,
+        pow: float = None,
+        cetner: int = None,
         clip_skip: Optional[int] = None,
         callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,
         callback_on_step_end_tensor_inputs: List[str] = ["latents"],
@@ -908,26 +915,77 @@
         # 7. Denoising loop
         num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
         self._num_timesteps = len(timesteps)
+
+        prv_features = None
+        latents_list = [latents]
+        init_latents = latents.detach().clone()
+
+        if cache_interval == 1:
+            interval_seq = list(range(num_inference_steps))
+        else:
+            if uniform:
+                interval_seq = list(range(0, num_inference_steps, cache_interval))
+            else:
+                num_slow_step = num_inference_steps // cache_interval
+                if num_inference_steps % cache_interval != 0:
+                    num_slow_step += 1
+
+                interval_seq, pow = sample_from_quad_center(num_inference_steps, num_slow_step, center=center, pow=pow)
+                interval_seq = sorted(interval_seq)
+        interval_seq = [0, 1, 3, 6, 9, 11, 13, 15, 17, 18, 19, 21, 23, 25, 27, 29, 31, 32, 33, 35, 37, 39, 41, 43, 46, 49, 50]
         with self.progress_bar(total=num_inference_steps) as progress_bar:
             for i, t in enumerate(timesteps):
                 # expand the latents if we are doing classifier free guidance
-                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents
+                import torch.distributed as dist
+                import os
+                rank_id = int(os.getenv("LOCAL_RANK", "0"))
+                world_size = int(os.getenv("WORLD_SIZE", "1"))
+
+                if world_size > 1:
+                    latent_model_input = latents
+                    prompt_embed = prompt_embeds[rank_id].unsqueeze(0)
+                else:
+                    latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents
+                    prompt_embed = prompt_embeds
                 latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)
 
-                # predict the noise residual
-                noise_pred = self.unet(
-                    latent_model_input,
-                    t,
-                    encoder_hidden_states=prompt_embeds,
-                    timestep_cond=timestep_cond,
-                    cross_attention_kwargs=self.cross_attention_kwargs,
-                    added_cond_kwargs=added_cond_kwargs,
-                    return_dict=False,
-                )[0]
+                if i in interval_seq:
+                    prv_features = None
+
+                    # predict the noise residual
+                if cache_interval > 1:
+                    noise_pred, prv_features = self.unet.forward_deepcache(
+                        latent_model_input,
+                        t,
+                        encoder_hidden_states=prompt_embed,
+                        timestep_cond=timestep_cond,
+                        cross_attention_kwargs=self.cross_attention_kwargs,
+                        added_cond_kwargs=added_cond_kwargs,
+                        replicate_prv_feature=prv_features,
+                        quick_replicate=True,
+                        cache_layer_id=cache_layer_id,
+                        cache_block_id=cache_block_id,
+                        return_dict=False,
+                    )
+                else:
+                    noise_pred = self.unet(
+                        latent_model_input,
+                        t,
+                        encoder_hidden_states=prompt_embed,
+                        timestep_cond=timestep_cond,
+                        cross_attention_kwargs=self.cross_attention_kwargs,
+                        added_cond_kwargs=added_cond_kwargs,
+                        return_dict=False,
+                    )[0]
 
                 # perform guidance
                 if self.do_classifier_free_guidance:
-                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
+                    if world_size > 1:
+                        noise_preds = [torch.zeros_like(noise_pred) for _ in range(2)]
+                        dist.all_gather(noise_preds, noise_pred)
+                        noise_pred_uncond, noise_pred_text = noise_preds
+                    else:
+                        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                     noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)
 
                 if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:
diff -urN /home/ascend/.local/lib/python3.8/site-packages/diffusers/schedulers/scheduling_pndm.py /home/ascend/diffusers/schedulers/scheduling_pndm.py
--- /home/ascend/.local/lib/python3.8/site-packages/diffusers/schedulers/scheduling_pndm.py	2024-05-27 11:21:55.289469400 +0800
+++ /home/ascend/diffusers/schedulers/scheduling_pndm.py	2024-05-28 17:05:52.309469400 +0800
@@ -219,7 +219,7 @@
             ].copy()  # we copy to avoid having negative strides which are not supported by torch.from_numpy
 
         timesteps = np.concatenate([self.prk_timesteps, self.plms_timesteps]).astype(np.int64)
-        self.timesteps = torch.from_numpy(timesteps).to(device)
+        self.timesteps = torch.from_numpy(timesteps)#.to(device)
 
         self.ets = []
         self.counter = 0
